{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {},
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "# os.environ[\"BART_TOOLBOX_PATH\"] = \"/Users/chunxuguo/bart\"\n",
    "import torch\n",
    "import torchopt\n",
    "from einops import rearrange, reduce\n",
    "from fastmri import complex_abs\n",
    "from fastmri.data import mri_data, subsample, transforms\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ese5934_project.datasets.csm_estimation import espirit_csm_estimation\n",
    "from ese5934_project.evaluate_metric import Evaluate_MT1, Evaluate_MT2\n",
    "from ese5934_project.models.GridField import Grid\n",
    "from ese5934_project.models.operators import (\n",
    "    C_adj,\n",
    "    F_adj,\n",
    "    ForwardModel,\n",
    "    MaskedForwardModel,\n",
    ")\n",
    "from ese5934_project.models.SIREN import Siren, get_coordinates\n",
    "from ese5934_project.tasks.mri_reconstruction_2d import reconstruct\n",
    "\n",
    "# Create a mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def data_transform_2(\n",
    "    kspace,\n",
    "    mask,\n",
    "    target,\n",
    "    data_attributes,\n",
    "    filename,\n",
    "    slice_num,\n",
    "):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[4])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "def data_transform_4(\n",
    "    kspace,\n",
    "    mask,\n",
    "    target,\n",
    "    data_attributes,\n",
    "    filename,\n",
    "    slice_num,\n",
    "):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[4])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "def data_transform_8(\n",
    "    kspace,\n",
    "    mask,\n",
    "    target,\n",
    "    data_attributes,\n",
    "    filename,\n",
    "    slice_num,\n",
    "):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[4])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "def data_transform_gt(kspace, mask, target, data_attributes, filename, slice_num):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[1])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "dataset_gt = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_gt,\n",
    "    challenge=\"multicoil\",\n",
    ")\n",
    "dataset_2 = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_2,\n",
    "    challenge=\"multicoil\",\n",
    ")\n",
    "dataset_4 = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_4,\n",
    "    challenge=\"multicoil\",\n",
    ")\n",
    "dataset_8 = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_8,\n",
    "    challenge=\"multicoil\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 367.5, 639.5, -0.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_gt[15]\n",
    "f_adj = F_adj()\n",
    "c_adj = C_adj()\n",
    "image_gt = c_adj(f_adj(kspace), csm)\n",
    "ground_truth_image = image_gt.squeeze()\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(torch.view_as_complex(image_gt).abs()[0], cmap=\"gray\", vmax=25)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Case Images with Acceleration Rate of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Field (Voxel Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {},
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 367.5, 639.5, -0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scheduler = lambda t: 0.8 ** (t // 400) * 1e-1\n",
    "optimizer = torchopt.adam(lr=0.1)\n",
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_4[15]\n",
    "field = Grid((640, 368), mean, std)\n",
    "params, image_list_ADAM = reconstruct(\n",
    "    field,\n",
    "    torch.rand(1, 2),\n",
    "    masked_kspace,\n",
    "    csm,\n",
    "    mask,\n",
    "    alpha=0.005,\n",
    "    optimizer=optimizer,\n",
    "    iterations=4000,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n",
    "plt.imshow(complex_abs(image_list_ADAM[-1]), cmap=\"gray\", vmax=25)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {},
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3c28be950>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "size = (640, 368)\n",
    "from ese5934_project.models.SIREN import Siren, get_coordinates\n",
    "\n",
    "coords = get_coordinates(size)\n",
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_4[15]\n",
    "field = Siren(\n",
    "    size,\n",
    "    mean.to(device),\n",
    "    std.to(device),\n",
    "    in_features=2,\n",
    "    out_features=2,\n",
    "    hidden_features=256,\n",
    "    hidden_layers=4,\n",
    "    outermost_linear=True,\n",
    "    first_omega_0=25,\n",
    "    hidden_omega_0=25,\n",
    ")\n",
    "lr_scheduler = lambda t: 0.8 ** (t // 400) * 1e-4\n",
    "optimizer = torchopt.adamw(lr=lr_scheduler)\n",
    "# 1e-4 1.092077389\n",
    "# 1e-3 0.08540542\n",
    "params, image_list_SIREN = reconstruct(\n",
    "    field,\n",
    "    coords,\n",
    "    masked_kspace,\n",
    "    csm,\n",
    "    mask,\n",
    "    alpha=0.01,\n",
    "    optimizer=optimizer,\n",
    "    iterations=4000,\n",
    "    device=device,\n",
    ")\n",
    "plt.imshow(complex_abs(image_list_SIREN[-1]), cmap=\"gray\", vmax=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DictField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:0')\n",
      "ic| self.coeff_reso: [40, 24]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 40, 24])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> total parameters:  361264\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.9792377948760986, tv_loss: 7.349102816078812e-05\n",
      "iteration 2, dc_loss: 2.9729835987091064, tv_loss: 0.00019470005645416677\n",
      "iteration 3, dc_loss: 2.9666202068328857, tv_loss: 0.0003167290415149182\n",
      "iteration 4, dc_loss: 2.9602725505828857, tv_loss: 0.00043874196126125753\n",
      "iteration 5, dc_loss: 2.953824043273926, tv_loss: 0.0005637682625092566\n",
      "iteration 6, dc_loss: 2.947174072265625, tv_loss: 0.0006920791347511113\n",
      "iteration 7, dc_loss: 2.9403083324432373, tv_loss: 0.0008243373013101518\n",
      "iteration 8, dc_loss: 2.9331324100494385, tv_loss: 0.0009631947032175958\n",
      "iteration 9, dc_loss: 2.925509452819824, tv_loss: 0.0011102133430540562\n",
      "iteration 10, dc_loss: 2.9172921180725098, tv_loss: 0.0012656308244913816\n",
      "iteration 11, dc_loss: 2.9083616733551025, tv_loss: 0.001429490395821631\n",
      "iteration 12, dc_loss: 2.8985307216644287, tv_loss: 0.0016029769321903586\n",
      "iteration 13, dc_loss: 2.8875598907470703, tv_loss: 0.0017875045305117965\n",
      "iteration 14, dc_loss: 2.875255823135376, tv_loss: 0.0019841939210891724\n",
      "iteration 15, dc_loss: 2.861395835876465, tv_loss: 0.0021931463852524757\n",
      "iteration 16, dc_loss: 2.845673084259033, tv_loss: 0.0024146647192537785\n",
      "iteration 17, dc_loss: 2.8278048038482666, tv_loss: 0.0026482173707336187\n",
      "iteration 18, dc_loss: 2.8074867725372314, tv_loss: 0.002892996883019805\n",
      "iteration 19, dc_loss: 2.7843844890594482, tv_loss: 0.003147834213450551\n",
      "iteration 20, dc_loss: 2.7581350803375244, tv_loss: 0.0034110541455447674\n",
      "iteration 21, dc_loss: 2.7283525466918945, tv_loss: 0.003680485999211669\n",
      "iteration 22, dc_loss: 2.6946353912353516, tv_loss: 0.00395341869443655\n",
      "iteration 23, dc_loss: 2.6565752029418945, tv_loss: 0.004225796554237604\n",
      "iteration 24, dc_loss: 2.6137781143188477, tv_loss: 0.004492885898798704\n",
      "iteration 25, dc_loss: 2.5658915042877197, tv_loss: 0.004750553518533707\n",
      "iteration 26, dc_loss: 2.512624740600586, tv_loss: 0.004992270842194557\n",
      "iteration 27, dc_loss: 2.4537758827209473, tv_loss: 0.0052330223843455315\n",
      "iteration 28, dc_loss: 2.38916277885437, tv_loss: 0.005544155836105347\n",
      "iteration 29, dc_loss: 2.318795919418335, tv_loss: 0.005946681834757328\n",
      "iteration 30, dc_loss: 2.242918014526367, tv_loss: 0.00652123661711812\n",
      "iteration 31, dc_loss: 2.161898612976074, tv_loss: 0.007276355288922787\n",
      "iteration 32, dc_loss: 2.0760345458984375, tv_loss: 0.008096645586192608\n",
      "iteration 33, dc_loss: 1.9860095977783203, tv_loss: 0.008976309560239315\n",
      "iteration 34, dc_loss: 1.892735242843628, tv_loss: 0.009908090345561504\n",
      "iteration 35, dc_loss: 1.7973870038986206, tv_loss: 0.010883238166570663\n",
      "iteration 36, dc_loss: 1.7014925479888916, tv_loss: 0.011897142976522446\n",
      "iteration 37, dc_loss: 1.6069927215576172, tv_loss: 0.012949020601809025\n",
      "iteration 38, dc_loss: 1.51633882522583, tv_loss: 0.01402956247329712\n",
      "iteration 39, dc_loss: 1.4322285652160645, tv_loss: 0.015127208083868027\n",
      "iteration 40, dc_loss: 1.3574731349945068, tv_loss: 0.01622314192354679\n",
      "iteration 41, dc_loss: 1.2946521043777466, tv_loss: 0.017287718132138252\n",
      "iteration 42, dc_loss: 1.2455157041549683, tv_loss: 0.018282808363437653\n",
      "iteration 43, dc_loss: 1.2100874185562134, tv_loss: 0.01916787028312683\n",
      "iteration 44, dc_loss: 1.1860671043395996, tv_loss: 0.019880184903740883\n",
      "iteration 45, dc_loss: 1.1685272455215454, tv_loss: 0.020364083349704742\n",
      "iteration 46, dc_loss: 1.1516591310501099, tv_loss: 0.020633939653635025\n",
      "iteration 47, dc_loss: 1.1307581663131714, tv_loss: 0.020637648180127144\n",
      "iteration 48, dc_loss: 1.1033411026000977, tv_loss: 0.020383480936288834\n",
      "iteration 49, dc_loss: 1.0698738098144531, tv_loss: 0.019920336082577705\n",
      "iteration 50, dc_loss: 1.0324541330337524, tv_loss: 0.019303277134895325\n",
      "iteration 51, dc_loss: 0.9940906167030334, tv_loss: 0.018591364845633507\n",
      "iteration 52, dc_loss: 0.9574493765830994, tv_loss: 0.017854485660791397\n",
      "iteration 53, dc_loss: 0.9242008328437805, tv_loss: 0.01716776378452778\n",
      "iteration 54, dc_loss: 0.8945776224136353, tv_loss: 0.016539424657821655\n",
      "iteration 55, dc_loss: 0.8678338527679443, tv_loss: 0.015976237133145332\n",
      "iteration 56, dc_loss: 0.8424965739250183, tv_loss: 0.015498367138206959\n",
      "iteration 57, dc_loss: 0.8168088793754578, tv_loss: 0.015114814043045044\n",
      "iteration 58, dc_loss: 0.7890077829360962, tv_loss: 0.014830387197434902\n",
      "iteration 59, dc_loss: 0.7578349709510803, tv_loss: 0.014647330157458782\n",
      "iteration 60, dc_loss: 0.7223443984985352, tv_loss: 0.01457265019416809\n",
      "iteration 61, dc_loss: 0.6827777028083801, tv_loss: 0.014586604200303555\n",
      "iteration 62, dc_loss: 0.639896810054779, tv_loss: 0.01466288510710001\n",
      "iteration 63, dc_loss: 0.5946411490440369, tv_loss: 0.01480405405163765\n",
      "iteration 64, dc_loss: 0.5482258200645447, tv_loss: 0.0150053221732378\n",
      "iteration 65, dc_loss: 0.5015916228294373, tv_loss: 0.015246184542775154\n",
      "iteration 66, dc_loss: 0.45534566044807434, tv_loss: 0.015488139353692532\n",
      "iteration 67, dc_loss: 0.4097108244895935, tv_loss: 0.01570977456867695\n",
      "iteration 68, dc_loss: 0.3647935688495636, tv_loss: 0.015895871445536613\n",
      "iteration 69, dc_loss: 0.32086488604545593, tv_loss: 0.016045965254306793\n",
      "iteration 70, dc_loss: 0.278525173664093, tv_loss: 0.01616179384291172\n",
      "iteration 71, dc_loss: 0.23882436752319336, tv_loss: 0.016259407624602318\n",
      "iteration 72, dc_loss: 0.2031148076057434, tv_loss: 0.01635390892624855\n",
      "iteration 73, dc_loss: 0.17270527780056, tv_loss: 0.01646251231431961\n",
      "iteration 74, dc_loss: 0.1485956609249115, tv_loss: 0.016619520261883736\n",
      "iteration 75, dc_loss: 0.13130563497543335, tv_loss: 0.01679975911974907\n",
      "iteration 76, dc_loss: 0.12063394486904144, tv_loss: 0.01695903390645981\n",
      "iteration 77, dc_loss: 0.11560951173305511, tv_loss: 0.01708184741437435\n",
      "iteration 78, dc_loss: 0.11470388621091843, tv_loss: 0.017162930220365524\n",
      "iteration 79, dc_loss: 0.11613055318593979, tv_loss: 0.017192725092172623\n",
      "iteration 80, dc_loss: 0.11817914992570877, tv_loss: 0.017176717519760132\n",
      "iteration 81, dc_loss: 0.1194392517209053, tv_loss: 0.017129434272646904\n",
      "iteration 82, dc_loss: 0.11892078071832657, tv_loss: 0.017059946432709694\n",
      "iteration 83, dc_loss: 0.11619564890861511, tv_loss: 0.016950005665421486\n",
      "iteration 84, dc_loss: 0.11143170297145844, tv_loss: 0.016820205375552177\n",
      "iteration 85, dc_loss: 0.10529157519340515, tv_loss: 0.01669168286025524\n",
      "iteration 86, dc_loss: 0.09866661578416824, tv_loss: 0.016569267958402634\n",
      "iteration 87, dc_loss: 0.09236612170934677, tv_loss: 0.01646069437265396\n",
      "iteration 88, dc_loss: 0.08706510812044144, tv_loss: 0.016378119587898254\n",
      "iteration 89, dc_loss: 0.0831327810883522, tv_loss: 0.016321422532200813\n",
      "iteration 90, dc_loss: 0.08061850070953369, tv_loss: 0.01627759449183941\n",
      "iteration 91, dc_loss: 0.0793127790093422, tv_loss: 0.016255302354693413\n",
      "iteration 92, dc_loss: 0.07882025092840195, tv_loss: 0.016246924176812172\n",
      "iteration 93, dc_loss: 0.07871540635824203, tv_loss: 0.01622730679810047\n",
      "iteration 94, dc_loss: 0.07868959754705429, tv_loss: 0.016196297481656075\n",
      "iteration 95, dc_loss: 0.07858963310718536, tv_loss: 0.016150962561368942\n",
      "iteration 96, dc_loss: 0.07827525585889816, tv_loss: 0.016099022701382637\n",
      "iteration 97, dc_loss: 0.077619768679142, tv_loss: 0.016052134335041046\n",
      "iteration 98, dc_loss: 0.07654686272144318, tv_loss: 0.01603155955672264\n",
      "iteration 99, dc_loss: 0.07504882663488388, tv_loss: 0.016031119972467422\n",
      "iteration 100, dc_loss: 0.07322750985622406, tv_loss: 0.01605646125972271\n",
      "iteration 101, dc_loss: 0.07126082479953766, tv_loss: 0.016118420287966728\n",
      "iteration 102, dc_loss: 0.06934134662151337, tv_loss: 0.016192711889743805\n",
      "iteration 103, dc_loss: 0.06763801723718643, tv_loss: 0.016307992860674858\n",
      "iteration 104, dc_loss: 0.06626627594232559, tv_loss: 0.016421014443039894\n",
      "iteration 105, dc_loss: 0.065265953540802, tv_loss: 0.016539057716727257\n",
      "iteration 106, dc_loss: 0.06459470838308334, tv_loss: 0.016650410369038582\n",
      "iteration 107, dc_loss: 0.06417772173881531, tv_loss: 0.01674686186015606\n",
      "iteration 108, dc_loss: 0.06391625851392746, tv_loss: 0.01680859364569187\n",
      "iteration 109, dc_loss: 0.0637129470705986, tv_loss: 0.01683332957327366\n",
      "iteration 110, dc_loss: 0.06349340081214905, tv_loss: 0.01683131232857704\n",
      "iteration 111, dc_loss: 0.06320308893918991, tv_loss: 0.01680733449757099\n",
      "iteration 112, dc_loss: 0.06281290203332901, tv_loss: 0.016768179833889008\n",
      "iteration 113, dc_loss: 0.06232394278049469, tv_loss: 0.01672378182411194\n",
      "iteration 114, dc_loss: 0.06177040934562683, tv_loss: 0.01667836867272854\n",
      "iteration 115, dc_loss: 0.06120435521006584, tv_loss: 0.01663941517472267\n",
      "iteration 116, dc_loss: 0.060671187937259674, tv_loss: 0.016610674560070038\n",
      "iteration 117, dc_loss: 0.06019732356071472, tv_loss: 0.016591927036643028\n",
      "iteration 118, dc_loss: 0.059785470366477966, tv_loss: 0.01658220775425434\n",
      "iteration 119, dc_loss: 0.05942255258560181, tv_loss: 0.016580112278461456\n",
      "iteration 120, dc_loss: 0.05909545719623566, tv_loss: 0.01658639684319496\n",
      "iteration 121, dc_loss: 0.05879716947674751, tv_loss: 0.01659409888088703\n",
      "iteration 122, dc_loss: 0.05852028355002403, tv_loss: 0.016596127301454544\n",
      "iteration 123, dc_loss: 0.058255184441804886, tv_loss: 0.016598323360085487\n",
      "iteration 124, dc_loss: 0.05798972025513649, tv_loss: 0.016601962968707085\n",
      "iteration 125, dc_loss: 0.057710736989974976, tv_loss: 0.016601476818323135\n",
      "iteration 126, dc_loss: 0.05741027742624283, tv_loss: 0.01660025306046009\n",
      "iteration 127, dc_loss: 0.05709007382392883, tv_loss: 0.01660262979567051\n",
      "iteration 128, dc_loss: 0.05676110088825226, tv_loss: 0.016607563942670822\n",
      "iteration 129, dc_loss: 0.05643783137202263, tv_loss: 0.016617145389318466\n",
      "iteration 130, dc_loss: 0.05613010376691818, tv_loss: 0.016629211604595184\n",
      "iteration 131, dc_loss: 0.05583775416016579, tv_loss: 0.016642708331346512\n",
      "iteration 132, dc_loss: 0.05555617809295654, tv_loss: 0.01665843091905117\n",
      "iteration 133, dc_loss: 0.055281054228544235, tv_loss: 0.016675151884555817\n",
      "iteration 134, dc_loss: 0.055013734847307205, tv_loss: 0.016692642122507095\n",
      "iteration 135, dc_loss: 0.054755330085754395, tv_loss: 0.016708631068468094\n",
      "iteration 136, dc_loss: 0.05450539290904999, tv_loss: 0.01672138087451458\n",
      "iteration 137, dc_loss: 0.05426076054573059, tv_loss: 0.016729434952139854\n",
      "iteration 138, dc_loss: 0.05401867628097534, tv_loss: 0.016734179109334946\n",
      "iteration 139, dc_loss: 0.05377675220370293, tv_loss: 0.016734939068555832\n",
      "iteration 140, dc_loss: 0.053533878177404404, tv_loss: 0.01673142984509468\n",
      "iteration 141, dc_loss: 0.05329133942723274, tv_loss: 0.016725784167647362\n",
      "iteration 142, dc_loss: 0.053051602095365524, tv_loss: 0.016719238832592964\n",
      "iteration 143, dc_loss: 0.05281724035739899, tv_loss: 0.016711633652448654\n",
      "iteration 144, dc_loss: 0.05258859694004059, tv_loss: 0.016705047339200974\n",
      "iteration 145, dc_loss: 0.052357014268636703, tv_loss: 0.016705503687262535\n",
      "iteration 146, dc_loss: 0.05212925747036934, tv_loss: 0.01670529507100582\n",
      "iteration 147, dc_loss: 0.05190376192331314, tv_loss: 0.016705278307199478\n",
      "iteration 148, dc_loss: 0.0516803041100502, tv_loss: 0.016706092283129692\n",
      "iteration 149, dc_loss: 0.05145520716905594, tv_loss: 0.01670984737575054\n",
      "iteration 150, dc_loss: 0.05122813582420349, tv_loss: 0.01671576127409935\n",
      "iteration 151, dc_loss: 0.050997842103242874, tv_loss: 0.016723088920116425\n",
      "iteration 152, dc_loss: 0.05076577514410019, tv_loss: 0.016730057075619698\n",
      "iteration 153, dc_loss: 0.05053482577204704, tv_loss: 0.01673363707959652\n",
      "iteration 154, dc_loss: 0.05030624195933342, tv_loss: 0.016734110191464424\n",
      "iteration 155, dc_loss: 0.05007835105061531, tv_loss: 0.016734540462493896\n",
      "iteration 156, dc_loss: 0.04984801262617111, tv_loss: 0.01673823595046997\n",
      "iteration 157, dc_loss: 0.04961463063955307, tv_loss: 0.016745615750551224\n",
      "iteration 158, dc_loss: 0.049382541328668594, tv_loss: 0.01675163395702839\n",
      "iteration 159, dc_loss: 0.04915421083569527, tv_loss: 0.016753409057855606\n",
      "iteration 160, dc_loss: 0.04892531782388687, tv_loss: 0.016754230484366417\n",
      "iteration 161, dc_loss: 0.048691049218177795, tv_loss: 0.016757922247052193\n",
      "iteration 162, dc_loss: 0.04845526069402695, tv_loss: 0.01676148548722267\n",
      "iteration 163, dc_loss: 0.04822375997900963, tv_loss: 0.016760289669036865\n",
      "iteration 164, dc_loss: 0.04799029231071472, tv_loss: 0.01676197163760662\n",
      "iteration 165, dc_loss: 0.047747835516929626, tv_loss: 0.01677107997238636\n",
      "iteration 166, dc_loss: 0.04751221463084221, tv_loss: 0.016772223636507988\n",
      "iteration 167, dc_loss: 0.04728405550122261, tv_loss: 0.01676546223461628\n",
      "iteration 168, dc_loss: 0.0470423698425293, tv_loss: 0.016770942136645317\n",
      "iteration 169, dc_loss: 0.04679803177714348, tv_loss: 0.01677827350795269\n",
      "iteration 170, dc_loss: 0.04656388238072395, tv_loss: 0.01677459292113781\n",
      "iteration 171, dc_loss: 0.0463259257376194, tv_loss: 0.01677420549094677\n",
      "iteration 172, dc_loss: 0.046079762279987335, tv_loss: 0.016781184822320938\n",
      "iteration 173, dc_loss: 0.04584350064396858, tv_loss: 0.01677744835615158\n",
      "iteration 174, dc_loss: 0.0456070639193058, tv_loss: 0.016773872077465057\n",
      "iteration 175, dc_loss: 0.04535980895161629, tv_loss: 0.016781389713287354\n",
      "iteration 176, dc_loss: 0.045125626027584076, tv_loss: 0.016775576397776604\n",
      "iteration 177, dc_loss: 0.04488050565123558, tv_loss: 0.016781093552708626\n",
      "iteration 178, dc_loss: 0.04463598132133484, tv_loss: 0.01678653620183468\n",
      "iteration 179, dc_loss: 0.04440143704414368, tv_loss: 0.016781983897089958\n",
      "iteration 180, dc_loss: 0.044151484966278076, tv_loss: 0.01679360307753086\n",
      "iteration 181, dc_loss: 0.04392130672931671, tv_loss: 0.016785843297839165\n",
      "iteration 182, dc_loss: 0.04367543384432793, tv_loss: 0.01679348573088646\n",
      "iteration 183, dc_loss: 0.043442629277706146, tv_loss: 0.01678965799510479\n",
      "iteration 184, dc_loss: 0.043196067214012146, tv_loss: 0.016801046207547188\n",
      "iteration 185, dc_loss: 0.04296642169356346, tv_loss: 0.016795380041003227\n",
      "iteration 186, dc_loss: 0.04271497577428818, tv_loss: 0.01681402325630188\n",
      "iteration 187, dc_loss: 0.04250193387269974, tv_loss: 0.01679639331996441\n",
      "iteration 188, dc_loss: 0.042243827134370804, tv_loss: 0.016825443133711815\n",
      "iteration 189, dc_loss: 0.04203978553414345, tv_loss: 0.016799667850136757\n",
      "iteration 190, dc_loss: 0.04180051013827324, tv_loss: 0.01681128516793251\n",
      "iteration 191, dc_loss: 0.04156430810689926, tv_loss: 0.016823649406433105\n",
      "iteration 192, dc_loss: 0.041359517723321915, tv_loss: 0.016804924234747887\n",
      "iteration 193, dc_loss: 0.04111155867576599, tv_loss: 0.01682952046394348\n",
      "iteration 194, dc_loss: 0.04089979827404022, tv_loss: 0.01682036742568016\n",
      "iteration 195, dc_loss: 0.04068611562252045, tv_loss: 0.01681540347635746\n",
      "iteration 196, dc_loss: 0.04045293107628822, tv_loss: 0.01683148182928562\n",
      "iteration 197, dc_loss: 0.040256962180137634, tv_loss: 0.016813110560178757\n",
      "iteration 198, dc_loss: 0.04001854360103607, tv_loss: 0.016841372475028038\n",
      "iteration 199, dc_loss: 0.039834219962358475, tv_loss: 0.01681891642510891\n",
      "iteration 200, dc_loss: 0.03959117829799652, tv_loss: 0.016852276399731636\n",
      "iteration 201, dc_loss: 0.03941291570663452, tv_loss: 0.016825739294290543\n",
      "iteration 202, dc_loss: 0.039194028824567795, tv_loss: 0.016840146854519844\n",
      "iteration 203, dc_loss: 0.03897906839847565, tv_loss: 0.01685325987637043\n",
      "iteration 204, dc_loss: 0.03880228474736214, tv_loss: 0.016834452748298645\n",
      "iteration 205, dc_loss: 0.03857101500034332, tv_loss: 0.01686665043234825\n",
      "iteration 206, dc_loss: 0.03838984668254852, tv_loss: 0.016854265704751015\n",
      "iteration 207, dc_loss: 0.038198042660951614, tv_loss: 0.016855599358677864\n",
      "iteration 208, dc_loss: 0.03799961879849434, tv_loss: 0.016864698380231857\n",
      "iteration 209, dc_loss: 0.037831861525774, tv_loss: 0.01684795878827572\n",
      "iteration 210, dc_loss: 0.03761978819966316, tv_loss: 0.016875015571713448\n",
      "iteration 211, dc_loss: 0.0374779999256134, tv_loss: 0.01683639921247959\n",
      "iteration 212, dc_loss: 0.03725995868444443, tv_loss: 0.016875257715582848\n",
      "iteration 213, dc_loss: 0.037115003913640976, tv_loss: 0.01684178225696087\n",
      "iteration 214, dc_loss: 0.03692392259836197, tv_loss: 0.016858648508787155\n",
      "iteration 215, dc_loss: 0.03674876689910889, tv_loss: 0.01686057262122631\n",
      "iteration 216, dc_loss: 0.03659013286232948, tv_loss: 0.01684987172484398\n",
      "iteration 217, dc_loss: 0.03639676049351692, tv_loss: 0.01687711477279663\n",
      "iteration 218, dc_loss: 0.036275073885917664, tv_loss: 0.016834799200296402\n",
      "iteration 219, dc_loss: 0.036054566502571106, tv_loss: 0.01689549721777439\n",
      "iteration 220, dc_loss: 0.03595920279622078, tv_loss: 0.016830764710903168\n",
      "iteration 221, dc_loss: 0.035738661885261536, tv_loss: 0.016887839883565903\n",
      "iteration 222, dc_loss: 0.03560086339712143, tv_loss: 0.0168662890791893\n",
      "iteration 223, dc_loss: 0.03546635061502457, tv_loss: 0.016848204657435417\n",
      "iteration 224, dc_loss: 0.03527205064892769, tv_loss: 0.016893677413463593\n",
      "iteration 225, dc_loss: 0.03516805171966553, tv_loss: 0.01684693992137909\n",
      "iteration 226, dc_loss: 0.03498302772641182, tv_loss: 0.016879349946975708\n",
      "iteration 227, dc_loss: 0.03483930975198746, tv_loss: 0.016876190900802612\n",
      "iteration 228, dc_loss: 0.03472184017300606, tv_loss: 0.016851985827088356\n",
      "iteration 229, dc_loss: 0.0345364548265934, tv_loss: 0.016895245760679245\n",
      "iteration 230, dc_loss: 0.0344335176050663, tv_loss: 0.016855187714099884\n",
      "iteration 231, dc_loss: 0.03426454961299896, tv_loss: 0.016882436349987984\n",
      "iteration 232, dc_loss: 0.034123849123716354, tv_loss: 0.01688516139984131\n",
      "iteration 233, dc_loss: 0.03401445224881172, tv_loss: 0.01686016470193863\n",
      "iteration 234, dc_loss: 0.03383840620517731, tv_loss: 0.016902660951018333\n",
      "iteration 235, dc_loss: 0.03374569118022919, tv_loss: 0.016860855743288994\n",
      "iteration 236, dc_loss: 0.03357771039009094, tv_loss: 0.016894616186618805\n",
      "iteration 237, dc_loss: 0.03345568850636482, tv_loss: 0.01688482239842415\n",
      "iteration 238, dc_loss: 0.03333292901515961, tv_loss: 0.01687832735478878\n",
      "iteration 239, dc_loss: 0.03317876160144806, tv_loss: 0.016905203461647034\n",
      "iteration 240, dc_loss: 0.03308678790926933, tv_loss: 0.01687132567167282\n",
      "iteration 241, dc_loss: 0.03291715309023857, tv_loss: 0.016915518790483475\n",
      "iteration 242, dc_loss: 0.03283337503671646, tv_loss: 0.016873693093657494\n",
      "iteration 243, dc_loss: 0.032667480409145355, tv_loss: 0.01691419631242752\n",
      "iteration 244, dc_loss: 0.03257053345441818, tv_loss: 0.016886821016669273\n",
      "iteration 245, dc_loss: 0.03242999687790871, tv_loss: 0.016904544085264206\n",
      "iteration 246, dc_loss: 0.032309893518686295, tv_loss: 0.01690348982810974\n",
      "iteration 247, dc_loss: 0.032200802117586136, tv_loss: 0.01689303293824196\n",
      "iteration 248, dc_loss: 0.03205649554729462, tv_loss: 0.016919277608394623\n",
      "iteration 249, dc_loss: 0.03197557106614113, tv_loss: 0.016883784905076027\n",
      "iteration 250, dc_loss: 0.031809303909540176, tv_loss: 0.01693568378686905\n",
      "iteration 251, dc_loss: 0.031760074198246, tv_loss: 0.016872694715857506\n",
      "iteration 252, dc_loss: 0.031569577753543854, tv_loss: 0.016950197517871857\n",
      "iteration 253, dc_loss: 0.031521331518888474, tv_loss: 0.016881339251995087\n",
      "iteration 254, dc_loss: 0.03135034441947937, tv_loss: 0.016932783648371696\n",
      "iteration 255, dc_loss: 0.03124973364174366, tv_loss: 0.0169181190431118\n",
      "iteration 256, dc_loss: 0.031158316880464554, tv_loss: 0.016900597140192986\n",
      "iteration 257, dc_loss: 0.031003084033727646, tv_loss: 0.016949117183685303\n",
      "iteration 258, dc_loss: 0.0309465192258358, tv_loss: 0.016896717250347137\n",
      "iteration 259, dc_loss: 0.030785975977778435, tv_loss: 0.016945084556937218\n",
      "iteration 260, dc_loss: 0.030701057985424995, tv_loss: 0.016918359324336052\n",
      "iteration 261, dc_loss: 0.030591409653425217, tv_loss: 0.016920318827033043\n",
      "iteration 262, dc_loss: 0.030461765825748444, tv_loss: 0.016945799812674522\n",
      "iteration 263, dc_loss: 0.030396709218621254, tv_loss: 0.016907505691051483\n",
      "iteration 264, dc_loss: 0.030243705958127975, tv_loss: 0.016955707222223282\n",
      "iteration 265, dc_loss: 0.030179549008607864, tv_loss: 0.01691371016204357\n",
      "iteration 266, dc_loss: 0.030042748898267746, tv_loss: 0.016944853588938713\n",
      "iteration 267, dc_loss: 0.029950467869639397, tv_loss: 0.016933590173721313\n",
      "iteration 268, dc_loss: 0.02985255979001522, tv_loss: 0.016930341720581055\n",
      "iteration 269, dc_loss: 0.029728170484304428, tv_loss: 0.01695534959435463\n",
      "iteration 270, dc_loss: 0.029664261266589165, tv_loss: 0.016920922324061394\n",
      "iteration 271, dc_loss: 0.029519282281398773, tv_loss: 0.01696804165840149\n",
      "iteration 272, dc_loss: 0.029470348730683327, tv_loss: 0.016919182613492012\n",
      "iteration 273, dc_loss: 0.029321175068616867, tv_loss: 0.016970297321677208\n",
      "iteration 274, dc_loss: 0.02926531806588173, tv_loss: 0.016928093507885933\n",
      "iteration 275, dc_loss: 0.02913506329059601, tv_loss: 0.01696108840405941\n",
      "iteration 276, dc_loss: 0.029056023806333542, tv_loss: 0.016944345086812973\n",
      "iteration 277, dc_loss: 0.028955824673175812, tv_loss: 0.016950469464063644\n",
      "iteration 278, dc_loss: 0.02885436825454235, tv_loss: 0.016959376633167267\n",
      "iteration 279, dc_loss: 0.028780771419405937, tv_loss: 0.016941921785473824\n",
      "iteration 280, dc_loss: 0.028659852221608162, tv_loss: 0.016973406076431274\n",
      "iteration 281, dc_loss: 0.02861289121210575, tv_loss: 0.01693294197320938\n",
      "iteration 282, dc_loss: 0.028469091281294823, tv_loss: 0.016991805285215378\n",
      "iteration 283, dc_loss: 0.0284557044506073, tv_loss: 0.016922123730182648\n",
      "iteration 284, dc_loss: 0.028286313638091087, tv_loss: 0.017007652670145035\n",
      "iteration 285, dc_loss: 0.02828212082386017, tv_loss: 0.0169234462082386\n",
      "iteration 286, dc_loss: 0.02812008559703827, tv_loss: 0.016992850229144096\n",
      "iteration 287, dc_loss: 0.028064511716365814, tv_loss: 0.016958629712462425\n",
      "iteration 288, dc_loss: 0.02798778936266899, tv_loss: 0.01695360243320465\n",
      "iteration 289, dc_loss: 0.027867740020155907, tv_loss: 0.016997328028082848\n",
      "iteration 290, dc_loss: 0.027851557359099388, tv_loss: 0.016936568543314934\n",
      "iteration 291, dc_loss: 0.027703991159796715, tv_loss: 0.017003145068883896\n",
      "iteration 292, dc_loss: 0.027665037661790848, tv_loss: 0.016958681866526604\n",
      "iteration 293, dc_loss: 0.027568500488996506, tv_loss: 0.016974521800875664\n",
      "iteration 294, dc_loss: 0.027477553114295006, tv_loss: 0.016989443451166153\n",
      "iteration 295, dc_loss: 0.027440844103693962, tv_loss: 0.016952965408563614\n",
      "iteration 296, dc_loss: 0.027317620813846588, tv_loss: 0.017002973705530167\n",
      "iteration 297, dc_loss: 0.0272834450006485, tv_loss: 0.016962546855211258\n",
      "iteration 298, dc_loss: 0.027178825810551643, tv_loss: 0.016991589218378067\n",
      "iteration 299, dc_loss: 0.02711435593664646, tv_loss: 0.016981661319732666\n",
      "iteration 300, dc_loss: 0.02705238200724125, tv_loss: 0.01697246916592121\n",
      "iteration 301, dc_loss: 0.026958078145980835, tv_loss: 0.016998136416077614\n",
      "iteration 302, dc_loss: 0.026920124888420105, tv_loss: 0.016968436539173126\n",
      "iteration 303, dc_loss: 0.026814838871359825, tv_loss: 0.017005497589707375\n",
      "iteration 304, dc_loss: 0.02677863836288452, tv_loss: 0.016972169280052185\n",
      "iteration 305, dc_loss: 0.02668301947414875, tv_loss: 0.016999028623104095\n",
      "iteration 306, dc_loss: 0.026633072644472122, tv_loss: 0.016981929540634155\n",
      "iteration 307, dc_loss: 0.026558928191661835, tv_loss: 0.016990795731544495\n",
      "iteration 308, dc_loss: 0.026489974930882454, tv_loss: 0.01699581928551197\n",
      "iteration 309, dc_loss: 0.026440132409334183, tv_loss: 0.01698267087340355\n",
      "iteration 310, dc_loss: 0.026352912187576294, tv_loss: 0.01700773276388645\n",
      "iteration 311, dc_loss: 0.02632489986717701, tv_loss: 0.01697450876235962\n",
      "iteration 312, dc_loss: 0.026221994310617447, tv_loss: 0.01701710931956768\n",
      "iteration 313, dc_loss: 0.026211807504296303, tv_loss: 0.016967887058854103\n",
      "iteration 314, dc_loss: 0.026096433401107788, tv_loss: 0.017024382948875427\n",
      "iteration 315, dc_loss: 0.026094770058989525, tv_loss: 0.016967175528407097\n",
      "iteration 316, dc_loss: 0.02597782388329506, tv_loss: 0.01702486351132393\n",
      "iteration 317, dc_loss: 0.02596721053123474, tv_loss: 0.016976065933704376\n",
      "iteration 318, dc_loss: 0.025869518518447876, tv_loss: 0.017015304416418076\n",
      "iteration 319, dc_loss: 0.0258342232555151, tv_loss: 0.016993794590234756\n",
      "iteration 320, dc_loss: 0.025768296793103218, tv_loss: 0.017004240304231644\n",
      "iteration 321, dc_loss: 0.025708921253681183, tv_loss: 0.017008347436785698\n",
      "iteration 322, dc_loss: 0.02566782385110855, tv_loss: 0.016994688659906387\n",
      "iteration 323, dc_loss: 0.025592699646949768, tv_loss: 0.017015967518091202\n",
      "iteration 324, dc_loss: 0.025565944612026215, tv_loss: 0.016990162432193756\n",
      "iteration 325, dc_loss: 0.025481535121798515, tv_loss: 0.017023511230945587\n",
      "iteration 326, dc_loss: 0.02546692080795765, tv_loss: 0.01698855683207512\n",
      "iteration 327, dc_loss: 0.025372274219989777, tv_loss: 0.017034225165843964\n",
      "iteration 328, dc_loss: 0.025374630466103554, tv_loss: 0.01698206178843975\n",
      "iteration 329, dc_loss: 0.025265546515583992, tv_loss: 0.017041729763150215\n",
      "iteration 330, dc_loss: 0.025283465161919594, tv_loss: 0.016975179314613342\n",
      "iteration 331, dc_loss: 0.02516433410346508, tv_loss: 0.01704537868499756\n",
      "iteration 332, dc_loss: 0.025179749354720116, tv_loss: 0.016979487612843513\n",
      "iteration 333, dc_loss: 0.025073101744055748, tv_loss: 0.017035219818353653\n",
      "iteration 334, dc_loss: 0.025060083717107773, tv_loss: 0.016997884958982468\n",
      "iteration 335, dc_loss: 0.024990715086460114, tv_loss: 0.017018845304846764\n",
      "iteration 336, dc_loss: 0.024945221841335297, tv_loss: 0.01701813004910946\n",
      "iteration 337, dc_loss: 0.024917008355259895, tv_loss: 0.017001811414957047\n",
      "iteration 338, dc_loss: 0.024840226396918297, tv_loss: 0.017035286873579025\n",
      "iteration 339, dc_loss: 0.024837031960487366, tv_loss: 0.01699514128267765\n",
      "iteration 340, dc_loss: 0.024748485535383224, tv_loss: 0.01703992672264576\n",
      "iteration 341, dc_loss: 0.024745795875787735, tv_loss: 0.016998527571558952\n",
      "iteration 342, dc_loss: 0.024663008749485016, tv_loss: 0.01703624054789543\n",
      "iteration 343, dc_loss: 0.024646509438753128, tv_loss: 0.01700826734304428\n",
      "iteration 344, dc_loss: 0.024584654718637466, tv_loss: 0.017026584595441818\n",
      "iteration 345, dc_loss: 0.024542421102523804, tv_loss: 0.017026016488671303\n",
      "iteration 346, dc_loss: 0.024519436061382294, tv_loss: 0.01700824871659279\n",
      "iteration 347, dc_loss: 0.024447400122880936, tv_loss: 0.017040014266967773\n",
      "iteration 348, dc_loss: 0.02444501221179962, tv_loss: 0.01700284704566002\n",
      "iteration 349, dc_loss: 0.024366578087210655, tv_loss: 0.01704467087984085\n",
      "iteration 350, dc_loss: 0.024380996823310852, tv_loss: 0.016996018588542938\n",
      "iteration 351, dc_loss: 0.024271752685308456, tv_loss: 0.017071284353733063\n",
      "iteration 352, dc_loss: 0.024332230910658836, tv_loss: 0.016975784674286842\n",
      "iteration 353, dc_loss: 0.024191202595829964, tv_loss: 0.017073005437850952\n",
      "iteration 354, dc_loss: 0.024214724078774452, tv_loss: 0.017002355307340622\n",
      "iteration 355, dc_loss: 0.024139070883393288, tv_loss: 0.017032567411661148\n",
      "iteration 356, dc_loss: 0.024098491296172142, tv_loss: 0.017033929005265236\n",
      "iteration 357, dc_loss: 0.024089964106678963, tv_loss: 0.017010288313031197\n",
      "iteration 358, dc_loss: 0.02400849014520645, tv_loss: 0.017058538272976875\n",
      "iteration 359, dc_loss: 0.024033237248659134, tv_loss: 0.0169985294342041\n",
      "iteration 360, dc_loss: 0.0239335298538208, tv_loss: 0.01706036739051342\n",
      "iteration 361, dc_loss: 0.023931309580802917, tv_loss: 0.017022013664245605\n",
      "iteration 362, dc_loss: 0.023891856893897057, tv_loss: 0.017023945227265358\n",
      "iteration 363, dc_loss: 0.023831868544220924, tv_loss: 0.017049312591552734\n",
      "iteration 364, dc_loss: 0.023835439234972, tv_loss: 0.017012331634759903\n",
      "iteration 365, dc_loss: 0.023762458935379982, tv_loss: 0.017052864655852318\n",
      "iteration 366, dc_loss: 0.023758234456181526, tv_loss: 0.017023393884301186\n",
      "iteration 367, dc_loss: 0.023699257522821426, tv_loss: 0.017046121880412102\n",
      "iteration 368, dc_loss: 0.023681016638875008, tv_loss: 0.01702834852039814\n",
      "iteration 369, dc_loss: 0.023639002814888954, tv_loss: 0.017036568373441696\n",
      "iteration 370, dc_loss: 0.023601550608873367, tv_loss: 0.017041370272636414\n",
      "iteration 371, dc_loss: 0.023585902526974678, tv_loss: 0.01702524535357952\n",
      "iteration 372, dc_loss: 0.02352483756840229, tv_loss: 0.01705564372241497\n",
      "iteration 373, dc_loss: 0.023529700934886932, tv_loss: 0.017020005732774734\n",
      "iteration 374, dc_loss: 0.023460103198885918, tv_loss: 0.017057569697499275\n",
      "iteration 375, dc_loss: 0.0234576053917408, tv_loss: 0.017026761546730995\n",
      "iteration 376, dc_loss: 0.02340485155582428, tv_loss: 0.01704673282802105\n",
      "iteration 377, dc_loss: 0.023380812257528305, tv_loss: 0.01703927479684353\n",
      "iteration 378, dc_loss: 0.02335384301841259, tv_loss: 0.01703594997525215\n",
      "iteration 379, dc_loss: 0.023312324658036232, tv_loss: 0.017048057168722153\n",
      "iteration 380, dc_loss: 0.023298725485801697, tv_loss: 0.017032848671078682\n",
      "iteration 381, dc_loss: 0.023249609395861626, tv_loss: 0.01705334149301052\n",
      "iteration 382, dc_loss: 0.023236799985170364, tv_loss: 0.017036674544215202\n",
      "iteration 383, dc_loss: 0.023191560059785843, tv_loss: 0.017051732167601585\n",
      "iteration 384, dc_loss: 0.02317414991557598, tv_loss: 0.017039000988006592\n",
      "iteration 385, dc_loss: 0.02313719317317009, tv_loss: 0.01704665832221508\n",
      "iteration 386, dc_loss: 0.02311304584145546, tv_loss: 0.017042361199855804\n",
      "iteration 387, dc_loss: 0.023082049563527107, tv_loss: 0.017045775428414345\n",
      "iteration 388, dc_loss: 0.023054808378219604, tv_loss: 0.0170464888215065\n",
      "iteration 389, dc_loss: 0.0230299960821867, tv_loss: 0.017046501860022545\n",
      "iteration 390, dc_loss: 0.02300861105322838, tv_loss: 0.017046310007572174\n",
      "iteration 391, dc_loss: 0.022975588217377663, tv_loss: 0.017057860270142555\n",
      "iteration 392, dc_loss: 0.02299100160598755, tv_loss: 0.0170206930488348\n",
      "iteration 393, dc_loss: 0.02289217710494995, tv_loss: 0.017099911347031593\n",
      "iteration 394, dc_loss: 0.023003123700618744, tv_loss: 0.016975218430161476\n",
      "iteration 395, dc_loss: 0.022832410410046577, tv_loss: 0.017118262127041817\n",
      "iteration 396, dc_loss: 0.02289849892258644, tv_loss: 0.017009494826197624\n",
      "iteration 397, dc_loss: 0.022818133234977722, tv_loss: 0.017054466530680656\n",
      "iteration 398, dc_loss: 0.022768672555685043, tv_loss: 0.017076291143894196\n",
      "iteration 399, dc_loss: 0.022809570655226707, tv_loss: 0.017015159130096436\n",
      "iteration 400, dc_loss: 0.02272910065948963, tv_loss: 0.017080778256058693\n",
      "iteration 401, dc_loss: 0.022747784852981567, tv_loss: 0.0170355923473835\n",
      "iteration 402, dc_loss: 0.02268911898136139, tv_loss: 0.01705746166408062\n",
      "iteration 403, dc_loss: 0.02267305925488472, tv_loss: 0.017064929008483887\n",
      "iteration 404, dc_loss: 0.022689182311296463, tv_loss: 0.017028534784913063\n",
      "iteration 405, dc_loss: 0.022629357874393463, tv_loss: 0.01706215739250183\n",
      "iteration 406, dc_loss: 0.022606607526540756, tv_loss: 0.017070597037672997\n",
      "iteration 407, dc_loss: 0.022633658722043037, tv_loss: 0.01702580600976944\n",
      "iteration 408, dc_loss: 0.02257932536303997, tv_loss: 0.017055589705705643\n",
      "iteration 409, dc_loss: 0.02254236303269863, tv_loss: 0.01707896590232849\n",
      "iteration 410, dc_loss: 0.022571317851543427, tv_loss: 0.017031220719218254\n",
      "iteration 411, dc_loss: 0.022535327821969986, tv_loss: 0.017044363543391228\n",
      "iteration 412, dc_loss: 0.022487642243504524, tv_loss: 0.017078999429941177\n",
      "iteration 413, dc_loss: 0.022502949461340904, tv_loss: 0.017043180763721466\n",
      "iteration 414, dc_loss: 0.022488202899694443, tv_loss: 0.01703949086368084\n",
      "iteration 415, dc_loss: 0.02243940904736519, tv_loss: 0.017071768641471863\n",
      "iteration 416, dc_loss: 0.02243988960981369, tv_loss: 0.017053501680493355\n",
      "iteration 417, dc_loss: 0.022432245314121246, tv_loss: 0.017042549327015877\n",
      "iteration 418, dc_loss: 0.022394979372620583, tv_loss: 0.017062997445464134\n",
      "iteration 419, dc_loss: 0.022384585812687874, tv_loss: 0.017056968063116074\n",
      "iteration 420, dc_loss: 0.02237231656908989, tv_loss: 0.017050303518772125\n",
      "iteration 421, dc_loss: 0.02234884537756443, tv_loss: 0.01705802232027054\n",
      "iteration 422, dc_loss: 0.02233470417559147, tv_loss: 0.017054997384548187\n",
      "iteration 423, dc_loss: 0.022314468398690224, tv_loss: 0.01705765724182129\n",
      "iteration 424, dc_loss: 0.022298671305179596, tv_loss: 0.017057480290532112\n",
      "iteration 425, dc_loss: 0.022288840264081955, tv_loss: 0.01705041155219078\n",
      "iteration 426, dc_loss: 0.022260699421167374, tv_loss: 0.017061898484826088\n",
      "iteration 427, dc_loss: 0.0222482867538929, tv_loss: 0.01705790124833584\n",
      "iteration 428, dc_loss: 0.022241510450839996, tv_loss: 0.01704847626388073\n",
      "iteration 429, dc_loss: 0.02220877818763256, tv_loss: 0.01706496998667717\n",
      "iteration 430, dc_loss: 0.02219819277524948, tv_loss: 0.017059210687875748\n",
      "iteration 431, dc_loss: 0.022192593663930893, tv_loss: 0.017049157992005348\n",
      "iteration 432, dc_loss: 0.022160524502396584, tv_loss: 0.01706509105861187\n",
      "iteration 433, dc_loss: 0.02215063013136387, tv_loss: 0.01705894246697426\n",
      "iteration 434, dc_loss: 0.022141089662909508, tv_loss: 0.01705300621688366\n",
      "iteration 435, dc_loss: 0.022114893421530724, tv_loss: 0.01706347055733204\n",
      "iteration 436, dc_loss: 0.022103799507021904, tv_loss: 0.017058925703167915\n",
      "iteration 437, dc_loss: 0.022090701386332512, tv_loss: 0.017056548967957497\n",
      "iteration 438, dc_loss: 0.0220715943723917, tv_loss: 0.01706014759838581\n",
      "iteration 439, dc_loss: 0.022054746747016907, tv_loss: 0.017061837017536163\n",
      "iteration 440, dc_loss: 0.022042840719223022, tv_loss: 0.017058663070201874\n",
      "iteration 441, dc_loss: 0.022028131410479546, tv_loss: 0.01705818437039852\n",
      "iteration 442, dc_loss: 0.02200552076101303, tv_loss: 0.0170658677816391\n",
      "iteration 443, dc_loss: 0.022002672776579857, tv_loss: 0.017053822055459023\n",
      "iteration 444, dc_loss: 0.021976763382554054, tv_loss: 0.01706531085073948\n",
      "iteration 445, dc_loss: 0.021970996633172035, tv_loss: 0.01705734245479107\n",
      "iteration 446, dc_loss: 0.02194770611822605, tv_loss: 0.017068350687623024\n",
      "iteration 447, dc_loss: 0.021951831877231598, tv_loss: 0.01705058105289936\n",
      "iteration 448, dc_loss: 0.021911947056651115, tv_loss: 0.017074424773454666\n",
      "iteration 449, dc_loss: 0.02191684953868389, tv_loss: 0.017052019014954567\n",
      "iteration 450, dc_loss: 0.021891770884394646, tv_loss: 0.01706155575811863\n",
      "iteration 451, dc_loss: 0.021869255229830742, tv_loss: 0.017071343958377838\n",
      "iteration 452, dc_loss: 0.021879376843571663, tv_loss: 0.017050396651029587\n",
      "iteration 453, dc_loss: 0.02184336632490158, tv_loss: 0.017076928168535233\n",
      "iteration 454, dc_loss: 0.021855374798178673, tv_loss: 0.017052000388503075\n",
      "iteration 455, dc_loss: 0.02181580848991871, tv_loss: 0.01707269810140133\n",
      "iteration 456, dc_loss: 0.021812109276652336, tv_loss: 0.01705845631659031\n",
      "iteration 457, dc_loss: 0.02179974503815174, tv_loss: 0.017058957368135452\n",
      "iteration 458, dc_loss: 0.021769758313894272, tv_loss: 0.017078768461942673\n",
      "iteration 459, dc_loss: 0.021788261830806732, tv_loss: 0.01704660803079605\n",
      "iteration 460, dc_loss: 0.021739691495895386, tv_loss: 0.017078224569559097\n",
      "iteration 461, dc_loss: 0.021735621616244316, tv_loss: 0.017066940665245056\n",
      "iteration 462, dc_loss: 0.021737869828939438, tv_loss: 0.017052998766303062\n",
      "iteration 463, dc_loss: 0.02170080505311489, tv_loss: 0.0170787014067173\n",
      "iteration 464, dc_loss: 0.02171187661588192, tv_loss: 0.01705418899655342\n",
      "iteration 465, dc_loss: 0.021682430058717728, tv_loss: 0.017068831250071526\n",
      "iteration 466, dc_loss: 0.02166922576725483, tv_loss: 0.017067354172468185\n",
      "iteration 467, dc_loss: 0.021658873185515404, tv_loss: 0.01706523448228836\n",
      "iteration 468, dc_loss: 0.021642334759235382, tv_loss: 0.017070680856704712\n",
      "iteration 469, dc_loss: 0.021642636507749557, tv_loss: 0.017058150842785835\n",
      "iteration 470, dc_loss: 0.02161175198853016, tv_loss: 0.017075588926672935\n",
      "iteration 471, dc_loss: 0.021615417674183846, tv_loss: 0.017057990655303\n",
      "iteration 472, dc_loss: 0.02159000374376774, tv_loss: 0.01707005500793457\n",
      "iteration 473, dc_loss: 0.021576739847660065, tv_loss: 0.01707116886973381\n",
      "iteration 474, dc_loss: 0.021576767787337303, tv_loss: 0.017059778794646263\n",
      "iteration 475, dc_loss: 0.021549470722675323, tv_loss: 0.017075641080737114\n",
      "iteration 476, dc_loss: 0.021551955491304398, tv_loss: 0.0170612595975399\n",
      "iteration 477, dc_loss: 0.02152848057448864, tv_loss: 0.01707245223224163\n",
      "iteration 478, dc_loss: 0.0215256717056036, tv_loss: 0.01706264354288578\n",
      "iteration 479, dc_loss: 0.021501552313566208, tv_loss: 0.017074191942811012\n",
      "iteration 480, dc_loss: 0.021501384675502777, tv_loss: 0.017062170431017876\n",
      "iteration 481, dc_loss: 0.021479181945323944, tv_loss: 0.017072515562176704\n",
      "iteration 482, dc_loss: 0.021471621468663216, tv_loss: 0.017068419605493546\n",
      "iteration 483, dc_loss: 0.021460358053445816, tv_loss: 0.017067840322852135\n",
      "iteration 484, dc_loss: 0.021444842219352722, tv_loss: 0.017071310430765152\n",
      "iteration 485, dc_loss: 0.02143489196896553, tv_loss: 0.017069121822714806\n",
      "iteration 486, dc_loss: 0.021424906328320503, tv_loss: 0.01706724800169468\n",
      "iteration 487, dc_loss: 0.021407591179013252, tv_loss: 0.017073065042495728\n",
      "iteration 488, dc_loss: 0.021404678001999855, tv_loss: 0.01706480048596859\n",
      "iteration 489, dc_loss: 0.021384108811616898, tv_loss: 0.017074763774871826\n",
      "iteration 490, dc_loss: 0.021385284140706062, tv_loss: 0.017064126208424568\n",
      "iteration 491, dc_loss: 0.021362295374274254, tv_loss: 0.01708023063838482\n",
      "iteration 492, dc_loss: 0.021384630352258682, tv_loss: 0.01705498807132244\n",
      "iteration 493, dc_loss: 0.021345775574445724, tv_loss: 0.017094343900680542\n",
      "iteration 494, dc_loss: 0.021390868350863457, tv_loss: 0.01704074628651142\n",
      "iteration 495, dc_loss: 0.021308813244104385, tv_loss: 0.01709778420627117\n",
      "iteration 496, dc_loss: 0.02132543921470642, tv_loss: 0.017056621611118317\n",
      "iteration 497, dc_loss: 0.02131323702633381, tv_loss: 0.01706179790198803\n",
      "iteration 498, dc_loss: 0.021275660023093224, tv_loss: 0.017100300639867783\n",
      "iteration 499, dc_loss: 0.021327706053853035, tv_loss: 0.01703754812479019\n",
      "iteration 500, dc_loss: 0.021251242607831955, tv_loss: 0.017091967165470123\n",
      "iteration 501, dc_loss: 0.021250830963253975, tv_loss: 0.01707795262336731\n",
      "iteration 502, dc_loss: 0.02127869799733162, tv_loss: 0.017047088593244553\n",
      "iteration 503, dc_loss: 0.021221036091446877, tv_loss: 0.017097236588597298\n",
      "iteration 504, dc_loss: 0.021244052797555923, tv_loss: 0.01705721765756607\n",
      "iteration 505, dc_loss: 0.021220417693257332, tv_loss: 0.017065895721316338\n",
      "iteration 506, dc_loss: 0.021192342042922974, tv_loss: 0.01708713360130787\n",
      "iteration 507, dc_loss: 0.021217316389083862, tv_loss: 0.01705641858279705\n",
      "iteration 508, dc_loss: 0.02117954194545746, tv_loss: 0.017082393169403076\n",
      "iteration 509, dc_loss: 0.02117607556283474, tv_loss: 0.01707182638347149\n",
      "iteration 510, dc_loss: 0.021171357482671738, tv_loss: 0.017067397013306618\n",
      "iteration 511, dc_loss: 0.021147921681404114, tv_loss: 0.017083942890167236\n",
      "iteration 512, dc_loss: 0.0211612731218338, tv_loss: 0.017059840261936188\n",
      "iteration 513, dc_loss: 0.021125074476003647, tv_loss: 0.017082681879401207\n",
      "iteration 514, dc_loss: 0.02112616039812565, tv_loss: 0.01707059144973755\n",
      "iteration 515, dc_loss: 0.021123945713043213, tv_loss: 0.017065051943063736\n",
      "iteration 516, dc_loss: 0.021093837916851044, tv_loss: 0.01708749681711197\n",
      "iteration 517, dc_loss: 0.02111179567873478, tv_loss: 0.017059776932001114\n",
      "iteration 518, dc_loss: 0.02107955887913704, tv_loss: 0.017081240192055702\n",
      "iteration 519, dc_loss: 0.02107332833111286, tv_loss: 0.01707751490175724\n",
      "iteration 520, dc_loss: 0.021077986806631088, tv_loss: 0.017064057290554047\n",
      "iteration 521, dc_loss: 0.021048519760370255, tv_loss: 0.017084190621972084\n",
      "iteration 522, dc_loss: 0.02105575241148472, tv_loss: 0.017066510394215584\n",
      "iteration 523, dc_loss: 0.02103724330663681, tv_loss: 0.017074618488550186\n",
      "iteration 524, dc_loss: 0.021026354283094406, tv_loss: 0.01707638055086136\n",
      "iteration 525, dc_loss: 0.021025031805038452, tv_loss: 0.017069991677999496\n",
      "iteration 526, dc_loss: 0.021007342264056206, tv_loss: 0.017080269753932953\n",
      "iteration 527, dc_loss: 0.021007036790251732, tv_loss: 0.017071833834052086\n",
      "iteration 528, dc_loss: 0.020989801734685898, tv_loss: 0.01707865484058857\n",
      "iteration 529, dc_loss: 0.02098563313484192, tv_loss: 0.017072239890694618\n",
      "iteration 530, dc_loss: 0.020973000675439835, tv_loss: 0.01707516424357891\n",
      "iteration 531, dc_loss: 0.02096347138285637, tv_loss: 0.017075879499316216\n",
      "iteration 532, dc_loss: 0.020959945395588875, tv_loss: 0.017071083188056946\n",
      "iteration 533, dc_loss: 0.02094196528196335, tv_loss: 0.01708107255399227\n",
      "iteration 534, dc_loss: 0.020945053547620773, tv_loss: 0.017070291563868523\n",
      "iteration 535, dc_loss: 0.020926162600517273, tv_loss: 0.017081215977668762\n",
      "iteration 536, dc_loss: 0.020924212411046028, tv_loss: 0.01707439310848713\n",
      "iteration 537, dc_loss: 0.020911840721964836, tv_loss: 0.017077771946787834\n",
      "iteration 538, dc_loss: 0.020908311009407043, tv_loss: 0.017072850838303566\n",
      "iteration 539, dc_loss: 0.020889759063720703, tv_loss: 0.01708284206688404\n",
      "iteration 540, dc_loss: 0.020898832008242607, tv_loss: 0.017065001651644707\n",
      "iteration 541, dc_loss: 0.020867163315415382, tv_loss: 0.017087794840335846\n",
      "iteration 542, dc_loss: 0.020878413692116737, tv_loss: 0.017067698761820793\n",
      "iteration 543, dc_loss: 0.020857030525803566, tv_loss: 0.01708107441663742\n",
      "iteration 544, dc_loss: 0.02085035666823387, tv_loss: 0.017079805955290794\n",
      "iteration 545, dc_loss: 0.020848343148827553, tv_loss: 0.01707318238914013\n",
      "iteration 546, dc_loss: 0.020831797271966934, tv_loss: 0.017081042751669884\n",
      "iteration 547, dc_loss: 0.020832844078540802, tv_loss: 0.017072061076760292\n",
      "iteration 548, dc_loss: 0.02081574685871601, tv_loss: 0.017081715166568756\n",
      "iteration 549, dc_loss: 0.02082044817507267, tv_loss: 0.017069721594452858\n",
      "iteration 550, dc_loss: 0.02079540304839611, tv_loss: 0.017087383195757866\n",
      "iteration 551, dc_loss: 0.020812859758734703, tv_loss: 0.01706288941204548\n",
      "iteration 552, dc_loss: 0.020776325836777687, tv_loss: 0.017092645168304443\n",
      "iteration 553, dc_loss: 0.0207980964332819, tv_loss: 0.017063867300748825\n",
      "iteration 554, dc_loss: 0.020761879161000252, tv_loss: 0.017092019319534302\n",
      "iteration 555, dc_loss: 0.020775675773620605, tv_loss: 0.017068684101104736\n",
      "iteration 556, dc_loss: 0.02074778638780117, tv_loss: 0.017086515203118324\n",
      "iteration 557, dc_loss: 0.020754022523760796, tv_loss: 0.017070692032575607\n",
      "iteration 558, dc_loss: 0.020734116435050964, tv_loss: 0.017081741243600845\n",
      "iteration 559, dc_loss: 0.020732339471578598, tv_loss: 0.0170753076672554\n",
      "iteration 560, dc_loss: 0.02072271704673767, tv_loss: 0.01707720384001732\n",
      "iteration 561, dc_loss: 0.02071116492152214, tv_loss: 0.01708145998418331\n",
      "iteration 562, dc_loss: 0.020713964477181435, tv_loss: 0.01707187294960022\n",
      "iteration 563, dc_loss: 0.020692940801382065, tv_loss: 0.01708686165511608\n",
      "iteration 564, dc_loss: 0.020705096423625946, tv_loss: 0.017069436609745026\n",
      "iteration 565, dc_loss: 0.020675884559750557, tv_loss: 0.017093269154429436\n",
      "iteration 566, dc_loss: 0.02069850265979767, tv_loss: 0.017064757645130157\n",
      "iteration 567, dc_loss: 0.020661665126681328, tv_loss: 0.017096150666475296\n",
      "iteration 568, dc_loss: 0.020693348720669746, tv_loss: 0.01705958880484104\n",
      "iteration 569, dc_loss: 0.020649608224630356, tv_loss: 0.01709754392504692\n",
      "iteration 570, dc_loss: 0.020679745823144913, tv_loss: 0.017059000208973885\n",
      "iteration 571, dc_loss: 0.02063182182610035, tv_loss: 0.01709546521306038\n",
      "iteration 572, dc_loss: 0.02064712904393673, tv_loss: 0.017068983986973763\n",
      "iteration 573, dc_loss: 0.02062273398041725, tv_loss: 0.017084280028939247\n",
      "iteration 574, dc_loss: 0.02061283029615879, tv_loss: 0.017086466774344444\n",
      "iteration 575, dc_loss: 0.0206291526556015, tv_loss: 0.017064379528164864\n",
      "iteration 576, dc_loss: 0.020591940730810165, tv_loss: 0.01709674298763275\n",
      "iteration 577, dc_loss: 0.020621556788682938, tv_loss: 0.017061062157154083\n",
      "iteration 578, dc_loss: 0.020581966266036034, tv_loss: 0.017092658206820488\n",
      "iteration 579, dc_loss: 0.020593587309122086, tv_loss: 0.017072414979338646\n",
      "iteration 580, dc_loss: 0.020575489848852158, tv_loss: 0.017082851380109787\n",
      "iteration 581, dc_loss: 0.020566022023558617, tv_loss: 0.017084568738937378\n",
      "iteration 582, dc_loss: 0.020568447187542915, tv_loss: 0.017073968425393105\n",
      "iteration 583, dc_loss: 0.02054775133728981, tv_loss: 0.017087945714592934\n",
      "iteration 584, dc_loss: 0.020560501143336296, tv_loss: 0.017069412395358086\n",
      "iteration 585, dc_loss: 0.02053474634885788, tv_loss: 0.01708959974348545\n",
      "iteration 586, dc_loss: 0.020549802109599113, tv_loss: 0.017068887129426003\n",
      "iteration 587, dc_loss: 0.020521627739071846, tv_loss: 0.017091648653149605\n",
      "iteration 588, dc_loss: 0.02053273655474186, tv_loss: 0.017073718830943108\n",
      "iteration 589, dc_loss: 0.020509321242570877, tv_loss: 0.017087731510400772\n",
      "iteration 590, dc_loss: 0.02051195502281189, tv_loss: 0.017076078802347183\n",
      "iteration 591, dc_loss: 0.020499417558312416, tv_loss: 0.017080767080187798\n",
      "iteration 592, dc_loss: 0.020494109019637108, tv_loss: 0.017079122364521027\n",
      "iteration 593, dc_loss: 0.020488576963543892, tv_loss: 0.017078351229429245\n",
      "iteration 594, dc_loss: 0.02047945186495781, tv_loss: 0.017081864178180695\n",
      "iteration 595, dc_loss: 0.02047787792980671, tv_loss: 0.017078924924135208\n",
      "iteration 596, dc_loss: 0.020465552806854248, tv_loss: 0.017086854204535484\n",
      "iteration 597, dc_loss: 0.020470991730690002, tv_loss: 0.01707570254802704\n",
      "iteration 598, dc_loss: 0.02045081928372383, tv_loss: 0.017090098932385445\n",
      "iteration 599, dc_loss: 0.020470086485147476, tv_loss: 0.017066141590476036\n",
      "iteration 600, dc_loss: 0.020433885976672173, tv_loss: 0.01709853485226631\n",
      "iteration 601, dc_loss: 0.020469721406698227, tv_loss: 0.017058098688721657\n",
      "iteration 602, dc_loss: 0.020419225096702576, tv_loss: 0.01710229180753231\n",
      "iteration 603, dc_loss: 0.020452778786420822, tv_loss: 0.017059916630387306\n",
      "iteration 604, dc_loss: 0.020408624783158302, tv_loss: 0.01709410920739174\n",
      "iteration 605, dc_loss: 0.020419398322701454, tv_loss: 0.017074650153517723\n",
      "iteration 606, dc_loss: 0.02040606550872326, tv_loss: 0.017081599682569504\n",
      "iteration 607, dc_loss: 0.020391467958688736, tv_loss: 0.01709076575934887\n",
      "iteration 608, dc_loss: 0.020408717915415764, tv_loss: 0.017068112269043922\n",
      "iteration 609, dc_loss: 0.02037491649389267, tv_loss: 0.017096910625696182\n",
      "iteration 610, dc_loss: 0.02040238492190838, tv_loss: 0.01706424541771412\n",
      "iteration 611, dc_loss: 0.020365795120596886, tv_loss: 0.017094837501645088\n",
      "iteration 612, dc_loss: 0.020384838804602623, tv_loss: 0.01706882007420063\n",
      "iteration 613, dc_loss: 0.020356666296720505, tv_loss: 0.017089489847421646\n",
      "iteration 614, dc_loss: 0.020364535972476006, tv_loss: 0.017074260860681534\n",
      "iteration 615, dc_loss: 0.02034626342356205, tv_loss: 0.017086539417505264\n",
      "iteration 616, dc_loss: 0.020347541198134422, tv_loss: 0.017080526798963547\n",
      "iteration 617, dc_loss: 0.020339401438832283, tv_loss: 0.01708361878991127\n",
      "iteration 618, dc_loss: 0.02033214457333088, tv_loss: 0.017085041850805283\n",
      "iteration 619, dc_loss: 0.020336735993623734, tv_loss: 0.017074987292289734\n",
      "iteration 620, dc_loss: 0.02031487599015236, tv_loss: 0.01709197275340557\n",
      "iteration 621, dc_loss: 0.02033550664782524, tv_loss: 0.017066646367311478\n",
      "iteration 622, dc_loss: 0.02029905654489994, tv_loss: 0.017098454758524895\n",
      "iteration 623, dc_loss: 0.0203270111232996, tv_loss: 0.017065681517124176\n",
      "iteration 624, dc_loss: 0.02029031328856945, tv_loss: 0.01709718070924282\n",
      "iteration 625, dc_loss: 0.020307186990976334, tv_loss: 0.017073726281523705\n",
      "iteration 626, dc_loss: 0.020285392180085182, tv_loss: 0.01708763837814331\n",
      "iteration 627, dc_loss: 0.02028551697731018, tv_loss: 0.01707967184484005\n",
      "iteration 628, dc_loss: 0.02027612179517746, tv_loss: 0.017081934958696365\n",
      "iteration 629, dc_loss: 0.02027043141424656, tv_loss: 0.017081409692764282\n",
      "iteration 630, dc_loss: 0.02026592195034027, tv_loss: 0.017080582678318024\n",
      "iteration 631, dc_loss: 0.020257238298654556, tv_loss: 0.017084620893001556\n",
      "iteration 632, dc_loss: 0.020261017605662346, tv_loss: 0.017076700925827026\n",
      "iteration 633, dc_loss: 0.020239900797605515, tv_loss: 0.01709410361945629\n",
      "iteration 634, dc_loss: 0.020261744037270546, tv_loss: 0.017068026587367058\n",
      "iteration 635, dc_loss: 0.02022322453558445, tv_loss: 0.017101861536502838\n",
      "iteration 636, dc_loss: 0.02025752142071724, tv_loss: 0.017062518745660782\n",
      "iteration 637, dc_loss: 0.020214790478348732, tv_loss: 0.01710032857954502\n",
      "iteration 638, dc_loss: 0.02024436928331852, tv_loss: 0.017066089436411858\n",
      "iteration 639, dc_loss: 0.020210742950439453, tv_loss: 0.017095308750867844\n",
      "iteration 640, dc_loss: 0.020232170820236206, tv_loss: 0.017068855464458466\n",
      "iteration 641, dc_loss: 0.02019907720386982, tv_loss: 0.017096102237701416\n",
      "iteration 642, dc_loss: 0.02022242732346058, tv_loss: 0.017066368833184242\n",
      "iteration 643, dc_loss: 0.0201822891831398, tv_loss: 0.017100252211093903\n",
      "iteration 644, dc_loss: 0.02020898088812828, tv_loss: 0.017067326232790947\n",
      "iteration 645, dc_loss: 0.020174387842416763, tv_loss: 0.01709546521306038\n",
      "iteration 646, dc_loss: 0.020186036825180054, tv_loss: 0.017077215015888214\n",
      "iteration 647, dc_loss: 0.02017616480588913, tv_loss: 0.017081398516893387\n",
      "iteration 648, dc_loss: 0.02016325108706951, tv_loss: 0.017089681699872017\n",
      "iteration 649, dc_loss: 0.020178234204649925, tv_loss: 0.01707075536251068\n",
      "iteration 650, dc_loss: 0.020150650292634964, tv_loss: 0.017094839364290237\n",
      "iteration 651, dc_loss: 0.02017305977642536, tv_loss: 0.017069371417164803\n",
      "iteration 652, dc_loss: 0.020144619047641754, tv_loss: 0.0170952919870615\n",
      "iteration 653, dc_loss: 0.020165234804153442, tv_loss: 0.01707221381366253\n",
      "iteration 654, dc_loss: 0.02013716846704483, tv_loss: 0.017096083611249924\n",
      "iteration 655, dc_loss: 0.020156975835561752, tv_loss: 0.01706990785896778\n",
      "iteration 656, dc_loss: 0.02012236788868904, tv_loss: 0.01709688827395439\n",
      "iteration 657, dc_loss: 0.02014545165002346, tv_loss: 0.017066095024347305\n",
      "iteration 658, dc_loss: 0.020108746364712715, tv_loss: 0.017095522955060005\n",
      "iteration 659, dc_loss: 0.02012680098414421, tv_loss: 0.017070867121219635\n",
      "iteration 660, dc_loss: 0.020106473937630653, tv_loss: 0.01708540879189968\n",
      "iteration 661, dc_loss: 0.02010306902229786, tv_loss: 0.017083995044231415\n",
      "iteration 662, dc_loss: 0.020110588520765305, tv_loss: 0.017072495073080063\n",
      "iteration 663, dc_loss: 0.020085163414478302, tv_loss: 0.017094336450099945\n",
      "iteration 664, dc_loss: 0.020109690725803375, tv_loss: 0.017066217958927155\n",
      "iteration 665, dc_loss: 0.020075563341379166, tv_loss: 0.01709665171802044\n",
      "iteration 666, dc_loss: 0.02010110579431057, tv_loss: 0.017067505046725273\n",
      "iteration 667, dc_loss: 0.02007122151553631, tv_loss: 0.01709429919719696\n",
      "iteration 668, dc_loss: 0.02008904330432415, tv_loss: 0.017074428498744965\n",
      "iteration 669, dc_loss: 0.020065588876605034, tv_loss: 0.017094330862164497\n",
      "iteration 670, dc_loss: 0.020077480003237724, tv_loss: 0.01707513816654682\n",
      "iteration 671, dc_loss: 0.020054558292031288, tv_loss: 0.01709059625864029\n",
      "iteration 672, dc_loss: 0.020065996795892715, tv_loss: 0.017072612419724464\n",
      "iteration 673, dc_loss: 0.020041612908244133, tv_loss: 0.017091283574700356\n",
      "iteration 674, dc_loss: 0.020055778324604034, tv_loss: 0.017072156071662903\n",
      "iteration 675, dc_loss: 0.020033739507198334, tv_loss: 0.017090054228901863\n",
      "iteration 676, dc_loss: 0.020039159804582596, tv_loss: 0.01708139106631279\n",
      "iteration 677, dc_loss: 0.020034559071063995, tv_loss: 0.0170818492770195\n",
      "iteration 678, dc_loss: 0.020019149407744408, tv_loss: 0.0170921478420496\n",
      "iteration 679, dc_loss: 0.020038120448589325, tv_loss: 0.017068611457943916\n",
      "iteration 680, dc_loss: 0.0200043935328722, tv_loss: 0.017098216339945793\n",
      "iteration 681, dc_loss: 0.020031999796628952, tv_loss: 0.017066607251763344\n",
      "iteration 682, dc_loss: 0.01999831385910511, tv_loss: 0.017097003757953644\n",
      "iteration 683, dc_loss: 0.020019296556711197, tv_loss: 0.017073314636945724\n",
      "iteration 684, dc_loss: 0.019995316863059998, tv_loss: 0.01709390990436077\n",
      "iteration 685, dc_loss: 0.020014718174934387, tv_loss: 0.01707090251147747\n",
      "iteration 686, dc_loss: 0.019985465332865715, tv_loss: 0.017097456380724907\n",
      "iteration 687, dc_loss: 0.02001979947090149, tv_loss: 0.01706116460263729\n",
      "iteration 688, dc_loss: 0.019971679896116257, tv_loss: 0.017107589170336723\n",
      "iteration 689, dc_loss: 0.020025109872221947, tv_loss: 0.0170518159866333\n",
      "iteration 690, dc_loss: 0.019961174577474594, tv_loss: 0.017111649736762047\n",
      "iteration 691, dc_loss: 0.02001085877418518, tv_loss: 0.01705600693821907\n",
      "iteration 692, dc_loss: 0.019954701885581017, tv_loss: 0.017104756087064743\n",
      "iteration 693, dc_loss: 0.019979190081357956, tv_loss: 0.017071006819605827\n",
      "iteration 694, dc_loss: 0.019954683259129524, tv_loss: 0.017087092623114586\n",
      "iteration 695, dc_loss: 0.01995256170630455, tv_loss: 0.017083916813135147\n",
      "iteration 696, dc_loss: 0.019960599020123482, tv_loss: 0.017073241993784904\n",
      "iteration 697, dc_loss: 0.019939027726650238, tv_loss: 0.01709355227649212\n",
      "iteration 698, dc_loss: 0.019963650032877922, tv_loss: 0.017067985609173775\n",
      "iteration 699, dc_loss: 0.019931044429540634, tv_loss: 0.017099320888519287\n",
      "iteration 700, dc_loss: 0.019957950338721275, tv_loss: 0.017069363966584206\n",
      "iteration 701, dc_loss: 0.01992332562804222, tv_loss: 0.01709689013659954\n",
      "iteration 702, dc_loss: 0.019940635189414024, tv_loss: 0.01707155816257\n",
      "iteration 703, dc_loss: 0.019916903227567673, tv_loss: 0.01708822324872017\n",
      "iteration 704, dc_loss: 0.01992121897637844, tv_loss: 0.017078379169106483\n",
      "iteration 705, dc_loss: 0.019915444776415825, tv_loss: 0.01708020083606243\n",
      "iteration 706, dc_loss: 0.01990492083132267, tv_loss: 0.017088251188397408\n",
      "iteration 707, dc_loss: 0.01991977170109749, tv_loss: 0.017072118818759918\n",
      "iteration 708, dc_loss: 0.01988961175084114, tv_loss: 0.017100542783737183\n",
      "iteration 709, dc_loss: 0.01992354728281498, tv_loss: 0.017062721773982048\n",
      "iteration 710, dc_loss: 0.019878318533301353, tv_loss: 0.017103424295783043\n",
      "iteration 711, dc_loss: 0.01991521567106247, tv_loss: 0.017061704769730568\n",
      "iteration 712, dc_loss: 0.019875014200806618, tv_loss: 0.017096973955631256\n",
      "iteration 713, dc_loss: 0.019896483048796654, tv_loss: 0.017070917412638664\n",
      "iteration 714, dc_loss: 0.019874615594744682, tv_loss: 0.01708894968032837\n",
      "iteration 715, dc_loss: 0.019879724830389023, tv_loss: 0.017079981043934822\n",
      "iteration 716, dc_loss: 0.019868910312652588, tv_loss: 0.017085863277316093\n",
      "iteration 717, dc_loss: 0.01987103559076786, tv_loss: 0.017078757286071777\n",
      "iteration 718, dc_loss: 0.019860148429870605, tv_loss: 0.01708557829260826\n",
      "iteration 719, dc_loss: 0.019864648580551147, tv_loss: 0.01707776077091694\n",
      "iteration 720, dc_loss: 0.01985562965273857, tv_loss: 0.01708405092358589\n",
      "iteration 721, dc_loss: 0.019854197278618813, tv_loss: 0.01708335243165493\n",
      "iteration 722, dc_loss: 0.019854597747325897, tv_loss: 0.017080623656511307\n",
      "iteration 723, dc_loss: 0.019841806963086128, tv_loss: 0.017089588567614555\n",
      "iteration 724, dc_loss: 0.01985720545053482, tv_loss: 0.017070718109607697\n",
      "iteration 725, dc_loss: 0.01982886716723442, tv_loss: 0.017096512019634247\n",
      "iteration 726, dc_loss: 0.019861597567796707, tv_loss: 0.017062019556760788\n",
      "iteration 727, dc_loss: 0.019819188863039017, tv_loss: 0.017102878540754318\n",
      "iteration 728, dc_loss: 0.01986105926334858, tv_loss: 0.01705932430922985\n",
      "iteration 729, dc_loss: 0.01981450244784355, tv_loss: 0.017103593796491623\n",
      "iteration 730, dc_loss: 0.01984948106110096, tv_loss: 0.017064668238162994\n",
      "iteration 731, dc_loss: 0.019810661673545837, tv_loss: 0.01709682121872902\n",
      "iteration 732, dc_loss: 0.019828539341688156, tv_loss: 0.017071345821022987\n",
      "iteration 733, dc_loss: 0.019805509597063065, tv_loss: 0.01708720065653324\n",
      "iteration 734, dc_loss: 0.019808636978268623, tv_loss: 0.017078479751944542\n",
      "iteration 735, dc_loss: 0.019803330302238464, tv_loss: 0.017080096527934074\n",
      "iteration 736, dc_loss: 0.019794441759586334, tv_loss: 0.017086954787373543\n",
      "iteration 737, dc_loss: 0.01980767212808132, tv_loss: 0.01707245036959648\n",
      "iteration 738, dc_loss: 0.019779589027166367, tv_loss: 0.017098834738135338\n",
      "iteration 739, dc_loss: 0.019815288484096527, tv_loss: 0.017060641199350357\n",
      "iteration 740, dc_loss: 0.019767533987760544, tv_loss: 0.01710541732609272\n",
      "iteration 741, dc_loss: 0.019812900573015213, tv_loss: 0.01705642230808735\n",
      "iteration 742, dc_loss: 0.019763117656111717, tv_loss: 0.01710176095366478\n",
      "iteration 743, dc_loss: 0.01979491300880909, tv_loss: 0.017065417021512985\n",
      "iteration 744, dc_loss: 0.019763849675655365, tv_loss: 0.01709221675992012\n",
      "iteration 745, dc_loss: 0.019774815067648888, tv_loss: 0.017076747491955757\n",
      "iteration 746, dc_loss: 0.019762050360441208, tv_loss: 0.017084509134292603\n",
      "iteration 747, dc_loss: 0.01976277492940426, tv_loss: 0.017079094424843788\n",
      "iteration 748, dc_loss: 0.019756397232413292, tv_loss: 0.017081746831536293\n",
      "iteration 749, dc_loss: 0.01975587196648121, tv_loss: 0.017079411074519157\n",
      "iteration 750, dc_loss: 0.01975230686366558, tv_loss: 0.017080701887607574\n",
      "iteration 751, dc_loss: 0.01974833570420742, tv_loss: 0.01708299107849598\n",
      "iteration 752, dc_loss: 0.019751595333218575, tv_loss: 0.017078589648008347\n",
      "iteration 753, dc_loss: 0.01973838172852993, tv_loss: 0.01708942838013172\n",
      "iteration 754, dc_loss: 0.0197538323700428, tv_loss: 0.017070649191737175\n",
      "iteration 755, dc_loss: 0.019726764410734177, tv_loss: 0.017094686627388\n",
      "iteration 756, dc_loss: 0.019757525995373726, tv_loss: 0.017061471939086914\n",
      "iteration 757, dc_loss: 0.019715774804353714, tv_loss: 0.017100801691412926\n",
      "iteration 758, dc_loss: 0.019756486639380455, tv_loss: 0.01705733872950077\n",
      "iteration 759, dc_loss: 0.019708963111042976, tv_loss: 0.01710144802927971\n",
      "iteration 760, dc_loss: 0.019744474440813065, tv_loss: 0.01706225797533989\n",
      "iteration 761, dc_loss: 0.019707240164279938, tv_loss: 0.01709548383951187\n",
      "iteration 762, dc_loss: 0.019724158570170403, tv_loss: 0.01707356795668602\n",
      "iteration 763, dc_loss: 0.019708670675754547, tv_loss: 0.01708330027759075\n",
      "iteration 764, dc_loss: 0.019704287871718407, tv_loss: 0.017082741484045982\n",
      "iteration 765, dc_loss: 0.019708750769495964, tv_loss: 0.01707441918551922\n",
      "iteration 766, dc_loss: 0.019691795110702515, tv_loss: 0.01708865538239479\n",
      "iteration 767, dc_loss: 0.01970764808356762, tv_loss: 0.01707102730870247\n",
      "iteration 768, dc_loss: 0.019683143123984337, tv_loss: 0.017093906179070473\n",
      "iteration 769, dc_loss: 0.01970835216343403, tv_loss: 0.017066264525055885\n",
      "iteration 770, dc_loss: 0.01967480033636093, tv_loss: 0.017097502946853638\n",
      "iteration 771, dc_loss: 0.019709868356585503, tv_loss: 0.01706037111580372\n",
      "iteration 772, dc_loss: 0.01966710016131401, tv_loss: 0.01710118167102337\n",
      "iteration 773, dc_loss: 0.019708340987563133, tv_loss: 0.017057882621884346\n",
      "iteration 774, dc_loss: 0.019662393257021904, tv_loss: 0.0171014703810215\n",
      "iteration 775, dc_loss: 0.019700029864907265, tv_loss: 0.01706094853579998\n",
      "iteration 776, dc_loss: 0.019657809287309647, tv_loss: 0.017098866403102875\n",
      "iteration 777, dc_loss: 0.01968594640493393, tv_loss: 0.017065132036805153\n",
      "iteration 778, dc_loss: 0.01965387724339962, tv_loss: 0.017091553658246994\n",
      "iteration 779, dc_loss: 0.019670192152261734, tv_loss: 0.017070207744836807\n",
      "iteration 780, dc_loss: 0.01965189166367054, tv_loss: 0.01708436757326126\n",
      "iteration 781, dc_loss: 0.019655995070934296, tv_loss: 0.017077049240469933\n",
      "iteration 782, dc_loss: 0.01965205743908882, tv_loss: 0.017078816890716553\n",
      "iteration 783, dc_loss: 0.01964595541357994, tv_loss: 0.017083758488297462\n",
      "iteration 784, dc_loss: 0.0196525901556015, tv_loss: 0.01707620359957218\n",
      "iteration 785, dc_loss: 0.019639139994978905, tv_loss: 0.01708739809691906\n",
      "iteration 786, dc_loss: 0.01965394988656044, tv_loss: 0.017070552334189415\n",
      "iteration 787, dc_loss: 0.01963283307850361, tv_loss: 0.01709025911986828\n",
      "iteration 788, dc_loss: 0.019657790660858154, tv_loss: 0.017064346000552177\n",
      "iteration 789, dc_loss: 0.01962287351489067, tv_loss: 0.017098339274525642\n",
      "iteration 790, dc_loss: 0.019661732017993927, tv_loss: 0.017058314755558968\n",
      "iteration 791, dc_loss: 0.019613107666373253, tv_loss: 0.017103884369134903\n",
      "iteration 792, dc_loss: 0.019654883071780205, tv_loss: 0.017056943848729134\n",
      "iteration 793, dc_loss: 0.01960766315460205, tv_loss: 0.01709781400859356\n",
      "iteration 794, dc_loss: 0.019634909927845, tv_loss: 0.017064303159713745\n",
      "iteration 795, dc_loss: 0.019607793539762497, tv_loss: 0.01708592101931572\n",
      "iteration 796, dc_loss: 0.019612422212958336, tv_loss: 0.01707722805440426\n",
      "iteration 797, dc_loss: 0.019613036885857582, tv_loss: 0.01707409881055355\n",
      "iteration 798, dc_loss: 0.01959611289203167, tv_loss: 0.017090218141674995\n",
      "iteration 799, dc_loss: 0.019620219245553017, tv_loss: 0.017065560445189476\n",
      "iteration 800, dc_loss: 0.01958508975803852, tv_loss: 0.017098601907491684\n",
      "iteration 801, dc_loss: 0.019623354077339172, tv_loss: 0.017057813704013824\n",
      "iteration 802, dc_loss: 0.01958427205681801, tv_loss: 0.017090506851673126\n",
      "iteration 803, dc_loss: 0.01958797127008438, tv_loss: 0.01708226092159748\n",
      "iteration 804, dc_loss: 0.01961057260632515, tv_loss: 0.017061350867152214\n",
      "iteration 805, dc_loss: 0.019577518105506897, tv_loss: 0.017091713845729828\n",
      "iteration 806, dc_loss: 0.019586671143770218, tv_loss: 0.017076948657631874\n",
      "iteration 807, dc_loss: 0.019597018137574196, tv_loss: 0.01706555485725403\n",
      "iteration 808, dc_loss: 0.01957184448838234, tv_loss: 0.01709011010825634\n",
      "iteration 809, dc_loss: 0.019583988934755325, tv_loss: 0.017074106261134148\n",
      "iteration 810, dc_loss: 0.01958453841507435, tv_loss: 0.017070574685931206\n",
      "iteration 811, dc_loss: 0.01956756040453911, tv_loss: 0.017086317762732506\n",
      "iteration 812, dc_loss: 0.019579317420721054, tv_loss: 0.01707216165959835\n",
      "iteration 813, dc_loss: 0.01957336813211441, tv_loss: 0.017075248062610626\n",
      "iteration 814, dc_loss: 0.019563905894756317, tv_loss: 0.017082950100302696\n",
      "iteration 815, dc_loss: 0.019572637975215912, tv_loss: 0.017072193324565887\n",
      "iteration 816, dc_loss: 0.019564537331461906, tv_loss: 0.017077304422855377\n",
      "iteration 817, dc_loss: 0.01956026814877987, tv_loss: 0.01707923226058483\n",
      "iteration 818, dc_loss: 0.019565224647521973, tv_loss: 0.01707291044294834\n",
      "iteration 819, dc_loss: 0.019556885585188866, tv_loss: 0.01707920804619789\n",
      "iteration 820, dc_loss: 0.019556604325771332, tv_loss: 0.017076632007956505\n",
      "iteration 821, dc_loss: 0.019556814804673195, tv_loss: 0.017074184492230415\n",
      "iteration 822, dc_loss: 0.01954931952059269, tv_loss: 0.017079927027225494\n",
      "iteration 823, dc_loss: 0.01955435238778591, tv_loss: 0.017072588205337524\n",
      "iteration 824, dc_loss: 0.019546492025256157, tv_loss: 0.0170779787003994\n",
      "iteration 825, dc_loss: 0.019545437768101692, tv_loss: 0.017077095806598663\n",
      "iteration 826, dc_loss: 0.019547879695892334, tv_loss: 0.017072970047593117\n",
      "iteration 827, dc_loss: 0.01953791454434395, tv_loss: 0.017080945894122124\n",
      "iteration 828, dc_loss: 0.019544048234820366, tv_loss: 0.017072681337594986\n",
      "iteration 829, dc_loss: 0.019536664709448814, tv_loss: 0.017078125849366188\n",
      "iteration 830, dc_loss: 0.019535431638360023, tv_loss: 0.017077408730983734\n",
      "iteration 831, dc_loss: 0.019536349922418594, tv_loss: 0.017074260860681534\n",
      "iteration 832, dc_loss: 0.019529929384589195, tv_loss: 0.017078323289752007\n",
      "iteration 833, dc_loss: 0.019530627876520157, tv_loss: 0.017075341194868088\n",
      "iteration 834, dc_loss: 0.019529398530721664, tv_loss: 0.017074426636099815\n",
      "iteration 835, dc_loss: 0.01952330581843853, tv_loss: 0.017078453674912453\n",
      "iteration 836, dc_loss: 0.019527679309248924, tv_loss: 0.017071979120373726\n",
      "iteration 837, dc_loss: 0.019519006833434105, tv_loss: 0.017078528180718422\n",
      "iteration 838, dc_loss: 0.019520824775099754, tv_loss: 0.01707461290061474\n",
      "iteration 839, dc_loss: 0.019519444555044174, tv_loss: 0.017073992639780045\n",
      "iteration 840, dc_loss: 0.01951335184276104, tv_loss: 0.017078187316656113\n",
      "iteration 841, dc_loss: 0.01951787807047367, tv_loss: 0.01707185059785843\n",
      "iteration 842, dc_loss: 0.01950896717607975, tv_loss: 0.017079157754778862\n",
      "iteration 843, dc_loss: 0.019511744379997253, tv_loss: 0.017075149342417717\n",
      "iteration 844, dc_loss: 0.01950772851705551, tv_loss: 0.017077475786209106\n",
      "iteration 845, dc_loss: 0.01950675994157791, tv_loss: 0.017075838521122932\n",
      "iteration 846, dc_loss: 0.019504157826304436, tv_loss: 0.01707584410905838\n",
      "iteration 847, dc_loss: 0.019502805545926094, tv_loss: 0.01707489974796772\n",
      "iteration 848, dc_loss: 0.019499843940138817, tv_loss: 0.017075711861252785\n",
      "iteration 849, dc_loss: 0.019499365240335464, tv_loss: 0.017074206843972206\n",
      "iteration 850, dc_loss: 0.019495181739330292, tv_loss: 0.017076576128602028\n",
      "iteration 851, dc_loss: 0.019497254863381386, tv_loss: 0.017072977498173714\n",
      "iteration 852, dc_loss: 0.019489798694849014, tv_loss: 0.017079319804906845\n",
      "iteration 853, dc_loss: 0.019494248554110527, tv_loss: 0.017073344439268112\n",
      "iteration 854, dc_loss: 0.019485285505652428, tv_loss: 0.01707988604903221\n",
      "iteration 855, dc_loss: 0.019490541890263557, tv_loss: 0.017071986570954323\n",
      "iteration 856, dc_loss: 0.019483312964439392, tv_loss: 0.017076928168535233\n",
      "iteration 857, dc_loss: 0.019483394920825958, tv_loss: 0.017074821516871452\n",
      "iteration 858, dc_loss: 0.019482726231217384, tv_loss: 0.01707368902862072\n",
      "iteration 859, dc_loss: 0.01947753317654133, tv_loss: 0.017077283933758736\n",
      "iteration 860, dc_loss: 0.01947891153395176, tv_loss: 0.01707456260919571\n",
      "iteration 861, dc_loss: 0.019476016983389854, tv_loss: 0.017075836658477783\n",
      "iteration 862, dc_loss: 0.019471870735287666, tv_loss: 0.017077775672078133\n",
      "iteration 863, dc_loss: 0.019476352259516716, tv_loss: 0.01707097329199314\n",
      "iteration 864, dc_loss: 0.019465303048491478, tv_loss: 0.017079897224903107\n",
      "iteration 865, dc_loss: 0.019472111016511917, tv_loss: 0.017071062698960304\n",
      "iteration 866, dc_loss: 0.019464528188109398, tv_loss: 0.017076823860406876\n",
      "iteration 867, dc_loss: 0.01946444623172283, tv_loss: 0.017075354233384132\n",
      "iteration 868, dc_loss: 0.019463637843728065, tv_loss: 0.01707465574145317\n",
      "iteration 869, dc_loss: 0.019460083916783333, tv_loss: 0.01707625202834606\n",
      "iteration 870, dc_loss: 0.01945854164659977, tv_loss: 0.017075641080737114\n",
      "iteration 871, dc_loss: 0.019459227100014687, tv_loss: 0.01707291789352894\n",
      "iteration 872, dc_loss: 0.019452553242444992, tv_loss: 0.017077786847949028\n",
      "iteration 873, dc_loss: 0.01945745013654232, tv_loss: 0.017071155831217766\n",
      "iteration 874, dc_loss: 0.019449494779109955, tv_loss: 0.017077436670660973\n",
      "iteration 875, dc_loss: 0.019450955092906952, tv_loss: 0.01707438938319683\n",
      "iteration 876, dc_loss: 0.019447840750217438, tv_loss: 0.01707567647099495\n",
      "iteration 877, dc_loss: 0.019446445629000664, tv_loss: 0.01707477681338787\n",
      "iteration 878, dc_loss: 0.019443903118371964, tv_loss: 0.017075031995773315\n",
      "iteration 879, dc_loss: 0.019443608820438385, tv_loss: 0.01707320474088192\n",
      "iteration 880, dc_loss: 0.01943858154118061, tv_loss: 0.017076311632990837\n",
      "iteration 881, dc_loss: 0.0194424856454134, tv_loss: 0.017070671543478966\n",
      "iteration 882, dc_loss: 0.019433757290244102, tv_loss: 0.017077786847949028\n",
      "iteration 883, dc_loss: 0.019439000636339188, tv_loss: 0.01707122102379799\n",
      "iteration 884, dc_loss: 0.019431324675679207, tv_loss: 0.0170778576284647\n",
      "iteration 885, dc_loss: 0.019433777779340744, tv_loss: 0.01707366481423378\n",
      "iteration 886, dc_loss: 0.019429607316851616, tv_loss: 0.01707545667886734\n",
      "iteration 887, dc_loss: 0.019428933039307594, tv_loss: 0.017073867842555046\n",
      "iteration 888, dc_loss: 0.0194269847124815, tv_loss: 0.01707381382584572\n",
      "iteration 889, dc_loss: 0.019424675032496452, tv_loss: 0.01707427203655243\n",
      "iteration 890, dc_loss: 0.019423995167016983, tv_loss: 0.01707322522997856\n",
      "iteration 891, dc_loss: 0.019421597942709923, tv_loss: 0.017074240371584892\n",
      "iteration 892, dc_loss: 0.019418448209762573, tv_loss: 0.01707630418241024\n",
      "iteration 893, dc_loss: 0.019421126693487167, tv_loss: 0.017071885988116264\n",
      "iteration 894, dc_loss: 0.019411377608776093, tv_loss: 0.017079420387744904\n",
      "iteration 895, dc_loss: 0.019421005621552467, tv_loss: 0.017067568376660347\n",
      "iteration 896, dc_loss: 0.019407493993639946, tv_loss: 0.017079057171940804\n",
      "iteration 897, dc_loss: 0.019414465874433517, tv_loss: 0.017070241272449493\n",
      "iteration 898, dc_loss: 0.019408419728279114, tv_loss: 0.017074791714549065\n",
      "iteration 899, dc_loss: 0.019406737759709358, tv_loss: 0.017075205221772194\n",
      "iteration 900, dc_loss: 0.019406838342547417, tv_loss: 0.017073649913072586\n",
      "iteration 901, dc_loss: 0.01940380595624447, tv_loss: 0.01707451045513153\n",
      "iteration 902, dc_loss: 0.01940142549574375, tv_loss: 0.01707480661571026\n",
      "iteration 903, dc_loss: 0.019403861835598946, tv_loss: 0.0170705858618021\n",
      "iteration 904, dc_loss: 0.019395694136619568, tv_loss: 0.017077377066016197\n",
      "iteration 905, dc_loss: 0.01940176822245121, tv_loss: 0.017070123925805092\n",
      "iteration 906, dc_loss: 0.01939404010772705, tv_loss: 0.017076779156923294\n",
      "iteration 907, dc_loss: 0.019395411014556885, tv_loss: 0.017073802649974823\n",
      "iteration 908, dc_loss: 0.019393928349018097, tv_loss: 0.017072945833206177\n",
      "iteration 909, dc_loss: 0.01938878931105137, tv_loss: 0.017075663432478905\n",
      "iteration 910, dc_loss: 0.01939159817993641, tv_loss: 0.01707070879638195\n",
      "iteration 911, dc_loss: 0.019385524094104767, tv_loss: 0.017074864357709885\n",
      "iteration 912, dc_loss: 0.019386475905776024, tv_loss: 0.017072247341275215\n",
      "iteration 913, dc_loss: 0.019384000450372696, tv_loss: 0.017073288559913635\n",
      "iteration 914, dc_loss: 0.01938052475452423, tv_loss: 0.017075704410672188\n",
      "iteration 915, dc_loss: 0.01938522420823574, tv_loss: 0.017069781199097633\n",
      "iteration 916, dc_loss: 0.019372889772057533, tv_loss: 0.01708040200173855\n",
      "iteration 917, dc_loss: 0.019385425373911858, tv_loss: 0.017065806314349174\n",
      "iteration 918, dc_loss: 0.01936984620988369, tv_loss: 0.017079351469874382\n",
      "iteration 919, dc_loss: 0.019377922639250755, tv_loss: 0.01706937700510025\n",
      "iteration 920, dc_loss: 0.01937173306941986, tv_loss: 0.01707405038177967\n",
      "iteration 921, dc_loss: 0.019369136542081833, tv_loss: 0.01707535982131958\n",
      "iteration 922, dc_loss: 0.01937248557806015, tv_loss: 0.01707075722515583\n",
      "iteration 923, dc_loss: 0.01936516910791397, tv_loss: 0.01707662269473076\n",
      "iteration 924, dc_loss: 0.019370168447494507, tv_loss: 0.017070267349481583\n",
      "iteration 925, dc_loss: 0.019364509731531143, tv_loss: 0.017074596136808395\n",
      "iteration 926, dc_loss: 0.019365299493074417, tv_loss: 0.0170725267380476\n",
      "iteration 927, dc_loss: 0.019363930448889732, tv_loss: 0.017072416841983795\n",
      "iteration 928, dc_loss: 0.019360054284334183, tv_loss: 0.017074519768357277\n",
      "iteration 929, dc_loss: 0.01936071366071701, tv_loss: 0.0170716755092144\n",
      "iteration 930, dc_loss: 0.019355108961462975, tv_loss: 0.017074957489967346\n",
      "iteration 931, dc_loss: 0.01935759373009205, tv_loss: 0.01707022450864315\n",
      "iteration 932, dc_loss: 0.0193499643355608, tv_loss: 0.017075952142477036\n",
      "iteration 933, dc_loss: 0.019355880096554756, tv_loss: 0.017068354412913322\n",
      "iteration 934, dc_loss: 0.019345808774232864, tv_loss: 0.017077013850212097\n",
      "iteration 935, dc_loss: 0.019354380667209625, tv_loss: 0.01706714555621147\n",
      "iteration 936, dc_loss: 0.01934182271361351, tv_loss: 0.01707853004336357\n",
      "iteration 937, dc_loss: 0.01935310661792755, tv_loss: 0.017066074535250664\n",
      "iteration 938, dc_loss: 0.019338350743055344, tv_loss: 0.01707945019006729\n",
      "iteration 939, dc_loss: 0.019348645582795143, tv_loss: 0.017067288979887962\n",
      "iteration 940, dc_loss: 0.019336696714162827, tv_loss: 0.01707712560892105\n",
      "iteration 941, dc_loss: 0.019343212246894836, tv_loss: 0.017068471759557724\n",
      "iteration 942, dc_loss: 0.0193348228931427, tv_loss: 0.017074910923838615\n",
      "iteration 943, dc_loss: 0.0193374864757061, tv_loss: 0.01707047037780285\n",
      "iteration 944, dc_loss: 0.019333934411406517, tv_loss: 0.01707252860069275\n",
      "iteration 945, dc_loss: 0.019333520904183388, tv_loss: 0.017071740701794624\n",
      "iteration 946, dc_loss: 0.019330350682139397, tv_loss: 0.01707414537668228\n",
      "iteration 947, dc_loss: 0.019333424046635628, tv_loss: 0.017070313915610313\n",
      "iteration 948, dc_loss: 0.01932673715054989, tv_loss: 0.017075907438993454\n",
      "iteration 949, dc_loss: 0.01933298632502556, tv_loss: 0.017068635672330856\n",
      "iteration 950, dc_loss: 0.019326139241456985, tv_loss: 0.017074551433324814\n",
      "iteration 951, dc_loss: 0.019327722489833832, tv_loss: 0.017071975395083427\n",
      "iteration 952, dc_loss: 0.019329551607370377, tv_loss: 0.01706896908581257\n",
      "iteration 953, dc_loss: 0.0193181075155735, tv_loss: 0.017078904435038567\n",
      "iteration 954, dc_loss: 0.019331011921167374, tv_loss: 0.017063798382878304\n",
      "iteration 955, dc_loss: 0.019311754032969475, tv_loss: 0.017080368474125862\n",
      "iteration 956, dc_loss: 0.01932508312165737, tv_loss: 0.017064256593585014\n",
      "iteration 957, dc_loss: 0.019311072304844856, tv_loss: 0.01707567647099495\n",
      "iteration 958, dc_loss: 0.019314317032694817, tv_loss: 0.017070382833480835\n",
      "iteration 959, dc_loss: 0.019315233454108238, tv_loss: 0.01706799305975437\n",
      "iteration 960, dc_loss: 0.019304106011986732, tv_loss: 0.01707829162478447\n",
      "iteration 961, dc_loss: 0.019318846985697746, tv_loss: 0.01706315390765667\n",
      "iteration 962, dc_loss: 0.019298642873764038, tv_loss: 0.017082659527659416\n",
      "iteration 963, dc_loss: 0.019317202270030975, tv_loss: 0.01706216111779213\n",
      "iteration 964, dc_loss: 0.019297650083899498, tv_loss: 0.017079569399356842\n",
      "iteration 965, dc_loss: 0.019310133531689644, tv_loss: 0.01706513576209545\n",
      "iteration 966, dc_loss: 0.019299400970339775, tv_loss: 0.017074231058359146\n",
      "iteration 967, dc_loss: 0.019302919507026672, tv_loss: 0.017069203779101372\n",
      "iteration 968, dc_loss: 0.019298691302537918, tv_loss: 0.017072046175599098\n",
      "iteration 969, dc_loss: 0.01930011250078678, tv_loss: 0.017069397494196892\n",
      "iteration 970, dc_loss: 0.01929299905896187, tv_loss: 0.017075499519705772\n",
      "iteration 971, dc_loss: 0.019301321357488632, tv_loss: 0.017065703868865967\n",
      "iteration 972, dc_loss: 0.01928512193262577, tv_loss: 0.017080115154385567\n",
      "iteration 973, dc_loss: 0.019302748143672943, tv_loss: 0.01706075295805931\n",
      "iteration 974, dc_loss: 0.019280869513750076, tv_loss: 0.01708102971315384\n",
      "iteration 975, dc_loss: 0.019297100603580475, tv_loss: 0.017063286155462265\n",
      "iteration 976, dc_loss: 0.019283229485154152, tv_loss: 0.017075903713703156\n",
      "iteration 977, dc_loss: 0.01928691752254963, tv_loss: 0.017070751637220383\n",
      "iteration 978, dc_loss: 0.01928682066500187, tv_loss: 0.01706928387284279\n",
      "iteration 979, dc_loss: 0.019280146807432175, tv_loss: 0.01707465760409832\n",
      "iteration 980, dc_loss: 0.0192888043820858, tv_loss: 0.017065413296222687\n",
      "iteration 981, dc_loss: 0.019277799874544144, tv_loss: 0.01707661896944046\n",
      "iteration 982, dc_loss: 0.019291169941425323, tv_loss: 0.017064251005649567\n",
      "iteration 983, dc_loss: 0.01927805505692959, tv_loss: 0.01707870326936245\n",
      "iteration 984, dc_loss: 0.019293751567602158, tv_loss: 0.017063353210687637\n",
      "iteration 985, dc_loss: 0.019275594502687454, tv_loss: 0.01707984320819378\n",
      "iteration 986, dc_loss: 0.019291672855615616, tv_loss: 0.017060384154319763\n",
      "iteration 987, dc_loss: 0.019266799092292786, tv_loss: 0.017080826684832573\n",
      "iteration 988, dc_loss: 0.01928297057747841, tv_loss: 0.017060449346899986\n",
      "iteration 989, dc_loss: 0.019262397661805153, tv_loss: 0.017077913507819176\n",
      "iteration 990, dc_loss: 0.01927206479012966, tv_loss: 0.01706654205918312\n",
      "iteration 991, dc_loss: 0.019269006326794624, tv_loss: 0.017068829387426376\n",
      "iteration 992, dc_loss: 0.019259510561823845, tv_loss: 0.017078204080462456\n",
      "iteration 993, dc_loss: 0.019280413165688515, tv_loss: 0.017057280987501144\n",
      "iteration 994, dc_loss: 0.01924866996705532, tv_loss: 0.01708858646452427\n",
      "iteration 995, dc_loss: 0.01928357221186161, tv_loss: 0.017052095383405685\n",
      "iteration 996, dc_loss: 0.01924705132842064, tv_loss: 0.017085930332541466\n",
      "iteration 997, dc_loss: 0.019269637763500214, tv_loss: 0.01706027425825596\n",
      "iteration 998, dc_loss: 0.019253812730312347, tv_loss: 0.01707356423139572\n",
      "iteration 999, dc_loss: 0.019253185018897057, tv_loss: 0.01707245595753193\n",
      "iteration 1000, dc_loss: 0.01926068216562271, tv_loss: 0.01706385426223278\n",
      "iteration 1001, dc_loss: 0.019246099516749382, tv_loss: 0.017077568918466568\n",
      "iteration 1002, dc_loss: 0.019260922446846962, tv_loss: 0.01706211268901825\n",
      "iteration 1003, dc_loss: 0.019247299060225487, tv_loss: 0.017075341194868088\n",
      "iteration 1004, dc_loss: 0.019255127757787704, tv_loss: 0.017067423090338707\n",
      "iteration 1005, dc_loss: 0.01925084739923477, tv_loss: 0.017071368172764778\n",
      "iteration 1006, dc_loss: 0.01924980990588665, tv_loss: 0.01707076095044613\n",
      "iteration 1007, dc_loss: 0.01924878917634487, tv_loss: 0.017069201916456223\n",
      "iteration 1008, dc_loss: 0.019244777038693428, tv_loss: 0.017070429399609566\n",
      "iteration 1009, dc_loss: 0.01924358867108822, tv_loss: 0.01706911437213421\n",
      "iteration 1010, dc_loss: 0.01924155466258526, tv_loss: 0.01706928014755249\n",
      "iteration 1011, dc_loss: 0.01923944614827633, tv_loss: 0.017070066183805466\n",
      "iteration 1012, dc_loss: 0.019237851724028587, tv_loss: 0.01707070879638195\n",
      "iteration 1013, dc_loss: 0.019240757450461388, tv_loss: 0.01706675812602043\n",
      "iteration 1014, dc_loss: 0.019229546189308167, tv_loss: 0.01707702875137329\n",
      "iteration 1015, dc_loss: 0.019248072057962418, tv_loss: 0.01705775409936905\n",
      "iteration 1016, dc_loss: 0.019220862537622452, tv_loss: 0.017084361985325813\n",
      "iteration 1017, dc_loss: 0.01925153099000454, tv_loss: 0.017052805051207542\n",
      "iteration 1018, dc_loss: 0.01921834424138069, tv_loss: 0.017084743827581406\n",
      "iteration 1019, dc_loss: 0.019244583323597908, tv_loss: 0.017056996002793312\n",
      "iteration 1020, dc_loss: 0.019221657887101173, tv_loss: 0.017078323289752007\n",
      "iteration 1021, dc_loss: 0.019233696162700653, tv_loss: 0.017064422369003296\n",
      "iteration 1022, dc_loss: 0.01922447979450226, tv_loss: 0.017071694135665894\n",
      "iteration 1023, dc_loss: 0.019226986914873123, tv_loss: 0.01706714555621147\n",
      "iteration 1024, dc_loss: 0.01922191120684147, tv_loss: 0.01707027107477188\n",
      "iteration 1025, dc_loss: 0.019224723801016808, tv_loss: 0.0170657467097044\n",
      "iteration 1026, dc_loss: 0.019216682761907578, tv_loss: 0.017072532325983047\n",
      "iteration 1027, dc_loss: 0.019225796684622765, tv_loss: 0.01706279069185257\n",
      "iteration 1028, dc_loss: 0.01921066828072071, tv_loss: 0.017077837139368057\n",
      "iteration 1029, dc_loss: 0.01922645978629589, tv_loss: 0.017060991376638412\n",
      "iteration 1030, dc_loss: 0.019208457320928574, tv_loss: 0.01707727275788784\n",
      "iteration 1031, dc_loss: 0.01922258548438549, tv_loss: 0.01706160604953766\n",
      "iteration 1032, dc_loss: 0.019210662692785263, tv_loss: 0.017072351649403572\n",
      "iteration 1033, dc_loss: 0.019213322550058365, tv_loss: 0.017069082707166672\n",
      "iteration 1034, dc_loss: 0.01921737752854824, tv_loss: 0.017064956948161125\n",
      "iteration 1035, dc_loss: 0.019204312935471535, tv_loss: 0.017077717930078506\n",
      "iteration 1036, dc_loss: 0.01922459527850151, tv_loss: 0.01705722138285637\n",
      "iteration 1037, dc_loss: 0.019200045615434647, tv_loss: 0.017081912606954575\n",
      "iteration 1038, dc_loss: 0.019229266792535782, tv_loss: 0.017052896320819855\n",
      "iteration 1039, dc_loss: 0.019197722896933556, tv_loss: 0.017084207385778427\n",
      "iteration 1040, dc_loss: 0.019227107986807823, tv_loss: 0.01705370470881462\n",
      "iteration 1041, dc_loss: 0.019196897745132446, tv_loss: 0.017081452533602715\n",
      "iteration 1042, dc_loss: 0.01921546645462513, tv_loss: 0.01705908216536045\n",
      "iteration 1043, dc_loss: 0.019196471199393272, tv_loss: 0.017074059695005417\n",
      "iteration 1044, dc_loss: 0.019201066344976425, tv_loss: 0.017066286876797676\n",
      "iteration 1045, dc_loss: 0.019200017675757408, tv_loss: 0.01706552878022194\n",
      "iteration 1046, dc_loss: 0.01919001154601574, tv_loss: 0.017075037583708763\n",
      "iteration 1047, dc_loss: 0.019207008183002472, tv_loss: 0.017058242112398148\n",
      "iteration 1048, dc_loss: 0.019182631745934486, tv_loss: 0.01708292029798031\n",
      "iteration 1049, dc_loss: 0.01921430043876171, tv_loss: 0.017051134258508682\n",
      "iteration 1050, dc_loss: 0.019178200513124466, tv_loss: 0.01708637736737728\n",
      "iteration 1051, dc_loss: 0.019212566316127777, tv_loss: 0.017050329595804214\n",
      "iteration 1052, dc_loss: 0.019177526235580444, tv_loss: 0.01708296127617359\n",
      "iteration 1053, dc_loss: 0.019201116636395454, tv_loss: 0.01705668680369854\n",
      "iteration 1054, dc_loss: 0.019180942326784134, tv_loss: 0.017074422910809517\n",
      "iteration 1055, dc_loss: 0.019187549129128456, tv_loss: 0.01706594228744507\n",
      "iteration 1056, dc_loss: 0.019185619428753853, tv_loss: 0.017066575586795807\n",
      "iteration 1057, dc_loss: 0.019180264323949814, tv_loss: 0.017071066424250603\n",
      "iteration 1058, dc_loss: 0.019186338409781456, tv_loss: 0.017064383253455162\n",
      "iteration 1059, dc_loss: 0.019178299233317375, tv_loss: 0.01707199029624462\n",
      "iteration 1060, dc_loss: 0.019186630845069885, tv_loss: 0.01706342212855816\n",
      "iteration 1061, dc_loss: 0.019177895039319992, tv_loss: 0.017071925103664398\n",
      "iteration 1062, dc_loss: 0.019186293706297874, tv_loss: 0.01706310734152794\n",
      "iteration 1063, dc_loss: 0.01917502097785473, tv_loss: 0.017073528841137886\n",
      "iteration 1064, dc_loss: 0.019186589866876602, tv_loss: 0.01706055738031864\n",
      "iteration 1065, dc_loss: 0.019168773666024208, tv_loss: 0.017076222226023674\n",
      "iteration 1066, dc_loss: 0.019184980541467667, tv_loss: 0.017057647928595543\n",
      "iteration 1067, dc_loss: 0.01916317455470562, tv_loss: 0.01707717776298523\n",
      "iteration 1068, dc_loss: 0.019181720912456512, tv_loss: 0.017056608572602272\n",
      "iteration 1069, dc_loss: 0.019161788746714592, tv_loss: 0.017074791714549065\n",
      "iteration 1070, dc_loss: 0.01917411759495735, tv_loss: 0.017061037942767143\n",
      "iteration 1071, dc_loss: 0.01916462741792202, tv_loss: 0.017069470137357712\n",
      "iteration 1072, dc_loss: 0.019164295867085457, tv_loss: 0.017069153487682343\n",
      "iteration 1073, dc_loss: 0.019171006977558136, tv_loss: 0.017061786726117134\n",
      "iteration 1074, dc_loss: 0.019153811037540436, tv_loss: 0.01707819290459156\n",
      "iteration 1075, dc_loss: 0.019179752096533775, tv_loss: 0.017051683738827705\n",
      "iteration 1076, dc_loss: 0.019146209582686424, tv_loss: 0.017084702849388123\n",
      "iteration 1077, dc_loss: 0.019181018695235252, tv_loss: 0.017049089074134827\n",
      "iteration 1078, dc_loss: 0.01914580725133419, tv_loss: 0.017083246260881424\n",
      "iteration 1079, dc_loss: 0.019172919914126396, tv_loss: 0.017054729163646698\n",
      "iteration 1080, dc_loss: 0.019150692969560623, tv_loss: 0.017075635492801666\n",
      "iteration 1081, dc_loss: 0.019164515659213066, tv_loss: 0.017060957849025726\n",
      "iteration 1082, dc_loss: 0.019154489040374756, tv_loss: 0.017070380970835686\n",
      "iteration 1083, dc_loss: 0.019162341952323914, tv_loss: 0.017061743885278702\n",
      "iteration 1084, dc_loss: 0.019150646403431892, tv_loss: 0.017072437331080437\n",
      "iteration 1085, dc_loss: 0.01916547864675522, tv_loss: 0.017056502401828766\n",
      "iteration 1086, dc_loss: 0.019140655174851418, tv_loss: 0.0170806385576725\n",
      "iteration 1087, dc_loss: 0.019171224907040596, tv_loss: 0.01704934984445572\n",
      "iteration 1088, dc_loss: 0.019132589921355247, tv_loss: 0.017086509615182877\n",
      "iteration 1089, dc_loss: 0.019170278683304787, tv_loss: 0.017046818509697914\n",
      "iteration 1090, dc_loss: 0.019131937995553017, tv_loss: 0.017082883045077324\n",
      "iteration 1091, dc_loss: 0.01915772259235382, tv_loss: 0.017054900527000427\n",
      "iteration 1092, dc_loss: 0.019139012321829796, tv_loss: 0.01707187294960022\n",
      "iteration 1093, dc_loss: 0.01914229616522789, tv_loss: 0.01706712506711483\n",
      "iteration 1094, dc_loss: 0.019146209582686424, tv_loss: 0.017061950638890266\n",
      "iteration 1095, dc_loss: 0.019134623929858208, tv_loss: 0.017072409391403198\n",
      "iteration 1096, dc_loss: 0.019146952778100967, tv_loss: 0.017059164121747017\n",
      "iteration 1097, dc_loss: 0.019133472815155983, tv_loss: 0.017072122544050217\n",
      "iteration 1098, dc_loss: 0.01914466731250286, tv_loss: 0.017061147838830948\n",
      "iteration 1099, dc_loss: 0.01913635991513729, tv_loss: 0.017070339992642403\n",
      "iteration 1100, dc_loss: 0.019143857061862946, tv_loss: 0.01706375740468502\n",
      "iteration 1101, dc_loss: 0.019139081239700317, tv_loss: 0.017069300636649132\n",
      "iteration 1102, dc_loss: 0.019147483631968498, tv_loss: 0.017061082646250725\n",
      "iteration 1103, dc_loss: 0.019133945927023888, tv_loss: 0.017073828727006912\n",
      "iteration 1104, dc_loss: 0.01915142685174942, tv_loss: 0.01705433987081051\n",
      "iteration 1105, dc_loss: 0.019121242687106133, tv_loss: 0.017081819474697113\n",
      "iteration 1106, dc_loss: 0.019154205918312073, tv_loss: 0.017045991495251656\n",
      "iteration 1107, dc_loss: 0.019112344831228256, tv_loss: 0.017085043713450432\n",
      "iteration 1108, dc_loss: 0.01914689503610134, tv_loss: 0.01704772189259529\n",
      "iteration 1109, dc_loss: 0.01911606825888157, tv_loss: 0.017075959593057632\n",
      "iteration 1110, dc_loss: 0.01912883296608925, tv_loss: 0.017061378806829453\n",
      "iteration 1111, dc_loss: 0.01912860944867134, tv_loss: 0.017061134800314903\n",
      "iteration 1112, dc_loss: 0.0191118735820055, tv_loss: 0.01707862876355648\n",
      "iteration 1113, dc_loss: 0.019143128767609596, tv_loss: 0.017047740519046783\n",
      "iteration 1114, dc_loss: 0.01910323277115822, tv_loss: 0.017087141051888466\n",
      "iteration 1115, dc_loss: 0.019145304337143898, tv_loss: 0.01704392209649086\n",
      "iteration 1116, dc_loss: 0.019105011597275734, tv_loss: 0.017082685604691505\n",
      "iteration 1117, dc_loss: 0.01913307048380375, tv_loss: 0.01705339550971985\n",
      "iteration 1118, dc_loss: 0.019113270565867424, tv_loss: 0.017072156071662903\n",
      "iteration 1119, dc_loss: 0.019121583551168442, tv_loss: 0.017062578350305557\n",
      "iteration 1120, dc_loss: 0.019116390496492386, tv_loss: 0.01706603169441223\n",
      "iteration 1121, dc_loss: 0.0191178098320961, tv_loss: 0.017062583938241005\n",
      "iteration 1122, dc_loss: 0.019109850749373436, tv_loss: 0.01706874929368496\n",
      "iteration 1123, dc_loss: 0.019120153039693832, tv_loss: 0.01705734059214592\n",
      "iteration 1124, dc_loss: 0.019101230427622795, tv_loss: 0.01707596145570278\n",
      "iteration 1125, dc_loss: 0.019123591482639313, tv_loss: 0.0170530267059803\n",
      "iteration 1126, dc_loss: 0.019097696989774704, tv_loss: 0.017077652737498283\n",
      "iteration 1127, dc_loss: 0.019120071083307266, tv_loss: 0.017053842544555664\n",
      "iteration 1128, dc_loss: 0.019100556150078773, tv_loss: 0.017072034999728203\n",
      "iteration 1129, dc_loss: 0.019109515473246574, tv_loss: 0.017062010243535042\n",
      "iteration 1130, dc_loss: 0.019107064232230186, tv_loss: 0.017063414677977562\n",
      "iteration 1131, dc_loss: 0.019098814576864243, tv_loss: 0.01707078516483307\n",
      "iteration 1132, dc_loss: 0.019114043563604355, tv_loss: 0.01705515757203102\n",
      "iteration 1133, dc_loss: 0.019093777984380722, tv_loss: 0.01707562804222107\n",
      "iteration 1134, dc_loss: 0.019118938595056534, tv_loss: 0.01705131307244301\n",
      "iteration 1135, dc_loss: 0.019092565402388573, tv_loss: 0.017079059034585953\n",
      "iteration 1136, dc_loss: 0.019123317673802376, tv_loss: 0.017049647867679596\n",
      "iteration 1137, dc_loss: 0.019093483686447144, tv_loss: 0.017080171033740044\n",
      "iteration 1138, dc_loss: 0.019124936312437057, tv_loss: 0.017048252746462822\n",
      "iteration 1139, dc_loss: 0.019091088324785233, tv_loss: 0.017080215737223625\n",
      "iteration 1140, dc_loss: 0.01912045292556286, tv_loss: 0.01704758033156395\n",
      "iteration 1141, dc_loss: 0.01908443681895733, tv_loss: 0.01707964763045311\n",
      "iteration 1142, dc_loss: 0.019109616056084633, tv_loss: 0.01705065369606018\n",
      "iteration 1143, dc_loss: 0.019084319472312927, tv_loss: 0.01707291603088379\n",
      "iteration 1144, dc_loss: 0.019094396382570267, tv_loss: 0.017060965299606323\n",
      "iteration 1145, dc_loss: 0.019094988703727722, tv_loss: 0.017059754580259323\n",
      "iteration 1146, dc_loss: 0.019080456346273422, tv_loss: 0.017074771225452423\n",
      "iteration 1147, dc_loss: 0.019109513610601425, tv_loss: 0.017046917229890823\n",
      "iteration 1148, dc_loss: 0.01907077431678772, tv_loss: 0.017086787149310112\n",
      "iteration 1149, dc_loss: 0.019117625430226326, tv_loss: 0.017039887607097626\n",
      "iteration 1150, dc_loss: 0.01906896010041237, tv_loss: 0.017086772248148918\n",
      "iteration 1151, dc_loss: 0.01910838671028614, tv_loss: 0.017044702544808388\n",
      "iteration 1152, dc_loss: 0.01907423511147499, tv_loss: 0.01707610860466957\n",
      "iteration 1153, dc_loss: 0.0190903227776289, tv_loss: 0.01705778017640114\n",
      "iteration 1154, dc_loss: 0.019082320854067802, tv_loss: 0.0170640479773283\n",
      "iteration 1155, dc_loss: 0.019078213721513748, tv_loss: 0.017066746950149536\n",
      "iteration 1156, dc_loss: 0.019084380939602852, tv_loss: 0.017059320583939552\n",
      "iteration 1157, dc_loss: 0.019075702875852585, tv_loss: 0.017067143693566322\n",
      "iteration 1158, dc_loss: 0.019081655889749527, tv_loss: 0.017060905694961548\n",
      "iteration 1159, dc_loss: 0.01907830685377121, tv_loss: 0.017064526677131653\n",
      "iteration 1160, dc_loss: 0.0190797857940197, tv_loss: 0.01706356182694435\n",
      "iteration 1161, dc_loss: 0.01907930336892605, tv_loss: 0.017064092680811882\n",
      "iteration 1162, dc_loss: 0.01908012665808201, tv_loss: 0.017062250524759293\n",
      "iteration 1163, dc_loss: 0.01907438412308693, tv_loss: 0.01706627756357193\n",
      "iteration 1164, dc_loss: 0.019081419333815575, tv_loss: 0.01705734059214592\n",
      "iteration 1165, dc_loss: 0.019065815955400467, tv_loss: 0.017071234062314034\n",
      "iteration 1166, dc_loss: 0.019083304330706596, tv_loss: 0.017052393406629562\n",
      "iteration 1167, dc_loss: 0.0190590713173151, tv_loss: 0.0170754324644804\n",
      "iteration 1168, dc_loss: 0.019082481041550636, tv_loss: 0.0170507300645113\n",
      "iteration 1169, dc_loss: 0.019058942794799805, tv_loss: 0.01707272417843342\n",
      "iteration 1170, dc_loss: 0.019074369221925735, tv_loss: 0.017055748030543327\n",
      "iteration 1171, dc_loss: 0.01906336471438408, tv_loss: 0.017065487802028656\n",
      "iteration 1172, dc_loss: 0.019064245745539665, tv_loss: 0.01706390641629696\n",
      "iteration 1173, dc_loss: 0.01907007396221161, tv_loss: 0.01705804094672203\n",
      "iteration 1174, dc_loss: 0.019055582582950592, tv_loss: 0.01707257144153118\n",
      "iteration 1175, dc_loss: 0.019076256081461906, tv_loss: 0.017050964757800102\n",
      "iteration 1176, dc_loss: 0.01905074343085289, tv_loss: 0.017075300216674805\n",
      "iteration 1177, dc_loss: 0.019073834642767906, tv_loss: 0.01705094613134861\n",
      "iteration 1178, dc_loss: 0.019051339477300644, tv_loss: 0.017072217538952827\n",
      "iteration 1179, dc_loss: 0.019065452739596367, tv_loss: 0.01705695502460003\n",
      "iteration 1180, dc_loss: 0.019055215641856194, tv_loss: 0.01706569828093052\n",
      "iteration 1181, dc_loss: 0.019057834520936012, tv_loss: 0.017061803489923477\n",
      "iteration 1182, dc_loss: 0.019057005643844604, tv_loss: 0.017061594873666763\n",
      "iteration 1183, dc_loss: 0.019055454060435295, tv_loss: 0.017062366008758545\n",
      "iteration 1184, dc_loss: 0.01905362494289875, tv_loss: 0.017063714563846588\n",
      "iteration 1185, dc_loss: 0.01905943639576435, tv_loss: 0.01705782301723957\n",
      "iteration 1186, dc_loss: 0.019045867025852203, tv_loss: 0.01707175187766552\n",
      "iteration 1187, dc_loss: 0.019071198999881744, tv_loss: 0.017047440633177757\n",
      "iteration 1188, dc_loss: 0.019036905840039253, tv_loss: 0.017083745449781418\n",
      "iteration 1189, dc_loss: 0.01908836141228676, tv_loss: 0.01703527383506298\n",
      "iteration 1190, dc_loss: 0.01903390698134899, tv_loss: 0.01709289290010929\n",
      "iteration 1191, dc_loss: 0.01909811981022358, tv_loss: 0.017031081020832062\n",
      "iteration 1192, dc_loss: 0.019038625061511993, tv_loss: 0.01709078997373581\n",
      "iteration 1193, dc_loss: 0.019089141860604286, tv_loss: 0.01703759841620922\n",
      "iteration 1194, dc_loss: 0.01904040202498436, tv_loss: 0.017080677673220634\n",
      "iteration 1195, dc_loss: 0.019065020605921745, tv_loss: 0.017048917710781097\n",
      "iteration 1196, dc_loss: 0.019039632752537727, tv_loss: 0.01706828363239765\n",
      "iteration 1197, dc_loss: 0.01904546655714512, tv_loss: 0.01705970987677574\n",
      "iteration 1198, dc_loss: 0.019046643748879433, tv_loss: 0.017059411853551865\n",
      "iteration 1199, dc_loss: 0.01904023066163063, tv_loss: 0.01706858165562153\n",
      "iteration 1200, dc_loss: 0.019058365374803543, tv_loss: 0.017052458599209785\n",
      "iteration 1201, dc_loss: 0.019035743549466133, tv_loss: 0.0170754287391901\n",
      "iteration 1202, dc_loss: 0.019051969051361084, tv_loss: 0.01705107092857361\n",
      "iteration 1203, dc_loss: 0.019040655344724655, tv_loss: 0.017059115692973137\n",
      "iteration 1204, dc_loss: 0.019033733755350113, tv_loss: 0.017071396112442017\n",
      "iteration 1205, dc_loss: 0.01905190572142601, tv_loss: 0.017049863934516907\n",
      "iteration 1206, dc_loss: 0.01903476193547249, tv_loss: 0.017062658444046974\n",
      "iteration 1207, dc_loss: 0.019031371921300888, tv_loss: 0.01706838421523571\n",
      "iteration 1208, dc_loss: 0.01904972828924656, tv_loss: 0.017048820853233337\n",
      "iteration 1209, dc_loss: 0.019028982147574425, tv_loss: 0.017066707834601402\n",
      "iteration 1210, dc_loss: 0.01903088390827179, tv_loss: 0.01706557162106037\n",
      "iteration 1211, dc_loss: 0.019047699868679047, tv_loss: 0.017048019915819168\n",
      "iteration 1212, dc_loss: 0.019024427980184555, tv_loss: 0.017068512737751007\n",
      "iteration 1213, dc_loss: 0.019031459465622902, tv_loss: 0.017061276361346245\n",
      "iteration 1214, dc_loss: 0.019043145701289177, tv_loss: 0.017050087451934814\n",
      "iteration 1215, dc_loss: 0.019020939245820045, tv_loss: 0.017070826143026352\n",
      "iteration 1216, dc_loss: 0.01903262734413147, tv_loss: 0.017057836055755615\n",
      "iteration 1217, dc_loss: 0.019037149846553802, tv_loss: 0.017053119838237762\n",
      "iteration 1218, dc_loss: 0.01902010850608349, tv_loss: 0.01706909015774727\n",
      "iteration 1219, dc_loss: 0.01903248205780983, tv_loss: 0.017055150121450424\n",
      "iteration 1220, dc_loss: 0.019031690433621407, tv_loss: 0.017055505886673927\n",
      "iteration 1221, dc_loss: 0.019018668681383133, tv_loss: 0.017068225890398026\n",
      "iteration 1222, dc_loss: 0.019032489508390427, tv_loss: 0.017053259536623955\n",
      "iteration 1223, dc_loss: 0.019026590511202812, tv_loss: 0.01705849915742874\n",
      "iteration 1224, dc_loss: 0.019017916172742844, tv_loss: 0.01706726662814617\n",
      "iteration 1225, dc_loss: 0.019031954929232597, tv_loss: 0.01705269142985344\n",
      "iteration 1226, dc_loss: 0.019020698964595795, tv_loss: 0.017062729224562645\n",
      "iteration 1227, dc_loss: 0.019019853323698044, tv_loss: 0.01706271804869175\n",
      "iteration 1228, dc_loss: 0.01902826502919197, tv_loss: 0.017053604125976562\n",
      "iteration 1229, dc_loss: 0.01901744306087494, tv_loss: 0.017063327133655548\n",
      "iteration 1230, dc_loss: 0.01902022957801819, tv_loss: 0.01705962046980858\n",
      "iteration 1231, dc_loss: 0.019024694338440895, tv_loss: 0.017054717987775803\n",
      "iteration 1232, dc_loss: 0.01901451125741005, tv_loss: 0.017064454033970833\n",
      "iteration 1233, dc_loss: 0.019021939486265182, tv_loss: 0.017056481912732124\n",
      "iteration 1234, dc_loss: 0.019018257036805153, tv_loss: 0.017059924080967903\n",
      "iteration 1235, dc_loss: 0.01901504211127758, tv_loss: 0.017062515020370483\n",
      "iteration 1236, dc_loss: 0.019019851461052895, tv_loss: 0.017056629061698914\n",
      "iteration 1237, dc_loss: 0.019015297293663025, tv_loss: 0.017060082405805588\n",
      "iteration 1238, dc_loss: 0.0190141424536705, tv_loss: 0.017060382291674614\n",
      "iteration 1239, dc_loss: 0.01901831664144993, tv_loss: 0.017055638134479523\n",
      "iteration 1240, dc_loss: 0.01901077851653099, tv_loss: 0.017062760889530182\n",
      "iteration 1241, dc_loss: 0.019016169011592865, tv_loss: 0.0170571468770504\n",
      "iteration 1242, dc_loss: 0.019012246280908585, tv_loss: 0.01706077717244625\n",
      "iteration 1243, dc_loss: 0.019012218341231346, tv_loss: 0.01705973409116268\n",
      "iteration 1244, dc_loss: 0.01901235803961754, tv_loss: 0.017058495432138443\n",
      "iteration 1245, dc_loss: 0.019011033698916435, tv_loss: 0.017058979719877243\n",
      "iteration 1246, dc_loss: 0.019009878858923912, tv_loss: 0.017059484496712685\n",
      "iteration 1247, dc_loss: 0.01901155337691307, tv_loss: 0.017057351768016815\n",
      "iteration 1248, dc_loss: 0.019007055088877678, tv_loss: 0.017061714082956314\n",
      "iteration 1249, dc_loss: 0.019011424854397774, tv_loss: 0.017057085409760475\n",
      "iteration 1250, dc_loss: 0.01900525577366352, tv_loss: 0.017062170431017876\n",
      "iteration 1251, dc_loss: 0.0190097875893116, tv_loss: 0.017056457698345184\n",
      "iteration 1252, dc_loss: 0.019005432724952698, tv_loss: 0.017059944570064545\n",
      "iteration 1253, dc_loss: 0.01900629512965679, tv_loss: 0.017058413475751877\n",
      "iteration 1254, dc_loss: 0.019005656242370605, tv_loss: 0.01705864816904068\n",
      "iteration 1255, dc_loss: 0.01900468021631241, tv_loss: 0.017059458419680595\n",
      "iteration 1256, dc_loss: 0.019003931432962418, tv_loss: 0.01705986075103283\n",
      "iteration 1257, dc_loss: 0.019004983827471733, tv_loss: 0.017057856544852257\n",
      "iteration 1258, dc_loss: 0.019000880420207977, tv_loss: 0.01706099696457386\n",
      "iteration 1259, dc_loss: 0.01900515891611576, tv_loss: 0.017055975273251534\n",
      "iteration 1260, dc_loss: 0.018999256193637848, tv_loss: 0.017061304301023483\n",
      "iteration 1261, dc_loss: 0.01900275982916355, tv_loss: 0.017057398334145546\n",
      "iteration 1262, dc_loss: 0.018999513238668442, tv_loss: 0.017060142010450363\n",
      "iteration 1263, dc_loss: 0.01900065876543522, tv_loss: 0.017058147117495537\n",
      "iteration 1264, dc_loss: 0.018998434767127037, tv_loss: 0.01705947332084179\n",
      "iteration 1265, dc_loss: 0.01899981126189232, tv_loss: 0.01705736666917801\n",
      "iteration 1266, dc_loss: 0.018996046856045723, tv_loss: 0.017060650512576103\n",
      "iteration 1267, dc_loss: 0.01899995654821396, tv_loss: 0.017056414857506752\n",
      "iteration 1268, dc_loss: 0.018994811922311783, tv_loss: 0.017061201855540276\n",
      "iteration 1269, dc_loss: 0.01899813860654831, tv_loss: 0.017057077959179878\n",
      "iteration 1270, dc_loss: 0.018994448706507683, tv_loss: 0.017059799283742905\n",
      "iteration 1271, dc_loss: 0.01899520494043827, tv_loss: 0.017058173194527626\n",
      "iteration 1272, dc_loss: 0.01899445801973343, tv_loss: 0.01705821231007576\n",
      "iteration 1273, dc_loss: 0.018993880599737167, tv_loss: 0.01705818809568882\n",
      "iteration 1274, dc_loss: 0.01899174228310585, tv_loss: 0.017059866338968277\n",
      "iteration 1275, dc_loss: 0.01899508200585842, tv_loss: 0.01705615408718586\n",
      "iteration 1276, dc_loss: 0.018988527357578278, tv_loss: 0.017062319442629814\n",
      "iteration 1277, dc_loss: 0.018995676189661026, tv_loss: 0.017054453492164612\n",
      "iteration 1278, dc_loss: 0.018987562507390976, tv_loss: 0.017061708495020866\n",
      "iteration 1279, dc_loss: 0.01899152621626854, tv_loss: 0.01705692522227764\n",
      "iteration 1280, dc_loss: 0.01899039000272751, tv_loss: 0.017057402059435844\n",
      "iteration 1281, dc_loss: 0.01898692548274994, tv_loss: 0.01706029660999775\n",
      "iteration 1282, dc_loss: 0.01899019256234169, tv_loss: 0.017056507989764214\n",
      "iteration 1283, dc_loss: 0.018987242132425308, tv_loss: 0.01705891266465187\n",
      "iteration 1284, dc_loss: 0.018986517563462257, tv_loss: 0.01705905795097351\n",
      "iteration 1285, dc_loss: 0.018988849595189095, tv_loss: 0.017056088894605637\n",
      "iteration 1286, dc_loss: 0.01898229494690895, tv_loss: 0.017062019556760788\n",
      "iteration 1287, dc_loss: 0.018989894539117813, tv_loss: 0.017053712159395218\n",
      "iteration 1288, dc_loss: 0.018981512635946274, tv_loss: 0.017061356455087662\n",
      "iteration 1289, dc_loss: 0.01898547075688839, tv_loss: 0.017056696116924286\n",
      "iteration 1290, dc_loss: 0.0189839918166399, tv_loss: 0.017057623714208603\n",
      "iteration 1291, dc_loss: 0.018982140347361565, tv_loss: 0.01705893874168396\n",
      "iteration 1292, dc_loss: 0.018982481211423874, tv_loss: 0.017058048397302628\n",
      "iteration 1293, dc_loss: 0.01898263581097126, tv_loss: 0.017057280987501144\n",
      "iteration 1294, dc_loss: 0.018980033695697784, tv_loss: 0.017059318721294403\n",
      "iteration 1295, dc_loss: 0.018983211368322372, tv_loss: 0.017055578529834747\n",
      "iteration 1296, dc_loss: 0.018977388739585876, tv_loss: 0.01706075482070446\n",
      "iteration 1297, dc_loss: 0.0189823005348444, tv_loss: 0.017055140808224678\n",
      "iteration 1298, dc_loss: 0.01897749863564968, tv_loss: 0.017059365287423134\n",
      "iteration 1299, dc_loss: 0.01897863857448101, tv_loss: 0.01705760881304741\n",
      "iteration 1300, dc_loss: 0.018977580592036247, tv_loss: 0.01705799251794815\n",
      "iteration 1301, dc_loss: 0.01897835172712803, tv_loss: 0.017056437209248543\n",
      "iteration 1302, dc_loss: 0.018974317237734795, tv_loss: 0.017059724777936935\n",
      "iteration 1303, dc_loss: 0.018978754058480263, tv_loss: 0.017054611817002296\n",
      "iteration 1304, dc_loss: 0.01897233910858631, tv_loss: 0.017060520127415657\n",
      "iteration 1305, dc_loss: 0.01897827535867691, tv_loss: 0.017054187133908272\n",
      "iteration 1306, dc_loss: 0.018971676006913185, tv_loss: 0.01706056296825409\n",
      "iteration 1307, dc_loss: 0.01897539384663105, tv_loss: 0.01705637015402317\n",
      "iteration 1308, dc_loss: 0.01897297613322735, tv_loss: 0.0170578733086586\n",
      "iteration 1309, dc_loss: 0.01897249184548855, tv_loss: 0.01705746538937092\n",
      "iteration 1310, dc_loss: 0.018972113728523254, tv_loss: 0.017057035118341446\n",
      "iteration 1311, dc_loss: 0.018971391022205353, tv_loss: 0.017057104036211967\n",
      "iteration 1312, dc_loss: 0.018970845267176628, tv_loss: 0.01705716736614704\n",
      "iteration 1313, dc_loss: 0.018970835953950882, tv_loss: 0.01705688238143921\n",
      "iteration 1314, dc_loss: 0.01896757259964943, tv_loss: 0.017059845849871635\n",
      "iteration 1315, dc_loss: 0.018973458558321, tv_loss: 0.017053259536623955\n",
      "iteration 1316, dc_loss: 0.01896345429122448, tv_loss: 0.017062487080693245\n",
      "iteration 1317, dc_loss: 0.018973032012581825, tv_loss: 0.017052151262760162\n",
      "iteration 1318, dc_loss: 0.018964484333992004, tv_loss: 0.01706002466380596\n",
      "iteration 1319, dc_loss: 0.018967648968100548, tv_loss: 0.01705636829137802\n",
      "iteration 1320, dc_loss: 0.01896713674068451, tv_loss: 0.01705656386911869\n",
      "iteration 1321, dc_loss: 0.018964001908898354, tv_loss: 0.01705915480852127\n",
      "iteration 1322, dc_loss: 0.018967222422361374, tv_loss: 0.01705525442957878\n",
      "iteration 1323, dc_loss: 0.01896391436457634, tv_loss: 0.017057863995432854\n",
      "iteration 1324, dc_loss: 0.01896381936967373, tv_loss: 0.017057305201888084\n",
      "iteration 1325, dc_loss: 0.01896580308675766, tv_loss: 0.017054744064807892\n",
      "iteration 1326, dc_loss: 0.01895947754383087, tv_loss: 0.0170606579631567\n",
      "iteration 1327, dc_loss: 0.01896672323346138, tv_loss: 0.017052948474884033\n",
      "iteration 1328, dc_loss: 0.018958009779453278, tv_loss: 0.01706097275018692\n",
      "iteration 1329, dc_loss: 0.018964841961860657, tv_loss: 0.017053326591849327\n",
      "iteration 1330, dc_loss: 0.01895829848945141, tv_loss: 0.017059095203876495\n",
      "iteration 1331, dc_loss: 0.018960926681756973, tv_loss: 0.017055777832865715\n",
      "iteration 1332, dc_loss: 0.018959349021315575, tv_loss: 0.01705675758421421\n",
      "iteration 1333, dc_loss: 0.01895919442176819, tv_loss: 0.01705637015402317\n",
      "iteration 1334, dc_loss: 0.01895737648010254, tv_loss: 0.017057787626981735\n",
      "iteration 1335, dc_loss: 0.018959995359182358, tv_loss: 0.017054837197065353\n",
      "iteration 1336, dc_loss: 0.018954547122120857, tv_loss: 0.017059870064258575\n",
      "iteration 1337, dc_loss: 0.018960798159241676, tv_loss: 0.017053116112947464\n",
      "iteration 1338, dc_loss: 0.018953073769807816, tv_loss: 0.017060162499547005\n",
      "iteration 1339, dc_loss: 0.018958628177642822, tv_loss: 0.01705382578074932\n",
      "iteration 1340, dc_loss: 0.01895439624786377, tv_loss: 0.01705726981163025\n",
      "iteration 1341, dc_loss: 0.018953649327158928, tv_loss: 0.017057353630661964\n",
      "iteration 1342, dc_loss: 0.018955528736114502, tv_loss: 0.017054907977581024\n",
      "iteration 1343, dc_loss: 0.018952153623104095, tv_loss: 0.01705772429704666\n",
      "iteration 1344, dc_loss: 0.018953049555420876, tv_loss: 0.01705624721944332\n",
      "iteration 1345, dc_loss: 0.018953967839479446, tv_loss: 0.017054755240678787\n",
      "iteration 1346, dc_loss: 0.018948275595903397, tv_loss: 0.01705998182296753\n",
      "iteration 1347, dc_loss: 0.01895669288933277, tv_loss: 0.017051175236701965\n",
      "iteration 1348, dc_loss: 0.018945349380373955, tv_loss: 0.017062075436115265\n",
      "iteration 1349, dc_loss: 0.018955480307340622, tv_loss: 0.01705138385295868\n",
      "iteration 1350, dc_loss: 0.018947558477520943, tv_loss: 0.017058610916137695\n",
      "iteration 1351, dc_loss: 0.018948562443256378, tv_loss: 0.017056934535503387\n",
      "iteration 1352, dc_loss: 0.018951335921883583, tv_loss: 0.017053527757525444\n",
      "iteration 1353, dc_loss: 0.018945105373859406, tv_loss: 0.017059095203876495\n",
      "iteration 1354, dc_loss: 0.018950466066598892, tv_loss: 0.017053043469786644\n",
      "iteration 1355, dc_loss: 0.018945761024951935, tv_loss: 0.01705707423388958\n",
      "iteration 1356, dc_loss: 0.018946729600429535, tv_loss: 0.0170555729418993\n",
      "iteration 1357, dc_loss: 0.01894710212945938, tv_loss: 0.017054859548807144\n",
      "iteration 1358, dc_loss: 0.018942251801490784, tv_loss: 0.01705952174961567\n",
      "iteration 1359, dc_loss: 0.018950264900922775, tv_loss: 0.01705087348818779\n",
      "iteration 1360, dc_loss: 0.018938397988677025, tv_loss: 0.017061876133084297\n",
      "iteration 1361, dc_loss: 0.018948765471577644, tv_loss: 0.017050666734576225\n",
      "iteration 1362, dc_loss: 0.01894049532711506, tv_loss: 0.01705826446413994\n",
      "iteration 1363, dc_loss: 0.01894369348883629, tv_loss: 0.01705457642674446\n",
      "iteration 1364, dc_loss: 0.018941720947623253, tv_loss: 0.01705624721944332\n",
      "iteration 1365, dc_loss: 0.018941916525363922, tv_loss: 0.017055654898285866\n",
      "iteration 1366, dc_loss: 0.018940450623631477, tv_loss: 0.01705627515912056\n",
      "iteration 1367, dc_loss: 0.018941735848784447, tv_loss: 0.017054203897714615\n",
      "iteration 1368, dc_loss: 0.018937233835458755, tv_loss: 0.017058124765753746\n",
      "iteration 1369, dc_loss: 0.018944012001156807, tv_loss: 0.017050983384251595\n",
      "iteration 1370, dc_loss: 0.018933558836579323, tv_loss: 0.0170612670481205\n",
      "iteration 1371, dc_loss: 0.018944356590509415, tv_loss: 0.017050277441740036\n",
      "iteration 1372, dc_loss: 0.018933847546577454, tv_loss: 0.01705997996032238\n",
      "iteration 1373, dc_loss: 0.018939996138215065, tv_loss: 0.01705295778810978\n",
      "iteration 1374, dc_loss: 0.018936585634946823, tv_loss: 0.01705574430525303\n",
      "iteration 1375, dc_loss: 0.018935006111860275, tv_loss: 0.017056945711374283\n",
      "iteration 1376, dc_loss: 0.018939785659313202, tv_loss: 0.01705186255276203\n",
      "iteration 1377, dc_loss: 0.018931685015559196, tv_loss: 0.017059581354260445\n",
      "iteration 1378, dc_loss: 0.018938403576612473, tv_loss: 0.017052238807082176\n",
      "iteration 1379, dc_loss: 0.0189330093562603, tv_loss: 0.017056796699762344\n",
      "iteration 1380, dc_loss: 0.018933219835162163, tv_loss: 0.01705572009086609\n",
      "iteration 1381, dc_loss: 0.01893569715321064, tv_loss: 0.017052631825208664\n",
      "iteration 1382, dc_loss: 0.01892806403338909, tv_loss: 0.017059890553355217\n",
      "iteration 1383, dc_loss: 0.018938938155770302, tv_loss: 0.017048727720975876\n",
      "iteration 1384, dc_loss: 0.018925268203020096, tv_loss: 0.017062054947018623\n",
      "iteration 1385, dc_loss: 0.018936948850750923, tv_loss: 0.017049770802259445\n",
      "iteration 1386, dc_loss: 0.018926911056041718, tv_loss: 0.017058949917554855\n",
      "iteration 1387, dc_loss: 0.018932055681943893, tv_loss: 0.017052991315722466\n",
      "iteration 1388, dc_loss: 0.018928978592157364, tv_loss: 0.017055371776223183\n",
      "iteration 1389, dc_loss: 0.01892904005944729, tv_loss: 0.017054660245776176\n",
      "iteration 1390, dc_loss: 0.018928788602352142, tv_loss: 0.017054330557584763\n",
      "iteration 1391, dc_loss: 0.018927717581391335, tv_loss: 0.017054932191967964\n",
      "iteration 1392, dc_loss: 0.01892610266804695, tv_loss: 0.01705627702176571\n",
      "iteration 1393, dc_loss: 0.018930234014987946, tv_loss: 0.017051998525857925\n",
      "iteration 1394, dc_loss: 0.01892140693962574, tv_loss: 0.017060404643416405\n",
      "iteration 1395, dc_loss: 0.018933316692709923, tv_loss: 0.017048010602593422\n",
      "iteration 1396, dc_loss: 0.018920445814728737, tv_loss: 0.017060313373804092\n",
      "iteration 1397, dc_loss: 0.018928395584225655, tv_loss: 0.01705174706876278\n",
      "iteration 1398, dc_loss: 0.01892506331205368, tv_loss: 0.0170546043664217\n",
      "iteration 1399, dc_loss: 0.018921131268143654, tv_loss: 0.017058104276657104\n",
      "iteration 1400, dc_loss: 0.018929127603769302, tv_loss: 0.017049474641680717\n",
      "iteration 1401, dc_loss: 0.01891811192035675, tv_loss: 0.017059700563549995\n",
      "iteration 1402, dc_loss: 0.018926862627267838, tv_loss: 0.017050104215741158\n",
      "iteration 1403, dc_loss: 0.018919849768280983, tv_loss: 0.017056358978152275\n",
      "iteration 1404, dc_loss: 0.01892058365046978, tv_loss: 0.017055081203579903\n",
      "iteration 1405, dc_loss: 0.018924305215477943, tv_loss: 0.017051177099347115\n",
      "iteration 1406, dc_loss: 0.018914908170700073, tv_loss: 0.017060503363609314\n",
      "iteration 1407, dc_loss: 0.018927736207842827, tv_loss: 0.017047233879566193\n",
      "iteration 1408, dc_loss: 0.018912408500909805, tv_loss: 0.017061747610569\n",
      "iteration 1409, dc_loss: 0.018924931064248085, tv_loss: 0.01704840362071991\n",
      "iteration 1410, dc_loss: 0.018914880231022835, tv_loss: 0.017057711258530617\n",
      "iteration 1411, dc_loss: 0.018918881192803383, tv_loss: 0.01705320179462433\n",
      "iteration 1412, dc_loss: 0.018918005749583244, tv_loss: 0.01705368608236313\n",
      "iteration 1413, dc_loss: 0.018915997818112373, tv_loss: 0.017055263742804527\n",
      "iteration 1414, dc_loss: 0.018916470929980278, tv_loss: 0.017054203897714615\n",
      "iteration 1415, dc_loss: 0.018917594105005264, tv_loss: 0.017052477225661278\n",
      "iteration 1416, dc_loss: 0.018912335857748985, tv_loss: 0.01705722138285637\n",
      "iteration 1417, dc_loss: 0.018920747563242912, tv_loss: 0.017048532143235207\n",
      "iteration 1418, dc_loss: 0.01890704594552517, tv_loss: 0.01706206239759922\n",
      "iteration 1419, dc_loss: 0.01892411895096302, tv_loss: 0.017044851556420326\n",
      "iteration 1420, dc_loss: 0.018905984237790108, tv_loss: 0.017062310129404068\n",
      "iteration 1421, dc_loss: 0.01891927793622017, tv_loss: 0.01704806089401245\n",
      "iteration 1422, dc_loss: 0.01890922710299492, tv_loss: 0.017057189717888832\n",
      "iteration 1423, dc_loss: 0.018912754952907562, tv_loss: 0.01705291122198105\n",
      "iteration 1424, dc_loss: 0.018912436440587044, tv_loss: 0.01705266907811165\n",
      "iteration 1425, dc_loss: 0.018908990547060966, tv_loss: 0.017055591568350792\n",
      "iteration 1426, dc_loss: 0.018912632018327713, tv_loss: 0.01705149933695793\n",
      "iteration 1427, dc_loss: 0.018908385187387466, tv_loss: 0.017055349424481392\n",
      "iteration 1428, dc_loss: 0.018909107893705368, tv_loss: 0.017054226249456406\n",
      "iteration 1429, dc_loss: 0.018911421298980713, tv_loss: 0.017051683738827705\n",
      "iteration 1430, dc_loss: 0.01890525594353676, tv_loss: 0.017057856544852257\n",
      "iteration 1431, dc_loss: 0.018914539366960526, tv_loss: 0.017048606649041176\n",
      "iteration 1432, dc_loss: 0.01890482008457184, tv_loss: 0.017058052122592926\n",
      "iteration 1433, dc_loss: 0.01891067996621132, tv_loss: 0.01705157198011875\n",
      "iteration 1434, dc_loss: 0.018908098340034485, tv_loss: 0.017053131014108658\n",
      "iteration 1435, dc_loss: 0.018903566524386406, tv_loss: 0.017056675627827644\n",
      "iteration 1436, dc_loss: 0.01891135983169079, tv_loss: 0.017048045992851257\n",
      "iteration 1437, dc_loss: 0.018899409100413322, tv_loss: 0.017059270292520523\n",
      "iteration 1438, dc_loss: 0.01891050860285759, tv_loss: 0.017047438770532608\n",
      "iteration 1439, dc_loss: 0.018900301307439804, tv_loss: 0.017056962475180626\n",
      "iteration 1440, dc_loss: 0.018904397264122963, tv_loss: 0.01705235242843628\n",
      "iteration 1441, dc_loss: 0.0189047921448946, tv_loss: 0.017051704227924347\n",
      "iteration 1442, dc_loss: 0.018897995352745056, tv_loss: 0.017058344557881355\n",
      "iteration 1443, dc_loss: 0.01891041174530983, tv_loss: 0.017045745626091957\n",
      "iteration 1444, dc_loss: 0.018893126398324966, tv_loss: 0.017062636092305183\n",
      "iteration 1445, dc_loss: 0.018911251798272133, tv_loss: 0.01704385131597519\n",
      "iteration 1446, dc_loss: 0.018894154578447342, tv_loss: 0.017060067504644394\n",
      "iteration 1447, dc_loss: 0.01890331692993641, tv_loss: 0.01705017313361168\n",
      "iteration 1448, dc_loss: 0.018899992108345032, tv_loss: 0.017052888870239258\n",
      "iteration 1449, dc_loss: 0.018896736204624176, tv_loss: 0.017055615782737732\n",
      "iteration 1450, dc_loss: 0.018902314826846123, tv_loss: 0.01704949513077736\n",
      "iteration 1451, dc_loss: 0.018895043060183525, tv_loss: 0.01705636829137802\n",
      "iteration 1452, dc_loss: 0.018901454284787178, tv_loss: 0.01704980619251728\n",
      "iteration 1453, dc_loss: 0.018896518275141716, tv_loss: 0.01705498807132244\n",
      "iteration 1454, dc_loss: 0.018900150433182716, tv_loss: 0.017051929607987404\n",
      "iteration 1455, dc_loss: 0.0188986174762249, tv_loss: 0.01705390028655529\n",
      "iteration 1456, dc_loss: 0.018899017944931984, tv_loss: 0.0170529056340456\n",
      "iteration 1457, dc_loss: 0.018898023292422295, tv_loss: 0.017052724957466125\n",
      "iteration 1458, dc_loss: 0.018896833062171936, tv_loss: 0.01705242320895195\n",
      "iteration 1459, dc_loss: 0.018894657492637634, tv_loss: 0.017053239047527313\n",
      "iteration 1460, dc_loss: 0.01889520324766636, tv_loss: 0.017051739618182182\n",
      "iteration 1461, dc_loss: 0.018892522901296616, tv_loss: 0.017053835093975067\n",
      "iteration 1462, dc_loss: 0.018893463537096977, tv_loss: 0.017052503302693367\n",
      "iteration 1463, dc_loss: 0.0188946183770895, tv_loss: 0.017051057890057564\n",
      "iteration 1464, dc_loss: 0.018888751044869423, tv_loss: 0.017056697979569435\n",
      "iteration 1465, dc_loss: 0.018901033326983452, tv_loss: 0.01704452745616436\n",
      "iteration 1466, dc_loss: 0.01888141967356205, tv_loss: 0.017064403742551804\n",
      "iteration 1467, dc_loss: 0.018907641991972923, tv_loss: 0.01703842170536518\n",
      "iteration 1468, dc_loss: 0.01887907087802887, tv_loss: 0.017066579312086105\n",
      "iteration 1469, dc_loss: 0.01890302263200283, tv_loss: 0.017041338607668877\n",
      "iteration 1470, dc_loss: 0.018883083015680313, tv_loss: 0.017059775069355965\n",
      "iteration 1471, dc_loss: 0.018892472609877586, tv_loss: 0.017049185931682587\n",
      "iteration 1472, dc_loss: 0.01888951286673546, tv_loss: 0.01705131307244301\n",
      "iteration 1473, dc_loss: 0.018885072320699692, tv_loss: 0.01705518737435341\n",
      "iteration 1474, dc_loss: 0.018891239538788795, tv_loss: 0.01704862155020237\n",
      "iteration 1475, dc_loss: 0.018883807584643364, tv_loss: 0.01705579087138176\n",
      "iteration 1476, dc_loss: 0.01888897269964218, tv_loss: 0.01705038920044899\n",
      "iteration 1477, dc_loss: 0.018886134028434753, tv_loss: 0.017053237184882164\n",
      "iteration 1478, dc_loss: 0.018886961042881012, tv_loss: 0.01705266162753105\n",
      "iteration 1479, dc_loss: 0.018889371305704117, tv_loss: 0.01705055870115757\n",
      "iteration 1480, dc_loss: 0.018885629251599312, tv_loss: 0.01705426163971424\n",
      "iteration 1481, dc_loss: 0.018887732177972794, tv_loss: 0.01705150492489338\n",
      "iteration 1482, dc_loss: 0.018886137753725052, tv_loss: 0.017051799222826958\n",
      "iteration 1483, dc_loss: 0.018882565200328827, tv_loss: 0.017053982242941856\n",
      "iteration 1484, dc_loss: 0.018887370824813843, tv_loss: 0.017048045992851257\n",
      "iteration 1485, dc_loss: 0.01887776516377926, tv_loss: 0.01705688051879406\n",
      "iteration 1486, dc_loss: 0.018887855112552643, tv_loss: 0.017046233639121056\n",
      "iteration 1487, dc_loss: 0.01887674070894718, tv_loss: 0.01705688051879406\n",
      "iteration 1488, dc_loss: 0.018884114921092987, tv_loss: 0.017048975452780724\n",
      "iteration 1489, dc_loss: 0.0188799649477005, tv_loss: 0.017052602022886276\n",
      "iteration 1490, dc_loss: 0.018878016620874405, tv_loss: 0.01705411821603775\n",
      "iteration 1491, dc_loss: 0.01888607256114483, tv_loss: 0.017045985907316208\n",
      "iteration 1492, dc_loss: 0.018870221450924873, tv_loss: 0.017062030732631683\n",
      "iteration 1493, dc_loss: 0.018893666565418243, tv_loss: 0.017038829624652863\n",
      "iteration 1494, dc_loss: 0.018865887075662613, tv_loss: 0.017066311091184616\n",
      "iteration 1495, dc_loss: 0.018893059343099594, tv_loss: 0.01703847199678421\n",
      "iteration 1496, dc_loss: 0.01886932924389839, tv_loss: 0.01706145517528057\n",
      "iteration 1497, dc_loss: 0.01888488233089447, tv_loss: 0.017045527696609497\n",
      "iteration 1498, dc_loss: 0.01887565106153488, tv_loss: 0.01705467328429222\n",
      "iteration 1499, dc_loss: 0.01888027973473072, tv_loss: 0.017049819231033325\n",
      "iteration 1500, dc_loss: 0.01887631043791771, tv_loss: 0.017053181305527687\n",
      "iteration 1501, dc_loss: 0.01888081431388855, tv_loss: 0.017047686502337456\n",
      "iteration 1502, dc_loss: 0.018869908526539803, tv_loss: 0.01705767959356308\n",
      "iteration 1503, dc_loss: 0.01888468861579895, tv_loss: 0.017042415216565132\n",
      "iteration 1504, dc_loss: 0.018863722681999207, tv_loss: 0.017063047736883163\n",
      "iteration 1505, dc_loss: 0.01888640783727169, tv_loss: 0.01703980751335621\n",
      "iteration 1506, dc_loss: 0.018863920122385025, tv_loss: 0.017061376944184303\n",
      "iteration 1507, dc_loss: 0.01887926459312439, tv_loss: 0.0170451533049345\n",
      "iteration 1508, dc_loss: 0.018870746716856956, tv_loss: 0.017052993178367615\n",
      "iteration 1509, dc_loss: 0.018869508057832718, tv_loss: 0.017053991556167603\n",
      "iteration 1510, dc_loss: 0.018878046423196793, tv_loss: 0.017045412212610245\n",
      "iteration 1511, dc_loss: 0.01886386051774025, tv_loss: 0.017059700563549995\n",
      "iteration 1512, dc_loss: 0.018881989642977715, tv_loss: 0.01704179309308529\n",
      "iteration 1513, dc_loss: 0.01886356621980667, tv_loss: 0.017060568556189537\n",
      "iteration 1514, dc_loss: 0.018882717937231064, tv_loss: 0.01704174466431141\n",
      "iteration 1515, dc_loss: 0.018865196034312248, tv_loss: 0.01705947518348694\n",
      "iteration 1516, dc_loss: 0.018880441784858704, tv_loss: 0.01704396679997444\n",
      "iteration 1517, dc_loss: 0.018865616992115974, tv_loss: 0.01705777272582054\n",
      "iteration 1518, dc_loss: 0.018876248970627785, tv_loss: 0.01704532839357853\n",
      "iteration 1519, dc_loss: 0.018864555284380913, tv_loss: 0.017055129632353783\n",
      "iteration 1520, dc_loss: 0.018870128318667412, tv_loss: 0.01704801246523857\n",
      "iteration 1521, dc_loss: 0.01886606588959694, tv_loss: 0.0170512106269598\n",
      "iteration 1522, dc_loss: 0.018863657489418983, tv_loss: 0.017053382471203804\n",
      "iteration 1523, dc_loss: 0.018872078508138657, tv_loss: 0.017045238986611366\n",
      "iteration 1524, dc_loss: 0.018856730312108994, tv_loss: 0.017061078920960426\n",
      "iteration 1525, dc_loss: 0.01888178661465645, tv_loss: 0.017036696895956993\n",
      "iteration 1526, dc_loss: 0.01885109767317772, tv_loss: 0.01706777513027191\n",
      "iteration 1527, dc_loss: 0.01888546161353588, tv_loss: 0.017033303156495094\n",
      "iteration 1528, dc_loss: 0.018852023407816887, tv_loss: 0.01706591621041298\n",
      "iteration 1529, dc_loss: 0.01887727901339531, tv_loss: 0.017039494588971138\n",
      "iteration 1530, dc_loss: 0.018857255578041077, tv_loss: 0.017058126628398895\n",
      "iteration 1531, dc_loss: 0.018867038190364838, tv_loss: 0.017046933993697166\n",
      "iteration 1532, dc_loss: 0.01886170729994774, tv_loss: 0.017050929367542267\n",
      "iteration 1533, dc_loss: 0.018860025331377983, tv_loss: 0.01705157570540905\n",
      "iteration 1534, dc_loss: 0.01886259950697422, tv_loss: 0.01704839989542961\n",
      "iteration 1535, dc_loss: 0.01885872147977352, tv_loss: 0.01705223321914673\n",
      "iteration 1536, dc_loss: 0.018862316384911537, tv_loss: 0.017049051821231842\n",
      "iteration 1537, dc_loss: 0.01885969005525112, tv_loss: 0.01705227605998516\n",
      "iteration 1538, dc_loss: 0.018863091245293617, tv_loss: 0.017048992216587067\n",
      "iteration 1539, dc_loss: 0.018860075622797012, tv_loss: 0.01705193892121315\n",
      "iteration 1540, dc_loss: 0.018863528966903687, tv_loss: 0.017048131674528122\n",
      "iteration 1541, dc_loss: 0.01885753683745861, tv_loss: 0.017053479328751564\n",
      "iteration 1542, dc_loss: 0.018864016979932785, tv_loss: 0.017046008259058\n",
      "iteration 1543, dc_loss: 0.01885249838232994, tv_loss: 0.017056375741958618\n",
      "iteration 1544, dc_loss: 0.018865292891860008, tv_loss: 0.017042452469468117\n",
      "iteration 1545, dc_loss: 0.018848106265068054, tv_loss: 0.017058761790394783\n",
      "iteration 1546, dc_loss: 0.01886531338095665, tv_loss: 0.01704084314405918\n",
      "iteration 1547, dc_loss: 0.018848273903131485, tv_loss: 0.017057249322533607\n",
      "iteration 1548, dc_loss: 0.018859561532735825, tv_loss: 0.017045367509126663\n",
      "iteration 1549, dc_loss: 0.018853334710001945, tv_loss: 0.017051182687282562\n",
      "iteration 1550, dc_loss: 0.01885143667459488, tv_loss: 0.017052873969078064\n",
      "iteration 1551, dc_loss: 0.01886073686182499, tv_loss: 0.017043497413396835\n",
      "iteration 1552, dc_loss: 0.018843239173293114, tv_loss: 0.017061101272702217\n",
      "iteration 1553, dc_loss: 0.018868686631321907, tv_loss: 0.017035815864801407\n",
      "iteration 1554, dc_loss: 0.01883903332054615, tv_loss: 0.01706526428461075\n",
      "iteration 1555, dc_loss: 0.01886781118810177, tv_loss: 0.017035989090800285\n",
      "iteration 1556, dc_loss: 0.01884222775697708, tv_loss: 0.01706078089773655\n",
      "iteration 1557, dc_loss: 0.018860135227441788, tv_loss: 0.017042558640241623\n",
      "iteration 1558, dc_loss: 0.018848517909646034, tv_loss: 0.017054330557584763\n",
      "iteration 1559, dc_loss: 0.018856899812817574, tv_loss: 0.01704629510641098\n",
      "iteration 1560, dc_loss: 0.018849927932024002, tv_loss: 0.017053334042429924\n",
      "iteration 1561, dc_loss: 0.01885910890996456, tv_loss: 0.01704382710158825\n",
      "iteration 1562, dc_loss: 0.018843011930584908, tv_loss: 0.017059391364455223\n",
      "iteration 1563, dc_loss: 0.01886601187288761, tv_loss: 0.017035916447639465\n",
      "iteration 1564, dc_loss: 0.018834754824638367, tv_loss: 0.017066581174731255\n",
      "iteration 1565, dc_loss: 0.018868524581193924, tv_loss: 0.017032047733664513\n",
      "iteration 1566, dc_loss: 0.018834004178643227, tv_loss: 0.01706524007022381\n",
      "iteration 1567, dc_loss: 0.018859636038541794, tv_loss: 0.017038103193044662\n",
      "iteration 1568, dc_loss: 0.018841514363884926, tv_loss: 0.017054865136742592\n",
      "iteration 1569, dc_loss: 0.0188454519957304, tv_loss: 0.01705016940832138\n",
      "iteration 1570, dc_loss: 0.018851201981306076, tv_loss: 0.017044134438037872\n",
      "iteration 1571, dc_loss: 0.018837561830878258, tv_loss: 0.0170577485114336\n",
      "iteration 1572, dc_loss: 0.018855653703212738, tv_loss: 0.017039747908711433\n",
      "iteration 1573, dc_loss: 0.018837306648492813, tv_loss: 0.01705845631659031\n",
      "iteration 1574, dc_loss: 0.018855342641472816, tv_loss: 0.017041150480508804\n",
      "iteration 1575, dc_loss: 0.018841160461306572, tv_loss: 0.017056329175829887\n",
      "iteration 1576, dc_loss: 0.01885441318154335, tv_loss: 0.01704360730946064\n",
      "iteration 1577, dc_loss: 0.018842967227101326, tv_loss: 0.017054975032806396\n",
      "iteration 1578, dc_loss: 0.01885383576154709, tv_loss: 0.01704312674701214\n",
      "iteration 1579, dc_loss: 0.018839368596673012, tv_loss: 0.017055919393897057\n",
      "iteration 1580, dc_loss: 0.018852192908525467, tv_loss: 0.01704118400812149\n",
      "iteration 1581, dc_loss: 0.018834121525287628, tv_loss: 0.017057674005627632\n",
      "iteration 1582, dc_loss: 0.018850507214665413, tv_loss: 0.017040163278579712\n",
      "iteration 1583, dc_loss: 0.01883319765329361, tv_loss: 0.017056697979569435\n",
      "iteration 1584, dc_loss: 0.018846573308110237, tv_loss: 0.017042677849531174\n",
      "iteration 1585, dc_loss: 0.018837401643395424, tv_loss: 0.017051350325345993\n",
      "iteration 1586, dc_loss: 0.01883842423558235, tv_loss: 0.01705009676516056\n",
      "iteration 1587, dc_loss: 0.018845567479729652, tv_loss: 0.017043236643075943\n",
      "iteration 1588, dc_loss: 0.01882866770029068, tv_loss: 0.01706068031489849\n",
      "iteration 1589, dc_loss: 0.018856922164559364, tv_loss: 0.017032813280820847\n",
      "iteration 1590, dc_loss: 0.018822330981492996, tv_loss: 0.017067641019821167\n",
      "iteration 1591, dc_loss: 0.018860215321183205, tv_loss: 0.017029693350195885\n",
      "iteration 1592, dc_loss: 0.018824128434062004, tv_loss: 0.017065251246094704\n",
      "iteration 1593, dc_loss: 0.018851744011044502, tv_loss: 0.017036965116858482\n",
      "iteration 1594, dc_loss: 0.018831854686141014, tv_loss: 0.017056088894605637\n",
      "iteration 1595, dc_loss: 0.018842436373233795, tv_loss: 0.01704479195177555\n",
      "iteration 1596, dc_loss: 0.018836041912436485, tv_loss: 0.01705029234290123\n",
      "iteration 1597, dc_loss: 0.018838880583643913, tv_loss: 0.017046310007572174\n",
      "iteration 1598, dc_loss: 0.018832271918654442, tv_loss: 0.01705186255276203\n",
      "iteration 1599, dc_loss: 0.018840260803699493, tv_loss: 0.01704328879714012\n",
      "iteration 1600, dc_loss: 0.018826372921466827, tv_loss: 0.017056765034794807\n",
      "iteration 1601, dc_loss: 0.018844176083803177, tv_loss: 0.017038727179169655\n",
      "iteration 1602, dc_loss: 0.018827704712748528, tv_loss: 0.017053624615073204\n",
      "iteration 1603, dc_loss: 0.018830152228474617, tv_loss: 0.017049958929419518\n",
      "iteration 1604, dc_loss: 0.01884160190820694, tv_loss: 0.01703907549381256\n",
      "iteration 1605, dc_loss: 0.01882634311914444, tv_loss: 0.017054367810487747\n",
      "iteration 1606, dc_loss: 0.01883081905543804, tv_loss: 0.017048848792910576\n",
      "iteration 1607, dc_loss: 0.018838031217455864, tv_loss: 0.0170410368591547\n",
      "iteration 1608, dc_loss: 0.018826348707079887, tv_loss: 0.01705230213701725\n",
      "iteration 1609, dc_loss: 0.01883123815059662, tv_loss: 0.017047179862856865\n",
      "iteration 1610, dc_loss: 0.018834542483091354, tv_loss: 0.017044002190232277\n",
      "iteration 1611, dc_loss: 0.01882627233862877, tv_loss: 0.017051666975021362\n",
      "iteration 1612, dc_loss: 0.018831567838788033, tv_loss: 0.017045462504029274\n",
      "iteration 1613, dc_loss: 0.018831707537174225, tv_loss: 0.01704494096338749\n",
      "iteration 1614, dc_loss: 0.018825817853212357, tv_loss: 0.017050838097929955\n",
      "iteration 1615, dc_loss: 0.018832065165042877, tv_loss: 0.017044462263584137\n",
      "iteration 1616, dc_loss: 0.018828604370355606, tv_loss: 0.017047518864274025\n",
      "iteration 1617, dc_loss: 0.018825877457857132, tv_loss: 0.017049839720129967\n",
      "iteration 1618, dc_loss: 0.018832502886652946, tv_loss: 0.01704283431172371\n",
      "iteration 1619, dc_loss: 0.018825076520442963, tv_loss: 0.017049727961421013\n",
      "iteration 1620, dc_loss: 0.01882782392203808, tv_loss: 0.017046522349119186\n",
      "iteration 1621, dc_loss: 0.018829453736543655, tv_loss: 0.01704460382461548\n",
      "iteration 1622, dc_loss: 0.01882399246096611, tv_loss: 0.01704983226954937\n",
      "iteration 1623, dc_loss: 0.018828921020030975, tv_loss: 0.01704462245106697\n",
      "iteration 1624, dc_loss: 0.018826227635145187, tv_loss: 0.017047081142663956\n",
      "iteration 1625, dc_loss: 0.01882464624941349, tv_loss: 0.017048558220267296\n",
      "iteration 1626, dc_loss: 0.018828537315130234, tv_loss: 0.017044557258486748\n",
      "iteration 1627, dc_loss: 0.018822599202394485, tv_loss: 0.017050111666321754\n",
      "iteration 1628, dc_loss: 0.018827639520168304, tv_loss: 0.01704448275268078\n",
      "iteration 1629, dc_loss: 0.018824653699994087, tv_loss: 0.017046961933374405\n",
      "iteration 1630, dc_loss: 0.01882270909845829, tv_loss: 0.017048543319106102\n",
      "iteration 1631, dc_loss: 0.018827106803655624, tv_loss: 0.01704384945333004\n",
      "iteration 1632, dc_loss: 0.01882254332304001, tv_loss: 0.017048122361302376\n",
      "iteration 1633, dc_loss: 0.01882302016019821, tv_loss: 0.017047449946403503\n",
      "iteration 1634, dc_loss: 0.018825214356184006, tv_loss: 0.017045116052031517\n",
      "iteration 1635, dc_loss: 0.018821220844984055, tv_loss: 0.01704893261194229\n",
      "iteration 1636, dc_loss: 0.018825337290763855, tv_loss: 0.017044425010681152\n",
      "iteration 1637, dc_loss: 0.01882045343518257, tv_loss: 0.017048809677362442\n",
      "iteration 1638, dc_loss: 0.01882292702794075, tv_loss: 0.017045840620994568\n",
      "iteration 1639, dc_loss: 0.018822723999619484, tv_loss: 0.017045674845576286\n",
      "iteration 1640, dc_loss: 0.01881975308060646, tv_loss: 0.017048371955752373\n",
      "iteration 1641, dc_loss: 0.01882268860936165, tv_loss: 0.01704518310725689\n",
      "iteration 1642, dc_loss: 0.01882091350853443, tv_loss: 0.017046736553311348\n",
      "iteration 1643, dc_loss: 0.018819620832800865, tv_loss: 0.017047779634594917\n",
      "iteration 1644, dc_loss: 0.018822217360138893, tv_loss: 0.017044859007000923\n",
      "iteration 1645, dc_loss: 0.018817907199263573, tv_loss: 0.017048845067620277\n",
      "iteration 1646, dc_loss: 0.01882214844226837, tv_loss: 0.017044195905327797\n",
      "iteration 1647, dc_loss: 0.018818022683262825, tv_loss: 0.017047926783561707\n",
      "iteration 1648, dc_loss: 0.018819445744156837, tv_loss: 0.01704617403447628\n",
      "iteration 1649, dc_loss: 0.01881997659802437, tv_loss: 0.01704537868499756\n",
      "iteration 1650, dc_loss: 0.018816960975527763, tv_loss: 0.01704813726246357\n",
      "iteration 1651, dc_loss: 0.018818745389580727, tv_loss: 0.01704598031938076\n",
      "iteration 1652, dc_loss: 0.0188188087195158, tv_loss: 0.017045512795448303\n",
      "iteration 1653, dc_loss: 0.018815942108631134, tv_loss: 0.017048027366399765\n",
      "iteration 1654, dc_loss: 0.018819883465766907, tv_loss: 0.017043842002749443\n",
      "iteration 1655, dc_loss: 0.01881449483335018, tv_loss: 0.017048992216587067\n",
      "iteration 1656, dc_loss: 0.01881914958357811, tv_loss: 0.017044082283973694\n",
      "iteration 1657, dc_loss: 0.01881543919444084, tv_loss: 0.017047541216015816\n",
      "iteration 1658, dc_loss: 0.018815748393535614, tv_loss: 0.017046820372343063\n",
      "iteration 1659, dc_loss: 0.018817465752363205, tv_loss: 0.01704462431371212\n",
      "iteration 1660, dc_loss: 0.018813887611031532, tv_loss: 0.01704775169491768\n",
      "iteration 1661, dc_loss: 0.018815912306308746, tv_loss: 0.01704530045390129\n",
      "iteration 1662, dc_loss: 0.018815606832504272, tv_loss: 0.0170452818274498\n",
      "iteration 1663, dc_loss: 0.018813224509358406, tv_loss: 0.017047442495822906\n",
      "iteration 1664, dc_loss: 0.018816515803337097, tv_loss: 0.01704411767423153\n",
      "iteration 1665, dc_loss: 0.018811674788594246, tv_loss: 0.017048951238393784\n",
      "iteration 1666, dc_loss: 0.01881653256714344, tv_loss: 0.017043691128492355\n",
      "iteration 1667, dc_loss: 0.018811790272593498, tv_loss: 0.017047816887497902\n",
      "iteration 1668, dc_loss: 0.018813537433743477, tv_loss: 0.017045525833964348\n",
      "iteration 1669, dc_loss: 0.018813638016581535, tv_loss: 0.01704501174390316\n",
      "iteration 1670, dc_loss: 0.018811525776982307, tv_loss: 0.01704682596027851\n",
      "iteration 1671, dc_loss: 0.01881292648613453, tv_loss: 0.017045216634869576\n",
      "iteration 1672, dc_loss: 0.01881209947168827, tv_loss: 0.01704595610499382\n",
      "iteration 1673, dc_loss: 0.018811043351888657, tv_loss: 0.01704687997698784\n",
      "iteration 1674, dc_loss: 0.018812989816069603, tv_loss: 0.017044566571712494\n",
      "iteration 1675, dc_loss: 0.018808597698807716, tv_loss: 0.017048604786396027\n",
      "iteration 1676, dc_loss: 0.018814608454704285, tv_loss: 0.017042210325598717\n",
      "iteration 1677, dc_loss: 0.01880800537765026, tv_loss: 0.017048468813300133\n",
      "iteration 1678, dc_loss: 0.018810845911502838, tv_loss: 0.017045319080352783\n",
      "iteration 1679, dc_loss: 0.018811186775565147, tv_loss: 0.017044704407453537\n",
      "iteration 1680, dc_loss: 0.018808020278811455, tv_loss: 0.01704752817749977\n",
      "iteration 1681, dc_loss: 0.01881028525531292, tv_loss: 0.017044849693775177\n",
      "iteration 1682, dc_loss: 0.018809398636221886, tv_loss: 0.017045337706804276\n",
      "iteration 1683, dc_loss: 0.018807673826813698, tv_loss: 0.017046744003891945\n",
      "iteration 1684, dc_loss: 0.01881035603582859, tv_loss: 0.017043866217136383\n",
      "iteration 1685, dc_loss: 0.01880582608282566, tv_loss: 0.017048286274075508\n",
      "iteration 1686, dc_loss: 0.01881089247763157, tv_loss: 0.01704304851591587\n",
      "iteration 1687, dc_loss: 0.018805624917149544, tv_loss: 0.017047949135303497\n",
      "iteration 1688, dc_loss: 0.018808094784617424, tv_loss: 0.01704496517777443\n",
      "iteration 1689, dc_loss: 0.0188075453042984, tv_loss: 0.017045026645064354\n",
      "iteration 1690, dc_loss: 0.018805643543601036, tv_loss: 0.017046505585312843\n",
      "iteration 1691, dc_loss: 0.01880727894604206, tv_loss: 0.01704453118145466\n",
      "iteration 1692, dc_loss: 0.018806057050824165, tv_loss: 0.017045458778738976\n",
      "iteration 1693, dc_loss: 0.01880539022386074, tv_loss: 0.017045918852090836\n",
      "iteration 1694, dc_loss: 0.01880659908056259, tv_loss: 0.017044605687260628\n",
      "iteration 1695, dc_loss: 0.018803518265485764, tv_loss: 0.017047572880983353\n",
      "iteration 1696, dc_loss: 0.01880844309926033, tv_loss: 0.01704234816133976\n",
      "iteration 1697, dc_loss: 0.01880115084350109, tv_loss: 0.01704919897019863\n",
      "iteration 1698, dc_loss: 0.01880747452378273, tv_loss: 0.017042409628629684\n",
      "iteration 1699, dc_loss: 0.018803440034389496, tv_loss: 0.01704605668783188\n",
      "iteration 1700, dc_loss: 0.018802637234330177, tv_loss: 0.01704661175608635\n",
      "iteration 1701, dc_loss: 0.018805691972374916, tv_loss: 0.017043381929397583\n",
      "iteration 1702, dc_loss: 0.01880175806581974, tv_loss: 0.017047066241502762\n",
      "iteration 1703, dc_loss: 0.018804222345352173, tv_loss: 0.01704416796565056\n",
      "iteration 1704, dc_loss: 0.01880212314426899, tv_loss: 0.01704583317041397\n",
      "iteration 1705, dc_loss: 0.01880180463194847, tv_loss: 0.017045816406607628\n",
      "iteration 1706, dc_loss: 0.018804725259542465, tv_loss: 0.017042674124240875\n",
      "iteration 1707, dc_loss: 0.01879800483584404, tv_loss: 0.017049234360456467\n",
      "iteration 1708, dc_loss: 0.018805868923664093, tv_loss: 0.01704121008515358\n",
      "iteration 1709, dc_loss: 0.018798835575580597, tv_loss: 0.017047856003046036\n",
      "iteration 1710, dc_loss: 0.01880147121846676, tv_loss: 0.01704474724829197\n",
      "iteration 1711, dc_loss: 0.018800707533955574, tv_loss: 0.017045017331838608\n",
      "iteration 1712, dc_loss: 0.018800605088472366, tv_loss: 0.01704471744596958\n",
      "iteration 1713, dc_loss: 0.01880035735666752, tv_loss: 0.017044590786099434\n",
      "iteration 1714, dc_loss: 0.018799318000674248, tv_loss: 0.01704534702003002\n",
      "iteration 1715, dc_loss: 0.01879911869764328, tv_loss: 0.01704534702003002\n",
      "iteration 1716, dc_loss: 0.018801407888531685, tv_loss: 0.017043011263012886\n",
      "iteration 1717, dc_loss: 0.018795674666762352, tv_loss: 0.017048733308911324\n",
      "iteration 1718, dc_loss: 0.018803298473358154, tv_loss: 0.017040882259607315\n",
      "iteration 1719, dc_loss: 0.018794294446706772, tv_loss: 0.01704939641058445\n",
      "iteration 1720, dc_loss: 0.018802011385560036, tv_loss: 0.01704113744199276\n",
      "iteration 1721, dc_loss: 0.018796414136886597, tv_loss: 0.017046276479959488\n",
      "iteration 1722, dc_loss: 0.01879732310771942, tv_loss: 0.017045024782419205\n",
      "iteration 1723, dc_loss: 0.018798131495714188, tv_loss: 0.01704402081668377\n",
      "iteration 1724, dc_loss: 0.018796034157276154, tv_loss: 0.01704598404467106\n",
      "iteration 1725, dc_loss: 0.018797410652041435, tv_loss: 0.01704438030719757\n",
      "iteration 1726, dc_loss: 0.018797241151332855, tv_loss: 0.01704431138932705\n",
      "iteration 1727, dc_loss: 0.018794730305671692, tv_loss: 0.017046578228473663\n",
      "iteration 1728, dc_loss: 0.018798764795064926, tv_loss: 0.017042238265275955\n",
      "iteration 1729, dc_loss: 0.018793532624840736, tv_loss: 0.017047110944986343\n",
      "iteration 1730, dc_loss: 0.0187971293926239, tv_loss: 0.017043117433786392\n",
      "iteration 1731, dc_loss: 0.018793709576129913, tv_loss: 0.017046140506863594\n",
      "iteration 1732, dc_loss: 0.01879582367837429, tv_loss: 0.017043640837073326\n",
      "iteration 1733, dc_loss: 0.018793806433677673, tv_loss: 0.017045309767127037\n",
      "iteration 1734, dc_loss: 0.01879543997347355, tv_loss: 0.017043305560946465\n",
      "iteration 1735, dc_loss: 0.018791981041431427, tv_loss: 0.017046481370925903\n",
      "iteration 1736, dc_loss: 0.018796591088175774, tv_loss: 0.017041634768247604\n",
      "iteration 1737, dc_loss: 0.018789924681186676, tv_loss: 0.01704820804297924\n",
      "iteration 1738, dc_loss: 0.018796944990754128, tv_loss: 0.017041033133864403\n",
      "iteration 1739, dc_loss: 0.018789976835250854, tv_loss: 0.017047597095370293\n",
      "iteration 1740, dc_loss: 0.01879449374973774, tv_loss: 0.01704264245927334\n",
      "iteration 1741, dc_loss: 0.018791571259498596, tv_loss: 0.017045166343450546\n",
      "iteration 1742, dc_loss: 0.01879184879362583, tv_loss: 0.01704464480280876\n",
      "iteration 1743, dc_loss: 0.01879287324845791, tv_loss: 0.01704343967139721\n",
      "iteration 1744, dc_loss: 0.018790144473314285, tv_loss: 0.017045995220541954\n",
      "iteration 1745, dc_loss: 0.01879328489303589, tv_loss: 0.017042580991983414\n",
      "iteration 1746, dc_loss: 0.018789665773510933, tv_loss: 0.017045795917510986\n",
      "iteration 1747, dc_loss: 0.01879033073782921, tv_loss: 0.017044682055711746\n",
      "iteration 1748, dc_loss: 0.01879275217652321, tv_loss: 0.0170418880879879\n",
      "iteration 1749, dc_loss: 0.018786469474434853, tv_loss: 0.017047924920916557\n",
      "iteration 1750, dc_loss: 0.018794231116771698, tv_loss: 0.017039982602000237\n",
      "iteration 1751, dc_loss: 0.018785180523991585, tv_loss: 0.01704881340265274\n",
      "iteration 1752, dc_loss: 0.01879327930510044, tv_loss: 0.017040466889739037\n",
      "iteration 1753, dc_loss: 0.018786681815981865, tv_loss: 0.01704670488834381\n",
      "iteration 1754, dc_loss: 0.01878986693918705, tv_loss: 0.017043091356754303\n",
      "iteration 1755, dc_loss: 0.01878797821700573, tv_loss: 0.01704452559351921\n",
      "iteration 1756, dc_loss: 0.01878812350332737, tv_loss: 0.017043951898813248\n",
      "iteration 1757, dc_loss: 0.018787743523716927, tv_loss: 0.0170439463108778\n",
      "iteration 1758, dc_loss: 0.018788238987326622, tv_loss: 0.01704315096139908\n",
      "iteration 1759, dc_loss: 0.018785446882247925, tv_loss: 0.017045794054865837\n",
      "iteration 1760, dc_loss: 0.01879025436937809, tv_loss: 0.0170410368591547\n",
      "iteration 1761, dc_loss: 0.01878262124955654, tv_loss: 0.01704871468245983\n",
      "iteration 1762, dc_loss: 0.01879105344414711, tv_loss: 0.017039842903614044\n",
      "iteration 1763, dc_loss: 0.01878339610993862, tv_loss: 0.017046980559825897\n",
      "iteration 1764, dc_loss: 0.018787216395139694, tv_loss: 0.01704273372888565\n",
      "iteration 1765, dc_loss: 0.01878703571856022, tv_loss: 0.017042648047208786\n",
      "iteration 1766, dc_loss: 0.018782636150717735, tv_loss: 0.01704687625169754\n",
      "iteration 1767, dc_loss: 0.018788769841194153, tv_loss: 0.017040612176060677\n",
      "iteration 1768, dc_loss: 0.018782125785946846, tv_loss: 0.017046872526407242\n",
      "iteration 1769, dc_loss: 0.01878640614449978, tv_loss: 0.017042024061083794\n",
      "iteration 1770, dc_loss: 0.018784373998641968, tv_loss: 0.01704360358417034\n",
      "iteration 1771, dc_loss: 0.018781568855047226, tv_loss: 0.017046159133315086\n",
      "iteration 1772, dc_loss: 0.018788272514939308, tv_loss: 0.017039386555552483\n",
      "iteration 1773, dc_loss: 0.01877829246222973, tv_loss: 0.017049314454197884\n",
      "iteration 1774, dc_loss: 0.018788496032357216, tv_loss: 0.017038892954587936\n",
      "iteration 1775, dc_loss: 0.018779287114739418, tv_loss: 0.017047546803951263\n",
      "iteration 1776, dc_loss: 0.018784545361995697, tv_loss: 0.017041770741343498\n",
      "iteration 1777, dc_loss: 0.018782155588269234, tv_loss: 0.017043786123394966\n",
      "iteration 1778, dc_loss: 0.018781399354338646, tv_loss: 0.01704426482319832\n",
      "iteration 1779, dc_loss: 0.018782317638397217, tv_loss: 0.017043117433786392\n",
      "iteration 1780, dc_loss: 0.018781954422593117, tv_loss: 0.017043260857462883\n",
      "iteration 1781, dc_loss: 0.018780088052153587, tv_loss: 0.017044955864548683\n",
      "iteration 1782, dc_loss: 0.01878397725522518, tv_loss: 0.017040930688381195\n",
      "iteration 1783, dc_loss: 0.018776381388306618, tv_loss: 0.017048358917236328\n",
      "iteration 1784, dc_loss: 0.018787341192364693, tv_loss: 0.01703713648021221\n",
      "iteration 1785, dc_loss: 0.018773924559354782, tv_loss: 0.01705016754567623\n",
      "iteration 1786, dc_loss: 0.018784798681735992, tv_loss: 0.017038820311427116\n",
      "iteration 1787, dc_loss: 0.01877792924642563, tv_loss: 0.017045270651578903\n",
      "iteration 1788, dc_loss: 0.01877852901816368, tv_loss: 0.01704440265893936\n",
      "iteration 1789, dc_loss: 0.018781717866659164, tv_loss: 0.017041055485606194\n",
      "iteration 1790, dc_loss: 0.01877633109688759, tv_loss: 0.01704634353518486\n",
      "iteration 1791, dc_loss: 0.018781865015625954, tv_loss: 0.01704072393476963\n",
      "iteration 1792, dc_loss: 0.01877691224217415, tv_loss: 0.017045490443706512\n",
      "iteration 1793, dc_loss: 0.018780017271637917, tv_loss: 0.017042022198438644\n",
      "iteration 1794, dc_loss: 0.018778139725327492, tv_loss: 0.017043447121977806\n",
      "iteration 1795, dc_loss: 0.018775908276438713, tv_loss: 0.017045294865965843\n",
      "iteration 1796, dc_loss: 0.0187811441719532, tv_loss: 0.017039703205227852\n",
      "iteration 1797, dc_loss: 0.018772276118397713, tv_loss: 0.017048226669430733\n",
      "iteration 1798, dc_loss: 0.01878219097852707, tv_loss: 0.01703806407749653\n",
      "iteration 1799, dc_loss: 0.01877213455736637, tv_loss: 0.017047913745045662\n",
      "iteration 1800, dc_loss: 0.018780527636408806, tv_loss: 0.017039259895682335\n",
      "iteration 1801, dc_loss: 0.01877332478761673, tv_loss: 0.01704614982008934\n",
      "iteration 1802, dc_loss: 0.01877806894481182, tv_loss: 0.017041046172380447\n",
      "iteration 1803, dc_loss: 0.0187741257250309, tv_loss: 0.017044570297002792\n",
      "iteration 1804, dc_loss: 0.018775932490825653, tv_loss: 0.01704234816133976\n",
      "iteration 1805, dc_loss: 0.018773987889289856, tv_loss: 0.017044005915522575\n",
      "iteration 1806, dc_loss: 0.01877622678875923, tv_loss: 0.017041657119989395\n",
      "iteration 1807, dc_loss: 0.01877198927104473, tv_loss: 0.01704588159918785\n",
      "iteration 1808, dc_loss: 0.018777912482619286, tv_loss: 0.01703997142612934\n",
      "iteration 1809, dc_loss: 0.01877186819911003, tv_loss: 0.017045708373188972\n",
      "iteration 1810, dc_loss: 0.018775131553411484, tv_loss: 0.017041904851794243\n",
      "iteration 1811, dc_loss: 0.018773632124066353, tv_loss: 0.017042797058820724\n",
      "iteration 1812, dc_loss: 0.01877165399491787, tv_loss: 0.017044272273778915\n",
      "iteration 1813, dc_loss: 0.018774790689349174, tv_loss: 0.017040738835930824\n",
      "iteration 1814, dc_loss: 0.018770715221762657, tv_loss: 0.01704450137913227\n",
      "iteration 1815, dc_loss: 0.01877325400710106, tv_loss: 0.017041677609086037\n",
      "iteration 1816, dc_loss: 0.018771477043628693, tv_loss: 0.0170432198792696\n",
      "iteration 1817, dc_loss: 0.018770642578601837, tv_loss: 0.017043916508555412\n",
      "iteration 1818, dc_loss: 0.018774528056383133, tv_loss: 0.017040027305483818\n",
      "iteration 1819, dc_loss: 0.01876607909798622, tv_loss: 0.017048489302396774\n",
      "iteration 1820, dc_loss: 0.018779663369059563, tv_loss: 0.01703479327261448\n",
      "iteration 1821, dc_loss: 0.01876271888613701, tv_loss: 0.017051460221409798\n",
      "iteration 1822, dc_loss: 0.018777860328555107, tv_loss: 0.017035838216543198\n",
      "iteration 1823, dc_loss: 0.01876647397875786, tv_loss: 0.017046762630343437\n",
      "iteration 1824, dc_loss: 0.018771223723888397, tv_loss: 0.017041662707924843\n",
      "iteration 1825, dc_loss: 0.018770603463053703, tv_loss: 0.01704198308289051\n",
      "iteration 1826, dc_loss: 0.018767986446619034, tv_loss: 0.017044194042682648\n",
      "iteration 1827, dc_loss: 0.018770497292280197, tv_loss: 0.017041189596056938\n",
      "iteration 1828, dc_loss: 0.018768124282360077, tv_loss: 0.017043067142367363\n",
      "iteration 1829, dc_loss: 0.01876780018210411, tv_loss: 0.01704307086765766\n",
      "iteration 1830, dc_loss: 0.018770594149827957, tv_loss: 0.017040200531482697\n",
      "iteration 1831, dc_loss: 0.01876421831548214, tv_loss: 0.01704678125679493\n",
      "iteration 1832, dc_loss: 0.01877320371568203, tv_loss: 0.017037997022271156\n",
      "iteration 1833, dc_loss: 0.018763551488518715, tv_loss: 0.017047220841050148\n",
      "iteration 1834, dc_loss: 0.018770795315504074, tv_loss: 0.017039470374584198\n",
      "iteration 1835, dc_loss: 0.018766911700367928, tv_loss: 0.017042921856045723\n",
      "iteration 1836, dc_loss: 0.018764372915029526, tv_loss: 0.017045246437191963\n",
      "iteration 1837, dc_loss: 0.01877237856388092, tv_loss: 0.0170371662825346\n",
      "iteration 1838, dc_loss: 0.01876041293144226, tv_loss: 0.017048852518200874\n",
      "iteration 1839, dc_loss: 0.018771694973111153, tv_loss: 0.017036955803632736\n",
      "iteration 1840, dc_loss: 0.018762538209557533, tv_loss: 0.017045482993125916\n",
      "iteration 1841, dc_loss: 0.018765846267342567, tv_loss: 0.017041711136698723\n",
      "iteration 1842, dc_loss: 0.018767137080430984, tv_loss: 0.017040202394127846\n",
      "iteration 1843, dc_loss: 0.01876116171479225, tv_loss: 0.01704629324376583\n",
      "iteration 1844, dc_loss: 0.018770450726151466, tv_loss: 0.017037229612469673\n",
      "iteration 1845, dc_loss: 0.018758133053779602, tv_loss: 0.017049042508006096\n",
      "iteration 1846, dc_loss: 0.018770495429635048, tv_loss: 0.017036067321896553\n",
      "iteration 1847, dc_loss: 0.01875980943441391, tv_loss: 0.017046289518475533\n",
      "iteration 1848, dc_loss: 0.018765201792120934, tv_loss: 0.017040634527802467\n",
      "iteration 1849, dc_loss: 0.01876348815858364, tv_loss: 0.01704232580959797\n",
      "iteration 1850, dc_loss: 0.01876283995807171, tv_loss: 0.017042821273207664\n",
      "iteration 1851, dc_loss: 0.01876298151910305, tv_loss: 0.017042405903339386\n",
      "iteration 1852, dc_loss: 0.018765022978186607, tv_loss: 0.01704011671245098\n",
      "iteration 1853, dc_loss: 0.018759140744805336, tv_loss: 0.017045902088284492\n",
      "iteration 1854, dc_loss: 0.018769176676869392, tv_loss: 0.017035918310284615\n",
      "iteration 1855, dc_loss: 0.01875460334122181, tv_loss: 0.017050670459866524\n",
      "iteration 1856, dc_loss: 0.01877244934439659, tv_loss: 0.017032712697982788\n",
      "iteration 1857, dc_loss: 0.01875351369380951, tv_loss: 0.01705067791044712\n",
      "iteration 1858, dc_loss: 0.01876705512404442, tv_loss: 0.017036186531186104\n",
      "iteration 1859, dc_loss: 0.018758678808808327, tv_loss: 0.017043916508555412\n",
      "iteration 1860, dc_loss: 0.018759556114673615, tv_loss: 0.017042847350239754\n",
      "iteration 1861, dc_loss: 0.01876330003142357, tv_loss: 0.0170392207801342\n",
      "iteration 1862, dc_loss: 0.01875637099146843, tv_loss: 0.01704607717692852\n",
      "iteration 1863, dc_loss: 0.018764417618513107, tv_loss: 0.017037834972143173\n",
      "iteration 1864, dc_loss: 0.018756898120045662, tv_loss: 0.017045404762029648\n",
      "iteration 1865, dc_loss: 0.018763268366456032, tv_loss: 0.017039213329553604\n",
      "iteration 1866, dc_loss: 0.018759101629257202, tv_loss: 0.017043529078364372\n",
      "iteration 1867, dc_loss: 0.018760401755571365, tv_loss: 0.01704186387360096\n",
      "iteration 1868, dc_loss: 0.018760256469249725, tv_loss: 0.017041044309735298\n",
      "iteration 1869, dc_loss: 0.018756620585918427, tv_loss: 0.01704368181526661\n",
      "iteration 1870, dc_loss: 0.018761033192276955, tv_loss: 0.017038719728589058\n",
      "iteration 1871, dc_loss: 0.01875435560941696, tv_loss: 0.01704537309706211\n",
      "iteration 1872, dc_loss: 0.01876216009259224, tv_loss: 0.017037758603692055\n",
      "iteration 1873, dc_loss: 0.018752261996269226, tv_loss: 0.01704762689769268\n",
      "iteration 1874, dc_loss: 0.018765361979603767, tv_loss: 0.01703447289764881\n",
      "iteration 1875, dc_loss: 0.018750354647636414, tv_loss: 0.0170493945479393\n",
      "iteration 1876, dc_loss: 0.018765222281217575, tv_loss: 0.017034262418746948\n",
      "iteration 1877, dc_loss: 0.018750622868537903, tv_loss: 0.01704845204949379\n",
      "iteration 1878, dc_loss: 0.018762098625302315, tv_loss: 0.017036322504281998\n",
      "iteration 1879, dc_loss: 0.018752414733171463, tv_loss: 0.017045138403773308\n",
      "iteration 1880, dc_loss: 0.018757110461592674, tv_loss: 0.017039818689227104\n",
      "iteration 1881, dc_loss: 0.01875518448650837, tv_loss: 0.017041515558958054\n",
      "iteration 1882, dc_loss: 0.018753981217741966, tv_loss: 0.017042802646756172\n",
      "iteration 1883, dc_loss: 0.018757358193397522, tv_loss: 0.017039617523550987\n",
      "iteration 1884, dc_loss: 0.018753021955490112, tv_loss: 0.01704409345984459\n",
      "iteration 1885, dc_loss: 0.018758002668619156, tv_loss: 0.017039094120264053\n",
      "iteration 1886, dc_loss: 0.01875278539955616, tv_loss: 0.017044108361005783\n",
      "iteration 1887, dc_loss: 0.01875794492661953, tv_loss: 0.017038527876138687\n",
      "iteration 1888, dc_loss: 0.018751297146081924, tv_loss: 0.017044639214873314\n",
      "iteration 1889, dc_loss: 0.018756607547402382, tv_loss: 0.01703859679400921\n",
      "iteration 1890, dc_loss: 0.01875125989317894, tv_loss: 0.01704327017068863\n",
      "iteration 1891, dc_loss: 0.018753575161099434, tv_loss: 0.01704041287302971\n",
      "iteration 1892, dc_loss: 0.01875239610671997, tv_loss: 0.017041286453604698\n",
      "iteration 1893, dc_loss: 0.018751204013824463, tv_loss: 0.017042366787791252\n",
      "iteration 1894, dc_loss: 0.01875581219792366, tv_loss: 0.017037823796272278\n",
      "iteration 1895, dc_loss: 0.01874559000134468, tv_loss: 0.01704832911491394\n",
      "iteration 1896, dc_loss: 0.018762748688459396, tv_loss: 0.017031414434313774\n",
      "iteration 1897, dc_loss: 0.01874120905995369, tv_loss: 0.017052913084626198\n",
      "iteration 1898, dc_loss: 0.018763689324259758, tv_loss: 0.017030110582709312\n",
      "iteration 1899, dc_loss: 0.018744084984064102, tv_loss: 0.017049243673682213\n",
      "iteration 1900, dc_loss: 0.018756235018372536, tv_loss: 0.01703660562634468\n",
      "iteration 1901, dc_loss: 0.018749069422483444, tv_loss: 0.017043309286236763\n",
      "iteration 1902, dc_loss: 0.018750857561826706, tv_loss: 0.017040956765413284\n",
      "iteration 1903, dc_loss: 0.01875106431543827, tv_loss: 0.017040075734257698\n",
      "iteration 1904, dc_loss: 0.018748681992292404, tv_loss: 0.017041806131601334\n",
      "iteration 1905, dc_loss: 0.018749242648482323, tv_loss: 0.017040852457284927\n",
      "iteration 1906, dc_loss: 0.018750572577118874, tv_loss: 0.01703949272632599\n",
      "iteration 1907, dc_loss: 0.018746042624115944, tv_loss: 0.017044393345713615\n",
      "iteration 1908, dc_loss: 0.01875373348593712, tv_loss: 0.017037002369761467\n",
      "iteration 1909, dc_loss: 0.018745146691799164, tv_loss: 0.01704532653093338\n",
      "iteration 1910, dc_loss: 0.01875247061252594, tv_loss: 0.017037611454725266\n",
      "iteration 1911, dc_loss: 0.018748003989458084, tv_loss: 0.017041681334376335\n",
      "iteration 1912, dc_loss: 0.018745699897408485, tv_loss: 0.017043713480234146\n",
      "iteration 1913, dc_loss: 0.018753590062260628, tv_loss: 0.01703559421002865\n",
      "iteration 1914, dc_loss: 0.018740683794021606, tv_loss: 0.01704811304807663\n",
      "iteration 1915, dc_loss: 0.018755268305540085, tv_loss: 0.01703307405114174\n",
      "iteration 1916, dc_loss: 0.01873994804918766, tv_loss: 0.017047816887497902\n",
      "iteration 1917, dc_loss: 0.018752528354525566, tv_loss: 0.017034724354743958\n",
      "iteration 1918, dc_loss: 0.018743028864264488, tv_loss: 0.017043838277459145\n",
      "iteration 1919, dc_loss: 0.01874610222876072, tv_loss: 0.017040694132447243\n",
      "iteration 1920, dc_loss: 0.01874825358390808, tv_loss: 0.017038606107234955\n",
      "iteration 1921, dc_loss: 0.018741073086857796, tv_loss: 0.017045583575963974\n",
      "iteration 1922, dc_loss: 0.01875273324549198, tv_loss: 0.017033729702234268\n",
      "iteration 1923, dc_loss: 0.01873847469687462, tv_loss: 0.017047753557562828\n",
      "iteration 1924, dc_loss: 0.0187503844499588, tv_loss: 0.01703559048473835\n",
      "iteration 1925, dc_loss: 0.01874164305627346, tv_loss: 0.017043793573975563\n",
      "iteration 1926, dc_loss: 0.018744753673672676, tv_loss: 0.017040126025676727\n",
      "iteration 1927, dc_loss: 0.01874629780650139, tv_loss: 0.0170382559299469\n",
      "iteration 1928, dc_loss: 0.01873953267931938, tv_loss: 0.017044981941580772\n",
      "iteration 1929, dc_loss: 0.018750004470348358, tv_loss: 0.017034726217389107\n",
      "iteration 1930, dc_loss: 0.01873784326016903, tv_loss: 0.017047343775629997\n",
      "iteration 1931, dc_loss: 0.01875205524265766, tv_loss: 0.017033593729138374\n",
      "iteration 1932, dc_loss: 0.01873861253261566, tv_loss: 0.017047541216015816\n",
      "iteration 1933, dc_loss: 0.01875261217355728, tv_loss: 0.017033729702234268\n",
      "iteration 1934, dc_loss: 0.018739093095064163, tv_loss: 0.017047030851244926\n",
      "iteration 1935, dc_loss: 0.018751315772533417, tv_loss: 0.017033996060490608\n",
      "iteration 1936, dc_loss: 0.01873752661049366, tv_loss: 0.0170466098934412\n",
      "iteration 1937, dc_loss: 0.018747864291071892, tv_loss: 0.017034942284226418\n",
      "iteration 1938, dc_loss: 0.01873723976314068, tv_loss: 0.01704459637403488\n",
      "iteration 1939, dc_loss: 0.018744032829999924, tv_loss: 0.01703728549182415\n",
      "iteration 1940, dc_loss: 0.018740056082606316, tv_loss: 0.01704111322760582\n",
      "iteration 1941, dc_loss: 0.018739785999059677, tv_loss: 0.01704147458076477\n",
      "iteration 1942, dc_loss: 0.0187455415725708, tv_loss: 0.017036031931638718\n",
      "iteration 1943, dc_loss: 0.018732789903879166, tv_loss: 0.017049307003617287\n",
      "iteration 1944, dc_loss: 0.018754661083221436, tv_loss: 0.01702774316072464\n",
      "iteration 1945, dc_loss: 0.018727155402302742, tv_loss: 0.01705518737435341\n",
      "iteration 1946, dc_loss: 0.01875581033527851, tv_loss: 0.017026040703058243\n",
      "iteration 1947, dc_loss: 0.01872990094125271, tv_loss: 0.01705108769237995\n",
      "iteration 1948, dc_loss: 0.01874578930437565, tv_loss: 0.017034415155649185\n",
      "iteration 1949, dc_loss: 0.018737468868494034, tv_loss: 0.017041929066181183\n",
      "iteration 1950, dc_loss: 0.018737155944108963, tv_loss: 0.017041636630892754\n",
      "iteration 1951, dc_loss: 0.018741929903626442, tv_loss: 0.01703636348247528\n",
      "iteration 1952, dc_loss: 0.01873410865664482, tv_loss: 0.01704377308487892\n",
      "iteration 1953, dc_loss: 0.01874062977731228, tv_loss: 0.017037037760019302\n",
      "iteration 1954, dc_loss: 0.018735835328698158, tv_loss: 0.01704193279147148\n",
      "iteration 1955, dc_loss: 0.018738150596618652, tv_loss: 0.017039941623806953\n",
      "iteration 1956, dc_loss: 0.018738694489002228, tv_loss: 0.017039699479937553\n",
      "iteration 1957, dc_loss: 0.018737668171525, tv_loss: 0.01704096607863903\n",
      "iteration 1958, dc_loss: 0.018739596009254456, tv_loss: 0.01703914999961853\n",
      "iteration 1959, dc_loss: 0.01873861439526081, tv_loss: 0.017039814963936806\n",
      "iteration 1960, dc_loss: 0.01873498409986496, tv_loss: 0.01704276166856289\n",
      "iteration 1961, dc_loss: 0.01874193549156189, tv_loss: 0.01703496277332306\n",
      "iteration 1962, dc_loss: 0.01872936263680458, tv_loss: 0.017046883702278137\n",
      "iteration 1963, dc_loss: 0.018744120374321938, tv_loss: 0.017031660303473473\n",
      "iteration 1964, dc_loss: 0.018727662041783333, tv_loss: 0.017047706991434097\n",
      "iteration 1965, dc_loss: 0.01874188706278801, tv_loss: 0.017033010721206665\n",
      "iteration 1966, dc_loss: 0.01873036101460457, tv_loss: 0.017044074833393097\n",
      "iteration 1967, dc_loss: 0.018735123798251152, tv_loss: 0.017038945108652115\n",
      "iteration 1968, dc_loss: 0.018736500293016434, tv_loss: 0.017037486657500267\n",
      "iteration 1969, dc_loss: 0.018728027120232582, tv_loss: 0.017046041786670685\n",
      "iteration 1970, dc_loss: 0.018743742257356644, tv_loss: 0.017030537128448486\n",
      "iteration 1971, dc_loss: 0.01872304268181324, tv_loss: 0.017051206901669502\n",
      "iteration 1972, dc_loss: 0.01874556392431259, tv_loss: 0.01702841743826866\n",
      "iteration 1973, dc_loss: 0.018724482506513596, tv_loss: 0.017049090936779976\n",
      "iteration 1974, dc_loss: 0.01873989962041378, tv_loss: 0.017033439129590988\n",
      "iteration 1975, dc_loss: 0.018729733303189278, tv_loss: 0.01704373024404049\n",
      "iteration 1976, dc_loss: 0.018736641854047775, tv_loss: 0.01703724078834057\n",
      "iteration 1977, dc_loss: 0.018732216209173203, tv_loss: 0.01704191043972969\n",
      "iteration 1978, dc_loss: 0.01873844675719738, tv_loss: 0.017035510390996933\n",
      "iteration 1979, dc_loss: 0.01872703991830349, tv_loss: 0.017046557739377022\n",
      "iteration 1980, dc_loss: 0.018744206055998802, tv_loss: 0.017029115930199623\n",
      "iteration 1981, dc_loss: 0.018719635903835297, tv_loss: 0.017053475603461266\n",
      "iteration 1982, dc_loss: 0.018747465685009956, tv_loss: 0.01702508144080639\n",
      "iteration 1983, dc_loss: 0.018719231709837914, tv_loss: 0.017052320763468742\n",
      "iteration 1984, dc_loss: 0.01873987913131714, tv_loss: 0.017030587419867516\n",
      "iteration 1985, dc_loss: 0.018726924434304237, tv_loss: 0.017042746767401695\n",
      "iteration 1986, dc_loss: 0.018727663904428482, tv_loss: 0.017041722312569618\n",
      "iteration 1987, dc_loss: 0.01873556151986122, tv_loss: 0.017033753916621208\n",
      "iteration 1988, dc_loss: 0.018722157925367355, tv_loss: 0.017047157511115074\n",
      "iteration 1989, dc_loss: 0.0187385156750679, tv_loss: 0.017030850052833557\n",
      "iteration 1990, dc_loss: 0.018722817301750183, tv_loss: 0.017046863213181496\n",
      "iteration 1991, dc_loss: 0.01873667538166046, tv_loss: 0.017033651471138\n",
      "iteration 1992, dc_loss: 0.01872715912759304, tv_loss: 0.01704396866261959\n",
      "iteration 1993, dc_loss: 0.01873534545302391, tv_loss: 0.01703592948615551\n",
      "iteration 1994, dc_loss: 0.01872795633971691, tv_loss: 0.017042743042111397\n",
      "iteration 1995, dc_loss: 0.01873406581580639, tv_loss: 0.017035456374287605\n",
      "iteration 1996, dc_loss: 0.01872469298541546, tv_loss: 0.017043551430106163\n",
      "iteration 1997, dc_loss: 0.018732579424977303, tv_loss: 0.0170346237719059\n",
      "iteration 1998, dc_loss: 0.018722645938396454, tv_loss: 0.01704389974474907\n",
      "iteration 1999, dc_loss: 0.01873128116130829, tv_loss: 0.017034845426678658\n",
      "iteration 2000, dc_loss: 0.018723785877227783, tv_loss: 0.017042092978954315\n",
      "iteration 2001, dc_loss: 0.018727760761976242, tv_loss: 0.01703793741762638\n",
      "iteration 2002, dc_loss: 0.018727893009781837, tv_loss: 0.017037006095051765\n",
      "iteration 2003, dc_loss: 0.018722953274846077, tv_loss: 0.01704133301973343\n",
      "iteration 2004, dc_loss: 0.018728194758296013, tv_loss: 0.017036233097314835\n",
      "iteration 2005, dc_loss: 0.018725989386439323, tv_loss: 0.017038360238075256\n",
      "iteration 2006, dc_loss: 0.018723124638199806, tv_loss: 0.017040882259607315\n",
      "iteration 2007, dc_loss: 0.018728870898485184, tv_loss: 0.01703493669629097\n",
      "iteration 2008, dc_loss: 0.018723011016845703, tv_loss: 0.01704041287302971\n",
      "iteration 2009, dc_loss: 0.018724028021097183, tv_loss: 0.0170389823615551\n",
      "iteration 2010, dc_loss: 0.01872820407152176, tv_loss: 0.01703467033803463\n",
      "iteration 2011, dc_loss: 0.018721463158726692, tv_loss: 0.01704138144850731\n",
      "iteration 2012, dc_loss: 0.018725479021668434, tv_loss: 0.017037244513630867\n",
      "iteration 2013, dc_loss: 0.018725844100117683, tv_loss: 0.017036767676472664\n",
      "iteration 2014, dc_loss: 0.01872124709188938, tv_loss: 0.017041070386767387\n",
      "iteration 2015, dc_loss: 0.018725447356700897, tv_loss: 0.017036473378539085\n",
      "iteration 2016, dc_loss: 0.018724113702774048, tv_loss: 0.017037469893693924\n",
      "iteration 2017, dc_loss: 0.018721798434853554, tv_loss: 0.017039544880390167\n",
      "iteration 2018, dc_loss: 0.018725164234638214, tv_loss: 0.017035994678735733\n",
      "iteration 2019, dc_loss: 0.01872168853878975, tv_loss: 0.017039312049746513\n",
      "iteration 2020, dc_loss: 0.018723882734775543, tv_loss: 0.017036978155374527\n",
      "iteration 2021, dc_loss: 0.018722694367170334, tv_loss: 0.01703815720975399\n",
      "iteration 2022, dc_loss: 0.018721409142017365, tv_loss: 0.017039502039551735\n",
      "iteration 2023, dc_loss: 0.01872519962489605, tv_loss: 0.01703552156686783\n",
      "iteration 2024, dc_loss: 0.018718725070357323, tv_loss: 0.017041612416505814\n",
      "iteration 2025, dc_loss: 0.01872466877102852, tv_loss: 0.017035264521837234\n",
      "iteration 2026, dc_loss: 0.018721632659435272, tv_loss: 0.017037993296980858\n",
      "iteration 2027, dc_loss: 0.018720176070928574, tv_loss: 0.01703927293419838\n",
      "iteration 2028, dc_loss: 0.01872309483587742, tv_loss: 0.017036257311701775\n",
      "iteration 2029, dc_loss: 0.018720468506217003, tv_loss: 0.017038878053426743\n",
      "iteration 2030, dc_loss: 0.018721356987953186, tv_loss: 0.017037957906723022\n",
      "iteration 2031, dc_loss: 0.01872129738330841, tv_loss: 0.01703779399394989\n",
      "iteration 2032, dc_loss: 0.018720155581831932, tv_loss: 0.017038654536008835\n",
      "iteration 2033, dc_loss: 0.018722444772720337, tv_loss: 0.017036087810993195\n",
      "iteration 2034, dc_loss: 0.018718058243393898, tv_loss: 0.017040221020579338\n",
      "iteration 2035, dc_loss: 0.01872231438755989, tv_loss: 0.017035789787769318\n",
      "iteration 2036, dc_loss: 0.018719390034675598, tv_loss: 0.017038587480783463\n",
      "iteration 2037, dc_loss: 0.018719565123319626, tv_loss: 0.01703830435872078\n",
      "iteration 2038, dc_loss: 0.018719885498285294, tv_loss: 0.017037726938724518\n",
      "iteration 2039, dc_loss: 0.018720297142863274, tv_loss: 0.017037011682987213\n",
      "iteration 2040, dc_loss: 0.018717976287007332, tv_loss: 0.017039088532328606\n",
      "iteration 2041, dc_loss: 0.018721090629696846, tv_loss: 0.017035849392414093\n",
      "iteration 2042, dc_loss: 0.018716931343078613, tv_loss: 0.017039936035871506\n",
      "iteration 2043, dc_loss: 0.018720712512731552, tv_loss: 0.01703605055809021\n",
      "iteration 2044, dc_loss: 0.018718231469392776, tv_loss: 0.0170383770018816\n",
      "iteration 2045, dc_loss: 0.018718000501394272, tv_loss: 0.017038332298398018\n",
      "iteration 2046, dc_loss: 0.018719181418418884, tv_loss: 0.0170368030667305\n",
      "iteration 2047, dc_loss: 0.018717389553785324, tv_loss: 0.01703828014433384\n",
      "iteration 2048, dc_loss: 0.01871810108423233, tv_loss: 0.017037296667695045\n",
      "iteration 2049, dc_loss: 0.01871856115758419, tv_loss: 0.01703665405511856\n",
      "iteration 2050, dc_loss: 0.0187159925699234, tv_loss: 0.017039114609360695\n",
      "iteration 2051, dc_loss: 0.018719563260674477, tv_loss: 0.017035601660609245\n",
      "iteration 2052, dc_loss: 0.01871465891599655, tv_loss: 0.017040545120835304\n",
      "iteration 2053, dc_loss: 0.018719928339123726, tv_loss: 0.017035018652677536\n",
      "iteration 2054, dc_loss: 0.01871543936431408, tv_loss: 0.017039069905877113\n",
      "iteration 2055, dc_loss: 0.01871652528643608, tv_loss: 0.0170376505702734\n",
      "iteration 2056, dc_loss: 0.018718430772423744, tv_loss: 0.01703551784157753\n",
      "iteration 2057, dc_loss: 0.018714239820837975, tv_loss: 0.01703956164419651\n",
      "iteration 2058, dc_loss: 0.01871749572455883, tv_loss: 0.017036162316799164\n",
      "iteration 2059, dc_loss: 0.01871638372540474, tv_loss: 0.017037222161889076\n",
      "iteration 2060, dc_loss: 0.01871461234986782, tv_loss: 0.01703893207013607\n",
      "iteration 2061, dc_loss: 0.01871740259230137, tv_loss: 0.017035892233252525\n",
      "iteration 2062, dc_loss: 0.018713241443037987, tv_loss: 0.017039773985743523\n",
      "iteration 2063, dc_loss: 0.01871822029352188, tv_loss: 0.017034487798810005\n",
      "iteration 2064, dc_loss: 0.018713241443037987, tv_loss: 0.0170392245054245\n",
      "iteration 2065, dc_loss: 0.018715346232056618, tv_loss: 0.01703696884214878\n",
      "iteration 2066, dc_loss: 0.01871509477496147, tv_loss: 0.017037106677889824\n",
      "iteration 2067, dc_loss: 0.018714288249611855, tv_loss: 0.01703779026865959\n",
      "iteration 2068, dc_loss: 0.018714144825935364, tv_loss: 0.017037687823176384\n",
      "iteration 2069, dc_loss: 0.018715571612119675, tv_loss: 0.017036015167832375\n",
      "iteration 2070, dc_loss: 0.018711799755692482, tv_loss: 0.017039595171809196\n",
      "iteration 2071, dc_loss: 0.01871703378856182, tv_loss: 0.01703423075377941\n",
      "iteration 2072, dc_loss: 0.018710928037762642, tv_loss: 0.017040185630321503\n",
      "iteration 2073, dc_loss: 0.018715715035796165, tv_loss: 0.017035244032740593\n",
      "iteration 2074, dc_loss: 0.018712302669882774, tv_loss: 0.01703847572207451\n",
      "iteration 2075, dc_loss: 0.01871330291032791, tv_loss: 0.01703718863427639\n",
      "iteration 2076, dc_loss: 0.018713869154453278, tv_loss: 0.017036326229572296\n",
      "iteration 2077, dc_loss: 0.0187121219933033, tv_loss: 0.017037875950336456\n",
      "iteration 2078, dc_loss: 0.018712887540459633, tv_loss: 0.01703702285885811\n",
      "iteration 2079, dc_loss: 0.018713312223553658, tv_loss: 0.01703653484582901\n",
      "iteration 2080, dc_loss: 0.01871124655008316, tv_loss: 0.017038509249687195\n",
      "iteration 2081, dc_loss: 0.01871427148580551, tv_loss: 0.017035335302352905\n",
      "iteration 2082, dc_loss: 0.018709778785705566, tv_loss: 0.01703949272632599\n",
      "iteration 2083, dc_loss: 0.018713895231485367, tv_loss: 0.01703505963087082\n",
      "iteration 2084, dc_loss: 0.018710363656282425, tv_loss: 0.017038343474268913\n",
      "iteration 2085, dc_loss: 0.01871228963136673, tv_loss: 0.017036197707057\n",
      "iteration 2086, dc_loss: 0.018710516393184662, tv_loss: 0.0170377679169178\n",
      "iteration 2087, dc_loss: 0.018712511286139488, tv_loss: 0.017035597935318947\n",
      "iteration 2088, dc_loss: 0.01870933175086975, tv_loss: 0.017038710415363312\n",
      "iteration 2089, dc_loss: 0.018712662160396576, tv_loss: 0.017035353928804398\n",
      "iteration 2090, dc_loss: 0.018708743155002594, tv_loss: 0.01703912578523159\n",
      "iteration 2091, dc_loss: 0.018712354823946953, tv_loss: 0.017035242170095444\n",
      "iteration 2092, dc_loss: 0.018709354102611542, tv_loss: 0.017037924379110336\n",
      "iteration 2093, dc_loss: 0.018710052594542503, tv_loss: 0.017036952078342438\n",
      "iteration 2094, dc_loss: 0.018710967153310776, tv_loss: 0.017035815864801407\n",
      "iteration 2095, dc_loss: 0.018708400428295135, tv_loss: 0.017038213089108467\n",
      "iteration 2096, dc_loss: 0.018710292875766754, tv_loss: 0.017036156728863716\n",
      "iteration 2097, dc_loss: 0.01870962604880333, tv_loss: 0.017036674544215202\n",
      "iteration 2098, dc_loss: 0.01870780438184738, tv_loss: 0.01703834906220436\n",
      "iteration 2099, dc_loss: 0.018711932003498077, tv_loss: 0.01703411526978016\n",
      "iteration 2100, dc_loss: 0.018705269321799278, tv_loss: 0.017040640115737915\n",
      "iteration 2101, dc_loss: 0.018712367862462997, tv_loss: 0.017033318057656288\n",
      "iteration 2102, dc_loss: 0.018706107512116432, tv_loss: 0.017039289698004723\n",
      "iteration 2103, dc_loss: 0.018709231168031693, tv_loss: 0.017035922035574913\n",
      "iteration 2104, dc_loss: 0.018708929419517517, tv_loss: 0.017036018893122673\n",
      "iteration 2105, dc_loss: 0.018706535920500755, tv_loss: 0.01703823357820511\n",
      "iteration 2106, dc_loss: 0.01870894618332386, tv_loss: 0.0170355886220932\n",
      "iteration 2107, dc_loss: 0.018707916140556335, tv_loss: 0.017036378383636475\n",
      "iteration 2108, dc_loss: 0.01870613545179367, tv_loss: 0.017037957906723022\n",
      "iteration 2109, dc_loss: 0.018709583207964897, tv_loss: 0.01703440211713314\n",
      "iteration 2110, dc_loss: 0.018704231828451157, tv_loss: 0.017039727419614792\n",
      "iteration 2111, dc_loss: 0.01871066726744175, tv_loss: 0.01703326590359211\n",
      "iteration 2112, dc_loss: 0.01870403252542019, tv_loss: 0.017039621248841286\n",
      "iteration 2113, dc_loss: 0.01870792917907238, tv_loss: 0.017035365104675293\n",
      "iteration 2114, dc_loss: 0.018706606701016426, tv_loss: 0.017036406323313713\n",
      "iteration 2115, dc_loss: 0.01870512031018734, tv_loss: 0.01703771948814392\n",
      "iteration 2116, dc_loss: 0.018707724288105965, tv_loss: 0.017035016790032387\n",
      "iteration 2117, dc_loss: 0.01870519295334816, tv_loss: 0.017037492245435715\n",
      "iteration 2118, dc_loss: 0.01870601624250412, tv_loss: 0.017036566510796547\n",
      "iteration 2119, dc_loss: 0.01870637759566307, tv_loss: 0.017036031931638718\n",
      "iteration 2120, dc_loss: 0.018703708425164223, tv_loss: 0.017038408666849136\n",
      "iteration 2121, dc_loss: 0.018708571791648865, tv_loss: 0.017033284530043602\n",
      "iteration 2122, dc_loss: 0.01870157942175865, tv_loss: 0.017040086910128593\n",
      "iteration 2123, dc_loss: 0.018707824870944023, tv_loss: 0.01703367568552494\n",
      "iteration 2124, dc_loss: 0.018703244626522064, tv_loss: 0.017038075253367424\n",
      "iteration 2125, dc_loss: 0.018705306574702263, tv_loss: 0.017035825178027153\n",
      "iteration 2126, dc_loss: 0.018704194575548172, tv_loss: 0.01703673042356968\n",
      "iteration 2127, dc_loss: 0.01870446465909481, tv_loss: 0.017036210745573044\n",
      "iteration 2128, dc_loss: 0.01870342157781124, tv_loss: 0.017037024721503258\n",
      "iteration 2129, dc_loss: 0.018705252557992935, tv_loss: 0.017035087570548058\n",
      "iteration 2130, dc_loss: 0.01870221272110939, tv_loss: 0.01703806221485138\n",
      "iteration 2131, dc_loss: 0.01870580203831196, tv_loss: 0.017034398391842842\n",
      "iteration 2132, dc_loss: 0.018701625987887383, tv_loss: 0.017038362100720406\n",
      "iteration 2133, dc_loss: 0.018704377114772797, tv_loss: 0.017035318538546562\n",
      "iteration 2134, dc_loss: 0.018703138455748558, tv_loss: 0.017036203294992447\n",
      "iteration 2135, dc_loss: 0.018701957538723946, tv_loss: 0.017037110403180122\n",
      "iteration 2136, dc_loss: 0.01870369352400303, tv_loss: 0.01703512668609619\n",
      "iteration 2137, dc_loss: 0.018702363595366478, tv_loss: 0.01703624054789543\n",
      "iteration 2138, dc_loss: 0.018701104447245598, tv_loss: 0.017037343233823776\n",
      "iteration 2139, dc_loss: 0.018704820424318314, tv_loss: 0.017033586278557777\n",
      "iteration 2140, dc_loss: 0.018698588013648987, tv_loss: 0.017039896920323372\n",
      "iteration 2141, dc_loss: 0.018706509843468666, tv_loss: 0.01703198254108429\n",
      "iteration 2142, dc_loss: 0.01869768090546131, tv_loss: 0.017040425911545753\n",
      "iteration 2143, dc_loss: 0.01870417408645153, tv_loss: 0.01703350804746151\n",
      "iteration 2144, dc_loss: 0.01870085671544075, tv_loss: 0.017036501318216324\n",
      "iteration 2145, dc_loss: 0.018700003623962402, tv_loss: 0.017037151381373405\n",
      "iteration 2146, dc_loss: 0.018703006207942963, tv_loss: 0.017034092918038368\n",
      "iteration 2147, dc_loss: 0.018699342384934425, tv_loss: 0.01703781448304653\n",
      "iteration 2148, dc_loss: 0.018701715394854546, tv_loss: 0.017035488039255142\n",
      "iteration 2149, dc_loss: 0.018700793385505676, tv_loss: 0.017036166042089462\n",
      "iteration 2150, dc_loss: 0.018700024113059044, tv_loss: 0.017036592587828636\n",
      "iteration 2151, dc_loss: 0.01870204694569111, tv_loss: 0.017034249380230904\n",
      "iteration 2152, dc_loss: 0.0186972264200449, tv_loss: 0.017038900405168533\n",
      "iteration 2153, dc_loss: 0.018703147768974304, tv_loss: 0.01703289896249771\n",
      "iteration 2154, dc_loss: 0.018697019666433334, tv_loss: 0.017038902267813683\n",
      "iteration 2155, dc_loss: 0.01870211772620678, tv_loss: 0.017033614218235016\n",
      "iteration 2156, dc_loss: 0.018697617575526237, tv_loss: 0.017037807032465935\n",
      "iteration 2157, dc_loss: 0.018700724467635155, tv_loss: 0.017034372314810753\n",
      "iteration 2158, dc_loss: 0.01869780384004116, tv_loss: 0.017037028446793556\n",
      "iteration 2159, dc_loss: 0.01869950070977211, tv_loss: 0.01703520677983761\n",
      "iteration 2160, dc_loss: 0.018698308616876602, tv_loss: 0.017036378383636475\n",
      "iteration 2161, dc_loss: 0.018699275329709053, tv_loss: 0.01703544519841671\n",
      "iteration 2162, dc_loss: 0.018697550520300865, tv_loss: 0.017037002369761467\n",
      "iteration 2163, dc_loss: 0.01869964599609375, tv_loss: 0.017034584656357765\n",
      "iteration 2164, dc_loss: 0.018696606159210205, tv_loss: 0.017037242650985718\n",
      "iteration 2165, dc_loss: 0.018698742613196373, tv_loss: 0.01703479513525963\n",
      "iteration 2166, dc_loss: 0.018697433173656464, tv_loss: 0.017035899683833122\n",
      "iteration 2167, dc_loss: 0.01869777962565422, tv_loss: 0.017035430297255516\n",
      "iteration 2168, dc_loss: 0.01869574375450611, tv_loss: 0.017037395387887955\n",
      "iteration 2169, dc_loss: 0.018700679764151573, tv_loss: 0.017032455652952194\n",
      "iteration 2170, dc_loss: 0.018692776560783386, tv_loss: 0.017040302976965904\n",
      "iteration 2171, dc_loss: 0.018701471388339996, tv_loss: 0.017031414434313774\n",
      "iteration 2172, dc_loss: 0.018692875280976295, tv_loss: 0.01703965663909912\n",
      "iteration 2173, dc_loss: 0.01869925856590271, tv_loss: 0.01703290455043316\n",
      "iteration 2174, dc_loss: 0.018695607781410217, tv_loss: 0.017036311328411102\n",
      "iteration 2175, dc_loss: 0.018695088103413582, tv_loss: 0.0170366819947958\n",
      "iteration 2176, dc_loss: 0.01869770884513855, tv_loss: 0.017034010961651802\n",
      "iteration 2177, dc_loss: 0.018694201484322548, tv_loss: 0.01703750528395176\n",
      "iteration 2178, dc_loss: 0.018697446212172508, tv_loss: 0.017034176737070084\n",
      "iteration 2179, dc_loss: 0.01869543455541134, tv_loss: 0.017036033794283867\n",
      "iteration 2180, dc_loss: 0.018694497644901276, tv_loss: 0.01703668385744095\n",
      "iteration 2181, dc_loss: 0.01869712769985199, tv_loss: 0.01703372597694397\n",
      "iteration 2182, dc_loss: 0.018692869693040848, tv_loss: 0.017037734389305115\n",
      "iteration 2183, dc_loss: 0.018697630614042282, tv_loss: 0.01703283004462719\n",
      "iteration 2184, dc_loss: 0.018691599369049072, tv_loss: 0.017038775607943535\n",
      "iteration 2185, dc_loss: 0.01869758404791355, tv_loss: 0.01703266054391861\n",
      "iteration 2186, dc_loss: 0.018692048266530037, tv_loss: 0.01703796163201332\n",
      "iteration 2187, dc_loss: 0.018696194514632225, tv_loss: 0.01703350991010666\n",
      "iteration 2188, dc_loss: 0.018692420795559883, tv_loss: 0.01703694276511669\n",
      "iteration 2189, dc_loss: 0.018695412203669548, tv_loss: 0.01703367568552494\n",
      "iteration 2190, dc_loss: 0.01869216188788414, tv_loss: 0.017036790028214455\n",
      "iteration 2191, dc_loss: 0.018694836646318436, tv_loss: 0.017034171149134636\n",
      "iteration 2192, dc_loss: 0.01869184710085392, tv_loss: 0.017037292942404747\n",
      "iteration 2193, dc_loss: 0.01869548298418522, tv_loss: 0.017033478245139122\n",
      "iteration 2194, dc_loss: 0.018691737204790115, tv_loss: 0.017036858946084976\n",
      "iteration 2195, dc_loss: 0.01869344897568226, tv_loss: 0.017034783959388733\n",
      "iteration 2196, dc_loss: 0.01869288645684719, tv_loss: 0.01703503169119358\n",
      "iteration 2197, dc_loss: 0.01869172789156437, tv_loss: 0.017036011442542076\n",
      "iteration 2198, dc_loss: 0.018693147227168083, tv_loss: 0.017034467309713364\n",
      "iteration 2199, dc_loss: 0.0186915285885334, tv_loss: 0.01703597605228424\n",
      "iteration 2200, dc_loss: 0.018691591918468475, tv_loss: 0.017035726457834244\n",
      "iteration 2201, dc_loss: 0.018693655729293823, tv_loss: 0.017033470794558525\n",
      "iteration 2202, dc_loss: 0.01868833228945732, tv_loss: 0.017038686200976372\n",
      "iteration 2203, dc_loss: 0.01869690790772438, tv_loss: 0.01703011803328991\n",
      "iteration 2204, dc_loss: 0.018685750663280487, tv_loss: 0.01704118773341179\n",
      "iteration 2205, dc_loss: 0.018696341663599014, tv_loss: 0.01703036203980446\n",
      "iteration 2206, dc_loss: 0.01868848130106926, tv_loss: 0.017037885263562202\n",
      "iteration 2207, dc_loss: 0.01869094930589199, tv_loss: 0.01703512668609619\n",
      "iteration 2208, dc_loss: 0.01869206130504608, tv_loss: 0.017033765092492104\n",
      "iteration 2209, dc_loss: 0.01868906058371067, tv_loss: 0.01703648455440998\n",
      "iteration 2210, dc_loss: 0.018691446632146835, tv_loss: 0.01703382283449173\n",
      "iteration 2211, dc_loss: 0.018689578399062157, tv_loss: 0.017035461962223053\n",
      "iteration 2212, dc_loss: 0.018689418211579323, tv_loss: 0.017035510390996933\n",
      "iteration 2213, dc_loss: 0.018691260367631912, tv_loss: 0.017033686861395836\n",
      "iteration 2214, dc_loss: 0.01868717186152935, tv_loss: 0.01703791692852974\n",
      "iteration 2215, dc_loss: 0.01869341731071472, tv_loss: 0.017031630501151085\n",
      "iteration 2216, dc_loss: 0.018686261028051376, tv_loss: 0.01703852228820324\n",
      "iteration 2217, dc_loss: 0.01869165152311325, tv_loss: 0.017032792791724205\n",
      "iteration 2218, dc_loss: 0.018689317628741264, tv_loss: 0.017034806311130524\n",
      "iteration 2219, dc_loss: 0.01868625544011593, tv_loss: 0.017037712037563324\n",
      "iteration 2220, dc_loss: 0.01869243197143078, tv_loss: 0.017031371593475342\n",
      "iteration 2221, dc_loss: 0.01868550106883049, tv_loss: 0.01703811250627041\n",
      "iteration 2222, dc_loss: 0.01869063451886177, tv_loss: 0.017032694071531296\n",
      "iteration 2223, dc_loss: 0.01868712157011032, tv_loss: 0.017035910859704018\n",
      "iteration 2224, dc_loss: 0.01868753507733345, tv_loss: 0.01703527942299843\n",
      "iteration 2225, dc_loss: 0.018689658492803574, tv_loss: 0.017033107578754425\n",
      "iteration 2226, dc_loss: 0.01868460141122341, tv_loss: 0.017038190737366676\n",
      "iteration 2227, dc_loss: 0.018692096695303917, tv_loss: 0.017030727118253708\n",
      "iteration 2228, dc_loss: 0.018683675676584244, tv_loss: 0.017038866877555847\n",
      "iteration 2229, dc_loss: 0.018689468502998352, tv_loss: 0.017032695934176445\n",
      "iteration 2230, dc_loss: 0.0186870489269495, tv_loss: 0.01703478954732418\n",
      "iteration 2231, dc_loss: 0.018684973940253258, tv_loss: 0.017036696895956993\n",
      "iteration 2232, dc_loss: 0.018689649179577827, tv_loss: 0.017031928524374962\n",
      "iteration 2233, dc_loss: 0.01868385076522827, tv_loss: 0.017037643119692802\n",
      "iteration 2234, dc_loss: 0.01868942193686962, tv_loss: 0.017031941562891006\n",
      "iteration 2235, dc_loss: 0.01868371106684208, tv_loss: 0.01703753136098385\n",
      "iteration 2236, dc_loss: 0.018688030540943146, tv_loss: 0.017033066600561142\n",
      "iteration 2237, dc_loss: 0.018685797229409218, tv_loss: 0.01703513227403164\n",
      "iteration 2238, dc_loss: 0.01868530549108982, tv_loss: 0.017035450786352158\n",
      "iteration 2239, dc_loss: 0.018687654286623, tv_loss: 0.017032913863658905\n",
      "iteration 2240, dc_loss: 0.018682202324271202, tv_loss: 0.017038114368915558\n",
      "iteration 2241, dc_loss: 0.01868944615125656, tv_loss: 0.017030619084835052\n",
      "iteration 2242, dc_loss: 0.018681449815630913, tv_loss: 0.01703839749097824\n",
      "iteration 2243, dc_loss: 0.01868799328804016, tv_loss: 0.017031697556376457\n",
      "iteration 2244, dc_loss: 0.01868264004588127, tv_loss: 0.017036892473697662\n",
      "iteration 2245, dc_loss: 0.018686994910240173, tv_loss: 0.017032422125339508\n",
      "iteration 2246, dc_loss: 0.018682386726140976, tv_loss: 0.017036866396665573\n",
      "iteration 2247, dc_loss: 0.01868688315153122, tv_loss: 0.01703214831650257\n",
      "iteration 2248, dc_loss: 0.01868145912885666, tv_loss: 0.017037348821759224\n",
      "iteration 2249, dc_loss: 0.018687795847654343, tv_loss: 0.017030879855155945\n",
      "iteration 2250, dc_loss: 0.018679630011320114, tv_loss: 0.017038961872458458\n",
      "iteration 2251, dc_loss: 0.018687911331653595, tv_loss: 0.017030488699674606\n",
      "iteration 2252, dc_loss: 0.018679989501833916, tv_loss: 0.017038125544786453\n",
      "iteration 2253, dc_loss: 0.018685350194573402, tv_loss: 0.01703251153230667\n",
      "iteration 2254, dc_loss: 0.018683774396777153, tv_loss: 0.017033899202942848\n",
      "iteration 2255, dc_loss: 0.018680928274989128, tv_loss: 0.017036693170666695\n",
      "iteration 2256, dc_loss: 0.018686536699533463, tv_loss: 0.017031051218509674\n",
      "iteration 2257, dc_loss: 0.01867925189435482, tv_loss: 0.01703832298517227\n",
      "iteration 2258, dc_loss: 0.018686557188630104, tv_loss: 0.01703081652522087\n",
      "iteration 2259, dc_loss: 0.018679138273000717, tv_loss: 0.0170378889888525\n",
      "iteration 2260, dc_loss: 0.018685266375541687, tv_loss: 0.017031345516443253\n",
      "iteration 2261, dc_loss: 0.018680432811379433, tv_loss: 0.017035825178027153\n",
      "iteration 2262, dc_loss: 0.01868140511214733, tv_loss: 0.0170346200466156\n",
      "iteration 2263, dc_loss: 0.01868426613509655, tv_loss: 0.017031732946634293\n",
      "iteration 2264, dc_loss: 0.01867689937353134, tv_loss: 0.017039267346262932\n",
      "iteration 2265, dc_loss: 0.018687978386878967, tv_loss: 0.01702822931110859\n",
      "iteration 2266, dc_loss: 0.018674978986382484, tv_loss: 0.01704094558954239\n",
      "iteration 2267, dc_loss: 0.018687065690755844, tv_loss: 0.017028525471687317\n",
      "iteration 2268, dc_loss: 0.01867769844830036, tv_loss: 0.0170376505702734\n",
      "iteration 2269, dc_loss: 0.018681975081562996, tv_loss: 0.01703328639268875\n",
      "iteration 2270, dc_loss: 0.01868140511214733, tv_loss: 0.017033804208040237\n",
      "iteration 2271, dc_loss: 0.018680496141314507, tv_loss: 0.01703452318906784\n",
      "iteration 2272, dc_loss: 0.018679620698094368, tv_loss: 0.017035041004419327\n",
      "iteration 2273, dc_loss: 0.018681839108467102, tv_loss: 0.017032451927661896\n",
      "iteration 2274, dc_loss: 0.018677202984690666, tv_loss: 0.017036883160471916\n",
      "iteration 2275, dc_loss: 0.018683919683098793, tv_loss: 0.01703020930290222\n",
      "iteration 2276, dc_loss: 0.018674731254577637, tv_loss: 0.017039459198713303\n",
      "iteration 2277, dc_loss: 0.01868453621864319, tv_loss: 0.01702948473393917\n",
      "iteration 2278, dc_loss: 0.01867702417075634, tv_loss: 0.017036613076925278\n",
      "iteration 2279, dc_loss: 0.018678786233067513, tv_loss: 0.017034543678164482\n",
      "iteration 2280, dc_loss: 0.018682053312659264, tv_loss: 0.017031144350767136\n",
      "iteration 2281, dc_loss: 0.01867453195154667, tv_loss: 0.017038622871041298\n",
      "iteration 2282, dc_loss: 0.018684174865484238, tv_loss: 0.017028935253620148\n",
      "iteration 2283, dc_loss: 0.01867416501045227, tv_loss: 0.017038820311427116\n",
      "iteration 2284, dc_loss: 0.018682317808270454, tv_loss: 0.017030445858836174\n",
      "iteration 2285, dc_loss: 0.0186761487275362, tv_loss: 0.017036333680152893\n",
      "iteration 2286, dc_loss: 0.01867861859500408, tv_loss: 0.017033617943525314\n",
      "iteration 2287, dc_loss: 0.01867964118719101, tv_loss: 0.017032440751791\n",
      "iteration 2288, dc_loss: 0.018674446269869804, tv_loss: 0.01703747548162937\n",
      "iteration 2289, dc_loss: 0.018682166934013367, tv_loss: 0.017029551789164543\n",
      "iteration 2290, dc_loss: 0.01867278479039669, tv_loss: 0.017038684338331223\n",
      "iteration 2291, dc_loss: 0.018681589514017105, tv_loss: 0.01702967844903469\n",
      "iteration 2292, dc_loss: 0.018673870712518692, tv_loss: 0.01703723892569542\n",
      "iteration 2293, dc_loss: 0.01867976412177086, tv_loss: 0.01703128032386303\n",
      "iteration 2294, dc_loss: 0.018674762919545174, tv_loss: 0.01703624427318573\n",
      "iteration 2295, dc_loss: 0.0186797883361578, tv_loss: 0.017031127586960793\n",
      "iteration 2296, dc_loss: 0.018673736602067947, tv_loss: 0.017037002369761467\n",
      "iteration 2297, dc_loss: 0.018680624663829803, tv_loss: 0.01702992245554924\n",
      "iteration 2298, dc_loss: 0.018671538680791855, tv_loss: 0.017038853839039803\n",
      "iteration 2299, dc_loss: 0.018682196736335754, tv_loss: 0.017028024420142174\n",
      "iteration 2300, dc_loss: 0.018669674172997475, tv_loss: 0.017040245234966278\n",
      "iteration 2301, dc_loss: 0.01868133433163166, tv_loss: 0.017028238624334335\n",
      "iteration 2302, dc_loss: 0.018672484904527664, tv_loss: 0.017036758363246918\n",
      "iteration 2303, dc_loss: 0.01867539994418621, tv_loss: 0.017033670097589493\n",
      "iteration 2304, dc_loss: 0.018677355721592903, tv_loss: 0.017031695693731308\n",
      "iteration 2305, dc_loss: 0.018671974539756775, tv_loss: 0.017037183046340942\n",
      "iteration 2306, dc_loss: 0.018680106848478317, tv_loss: 0.017029080539941788\n",
      "iteration 2307, dc_loss: 0.018670199438929558, tv_loss: 0.017038971185684204\n",
      "iteration 2308, dc_loss: 0.018680384382605553, tv_loss: 0.01702864281833172\n",
      "iteration 2309, dc_loss: 0.01867125742137432, tv_loss: 0.01703750155866146\n",
      "iteration 2310, dc_loss: 0.018677396699786186, tv_loss: 0.017030958086252213\n",
      "iteration 2311, dc_loss: 0.018673328682780266, tv_loss: 0.01703461818397045\n",
      "iteration 2312, dc_loss: 0.01867368072271347, tv_loss: 0.01703396998345852\n",
      "iteration 2313, dc_loss: 0.018675290048122406, tv_loss: 0.01703212596476078\n",
      "iteration 2314, dc_loss: 0.018669992685317993, tv_loss: 0.0170372873544693\n",
      "iteration 2315, dc_loss: 0.018680095672607422, tv_loss: 0.017027275636792183\n",
      "iteration 2316, dc_loss: 0.01866610161960125, tv_loss: 0.01704142615199089\n",
      "iteration 2317, dc_loss: 0.018681863322854042, tv_loss: 0.017025766894221306\n",
      "iteration 2318, dc_loss: 0.018667396157979965, tv_loss: 0.017040163278579712\n",
      "iteration 2319, dc_loss: 0.01867850311100483, tv_loss: 0.017028747126460075\n",
      "iteration 2320, dc_loss: 0.018669743090867996, tv_loss: 0.017036989331245422\n",
      "iteration 2321, dc_loss: 0.018674828112125397, tv_loss: 0.017031345516443253\n",
      "iteration 2322, dc_loss: 0.01867176592350006, tv_loss: 0.017033899202942848\n",
      "iteration 2323, dc_loss: 0.018672073259949684, tv_loss: 0.017033277079463005\n",
      "iteration 2324, dc_loss: 0.018672160804271698, tv_loss: 0.01703312061727047\n",
      "iteration 2325, dc_loss: 0.018671952188014984, tv_loss: 0.01703350991010666\n",
      "iteration 2326, dc_loss: 0.018671179190278053, tv_loss: 0.017034487798810005\n",
      "iteration 2327, dc_loss: 0.018673785030841827, tv_loss: 0.017031826078891754\n",
      "iteration 2328, dc_loss: 0.018670622259378433, tv_loss: 0.017034869641065598\n",
      "iteration 2329, dc_loss: 0.018672971054911613, tv_loss: 0.017032289877533913\n",
      "iteration 2330, dc_loss: 0.01867145486176014, tv_loss: 0.017033470794558525\n",
      "iteration 2331, dc_loss: 0.018670016899704933, tv_loss: 0.017034532502293587\n",
      "iteration 2332, dc_loss: 0.018673203885555267, tv_loss: 0.017030859366059303\n",
      "iteration 2333, dc_loss: 0.01866718754172325, tv_loss: 0.01703658699989319\n",
      "iteration 2334, dc_loss: 0.018673911690711975, tv_loss: 0.01702963002026081\n",
      "iteration 2335, dc_loss: 0.01866788975894451, tv_loss: 0.01703549735248089\n",
      "iteration 2336, dc_loss: 0.01867128536105156, tv_loss: 0.017031975090503693\n",
      "iteration 2337, dc_loss: 0.018670007586479187, tv_loss: 0.017033196985721588\n",
      "iteration 2338, dc_loss: 0.018668482080101967, tv_loss: 0.01703472249209881\n",
      "iteration 2339, dc_loss: 0.018673861399292946, tv_loss: 0.017029346898198128\n",
      "iteration 2340, dc_loss: 0.018662769347429276, tv_loss: 0.0170405525714159\n",
      "iteration 2341, dc_loss: 0.01868033967912197, tv_loss: 0.017023256048560143\n",
      "iteration 2342, dc_loss: 0.01866026222705841, tv_loss: 0.017043478786945343\n",
      "iteration 2343, dc_loss: 0.018678756430745125, tv_loss: 0.017024891451001167\n",
      "iteration 2344, dc_loss: 0.018664740025997162, tv_loss: 0.017038684338331223\n",
      "iteration 2345, dc_loss: 0.018673108890652657, tv_loss: 0.01702991873025894\n",
      "iteration 2346, dc_loss: 0.018668070435523987, tv_loss: 0.01703442446887493\n",
      "iteration 2347, dc_loss: 0.01866900734603405, tv_loss: 0.0170328039675951\n",
      "iteration 2348, dc_loss: 0.018668444827198982, tv_loss: 0.017032818868756294\n",
      "iteration 2349, dc_loss: 0.018668875098228455, tv_loss: 0.017032193019986153\n",
      "iteration 2350, dc_loss: 0.01866643689572811, tv_loss: 0.01703481748700142\n",
      "iteration 2351, dc_loss: 0.01867038384079933, tv_loss: 0.01703111082315445\n",
      "iteration 2352, dc_loss: 0.018666064366698265, tv_loss: 0.017035450786352158\n",
      "iteration 2353, dc_loss: 0.01867038756608963, tv_loss: 0.017030971124768257\n",
      "iteration 2354, dc_loss: 0.018667835742235184, tv_loss: 0.017033159732818604\n",
      "iteration 2355, dc_loss: 0.01866544410586357, tv_loss: 0.01703519932925701\n",
      "iteration 2356, dc_loss: 0.018671246245503426, tv_loss: 0.01702902652323246\n",
      "iteration 2357, dc_loss: 0.01866224966943264, tv_loss: 0.01703774183988571\n",
      "iteration 2358, dc_loss: 0.018672125414013863, tv_loss: 0.0170276015996933\n",
      "iteration 2359, dc_loss: 0.018662042915821075, tv_loss: 0.01703740656375885\n",
      "iteration 2360, dc_loss: 0.018669692799448967, tv_loss: 0.017029428854584694\n",
      "iteration 2361, dc_loss: 0.01866520382463932, tv_loss: 0.017033733427524567\n",
      "iteration 2362, dc_loss: 0.01866466924548149, tv_loss: 0.017034245654940605\n",
      "iteration 2363, dc_loss: 0.01866980455815792, tv_loss: 0.017029227688908577\n",
      "iteration 2364, dc_loss: 0.018659822642803192, tv_loss: 0.017039205878973007\n",
      "iteration 2365, dc_loss: 0.018674395978450775, tv_loss: 0.01702461764216423\n",
      "iteration 2366, dc_loss: 0.01865781471133232, tv_loss: 0.01704104244709015\n",
      "iteration 2367, dc_loss: 0.01867271028459072, tv_loss: 0.01702587679028511\n",
      "iteration 2368, dc_loss: 0.018661081790924072, tv_loss: 0.01703724078834057\n",
      "iteration 2369, dc_loss: 0.018667185679078102, tv_loss: 0.01703103445470333\n",
      "iteration 2370, dc_loss: 0.018665416166186333, tv_loss: 0.017032915726304054\n",
      "iteration 2371, dc_loss: 0.018666451796889305, tv_loss: 0.017031975090503693\n",
      "iteration 2372, dc_loss: 0.018663546070456505, tv_loss: 0.01703484170138836\n",
      "iteration 2373, dc_loss: 0.0186703372746706, tv_loss: 0.01702789217233658\n",
      "iteration 2374, dc_loss: 0.018658194690942764, tv_loss: 0.017040010541677475\n",
      "iteration 2375, dc_loss: 0.018674952909350395, tv_loss: 0.017023269087076187\n",
      "iteration 2376, dc_loss: 0.018654247745871544, tv_loss: 0.017043529078364372\n",
      "iteration 2377, dc_loss: 0.018674112856388092, tv_loss: 0.01702297478914261\n",
      "iteration 2378, dc_loss: 0.018658429384231567, tv_loss: 0.01703799143433571\n",
      "iteration 2379, dc_loss: 0.018664274364709854, tv_loss: 0.017031831666827202\n",
      "iteration 2380, dc_loss: 0.018666235730051994, tv_loss: 0.017029833048582077\n",
      "iteration 2381, dc_loss: 0.018658526241779327, tv_loss: 0.01703745685517788\n",
      "iteration 2382, dc_loss: 0.018668999895453453, tv_loss: 0.017026934772729874\n",
      "iteration 2383, dc_loss: 0.018657857552170753, tv_loss: 0.017038127407431602\n",
      "iteration 2384, dc_loss: 0.018669191747903824, tv_loss: 0.017027078196406364\n",
      "iteration 2385, dc_loss: 0.018660036846995354, tv_loss: 0.017036711797118187\n",
      "iteration 2386, dc_loss: 0.018667493015527725, tv_loss: 0.017029596492648125\n",
      "iteration 2387, dc_loss: 0.018661901354789734, tv_loss: 0.017034858465194702\n",
      "iteration 2388, dc_loss: 0.018665429204702377, tv_loss: 0.017030445858836174\n",
      "iteration 2389, dc_loss: 0.018660511821508408, tv_loss: 0.017034461721777916\n",
      "iteration 2390, dc_loss: 0.018664652481675148, tv_loss: 0.017029697075486183\n",
      "iteration 2391, dc_loss: 0.018659252673387527, tv_loss: 0.017034905031323433\n",
      "iteration 2392, dc_loss: 0.018663190305233, tv_loss: 0.017030887305736542\n",
      "iteration 2393, dc_loss: 0.018661800771951675, tv_loss: 0.01703217625617981\n",
      "iteration 2394, dc_loss: 0.018659712746739388, tv_loss: 0.017034215852618217\n",
      "iteration 2395, dc_loss: 0.018665693700313568, tv_loss: 0.017028307542204857\n",
      "iteration 2396, dc_loss: 0.018655620515346527, tv_loss: 0.017038652673363686\n",
      "iteration 2397, dc_loss: 0.01867193914949894, tv_loss: 0.01702282950282097\n",
      "iteration 2398, dc_loss: 0.01865030825138092, tv_loss: 0.01704462803900242\n",
      "iteration 2399, dc_loss: 0.018674621358513832, tv_loss: 0.017019862309098244\n",
      "iteration 2400, dc_loss: 0.018651776015758514, tv_loss: 0.017041916027665138\n",
      "iteration 2401, dc_loss: 0.0186662245541811, tv_loss: 0.01702677272260189\n",
      "iteration 2402, dc_loss: 0.01866033487021923, tv_loss: 0.017031658440828323\n",
      "iteration 2403, dc_loss: 0.018655823543667793, tv_loss: 0.01703588478267193\n",
      "iteration 2404, dc_loss: 0.018664797767996788, tv_loss: 0.01702718436717987\n",
      "iteration 2405, dc_loss: 0.018658587709069252, tv_loss: 0.01703321747481823\n",
      "iteration 2406, dc_loss: 0.018657267093658447, tv_loss: 0.017034169286489487\n",
      "iteration 2407, dc_loss: 0.01866339147090912, tv_loss: 0.017027853056788445\n",
      "iteration 2408, dc_loss: 0.018657447770237923, tv_loss: 0.017033608630299568\n",
      "iteration 2409, dc_loss: 0.01865861937403679, tv_loss: 0.017032289877533913\n",
      "iteration 2410, dc_loss: 0.01866169273853302, tv_loss: 0.017029225826263428\n",
      "iteration 2411, dc_loss: 0.018657101318240166, tv_loss: 0.017033707350492477\n",
      "iteration 2412, dc_loss: 0.018659308552742004, tv_loss: 0.017031099647283554\n",
      "iteration 2413, dc_loss: 0.01866006664931774, tv_loss: 0.01703001745045185\n",
      "iteration 2414, dc_loss: 0.018656929954886436, tv_loss: 0.017033111304044724\n",
      "iteration 2415, dc_loss: 0.018660040572285652, tv_loss: 0.017030013725161552\n",
      "iteration 2416, dc_loss: 0.01865795999765396, tv_loss: 0.017031997442245483\n",
      "iteration 2417, dc_loss: 0.018658176064491272, tv_loss: 0.01703163981437683\n",
      "iteration 2418, dc_loss: 0.018659304827451706, tv_loss: 0.01703040488064289\n",
      "iteration 2419, dc_loss: 0.01865583099424839, tv_loss: 0.01703374646604061\n",
      "iteration 2420, dc_loss: 0.018660666421055794, tv_loss: 0.017028676345944405\n",
      "iteration 2421, dc_loss: 0.01865653321146965, tv_loss: 0.017032528296113014\n",
      "iteration 2422, dc_loss: 0.018657261505723, tv_loss: 0.017031632363796234\n",
      "iteration 2423, dc_loss: 0.018659362569451332, tv_loss: 0.017029447481036186\n",
      "iteration 2424, dc_loss: 0.01865561492741108, tv_loss: 0.017033135518431664\n",
      "iteration 2425, dc_loss: 0.018658369779586792, tv_loss: 0.01703030988574028\n",
      "iteration 2426, dc_loss: 0.018657488748431206, tv_loss: 0.01703115552663803\n",
      "iteration 2427, dc_loss: 0.01865651272237301, tv_loss: 0.017032034695148468\n",
      "iteration 2428, dc_loss: 0.018657879903912544, tv_loss: 0.017030490562319756\n",
      "iteration 2429, dc_loss: 0.01865554042160511, tv_loss: 0.017032664269208908\n",
      "iteration 2430, dc_loss: 0.01865890994668007, tv_loss: 0.01702912338078022\n",
      "iteration 2431, dc_loss: 0.018654562532901764, tv_loss: 0.017033293843269348\n",
      "iteration 2432, dc_loss: 0.018657784909009933, tv_loss: 0.017029928043484688\n",
      "iteration 2433, dc_loss: 0.018656769767403603, tv_loss: 0.017030859366059303\n",
      "iteration 2434, dc_loss: 0.018654894083738327, tv_loss: 0.01703268475830555\n",
      "iteration 2435, dc_loss: 0.01865733228623867, tv_loss: 0.0170301366597414\n",
      "iteration 2436, dc_loss: 0.018656063824892044, tv_loss: 0.017031213268637657\n",
      "iteration 2437, dc_loss: 0.018654881045222282, tv_loss: 0.017032243311405182\n",
      "iteration 2438, dc_loss: 0.01865750178694725, tv_loss: 0.01702955923974514\n",
      "iteration 2439, dc_loss: 0.018654154613614082, tv_loss: 0.01703280210494995\n",
      "iteration 2440, dc_loss: 0.01865721493959427, tv_loss: 0.017029602080583572\n",
      "iteration 2441, dc_loss: 0.01865493319928646, tv_loss: 0.01703176647424698\n",
      "iteration 2442, dc_loss: 0.018654773011803627, tv_loss: 0.017031783238053322\n",
      "iteration 2443, dc_loss: 0.01865592785179615, tv_loss: 0.017030438408255577\n",
      "iteration 2444, dc_loss: 0.018654944375157356, tv_loss: 0.017031218856573105\n",
      "iteration 2445, dc_loss: 0.018654484301805496, tv_loss: 0.017031488940119743\n",
      "iteration 2446, dc_loss: 0.018656259402632713, tv_loss: 0.017029576003551483\n",
      "iteration 2447, dc_loss: 0.01865280047059059, tv_loss: 0.017032966017723083\n",
      "iteration 2448, dc_loss: 0.018656739965081215, tv_loss: 0.0170290470123291\n",
      "iteration 2449, dc_loss: 0.018652798607945442, tv_loss: 0.017032990232110023\n",
      "iteration 2450, dc_loss: 0.018655788153409958, tv_loss: 0.01702987216413021\n",
      "iteration 2451, dc_loss: 0.018654298037290573, tv_loss: 0.017031090334057808\n",
      "iteration 2452, dc_loss: 0.018652740865945816, tv_loss: 0.017032403498888016\n",
      "iteration 2453, dc_loss: 0.018655965104699135, tv_loss: 0.017028985545039177\n",
      "iteration 2454, dc_loss: 0.018653063103556633, tv_loss: 0.01703171245753765\n",
      "iteration 2455, dc_loss: 0.01865374483168125, tv_loss: 0.01703089475631714\n",
      "iteration 2456, dc_loss: 0.018654540181159973, tv_loss: 0.017030058428645134\n",
      "iteration 2457, dc_loss: 0.018652280792593956, tv_loss: 0.017032425850629807\n",
      "iteration 2458, dc_loss: 0.01865563355386257, tv_loss: 0.017029104754328728\n",
      "iteration 2459, dc_loss: 0.018650716170668602, tv_loss: 0.017033737152814865\n",
      "iteration 2460, dc_loss: 0.018655480816960335, tv_loss: 0.017028624191880226\n",
      "iteration 2461, dc_loss: 0.01865239068865776, tv_loss: 0.017031488940119743\n",
      "iteration 2462, dc_loss: 0.01865221932530403, tv_loss: 0.017031533643603325\n",
      "iteration 2463, dc_loss: 0.018654344603419304, tv_loss: 0.017029358074069023\n",
      "iteration 2464, dc_loss: 0.01865159533917904, tv_loss: 0.017032090574502945\n",
      "iteration 2465, dc_loss: 0.01865305006504059, tv_loss: 0.01703059673309326\n",
      "iteration 2466, dc_loss: 0.018653327599167824, tv_loss: 0.017030145972967148\n",
      "iteration 2467, dc_loss: 0.01865081675350666, tv_loss: 0.017032457515597343\n",
      "iteration 2468, dc_loss: 0.018654562532901764, tv_loss: 0.017028620466589928\n",
      "iteration 2469, dc_loss: 0.018649538978934288, tv_loss: 0.017033522948622704\n",
      "iteration 2470, dc_loss: 0.01865450292825699, tv_loss: 0.017028434202075005\n",
      "iteration 2471, dc_loss: 0.018651193007826805, tv_loss: 0.017031632363796234\n",
      "iteration 2472, dc_loss: 0.01865081675350666, tv_loss: 0.01703186146914959\n",
      "iteration 2473, dc_loss: 0.01865323819220066, tv_loss: 0.01702926680445671\n",
      "iteration 2474, dc_loss: 0.01865088753402233, tv_loss: 0.01703145168721676\n",
      "iteration 2475, dc_loss: 0.01865120232105255, tv_loss: 0.017031045630574226\n",
      "iteration 2476, dc_loss: 0.018652290105819702, tv_loss: 0.01702991873025894\n",
      "iteration 2477, dc_loss: 0.018650401383638382, tv_loss: 0.017031747847795486\n",
      "iteration 2478, dc_loss: 0.01865256391465664, tv_loss: 0.01702950894832611\n",
      "iteration 2479, dc_loss: 0.018649432808160782, tv_loss: 0.01703243888914585\n",
      "iteration 2480, dc_loss: 0.018652435392141342, tv_loss: 0.017029212787747383\n",
      "iteration 2481, dc_loss: 0.018649661913514137, tv_loss: 0.017031781375408173\n",
      "iteration 2482, dc_loss: 0.018651217222213745, tv_loss: 0.01703006401658058\n",
      "iteration 2483, dc_loss: 0.018650321289896965, tv_loss: 0.017030803486704826\n",
      "iteration 2484, dc_loss: 0.018651101738214493, tv_loss: 0.017029892653226852\n",
      "iteration 2485, dc_loss: 0.018648911267518997, tv_loss: 0.017032040283083916\n",
      "iteration 2486, dc_loss: 0.01865217462182045, tv_loss: 0.017028821632266045\n",
      "iteration 2487, dc_loss: 0.01864820346236229, tv_loss: 0.017032736912369728\n",
      "iteration 2488, dc_loss: 0.018651381134986877, tv_loss: 0.017029352486133575\n",
      "iteration 2489, dc_loss: 0.018649423494935036, tv_loss: 0.017031051218509674\n",
      "iteration 2490, dc_loss: 0.01864963211119175, tv_loss: 0.017030633985996246\n",
      "iteration 2491, dc_loss: 0.01865040510892868, tv_loss: 0.017029715701937675\n",
      "iteration 2492, dc_loss: 0.018647925928235054, tv_loss: 0.017032094299793243\n",
      "iteration 2493, dc_loss: 0.018650595098733902, tv_loss: 0.017029335722327232\n",
      "iteration 2494, dc_loss: 0.018649466335773468, tv_loss: 0.017030388116836548\n",
      "iteration 2495, dc_loss: 0.018647681921720505, tv_loss: 0.017032047733664513\n",
      "iteration 2496, dc_loss: 0.01865125074982643, tv_loss: 0.017028378322720528\n",
      "iteration 2497, dc_loss: 0.018646113574504852, tv_loss: 0.01703341118991375\n",
      "iteration 2498, dc_loss: 0.018651681020855904, tv_loss: 0.017027687281370163\n",
      "iteration 2499, dc_loss: 0.01864706352353096, tv_loss: 0.017032137140631676\n",
      "iteration 2500, dc_loss: 0.018648847937583923, tv_loss: 0.017030220478773117\n",
      "iteration 2501, dc_loss: 0.01864892989397049, tv_loss: 0.017030030488967896\n",
      "iteration 2502, dc_loss: 0.01864761859178543, tv_loss: 0.017031189054250717\n",
      "iteration 2503, dc_loss: 0.01864871010184288, tv_loss: 0.01702991873025894\n",
      "iteration 2504, dc_loss: 0.01864827238023281, tv_loss: 0.01703016832470894\n",
      "iteration 2505, dc_loss: 0.018647009506821632, tv_loss: 0.01703130081295967\n",
      "iteration 2506, dc_loss: 0.01864967681467533, tv_loss: 0.01702856831252575\n",
      "iteration 2507, dc_loss: 0.018645159900188446, tv_loss: 0.01703307405114174\n",
      "iteration 2508, dc_loss: 0.018650352954864502, tv_loss: 0.017027871683239937\n",
      "iteration 2509, dc_loss: 0.01864604279398918, tv_loss: 0.01703203096985817\n",
      "iteration 2510, dc_loss: 0.018647363409399986, tv_loss: 0.017030490562319756\n",
      "iteration 2511, dc_loss: 0.018648508936166763, tv_loss: 0.01702914759516716\n",
      "iteration 2512, dc_loss: 0.018645664677023888, tv_loss: 0.01703183725476265\n",
      "iteration 2513, dc_loss: 0.01864851638674736, tv_loss: 0.01702882908284664\n",
      "iteration 2514, dc_loss: 0.018646027892827988, tv_loss: 0.017031172290444374\n",
      "iteration 2515, dc_loss: 0.01864701695740223, tv_loss: 0.017030078917741776\n",
      "iteration 2516, dc_loss: 0.01864747330546379, tv_loss: 0.017029616981744766\n",
      "iteration 2517, dc_loss: 0.018644463270902634, tv_loss: 0.01703258417546749\n",
      "iteration 2518, dc_loss: 0.01864936575293541, tv_loss: 0.017027510330080986\n",
      "iteration 2519, dc_loss: 0.018643509596586227, tv_loss: 0.017033159732818604\n",
      "iteration 2520, dc_loss: 0.018648657947778702, tv_loss: 0.017027772963047028\n",
      "iteration 2521, dc_loss: 0.0186453964561224, tv_loss: 0.01703088916838169\n",
      "iteration 2522, dc_loss: 0.018645040690898895, tv_loss: 0.01703118532896042\n",
      "iteration 2523, dc_loss: 0.018646929413080215, tv_loss: 0.017029251903295517\n",
      "iteration 2524, dc_loss: 0.018645571544766426, tv_loss: 0.017030484974384308\n",
      "iteration 2525, dc_loss: 0.0186453964561224, tv_loss: 0.017030442133545876\n",
      "iteration 2526, dc_loss: 0.01864643022418022, tv_loss: 0.01702924631536007\n",
      "iteration 2527, dc_loss: 0.01864350028336048, tv_loss: 0.01703207939863205\n",
      "iteration 2528, dc_loss: 0.018648285418748856, tv_loss: 0.01702723652124405\n",
      "iteration 2529, dc_loss: 0.018642308190464973, tv_loss: 0.01703316904604435\n",
      "iteration 2530, dc_loss: 0.018647685647010803, tv_loss: 0.017027733847498894\n",
      "iteration 2531, dc_loss: 0.0186433345079422, tv_loss: 0.017031889408826828\n",
      "iteration 2532, dc_loss: 0.018645184114575386, tv_loss: 0.01702977530658245\n",
      "iteration 2533, dc_loss: 0.01864531636238098, tv_loss: 0.0170294139534235\n",
      "iteration 2534, dc_loss: 0.01864391751587391, tv_loss: 0.01703064888715744\n",
      "iteration 2535, dc_loss: 0.01864491030573845, tv_loss: 0.01702956110239029\n",
      "iteration 2536, dc_loss: 0.01864452473819256, tv_loss: 0.017029952257871628\n",
      "iteration 2537, dc_loss: 0.018643846735358238, tv_loss: 0.017030727118253708\n",
      "iteration 2538, dc_loss: 0.0186456385999918, tv_loss: 0.017028989270329475\n",
      "iteration 2539, dc_loss: 0.01864231377840042, tv_loss: 0.017031995579600334\n",
      "iteration 2540, dc_loss: 0.01864592917263508, tv_loss: 0.017028028145432472\n",
      "iteration 2541, dc_loss: 0.018642602488398552, tv_loss: 0.01703111082315445\n",
      "iteration 2542, dc_loss: 0.01864396221935749, tv_loss: 0.01702967844903469\n",
      "iteration 2543, dc_loss: 0.01864350214600563, tv_loss: 0.01703009381890297\n",
      "iteration 2544, dc_loss: 0.01864468678832054, tv_loss: 0.017028890550136566\n",
      "iteration 2545, dc_loss: 0.01864118129014969, tv_loss: 0.017032306641340256\n",
      "iteration 2546, dc_loss: 0.01864609494805336, tv_loss: 0.017027242109179497\n",
      "iteration 2547, dc_loss: 0.018640460446476936, tv_loss: 0.017032675445079803\n",
      "iteration 2548, dc_loss: 0.018645355477929115, tv_loss: 0.017027635127305984\n",
      "iteration 2549, dc_loss: 0.01864180900156498, tv_loss: 0.017031073570251465\n",
      "iteration 2550, dc_loss: 0.01864256151020527, tv_loss: 0.017030274495482445\n",
      "iteration 2551, dc_loss: 0.018643882125616074, tv_loss: 0.017028862610459328\n",
      "iteration 2552, dc_loss: 0.0186411514878273, tv_loss: 0.017031406983733177\n",
      "iteration 2553, dc_loss: 0.018643956631422043, tv_loss: 0.01702834852039814\n",
      "iteration 2554, dc_loss: 0.018641237169504166, tv_loss: 0.017030855640769005\n",
      "iteration 2555, dc_loss: 0.018642183393239975, tv_loss: 0.01702973246574402\n",
      "iteration 2556, dc_loss: 0.018643591552972794, tv_loss: 0.01702825352549553\n",
      "iteration 2557, dc_loss: 0.018639367073774338, tv_loss: 0.017032524570822716\n",
      "iteration 2558, dc_loss: 0.018645428121089935, tv_loss: 0.017026564106345177\n",
      "iteration 2559, dc_loss: 0.01863807998597622, tv_loss: 0.01703372411429882\n",
      "iteration 2560, dc_loss: 0.01864440180361271, tv_loss: 0.017027059569954872\n",
      "iteration 2561, dc_loss: 0.018641211092472076, tv_loss: 0.017030011862516403\n",
      "iteration 2562, dc_loss: 0.018640130758285522, tv_loss: 0.017030956223607063\n",
      "iteration 2563, dc_loss: 0.018642785027623177, tv_loss: 0.01702818274497986\n",
      "iteration 2564, dc_loss: 0.018640408292412758, tv_loss: 0.01703042909502983\n",
      "iteration 2565, dc_loss: 0.018640892580151558, tv_loss: 0.01702985353767872\n",
      "iteration 2566, dc_loss: 0.01864190585911274, tv_loss: 0.017028819769620895\n",
      "iteration 2567, dc_loss: 0.018639130517840385, tv_loss: 0.01703154668211937\n",
      "iteration 2568, dc_loss: 0.01864335872232914, tv_loss: 0.017027191817760468\n",
      "iteration 2569, dc_loss: 0.018637526780366898, tv_loss: 0.017032859846949577\n",
      "iteration 2570, dc_loss: 0.018643544986844063, tv_loss: 0.01702667959034443\n",
      "iteration 2571, dc_loss: 0.01863892935216427, tv_loss: 0.017031151801347733\n",
      "iteration 2572, dc_loss: 0.018639687448740005, tv_loss: 0.017030326649546623\n",
      "iteration 2573, dc_loss: 0.01864214800298214, tv_loss: 0.017027784138917923\n",
      "iteration 2574, dc_loss: 0.018637951463460922, tv_loss: 0.017031891271471977\n",
      "iteration 2575, dc_loss: 0.018641896545886993, tv_loss: 0.017027772963047028\n",
      "iteration 2576, dc_loss: 0.018639039248228073, tv_loss: 0.01703043468296528\n",
      "iteration 2577, dc_loss: 0.01863960362970829, tv_loss: 0.017029670998454094\n",
      "iteration 2578, dc_loss: 0.018640315160155296, tv_loss: 0.017028825357556343\n",
      "iteration 2579, dc_loss: 0.01863793469965458, tv_loss: 0.01703120581805706\n",
      "iteration 2580, dc_loss: 0.01864241436123848, tv_loss: 0.017026718705892563\n",
      "iteration 2581, dc_loss: 0.018635528162121773, tv_loss: 0.01703345775604248\n",
      "iteration 2582, dc_loss: 0.018642794340848923, tv_loss: 0.017026007175445557\n",
      "iteration 2583, dc_loss: 0.018636906519532204, tv_loss: 0.01703164540231228\n",
      "iteration 2584, dc_loss: 0.018639342859387398, tv_loss: 0.017029037699103355\n",
      "iteration 2585, dc_loss: 0.018639622256159782, tv_loss: 0.01702861301600933\n",
      "iteration 2586, dc_loss: 0.018637796863913536, tv_loss: 0.01703030988574028\n",
      "iteration 2587, dc_loss: 0.01863885298371315, tv_loss: 0.017029134556651115\n",
      "iteration 2588, dc_loss: 0.018638629466295242, tv_loss: 0.017029274255037308\n",
      "iteration 2589, dc_loss: 0.018637388944625854, tv_loss: 0.017030442133545876\n",
      "iteration 2590, dc_loss: 0.018639950081706047, tv_loss: 0.01702786237001419\n",
      "iteration 2591, dc_loss: 0.018635910004377365, tv_loss: 0.017031874507665634\n",
      "iteration 2592, dc_loss: 0.018640797585248947, tv_loss: 0.017026858404278755\n",
      "iteration 2593, dc_loss: 0.018636303022503853, tv_loss: 0.017031114548444748\n",
      "iteration 2594, dc_loss: 0.018637632951140404, tv_loss: 0.017029548063874245\n",
      "iteration 2595, dc_loss: 0.018639395013451576, tv_loss: 0.017027627676725388\n",
      "iteration 2596, dc_loss: 0.01863504759967327, tv_loss: 0.0170318391174078\n",
      "iteration 2597, dc_loss: 0.018639396876096725, tv_loss: 0.01702730730175972\n",
      "iteration 2598, dc_loss: 0.0186367966234684, tv_loss: 0.017029741778969765\n",
      "iteration 2599, dc_loss: 0.018636321648955345, tv_loss: 0.01703006774187088\n",
      "iteration 2600, dc_loss: 0.018638482317328453, tv_loss: 0.017027830705046654\n",
      "iteration 2601, dc_loss: 0.0186348557472229, tv_loss: 0.0170314721763134\n",
      "iteration 2602, dc_loss: 0.01864022947847843, tv_loss: 0.017026156187057495\n",
      "iteration 2603, dc_loss: 0.0186329148709774, tv_loss: 0.017033355310559273\n",
      "iteration 2604, dc_loss: 0.018640045076608658, tv_loss: 0.01702597364783287\n",
      "iteration 2605, dc_loss: 0.018634891137480736, tv_loss: 0.01703084260225296\n",
      "iteration 2606, dc_loss: 0.018636301159858704, tv_loss: 0.017029283568263054\n",
      "iteration 2607, dc_loss: 0.01863725855946541, tv_loss: 0.01702824793756008\n",
      "iteration 2608, dc_loss: 0.0186359453946352, tv_loss: 0.017029479146003723\n",
      "iteration 2609, dc_loss: 0.018635788932442665, tv_loss: 0.017029503360390663\n",
      "iteration 2610, dc_loss: 0.01863708905875683, tv_loss: 0.01702813431620598\n",
      "iteration 2611, dc_loss: 0.018633829429745674, tv_loss: 0.017031358554959297\n",
      "iteration 2612, dc_loss: 0.018638746812939644, tv_loss: 0.017026325687766075\n",
      "iteration 2613, dc_loss: 0.018632249906659126, tv_loss: 0.017032671719789505\n",
      "iteration 2614, dc_loss: 0.01863878406584263, tv_loss: 0.017025984823703766\n",
      "iteration 2615, dc_loss: 0.01863364316523075, tv_loss: 0.017030935734510422\n",
      "iteration 2616, dc_loss: 0.018635157495737076, tv_loss: 0.01702934131026268\n",
      "iteration 2617, dc_loss: 0.018637314438819885, tv_loss: 0.017027176916599274\n",
      "iteration 2618, dc_loss: 0.018632207065820694, tv_loss: 0.0170321986079216\n",
      "iteration 2619, dc_loss: 0.018637729808688164, tv_loss: 0.017026487737894058\n",
      "iteration 2620, dc_loss: 0.0186332855373621, tv_loss: 0.017030712217092514\n",
      "iteration 2621, dc_loss: 0.01863553561270237, tv_loss: 0.0170281995087862\n",
      "iteration 2622, dc_loss: 0.018634643405675888, tv_loss: 0.01702890172600746\n",
      "iteration 2623, dc_loss: 0.018633266910910606, tv_loss: 0.01703021675348282\n",
      "iteration 2624, dc_loss: 0.018637238070368767, tv_loss: 0.01702631637454033\n",
      "iteration 2625, dc_loss: 0.018630241975188255, tv_loss: 0.017033345997333527\n",
      "iteration 2626, dc_loss: 0.01863870769739151, tv_loss: 0.017024660483002663\n",
      "iteration 2627, dc_loss: 0.018630918115377426, tv_loss: 0.01703217439353466\n",
      "iteration 2628, dc_loss: 0.018635457381606102, tv_loss: 0.017027447000145912\n",
      "iteration 2629, dc_loss: 0.018633680418133736, tv_loss: 0.017029106616973877\n",
      "iteration 2630, dc_loss: 0.018633222207427025, tv_loss: 0.017029481008648872\n",
      "iteration 2631, dc_loss: 0.018634378910064697, tv_loss: 0.017028236761689186\n",
      "iteration 2632, dc_loss: 0.01863338239490986, tv_loss: 0.017029114067554474\n",
      "iteration 2633, dc_loss: 0.01863241381943226, tv_loss: 0.017029881477355957\n",
      "iteration 2634, dc_loss: 0.018635062500834465, tv_loss: 0.017027121037244797\n",
      "iteration 2635, dc_loss: 0.01863021031022072, tv_loss: 0.017031937837600708\n",
      "iteration 2636, dc_loss: 0.01863693818449974, tv_loss: 0.017025230452418327\n",
      "iteration 2637, dc_loss: 0.018629932776093483, tv_loss: 0.017032135277986526\n",
      "iteration 2638, dc_loss: 0.018634185194969177, tv_loss: 0.017027679830789566\n",
      "iteration 2639, dc_loss: 0.01863393560051918, tv_loss: 0.0170277152210474\n",
      "iteration 2640, dc_loss: 0.01862969435751438, tv_loss: 0.017031855881214142\n",
      "iteration 2641, dc_loss: 0.018636194989085197, tv_loss: 0.017025234177708626\n",
      "iteration 2642, dc_loss: 0.018629545345902443, tv_loss: 0.017031721770763397\n",
      "iteration 2643, dc_loss: 0.01863417960703373, tv_loss: 0.017026908695697784\n",
      "iteration 2644, dc_loss: 0.018631158396601677, tv_loss: 0.01702977530658245\n",
      "iteration 2645, dc_loss: 0.01863204501569271, tv_loss: 0.017028789967298508\n",
      "iteration 2646, dc_loss: 0.01863299496471882, tv_loss: 0.01702778972685337\n",
      "iteration 2647, dc_loss: 0.01862906850874424, tv_loss: 0.017031634226441383\n",
      "iteration 2648, dc_loss: 0.018636101856827736, tv_loss: 0.017024492844939232\n",
      "iteration 2649, dc_loss: 0.018627086654305458, tv_loss: 0.017033351585268974\n",
      "iteration 2650, dc_loss: 0.018634753301739693, tv_loss: 0.017025494948029518\n",
      "iteration 2651, dc_loss: 0.018630294129252434, tv_loss: 0.017029836773872375\n",
      "iteration 2652, dc_loss: 0.018630946055054665, tv_loss: 0.0170291718095541\n",
      "iteration 2653, dc_loss: 0.01863158494234085, tv_loss: 0.01702846772968769\n",
      "iteration 2654, dc_loss: 0.018631625920534134, tv_loss: 0.017028266564011574\n",
      "iteration 2655, dc_loss: 0.018629856407642365, tv_loss: 0.017029767856001854\n",
      "iteration 2656, dc_loss: 0.01863216981291771, tv_loss: 0.017027199268341064\n",
      "iteration 2657, dc_loss: 0.018628844991326332, tv_loss: 0.017030371353030205\n",
      "iteration 2658, dc_loss: 0.018633617088198662, tv_loss: 0.017025593668222427\n",
      "iteration 2659, dc_loss: 0.018626702949404716, tv_loss: 0.017032593488693237\n",
      "iteration 2660, dc_loss: 0.01863398402929306, tv_loss: 0.01702529564499855\n",
      "iteration 2661, dc_loss: 0.018628070130944252, tv_loss: 0.017030879855155945\n",
      "iteration 2662, dc_loss: 0.01863042823970318, tv_loss: 0.017028290778398514\n",
      "iteration 2663, dc_loss: 0.018631963059306145, tv_loss: 0.017026644200086594\n",
      "iteration 2664, dc_loss: 0.01862681843340397, tv_loss: 0.01703179068863392\n",
      "iteration 2665, dc_loss: 0.018633553758263588, tv_loss: 0.017025064677000046\n",
      "iteration 2666, dc_loss: 0.018626637756824493, tv_loss: 0.017031919211149216\n",
      "iteration 2667, dc_loss: 0.01863221451640129, tv_loss: 0.01702606864273548\n",
      "iteration 2668, dc_loss: 0.018628142774105072, tv_loss: 0.01702982559800148\n",
      "iteration 2669, dc_loss: 0.018628809601068497, tv_loss: 0.017028983682394028\n",
      "iteration 2670, dc_loss: 0.018631715327501297, tv_loss: 0.017026055604219437\n",
      "iteration 2671, dc_loss: 0.018625032156705856, tv_loss: 0.017032822594046593\n",
      "iteration 2672, dc_loss: 0.018634052947163582, tv_loss: 0.017023755237460136\n",
      "iteration 2673, dc_loss: 0.0186245609074831, tv_loss: 0.017033003270626068\n",
      "iteration 2674, dc_loss: 0.01863187924027443, tv_loss: 0.01702544093132019\n",
      "iteration 2675, dc_loss: 0.01862776279449463, tv_loss: 0.017029449343681335\n",
      "iteration 2676, dc_loss: 0.018628280609846115, tv_loss: 0.01702888496220112\n",
      "iteration 2677, dc_loss: 0.01862919330596924, tv_loss: 0.01702786423265934\n",
      "iteration 2678, dc_loss: 0.018628627061843872, tv_loss: 0.017028257250785828\n",
      "iteration 2679, dc_loss: 0.01862729899585247, tv_loss: 0.01702936179935932\n",
      "iteration 2680, dc_loss: 0.018629631027579308, tv_loss: 0.017026863992214203\n",
      "iteration 2681, dc_loss: 0.018625408411026, tv_loss: 0.01703103445470333\n",
      "iteration 2682, dc_loss: 0.018631432205438614, tv_loss: 0.017025040462613106\n",
      "iteration 2683, dc_loss: 0.018624600023031235, tv_loss: 0.017031872645020485\n",
      "iteration 2684, dc_loss: 0.018630219623446465, tv_loss: 0.017026100307703018\n",
      "iteration 2685, dc_loss: 0.018626924604177475, tv_loss: 0.01702909730374813\n",
      "iteration 2686, dc_loss: 0.018625928089022636, tv_loss: 0.01702987588942051\n",
      "iteration 2687, dc_loss: 0.018630797043442726, tv_loss: 0.017024856060743332\n",
      "iteration 2688, dc_loss: 0.01862308196723461, tv_loss: 0.017032483592629433\n",
      "iteration 2689, dc_loss: 0.018630878999829292, tv_loss: 0.017024559900164604\n",
      "iteration 2690, dc_loss: 0.01862470991909504, tv_loss: 0.0170306209474802\n",
      "iteration 2691, dc_loss: 0.0186274666339159, tv_loss: 0.017027778550982475\n",
      "iteration 2692, dc_loss: 0.01862764172255993, tv_loss: 0.017027530819177628\n",
      "iteration 2693, dc_loss: 0.018624551594257355, tv_loss: 0.017030542716383934\n",
      "iteration 2694, dc_loss: 0.01862996257841587, tv_loss: 0.017025066539645195\n",
      "iteration 2695, dc_loss: 0.01862286403775215, tv_loss: 0.017032001167535782\n",
      "iteration 2696, dc_loss: 0.018629170954227448, tv_loss: 0.01702546700835228\n",
      "iteration 2697, dc_loss: 0.018625492230057716, tv_loss: 0.01702893152832985\n",
      "iteration 2698, dc_loss: 0.018624646589159966, tv_loss: 0.01702961139380932\n",
      "iteration 2699, dc_loss: 0.018627654761075974, tv_loss: 0.01702643185853958\n",
      "iteration 2700, dc_loss: 0.018624642863869667, tv_loss: 0.017029276117682457\n",
      "iteration 2701, dc_loss: 0.018626146018505096, tv_loss: 0.017027607187628746\n",
      "iteration 2702, dc_loss: 0.018625479191541672, tv_loss: 0.017028160393238068\n",
      "iteration 2703, dc_loss: 0.018624916672706604, tv_loss: 0.017028650268912315\n",
      "iteration 2704, dc_loss: 0.01862703450024128, tv_loss: 0.017026640474796295\n",
      "iteration 2705, dc_loss: 0.01862214505672455, tv_loss: 0.017031749710440636\n",
      "iteration 2706, dc_loss: 0.0186317078769207, tv_loss: 0.01702236570417881\n",
      "iteration 2707, dc_loss: 0.0186186321079731, tv_loss: 0.01703546941280365\n",
      "iteration 2708, dc_loss: 0.018632330000400543, tv_loss: 0.01702156662940979\n",
      "iteration 2709, dc_loss: 0.018621057271957397, tv_loss: 0.017032423987984657\n",
      "iteration 2710, dc_loss: 0.018626883625984192, tv_loss: 0.017026159912347794\n",
      "iteration 2711, dc_loss: 0.018624713644385338, tv_loss: 0.017028052359819412\n",
      "iteration 2712, dc_loss: 0.018622910603880882, tv_loss: 0.01702970266342163\n",
      "iteration 2713, dc_loss: 0.018626943230628967, tv_loss: 0.01702561043202877\n",
      "iteration 2714, dc_loss: 0.018621819093823433, tv_loss: 0.017030786722898483\n",
      "iteration 2715, dc_loss: 0.018627159297466278, tv_loss: 0.017025507986545563\n",
      "iteration 2716, dc_loss: 0.01862252503633499, tv_loss: 0.017030108720064163\n",
      "iteration 2717, dc_loss: 0.018625743687152863, tv_loss: 0.017026713117957115\n",
      "iteration 2718, dc_loss: 0.01862373575568199, tv_loss: 0.017028426751494408\n",
      "iteration 2719, dc_loss: 0.01862277090549469, tv_loss: 0.017029011622071266\n",
      "iteration 2720, dc_loss: 0.018625862896442413, tv_loss: 0.017025751993060112\n",
      "iteration 2721, dc_loss: 0.018619569018483162, tv_loss: 0.017032084986567497\n",
      "iteration 2722, dc_loss: 0.01862950809299946, tv_loss: 0.017022311687469482\n",
      "iteration 2723, dc_loss: 0.018618015572428703, tv_loss: 0.017033899202942848\n",
      "iteration 2724, dc_loss: 0.01862836442887783, tv_loss: 0.017023427411913872\n",
      "iteration 2725, dc_loss: 0.01862015761435032, tv_loss: 0.017031291499733925\n",
      "iteration 2726, dc_loss: 0.018625369295477867, tv_loss: 0.017025679349899292\n",
      "iteration 2727, dc_loss: 0.018622009083628654, tv_loss: 0.017028676345944405\n",
      "iteration 2728, dc_loss: 0.018622325733304024, tv_loss: 0.017028162255883217\n",
      "iteration 2729, dc_loss: 0.018623603507876396, tv_loss: 0.017026817426085472\n",
      "iteration 2730, dc_loss: 0.01862141489982605, tv_loss: 0.01702912151813507\n",
      "iteration 2731, dc_loss: 0.01862361840903759, tv_loss: 0.01702709123492241\n",
      "iteration 2732, dc_loss: 0.018622055649757385, tv_loss: 0.01702863723039627\n",
      "iteration 2733, dc_loss: 0.01862320303916931, tv_loss: 0.017027346417307854\n",
      "iteration 2734, dc_loss: 0.018622152507305145, tv_loss: 0.017028143629431725\n",
      "iteration 2735, dc_loss: 0.018622050061821938, tv_loss: 0.017027948051691055\n",
      "iteration 2736, dc_loss: 0.01862228475511074, tv_loss: 0.01702747493982315\n",
      "iteration 2737, dc_loss: 0.018620263785123825, tv_loss: 0.017029305920004845\n",
      "iteration 2738, dc_loss: 0.018624287098646164, tv_loss: 0.01702519692480564\n",
      "iteration 2739, dc_loss: 0.01861809939146042, tv_loss: 0.017031408846378326\n",
      "iteration 2740, dc_loss: 0.018626518547534943, tv_loss: 0.017023129388689995\n",
      "iteration 2741, dc_loss: 0.0186159648001194, tv_loss: 0.017033766955137253\n",
      "iteration 2742, dc_loss: 0.018628330901265144, tv_loss: 0.01702135056257248\n",
      "iteration 2743, dc_loss: 0.018616393208503723, tv_loss: 0.017033014446496964\n",
      "iteration 2744, dc_loss: 0.01862422004342079, tv_loss: 0.01702474243938923\n",
      "iteration 2745, dc_loss: 0.018619751557707787, tv_loss: 0.017028799280524254\n",
      "iteration 2746, dc_loss: 0.01862017810344696, tv_loss: 0.017028125002980232\n",
      "iteration 2747, dc_loss: 0.01862209476530552, tv_loss: 0.017026130110025406\n",
      "iteration 2748, dc_loss: 0.018618609756231308, tv_loss: 0.017029685899615288\n",
      "iteration 2749, dc_loss: 0.018622800707817078, tv_loss: 0.017025653272867203\n",
      "iteration 2750, dc_loss: 0.018618840724229813, tv_loss: 0.017029736191034317\n",
      "iteration 2751, dc_loss: 0.018622590228915215, tv_loss: 0.017025917768478394\n",
      "iteration 2752, dc_loss: 0.01861930824816227, tv_loss: 0.017028992995619774\n",
      "iteration 2753, dc_loss: 0.018620815128087997, tv_loss: 0.017027119174599648\n",
      "iteration 2754, dc_loss: 0.018620314076542854, tv_loss: 0.01702728681266308\n",
      "iteration 2755, dc_loss: 0.018618665635585785, tv_loss: 0.017028696835041046\n",
      "iteration 2756, dc_loss: 0.018621759489178658, tv_loss: 0.01702556014060974\n",
      "iteration 2757, dc_loss: 0.0186157263815403, tv_loss: 0.017031662166118622\n",
      "iteration 2758, dc_loss: 0.01862569898366928, tv_loss: 0.017021851614117622\n",
      "iteration 2759, dc_loss: 0.018613461405038834, tv_loss: 0.01703418418765068\n",
      "iteration 2760, dc_loss: 0.01862640492618084, tv_loss: 0.017021209001541138\n",
      "iteration 2761, dc_loss: 0.018614361062645912, tv_loss: 0.017032980918884277\n",
      "iteration 2762, dc_loss: 0.018622979521751404, tv_loss: 0.017023971304297447\n",
      "iteration 2763, dc_loss: 0.018617652356624603, tv_loss: 0.01702887937426567\n",
      "iteration 2764, dc_loss: 0.01861841231584549, tv_loss: 0.01702784188091755\n",
      "iteration 2765, dc_loss: 0.018619809299707413, tv_loss: 0.01702628657221794\n",
      "iteration 2766, dc_loss: 0.018616650253534317, tv_loss: 0.017029408365488052\n",
      "iteration 2767, dc_loss: 0.018620990216732025, tv_loss: 0.017025161534547806\n",
      "iteration 2768, dc_loss: 0.018617035821080208, tv_loss: 0.017029261216521263\n",
      "iteration 2769, dc_loss: 0.018620001152157784, tv_loss: 0.0170263834297657\n",
      "iteration 2770, dc_loss: 0.018617747351527214, tv_loss: 0.017028532922267914\n",
      "iteration 2771, dc_loss: 0.01861957274377346, tv_loss: 0.017026308923959732\n",
      "iteration 2772, dc_loss: 0.018617376685142517, tv_loss: 0.017028069123625755\n",
      "iteration 2773, dc_loss: 0.018617235124111176, tv_loss: 0.01702788844704628\n",
      "iteration 2774, dc_loss: 0.018619250506162643, tv_loss: 0.017025742679834366\n",
      "iteration 2775, dc_loss: 0.01861485466361046, tv_loss: 0.017030203714966774\n",
      "iteration 2776, dc_loss: 0.018622638657689095, tv_loss: 0.017022645100951195\n",
      "iteration 2777, dc_loss: 0.018611401319503784, tv_loss: 0.01703416183590889\n",
      "iteration 2778, dc_loss: 0.018625734373927116, tv_loss: 0.017019856721162796\n",
      "iteration 2779, dc_loss: 0.018610790371894836, tv_loss: 0.01703454740345478\n",
      "iteration 2780, dc_loss: 0.01862313039600849, tv_loss: 0.01702178828418255\n",
      "iteration 2781, dc_loss: 0.018614087253808975, tv_loss: 0.017030365765094757\n",
      "iteration 2782, dc_loss: 0.01861746236681938, tv_loss: 0.017026621848344803\n",
      "iteration 2783, dc_loss: 0.018618127331137657, tv_loss: 0.01702575385570526\n",
      "iteration 2784, dc_loss: 0.018614228814840317, tv_loss: 0.017029616981744766\n",
      "iteration 2785, dc_loss: 0.01861935667693615, tv_loss: 0.017024559900164604\n",
      "iteration 2786, dc_loss: 0.01861409842967987, tv_loss: 0.01702994480729103\n",
      "iteration 2787, dc_loss: 0.018618881702423096, tv_loss: 0.01702524721622467\n",
      "iteration 2788, dc_loss: 0.01861511543393135, tv_loss: 0.01702902838587761\n",
      "iteration 2789, dc_loss: 0.018618512898683548, tv_loss: 0.01702549308538437\n",
      "iteration 2790, dc_loss: 0.018615297973155975, tv_loss: 0.01702842116355896\n",
      "iteration 2791, dc_loss: 0.01861618086695671, tv_loss: 0.017027167603373528\n",
      "iteration 2792, dc_loss: 0.018616637215018272, tv_loss: 0.017026333138346672\n",
      "iteration 2793, dc_loss: 0.01861356757581234, tv_loss: 0.017029201611876488\n",
      "iteration 2794, dc_loss: 0.018619339913129807, tv_loss: 0.01702348329126835\n",
      "iteration 2795, dc_loss: 0.018610255792737007, tv_loss: 0.017032772302627563\n",
      "iteration 2796, dc_loss: 0.018623115494847298, tv_loss: 0.017020191997289658\n",
      "iteration 2797, dc_loss: 0.01860872469842434, tv_loss: 0.0170347448438406\n",
      "iteration 2798, dc_loss: 0.01862335577607155, tv_loss: 0.017019985243678093\n",
      "iteration 2799, dc_loss: 0.01860973611474037, tv_loss: 0.017033090814948082\n",
      "iteration 2800, dc_loss: 0.018618619069457054, tv_loss: 0.017023591324687004\n",
      "iteration 2801, dc_loss: 0.018614480271935463, tv_loss: 0.01702728308737278\n",
      "iteration 2802, dc_loss: 0.018613360822200775, tv_loss: 0.01702798157930374\n",
      "iteration 2803, dc_loss: 0.01861587166786194, tv_loss: 0.017025334760546684\n",
      "iteration 2804, dc_loss: 0.018613873049616814, tv_loss: 0.01702730916440487\n",
      "iteration 2805, dc_loss: 0.01861436851322651, tv_loss: 0.017026761546730995\n",
      "iteration 2806, dc_loss: 0.018614884465932846, tv_loss: 0.017026135697960854\n",
      "iteration 2807, dc_loss: 0.01861368492245674, tv_loss: 0.017027147114276886\n",
      "iteration 2808, dc_loss: 0.018614178523421288, tv_loss: 0.01702643372118473\n",
      "iteration 2809, dc_loss: 0.018614590167999268, tv_loss: 0.01702592521905899\n",
      "iteration 2810, dc_loss: 0.018613319844007492, tv_loss: 0.017027229070663452\n",
      "iteration 2811, dc_loss: 0.018614403903484344, tv_loss: 0.017026130110025406\n",
      "iteration 2812, dc_loss: 0.018613740801811218, tv_loss: 0.017026616260409355\n",
      "iteration 2813, dc_loss: 0.018612977117300034, tv_loss: 0.01702718250453472\n",
      "iteration 2814, dc_loss: 0.018615443259477615, tv_loss: 0.01702461950480938\n",
      "iteration 2815, dc_loss: 0.018611762672662735, tv_loss: 0.0170282069593668\n",
      "iteration 2816, dc_loss: 0.01861417666077614, tv_loss: 0.017025640234351158\n",
      "iteration 2817, dc_loss: 0.018613647669553757, tv_loss: 0.017026031389832497\n",
      "iteration 2818, dc_loss: 0.018612533807754517, tv_loss: 0.017027050256729126\n",
      "iteration 2819, dc_loss: 0.018614355474710464, tv_loss: 0.01702517829835415\n",
      "iteration 2820, dc_loss: 0.018612109124660492, tv_loss: 0.017027385532855988\n",
      "iteration 2821, dc_loss: 0.018613994121551514, tv_loss: 0.017025481909513474\n",
      "iteration 2822, dc_loss: 0.018611937761306763, tv_loss: 0.01702752709388733\n",
      "iteration 2823, dc_loss: 0.01861346699297428, tv_loss: 0.0170259028673172\n",
      "iteration 2824, dc_loss: 0.01861266791820526, tv_loss: 0.017026495188474655\n",
      "iteration 2825, dc_loss: 0.018612802028656006, tv_loss: 0.01702621951699257\n",
      "iteration 2826, dc_loss: 0.018612239509820938, tv_loss: 0.017026692628860474\n",
      "iteration 2827, dc_loss: 0.018613392487168312, tv_loss: 0.01702544093132019\n",
      "iteration 2828, dc_loss: 0.018611613661050797, tv_loss: 0.0170271135866642\n",
      "iteration 2829, dc_loss: 0.01861310750246048, tv_loss: 0.01702558808028698\n",
      "iteration 2830, dc_loss: 0.018611706793308258, tv_loss: 0.017027007415890694\n",
      "iteration 2831, dc_loss: 0.018612680956721306, tv_loss: 0.017025984823703766\n",
      "iteration 2832, dc_loss: 0.018611382693052292, tv_loss: 0.01702709123492241\n",
      "iteration 2833, dc_loss: 0.018613306805491447, tv_loss: 0.017024992033839226\n",
      "iteration 2834, dc_loss: 0.018610388040542603, tv_loss: 0.01702776364982128\n",
      "iteration 2835, dc_loss: 0.018613168969750404, tv_loss: 0.017024870961904526\n",
      "iteration 2836, dc_loss: 0.01861112378537655, tv_loss: 0.017026841640472412\n",
      "iteration 2837, dc_loss: 0.018611745908856392, tv_loss: 0.01702621579170227\n",
      "iteration 2838, dc_loss: 0.018611643463373184, tv_loss: 0.01702633500099182\n",
      "iteration 2839, dc_loss: 0.018611839041113853, tv_loss: 0.01702607423067093\n",
      "iteration 2840, dc_loss: 0.018610883504152298, tv_loss: 0.017026878893375397\n",
      "iteration 2841, dc_loss: 0.018612034618854523, tv_loss: 0.017025569453835487\n",
      "iteration 2842, dc_loss: 0.018610535189509392, tv_loss: 0.017026931047439575\n",
      "iteration 2843, dc_loss: 0.01861213520169258, tv_loss: 0.017025219276547432\n",
      "iteration 2844, dc_loss: 0.018610306084156036, tv_loss: 0.017026979476213455\n",
      "iteration 2845, dc_loss: 0.0186116024851799, tv_loss: 0.017025692388415337\n",
      "iteration 2846, dc_loss: 0.01861017569899559, tv_loss: 0.017027078196406364\n",
      "iteration 2847, dc_loss: 0.018611839041113853, tv_loss: 0.017025260254740715\n",
      "iteration 2848, dc_loss: 0.01860964111983776, tv_loss: 0.017027277499437332\n",
      "iteration 2849, dc_loss: 0.018611986190080643, tv_loss: 0.017024807631969452\n",
      "iteration 2850, dc_loss: 0.018609128892421722, tv_loss: 0.017027564346790314\n",
      "iteration 2851, dc_loss: 0.018611691892147064, tv_loss: 0.017024917528033257\n",
      "iteration 2852, dc_loss: 0.018609944730997086, tv_loss: 0.017026634886860847\n",
      "iteration 2853, dc_loss: 0.018610455095767975, tv_loss: 0.017026137560606003\n",
      "iteration 2854, dc_loss: 0.018610302358865738, tv_loss: 0.017026200890541077\n",
      "iteration 2855, dc_loss: 0.018609998747706413, tv_loss: 0.01702634058892727\n",
      "iteration 2856, dc_loss: 0.01861022226512432, tv_loss: 0.017025962471961975\n",
      "iteration 2857, dc_loss: 0.018610432744026184, tv_loss: 0.017025629058480263\n",
      "iteration 2858, dc_loss: 0.018608734011650085, tv_loss: 0.017027227208018303\n",
      "iteration 2859, dc_loss: 0.018611514940857887, tv_loss: 0.01702442206442356\n",
      "iteration 2860, dc_loss: 0.018608029931783676, tv_loss: 0.017027903348207474\n",
      "iteration 2861, dc_loss: 0.018610933795571327, tv_loss: 0.0170249305665493\n",
      "iteration 2862, dc_loss: 0.018608830869197845, tv_loss: 0.01702684722840786\n",
      "iteration 2863, dc_loss: 0.01860995963215828, tv_loss: 0.017025548964738846\n",
      "iteration 2864, dc_loss: 0.018609091639518738, tv_loss: 0.017026254907250404\n",
      "iteration 2865, dc_loss: 0.01860942877829075, tv_loss: 0.017025791108608246\n",
      "iteration 2866, dc_loss: 0.018609030172228813, tv_loss: 0.017026090994477272\n",
      "iteration 2867, dc_loss: 0.018609657883644104, tv_loss: 0.017025424167513847\n",
      "iteration 2868, dc_loss: 0.01860792003571987, tv_loss: 0.017027202993631363\n",
      "iteration 2869, dc_loss: 0.018610751256346703, tv_loss: 0.017024459317326546\n",
      "iteration 2870, dc_loss: 0.0186066422611475, tv_loss: 0.017028437927365303\n",
      "iteration 2871, dc_loss: 0.018610648810863495, tv_loss: 0.01702420599758625\n",
      "iteration 2872, dc_loss: 0.018608566373586655, tv_loss: 0.01702609471976757\n",
      "iteration 2873, dc_loss: 0.01860741525888443, tv_loss: 0.017027132213115692\n",
      "iteration 2874, dc_loss: 0.018610160797834396, tv_loss: 0.017024319618940353\n",
      "iteration 2875, dc_loss: 0.01860729791224003, tv_loss: 0.017027126625180244\n",
      "iteration 2876, dc_loss: 0.018608560785651207, tv_loss: 0.017025809735059738\n",
      "iteration 2877, dc_loss: 0.018608661368489265, tv_loss: 0.017025601118803024\n",
      "iteration 2878, dc_loss: 0.018607154488563538, tv_loss: 0.017026983201503754\n",
      "iteration 2879, dc_loss: 0.01861012913286686, tv_loss: 0.017023947089910507\n",
      "iteration 2880, dc_loss: 0.018605519086122513, tv_loss: 0.017028484493494034\n",
      "iteration 2881, dc_loss: 0.018609927967190742, tv_loss: 0.017023958265781403\n",
      "iteration 2882, dc_loss: 0.018607081845402718, tv_loss: 0.017026711255311966\n",
      "iteration 2883, dc_loss: 0.018607350066304207, tv_loss: 0.017026355490088463\n",
      "iteration 2884, dc_loss: 0.01860876940190792, tv_loss: 0.017024803906679153\n",
      "iteration 2885, dc_loss: 0.018606433644890785, tv_loss: 0.017027001827955246\n",
      "iteration 2886, dc_loss: 0.01860806718468666, tv_loss: 0.017025254666805267\n",
      "iteration 2887, dc_loss: 0.018607720732688904, tv_loss: 0.01702556572854519\n",
      "iteration 2888, dc_loss: 0.0186066422611475, tv_loss: 0.017026614397764206\n",
      "iteration 2889, dc_loss: 0.01860816404223442, tv_loss: 0.01702507771551609\n",
      "iteration 2890, dc_loss: 0.018606174737215042, tv_loss: 0.01702701859176159\n",
      "iteration 2891, dc_loss: 0.018608538433909416, tv_loss: 0.017024481669068336\n",
      "iteration 2892, dc_loss: 0.018605651333928108, tv_loss: 0.01702718436717987\n",
      "iteration 2893, dc_loss: 0.018607569858431816, tv_loss: 0.01702512428164482\n",
      "iteration 2894, dc_loss: 0.018606310710310936, tv_loss: 0.01702626422047615\n",
      "iteration 2895, dc_loss: 0.01860741153359413, tv_loss: 0.017025047913193703\n",
      "iteration 2896, dc_loss: 0.01860593818128109, tv_loss: 0.017026439309120178\n",
      "iteration 2897, dc_loss: 0.01860749162733555, tv_loss: 0.017024891451001167\n",
      "iteration 2898, dc_loss: 0.01860496960580349, tv_loss: 0.017027447000145912\n",
      "iteration 2899, dc_loss: 0.018608229234814644, tv_loss: 0.017024099826812744\n",
      "iteration 2900, dc_loss: 0.018605072051286697, tv_loss: 0.01702709309756756\n",
      "iteration 2901, dc_loss: 0.01860653981566429, tv_loss: 0.01702544279396534\n",
      "iteration 2902, dc_loss: 0.01860661432147026, tv_loss: 0.017025215551257133\n",
      "iteration 2903, dc_loss: 0.018605053424835205, tv_loss: 0.017026692628860474\n",
      "iteration 2904, dc_loss: 0.018606960773468018, tv_loss: 0.01702471263706684\n",
      "iteration 2905, dc_loss: 0.018605206161737442, tv_loss: 0.01702640764415264\n",
      "iteration 2906, dc_loss: 0.018605662509799004, tv_loss: 0.01702587492763996\n",
      "iteration 2907, dc_loss: 0.018606500700116158, tv_loss: 0.017024947330355644\n",
      "iteration 2908, dc_loss: 0.018604222685098648, tv_loss: 0.01702713780105114\n",
      "iteration 2909, dc_loss: 0.018607573583722115, tv_loss: 0.017023753374814987\n",
      "iteration 2910, dc_loss: 0.018603041768074036, tv_loss: 0.017028173431754112\n",
      "iteration 2911, dc_loss: 0.018607160076498985, tv_loss: 0.017023945227265358\n",
      "iteration 2912, dc_loss: 0.018605178222060204, tv_loss: 0.017025824636220932\n",
      "iteration 2913, dc_loss: 0.018603820353746414, tv_loss: 0.017027132213115692\n",
      "iteration 2914, dc_loss: 0.01860685460269451, tv_loss: 0.01702401414513588\n",
      "iteration 2915, dc_loss: 0.018603885546326637, tv_loss: 0.017026832327246666\n",
      "iteration 2916, dc_loss: 0.018605230376124382, tv_loss: 0.01702531799674034\n",
      "iteration 2917, dc_loss: 0.0186053067445755, tv_loss: 0.017025098204612732\n",
      "iteration 2918, dc_loss: 0.018603460863232613, tv_loss: 0.017026862129569054\n",
      "iteration 2919, dc_loss: 0.01860661804676056, tv_loss: 0.01702369563281536\n",
      "iteration 2920, dc_loss: 0.018602361902594566, tv_loss: 0.017027931287884712\n",
      "iteration 2921, dc_loss: 0.018606748431921005, tv_loss: 0.01702350564301014\n",
      "iteration 2922, dc_loss: 0.018603097647428513, tv_loss: 0.017026998102664948\n",
      "iteration 2923, dc_loss: 0.018604235723614693, tv_loss: 0.01702568307518959\n",
      "iteration 2924, dc_loss: 0.018604764714837074, tv_loss: 0.017024975270032883\n",
      "iteration 2925, dc_loss: 0.018604101613163948, tv_loss: 0.017025509849190712\n",
      "iteration 2926, dc_loss: 0.01860373094677925, tv_loss: 0.01702575385570526\n",
      "iteration 2927, dc_loss: 0.018604682758450508, tv_loss: 0.01702474243938923\n",
      "iteration 2928, dc_loss: 0.01860259473323822, tv_loss: 0.017026815563440323\n",
      "iteration 2929, dc_loss: 0.01860557124018669, tv_loss: 0.017023921012878418\n",
      "iteration 2930, dc_loss: 0.018601693212985992, tv_loss: 0.017027782276272774\n",
      "iteration 2931, dc_loss: 0.01860598288476467, tv_loss: 0.017023282125592232\n",
      "iteration 2932, dc_loss: 0.0186017993837595, tv_loss: 0.01702726073563099\n",
      "iteration 2933, dc_loss: 0.018604198470711708, tv_loss: 0.0170246884226799\n",
      "iteration 2934, dc_loss: 0.018603701144456863, tv_loss: 0.017025087028741837\n",
      "iteration 2935, dc_loss: 0.018602579832077026, tv_loss: 0.017026163637638092\n",
      "iteration 2936, dc_loss: 0.018603507429361343, tv_loss: 0.017025219276547432\n",
      "iteration 2937, dc_loss: 0.018603652715682983, tv_loss: 0.017025042325258255\n",
      "iteration 2938, dc_loss: 0.018601734191179276, tv_loss: 0.017026826739311218\n",
      "iteration 2939, dc_loss: 0.018605045974254608, tv_loss: 0.01702340878546238\n",
      "iteration 2940, dc_loss: 0.01860038749873638, tv_loss: 0.01702801138162613\n",
      "iteration 2941, dc_loss: 0.018605509772896767, tv_loss: 0.017022809013724327\n",
      "iteration 2942, dc_loss: 0.018600814044475555, tv_loss: 0.017027419060468674\n",
      "iteration 2943, dc_loss: 0.018603159114718437, tv_loss: 0.017024967819452286\n",
      "iteration 2944, dc_loss: 0.01860327087342739, tv_loss: 0.017024707049131393\n",
      "iteration 2945, dc_loss: 0.01860135607421398, tv_loss: 0.01702645979821682\n",
      "iteration 2946, dc_loss: 0.018602795898914337, tv_loss: 0.017024876549839973\n",
      "iteration 2947, dc_loss: 0.018602684140205383, tv_loss: 0.017024869099259377\n",
      "iteration 2948, dc_loss: 0.01860102079808712, tv_loss: 0.017026478424668312\n",
      "iteration 2949, dc_loss: 0.018603801727294922, tv_loss: 0.017023717984557152\n",
      "iteration 2950, dc_loss: 0.018599901348352432, tv_loss: 0.017027704045176506\n",
      "iteration 2951, dc_loss: 0.01860453188419342, tv_loss: 0.01702296733856201\n",
      "iteration 2952, dc_loss: 0.01859956979751587, tv_loss: 0.017027700319886208\n",
      "iteration 2953, dc_loss: 0.018603231757879257, tv_loss: 0.017023835331201553\n",
      "iteration 2954, dc_loss: 0.01860164850950241, tv_loss: 0.017025286331772804\n",
      "iteration 2955, dc_loss: 0.018600599840283394, tv_loss: 0.017026273533701897\n",
      "iteration 2956, dc_loss: 0.018602699041366577, tv_loss: 0.01702411286532879\n",
      "iteration 2957, dc_loss: 0.018601033836603165, tv_loss: 0.017025722190737724\n",
      "iteration 2958, dc_loss: 0.01860104687511921, tv_loss: 0.0170256607234478\n",
      "iteration 2959, dc_loss: 0.01860208250582218, tv_loss: 0.017024530097842216\n",
      "iteration 2960, dc_loss: 0.018599875271320343, tv_loss: 0.017026621848344803\n",
      "iteration 2961, dc_loss: 0.018602881580591202, tv_loss: 0.01702355407178402\n",
      "iteration 2962, dc_loss: 0.018598604947328568, tv_loss: 0.017027761787176132\n",
      "iteration 2963, dc_loss: 0.018603280186653137, tv_loss: 0.017023013904690742\n",
      "iteration 2964, dc_loss: 0.01860017143189907, tv_loss: 0.017026018351316452\n",
      "iteration 2965, dc_loss: 0.0185996200889349, tv_loss: 0.017026478424668312\n",
      "iteration 2966, dc_loss: 0.018602726981043816, tv_loss: 0.0170232355594635\n",
      "iteration 2967, dc_loss: 0.01859903521835804, tv_loss: 0.01702677644789219\n",
      "iteration 2968, dc_loss: 0.01860113814473152, tv_loss: 0.017024511471390724\n",
      "iteration 2969, dc_loss: 0.0186004638671875, tv_loss: 0.017025064677000046\n",
      "iteration 2970, dc_loss: 0.018599679693579674, tv_loss: 0.017025766894221306\n",
      "iteration 2971, dc_loss: 0.018601350486278534, tv_loss: 0.01702410727739334\n",
      "iteration 2972, dc_loss: 0.0185982808470726, tv_loss: 0.01702723279595375\n",
      "iteration 2973, dc_loss: 0.01860286481678486, tv_loss: 0.017022574320435524\n",
      "iteration 2974, dc_loss: 0.018597079440951347, tv_loss: 0.01702815853059292\n",
      "iteration 2975, dc_loss: 0.01860208623111248, tv_loss: 0.017022937536239624\n",
      "iteration 2976, dc_loss: 0.018599478527903557, tv_loss: 0.017025377601385117\n",
      "iteration 2977, dc_loss: 0.018598362803459167, tv_loss: 0.017026416957378387\n",
      "iteration 2978, dc_loss: 0.018601354211568832, tv_loss: 0.01702340506017208\n",
      "iteration 2979, dc_loss: 0.018598558381199837, tv_loss: 0.01702624000608921\n",
      "iteration 2980, dc_loss: 0.018600091338157654, tv_loss: 0.017024733126163483\n",
      "iteration 2981, dc_loss: 0.018599312752485275, tv_loss: 0.017025379464030266\n",
      "iteration 2982, dc_loss: 0.01859951578080654, tv_loss: 0.017024975270032883\n",
      "iteration 2983, dc_loss: 0.0185998622328043, tv_loss: 0.01702447421848774\n",
      "iteration 2984, dc_loss: 0.018597327172756195, tv_loss: 0.017026925459504128\n",
      "iteration 2985, dc_loss: 0.018601736053824425, tv_loss: 0.01702248491346836\n",
      "iteration 2986, dc_loss: 0.0185964684933424, tv_loss: 0.017027728259563446\n",
      "iteration 2987, dc_loss: 0.01860079914331436, tv_loss: 0.017023272812366486\n",
      "iteration 2988, dc_loss: 0.018597902730107307, tv_loss: 0.017026007175445557\n",
      "iteration 2989, dc_loss: 0.01859925128519535, tv_loss: 0.017024481669068336\n",
      "iteration 2990, dc_loss: 0.018597956746816635, tv_loss: 0.01702561043202877\n",
      "iteration 2991, dc_loss: 0.018599271774291992, tv_loss: 0.017024192959070206\n",
      "iteration 2992, dc_loss: 0.018597960472106934, tv_loss: 0.017025455832481384\n",
      "iteration 2993, dc_loss: 0.018598943948745728, tv_loss: 0.01702452450990677\n",
      "iteration 2994, dc_loss: 0.018597448244690895, tv_loss: 0.017026053741574287\n",
      "iteration 2995, dc_loss: 0.018599534407258034, tv_loss: 0.01702386885881424\n",
      "iteration 2996, dc_loss: 0.018596649169921875, tv_loss: 0.017026538029313087\n",
      "iteration 2997, dc_loss: 0.01859947293996811, tv_loss: 0.017023511230945587\n",
      "iteration 2998, dc_loss: 0.018597537651658058, tv_loss: 0.01702527329325676\n",
      "iteration 2999, dc_loss: 0.018597135320305824, tv_loss: 0.017025629058480263\n",
      "iteration 3000, dc_loss: 0.018598318099975586, tv_loss: 0.017024394124746323\n",
      "iteration 3001, dc_loss: 0.018598221242427826, tv_loss: 0.017024463042616844\n",
      "iteration 3002, dc_loss: 0.018596366047859192, tv_loss: 0.0170262660831213\n",
      "iteration 3003, dc_loss: 0.018599361181259155, tv_loss: 0.017023149877786636\n",
      "iteration 3004, dc_loss: 0.01859506033360958, tv_loss: 0.017027391120791435\n",
      "iteration 3005, dc_loss: 0.01860053651034832, tv_loss: 0.017021827399730682\n",
      "iteration 3006, dc_loss: 0.018594371154904366, tv_loss: 0.01702789030969143\n",
      "iteration 3007, dc_loss: 0.018599357455968857, tv_loss: 0.017022818326950073\n",
      "iteration 3008, dc_loss: 0.018596552312374115, tv_loss: 0.017025470733642578\n",
      "iteration 3009, dc_loss: 0.01859588548541069, tv_loss: 0.01702597364783287\n",
      "iteration 3010, dc_loss: 0.018598783761262894, tv_loss: 0.017022941261529922\n",
      "iteration 3011, dc_loss: 0.018595699220895767, tv_loss: 0.017025930806994438\n",
      "iteration 3012, dc_loss: 0.018596937879920006, tv_loss: 0.0170246921479702\n",
      "iteration 3013, dc_loss: 0.018597301095724106, tv_loss: 0.017024356871843338\n",
      "iteration 3014, dc_loss: 0.01859644055366516, tv_loss: 0.01702520251274109\n",
      "iteration 3015, dc_loss: 0.01859707199037075, tv_loss: 0.017024502158164978\n",
      "iteration 3016, dc_loss: 0.018595164641737938, tv_loss: 0.0170261450111866\n",
      "iteration 3017, dc_loss: 0.018598275259137154, tv_loss: 0.017022831365466118\n",
      "iteration 3018, dc_loss: 0.018594354391098022, tv_loss: 0.017026668414473534\n",
      "iteration 3019, dc_loss: 0.018597902730107307, tv_loss: 0.017023099586367607\n",
      "iteration 3020, dc_loss: 0.0185948945581913, tv_loss: 0.01702609471976757\n",
      "iteration 3021, dc_loss: 0.018597617745399475, tv_loss: 0.017023349180817604\n",
      "iteration 3022, dc_loss: 0.01859409548342228, tv_loss: 0.01702674850821495\n",
      "iteration 3023, dc_loss: 0.018597979098558426, tv_loss: 0.01702265255153179\n",
      "iteration 3024, dc_loss: 0.018593929708003998, tv_loss: 0.017026515677571297\n",
      "iteration 3025, dc_loss: 0.01859653741121292, tv_loss: 0.017023826017975807\n",
      "iteration 3026, dc_loss: 0.018595663830637932, tv_loss: 0.017024705186486244\n",
      "iteration 3027, dc_loss: 0.018595566973090172, tv_loss: 0.017024822533130646\n",
      "iteration 3028, dc_loss: 0.01859544776380062, tv_loss: 0.01702488213777542\n",
      "iteration 3029, dc_loss: 0.018594952300190926, tv_loss: 0.01702512800693512\n",
      "iteration 3030, dc_loss: 0.018596040084958076, tv_loss: 0.017023811116814613\n",
      "iteration 3031, dc_loss: 0.018594376742839813, tv_loss: 0.017025332897901535\n",
      "iteration 3032, dc_loss: 0.018595445901155472, tv_loss: 0.017024192959070206\n",
      "iteration 3033, dc_loss: 0.01859532669186592, tv_loss: 0.01702430658042431\n",
      "iteration 3034, dc_loss: 0.018593398854136467, tv_loss: 0.017026308923959732\n",
      "iteration 3035, dc_loss: 0.018597818911075592, tv_loss: 0.01702180877327919\n",
      "iteration 3036, dc_loss: 0.018591100350022316, tv_loss: 0.017028428614139557\n",
      "iteration 3037, dc_loss: 0.018598180264234543, tv_loss: 0.01702122576534748\n",
      "iteration 3038, dc_loss: 0.018592344596982002, tv_loss: 0.01702694222331047\n",
      "iteration 3039, dc_loss: 0.018595434725284576, tv_loss: 0.01702382043004036\n",
      "iteration 3040, dc_loss: 0.01859508454799652, tv_loss: 0.01702406443655491\n",
      "iteration 3041, dc_loss: 0.018592966720461845, tv_loss: 0.0170260239392519\n",
      "iteration 3042, dc_loss: 0.018595173954963684, tv_loss: 0.017023619264364243\n",
      "iteration 3043, dc_loss: 0.01859460212290287, tv_loss: 0.017024043947458267\n",
      "iteration 3044, dc_loss: 0.018592458218336105, tv_loss: 0.01702611707150936\n",
      "iteration 3045, dc_loss: 0.018596330657601357, tv_loss: 0.01702231355011463\n",
      "iteration 3046, dc_loss: 0.018591146916151047, tv_loss: 0.017027579247951508\n",
      "iteration 3047, dc_loss: 0.01859702169895172, tv_loss: 0.01702151820063591\n",
      "iteration 3048, dc_loss: 0.01859176531434059, tv_loss: 0.017026538029313087\n",
      "iteration 3049, dc_loss: 0.018593868240714073, tv_loss: 0.017024289816617966\n",
      "iteration 3050, dc_loss: 0.018594752997159958, tv_loss: 0.0170233603566885\n",
      "iteration 3051, dc_loss: 0.018591560423374176, tv_loss: 0.017026569694280624\n",
      "iteration 3052, dc_loss: 0.01859571970999241, tv_loss: 0.01702236942946911\n",
      "iteration 3053, dc_loss: 0.01859154924750328, tv_loss: 0.017026398330926895\n",
      "iteration 3054, dc_loss: 0.018594147637486458, tv_loss: 0.017023639753460884\n",
      "iteration 3055, dc_loss: 0.01859368197619915, tv_loss: 0.017023973166942596\n",
      "iteration 3056, dc_loss: 0.01859159767627716, tv_loss: 0.017025984823703766\n",
      "iteration 3057, dc_loss: 0.018595244735479355, tv_loss: 0.017022352665662766\n",
      "iteration 3058, dc_loss: 0.018590113148093224, tv_loss: 0.01702743209898472\n",
      "iteration 3059, dc_loss: 0.01859595999121666, tv_loss: 0.01702141761779785\n",
      "iteration 3060, dc_loss: 0.01859036646783352, tv_loss: 0.017026860266923904\n",
      "iteration 3061, dc_loss: 0.018594032153487206, tv_loss: 0.017023097723722458\n",
      "iteration 3062, dc_loss: 0.01859222911298275, tv_loss: 0.017024818807840347\n",
      "iteration 3063, dc_loss: 0.018592597916722298, tv_loss: 0.017024384811520576\n",
      "iteration 3064, dc_loss: 0.018592212349176407, tv_loss: 0.017024677246809006\n",
      "iteration 3065, dc_loss: 0.01859329268336296, tv_loss: 0.0170234777033329\n",
      "iteration 3066, dc_loss: 0.018590064719319344, tv_loss: 0.01702660694718361\n",
      "iteration 3067, dc_loss: 0.018595309928059578, tv_loss: 0.017021331936120987\n",
      "iteration 3068, dc_loss: 0.018588921055197716, tv_loss: 0.0170277189463377\n",
      "iteration 3069, dc_loss: 0.018594656139612198, tv_loss: 0.017021866515278816\n",
      "iteration 3070, dc_loss: 0.01859070546925068, tv_loss: 0.017025668174028397\n",
      "iteration 3071, dc_loss: 0.01859111711382866, tv_loss: 0.017025085166096687\n",
      "iteration 3072, dc_loss: 0.018594177439808846, tv_loss: 0.017021911218762398\n",
      "iteration 3073, dc_loss: 0.018588658422231674, tv_loss: 0.01702733337879181\n",
      "iteration 3074, dc_loss: 0.018594197928905487, tv_loss: 0.017021695151925087\n",
      "iteration 3075, dc_loss: 0.01859026402235031, tv_loss: 0.01702554151415825\n",
      "iteration 3076, dc_loss: 0.01859169453382492, tv_loss: 0.017024051398038864\n",
      "iteration 3077, dc_loss: 0.018591823056340218, tv_loss: 0.01702391542494297\n",
      "iteration 3078, dc_loss: 0.018590005114674568, tv_loss: 0.017025696113705635\n",
      "iteration 3079, dc_loss: 0.018593372777104378, tv_loss: 0.017022183164954185\n",
      "iteration 3080, dc_loss: 0.018588021397590637, tv_loss: 0.017027365043759346\n",
      "iteration 3081, dc_loss: 0.01859390176832676, tv_loss: 0.01702134683728218\n",
      "iteration 3082, dc_loss: 0.018589496612548828, tv_loss: 0.017025647684931755\n",
      "iteration 3083, dc_loss: 0.018590757623314857, tv_loss: 0.017024368047714233\n",
      "iteration 3084, dc_loss: 0.018590927124023438, tv_loss: 0.017024146392941475\n",
      "iteration 3085, dc_loss: 0.018591178581118584, tv_loss: 0.01702377013862133\n",
      "iteration 3086, dc_loss: 0.018589021638035774, tv_loss: 0.017025819048285484\n",
      "iteration 3087, dc_loss: 0.01859293133020401, tv_loss: 0.01702183298766613\n",
      "iteration 3088, dc_loss: 0.018587548285722733, tv_loss: 0.01702716574072838\n",
      "iteration 3089, dc_loss: 0.018593868240714073, tv_loss: 0.017020845785737038\n",
      "iteration 3090, dc_loss: 0.018586907535791397, tv_loss: 0.017027754336595535\n",
      "iteration 3091, dc_loss: 0.018592866137623787, tv_loss: 0.017021629959344864\n",
      "iteration 3092, dc_loss: 0.018588455393910408, tv_loss: 0.0170257817953825\n",
      "iteration 3093, dc_loss: 0.018590150400996208, tv_loss: 0.017023896798491478\n",
      "iteration 3094, dc_loss: 0.018590934574604034, tv_loss: 0.017023026943206787\n",
      "iteration 3095, dc_loss: 0.018588295206427574, tv_loss: 0.017025673761963844\n",
      "iteration 3096, dc_loss: 0.018590880557894707, tv_loss: 0.01702316850423813\n",
      "iteration 3097, dc_loss: 0.01858934387564659, tv_loss: 0.017024802044034004\n",
      "iteration 3098, dc_loss: 0.01859019696712494, tv_loss: 0.01702384650707245\n",
      "iteration 3099, dc_loss: 0.0185894425958395, tv_loss: 0.017024360597133636\n",
      "iteration 3100, dc_loss: 0.018589114770293236, tv_loss: 0.017024457454681396\n",
      "iteration 3101, dc_loss: 0.0185902900993824, tv_loss: 0.017023136839270592\n",
      "iteration 3102, dc_loss: 0.01858719065785408, tv_loss: 0.01702621392905712\n",
      "iteration 3103, dc_loss: 0.018592389300465584, tv_loss: 0.01702106185257435\n",
      "iteration 3104, dc_loss: 0.018585536628961563, tv_loss: 0.01702790893614292\n",
      "iteration 3105, dc_loss: 0.01859278418123722, tv_loss: 0.017020586878061295\n",
      "iteration 3106, dc_loss: 0.018586138263344765, tv_loss: 0.017027053982019424\n",
      "iteration 3107, dc_loss: 0.018591003492474556, tv_loss: 0.01702198199927807\n",
      "iteration 3108, dc_loss: 0.018587417900562286, tv_loss: 0.01702539622783661\n",
      "iteration 3109, dc_loss: 0.018589282408356667, tv_loss: 0.01702340878546238\n",
      "iteration 3110, dc_loss: 0.018588583916425705, tv_loss: 0.01702403463423252\n",
      "iteration 3111, dc_loss: 0.018587762489914894, tv_loss: 0.017024848610162735\n",
      "iteration 3112, dc_loss: 0.01858920231461525, tv_loss: 0.01702340506017208\n",
      "iteration 3113, dc_loss: 0.01858811266720295, tv_loss: 0.017024455592036247\n",
      "iteration 3114, dc_loss: 0.018588831648230553, tv_loss: 0.01702360063791275\n",
      "iteration 3115, dc_loss: 0.01858789101243019, tv_loss: 0.017024371773004532\n",
      "iteration 3116, dc_loss: 0.0185873843729496, tv_loss: 0.01702466420829296\n",
      "iteration 3117, dc_loss: 0.018589304760098457, tv_loss: 0.017022596672177315\n",
      "iteration 3118, dc_loss: 0.01858593337237835, tv_loss: 0.01702595315873623\n",
      "iteration 3119, dc_loss: 0.01859111711382866, tv_loss: 0.01702079176902771\n",
      "iteration 3120, dc_loss: 0.018583469092845917, tv_loss: 0.017028504982590675\n",
      "iteration 3121, dc_loss: 0.01859223283827305, tv_loss: 0.017019717022776604\n",
      "iteration 3122, dc_loss: 0.01858457177877426, tv_loss: 0.0170272309333086\n",
      "iteration 3123, dc_loss: 0.018589509651064873, tv_loss: 0.01702205464243889\n",
      "iteration 3124, dc_loss: 0.01858687587082386, tv_loss: 0.01702447049319744\n",
      "iteration 3125, dc_loss: 0.01858695037662983, tv_loss: 0.01702420972287655\n",
      "iteration 3126, dc_loss: 0.018587838858366013, tv_loss: 0.01702316664159298\n",
      "iteration 3127, dc_loss: 0.01858615316450596, tv_loss: 0.01702478528022766\n",
      "iteration 3128, dc_loss: 0.01858796365559101, tv_loss: 0.017022978514432907\n",
      "iteration 3129, dc_loss: 0.01858636550605297, tv_loss: 0.0170246884226799\n",
      "iteration 3130, dc_loss: 0.01858745701611042, tv_loss: 0.01702369935810566\n",
      "iteration 3131, dc_loss: 0.018587322905659676, tv_loss: 0.017023740336298943\n",
      "iteration 3132, dc_loss: 0.018585901707410812, tv_loss: 0.017024917528033257\n",
      "iteration 3133, dc_loss: 0.018588148057460785, tv_loss: 0.017022376880049706\n",
      "iteration 3134, dc_loss: 0.018585169687867165, tv_loss: 0.017025163397192955\n",
      "iteration 3135, dc_loss: 0.018587443977594376, tv_loss: 0.01702285185456276\n",
      "iteration 3136, dc_loss: 0.018585000187158585, tv_loss: 0.01702537201344967\n",
      "iteration 3137, dc_loss: 0.018589165061712265, tv_loss: 0.01702127419412136\n",
      "iteration 3138, dc_loss: 0.018582506105303764, tv_loss: 0.01702793315052986\n",
      "iteration 3139, dc_loss: 0.018591221421957016, tv_loss: 0.01701909862458706\n",
      "iteration 3140, dc_loss: 0.018581656739115715, tv_loss: 0.01702847145497799\n",
      "iteration 3141, dc_loss: 0.01858951710164547, tv_loss: 0.01702035218477249\n",
      "iteration 3142, dc_loss: 0.018584072589874268, tv_loss: 0.01702561043202877\n",
      "iteration 3143, dc_loss: 0.018585799261927605, tv_loss: 0.017023805528879166\n",
      "iteration 3144, dc_loss: 0.018587181344628334, tv_loss: 0.017022429034113884\n",
      "iteration 3145, dc_loss: 0.018583403900265694, tv_loss: 0.017026223242282867\n",
      "iteration 3146, dc_loss: 0.018588494509458542, tv_loss: 0.017021089792251587\n",
      "iteration 3147, dc_loss: 0.018583353608846664, tv_loss: 0.017026124522089958\n",
      "iteration 3148, dc_loss: 0.018587136641144753, tv_loss: 0.017022132873535156\n",
      "iteration 3149, dc_loss: 0.01858486980199814, tv_loss: 0.017024198547005653\n",
      "iteration 3150, dc_loss: 0.018584178760647774, tv_loss: 0.017024779692292213\n",
      "iteration 3151, dc_loss: 0.018587715923786163, tv_loss: 0.017021259292960167\n",
      "iteration 3152, dc_loss: 0.018581287935376167, tv_loss: 0.01702769286930561\n",
      "iteration 3153, dc_loss: 0.018589865416288376, tv_loss: 0.01701912097632885\n",
      "iteration 3154, dc_loss: 0.01858038827776909, tv_loss: 0.017028536647558212\n",
      "iteration 3155, dc_loss: 0.018589045852422714, tv_loss: 0.01701975427567959\n",
      "iteration 3156, dc_loss: 0.018582791090011597, tv_loss: 0.017025848850607872\n",
      "iteration 3157, dc_loss: 0.01858510449528694, tv_loss: 0.01702333241701126\n",
      "iteration 3158, dc_loss: 0.01858491264283657, tv_loss: 0.01702333614230156\n",
      "iteration 3159, dc_loss: 0.018584027886390686, tv_loss: 0.017024053260684013\n",
      "iteration 3160, dc_loss: 0.01858459785580635, tv_loss: 0.017023393884301186\n",
      "iteration 3161, dc_loss: 0.018584096804261208, tv_loss: 0.017023902386426926\n",
      "iteration 3162, dc_loss: 0.018584338948130608, tv_loss: 0.017023740336298943\n",
      "iteration 3163, dc_loss: 0.01858496107161045, tv_loss: 0.01702316850423813\n",
      "iteration 3164, dc_loss: 0.01858324557542801, tv_loss: 0.017024796456098557\n",
      "iteration 3165, dc_loss: 0.018584897741675377, tv_loss: 0.017022905871272087\n",
      "iteration 3166, dc_loss: 0.018583161756396294, tv_loss: 0.01702435314655304\n",
      "iteration 3167, dc_loss: 0.018584568053483963, tv_loss: 0.017022743821144104\n",
      "iteration 3168, dc_loss: 0.018582910299301147, tv_loss: 0.017024321481585503\n",
      "iteration 3169, dc_loss: 0.018584569916129112, tv_loss: 0.01702266000211239\n",
      "iteration 3170, dc_loss: 0.018581632524728775, tv_loss: 0.017025643959641457\n",
      "iteration 3171, dc_loss: 0.018587889149785042, tv_loss: 0.017019471153616905\n",
      "iteration 3172, dc_loss: 0.018578168004751205, tv_loss: 0.01702929101884365\n",
      "iteration 3173, dc_loss: 0.018589157611131668, tv_loss: 0.01701822131872177\n",
      "iteration 3174, dc_loss: 0.018578892573714256, tv_loss: 0.017028184607625008\n",
      "iteration 3175, dc_loss: 0.018586188554763794, tv_loss: 0.01702052727341652\n",
      "iteration 3176, dc_loss: 0.018582485616207123, tv_loss: 0.01702401041984558\n",
      "iteration 3177, dc_loss: 0.018581394106149673, tv_loss: 0.01702500879764557\n",
      "iteration 3178, dc_loss: 0.018585093319416046, tv_loss: 0.01702132076025009\n",
      "iteration 3179, dc_loss: 0.018580473959445953, tv_loss: 0.017026033252477646\n",
      "iteration 3180, dc_loss: 0.018585754558444023, tv_loss: 0.017020871862769127\n",
      "iteration 3181, dc_loss: 0.018580589443445206, tv_loss: 0.017026036977767944\n",
      "iteration 3182, dc_loss: 0.01858457364141941, tv_loss: 0.017021892592310905\n",
      "iteration 3183, dc_loss: 0.018582111224532127, tv_loss: 0.017024068161845207\n",
      "iteration 3184, dc_loss: 0.01858193427324295, tv_loss: 0.017023973166942596\n",
      "iteration 3185, dc_loss: 0.018583981320261955, tv_loss: 0.017021771520376205\n",
      "iteration 3186, dc_loss: 0.018579088151454926, tv_loss: 0.017026683315634727\n",
      "iteration 3187, dc_loss: 0.01858675293624401, tv_loss: 0.017019135877490044\n",
      "iteration 3188, dc_loss: 0.018577508628368378, tv_loss: 0.01702849380671978\n",
      "iteration 3189, dc_loss: 0.018587296828627586, tv_loss: 0.01701865904033184\n",
      "iteration 3190, dc_loss: 0.018578216433525085, tv_loss: 0.01702754572033882\n",
      "iteration 3191, dc_loss: 0.018584858626127243, tv_loss: 0.017020566388964653\n",
      "iteration 3192, dc_loss: 0.01858036033809185, tv_loss: 0.017024753615260124\n",
      "iteration 3193, dc_loss: 0.018581561744213104, tv_loss: 0.01702333427965641\n",
      "iteration 3194, dc_loss: 0.018582336604595184, tv_loss: 0.017022494226694107\n",
      "iteration 3195, dc_loss: 0.018580377101898193, tv_loss: 0.017024507746100426\n",
      "iteration 3196, dc_loss: 0.018582789227366447, tv_loss: 0.017022250220179558\n",
      "iteration 3197, dc_loss: 0.018580559641122818, tv_loss: 0.01702454872429371\n",
      "iteration 3198, dc_loss: 0.018582437187433243, tv_loss: 0.01702255569398403\n",
      "iteration 3199, dc_loss: 0.018581312149763107, tv_loss: 0.017023442313075066\n",
      "iteration 3200, dc_loss: 0.018580524250864983, tv_loss: 0.017023958265781403\n",
      "iteration 3201, dc_loss: 0.018582243472337723, tv_loss: 0.017022037878632545\n",
      "iteration 3202, dc_loss: 0.01857958920300007, tv_loss: 0.017024395987391472\n",
      "iteration 3203, dc_loss: 0.018581563606858253, tv_loss: 0.017022211104631424\n",
      "iteration 3204, dc_loss: 0.018580835312604904, tv_loss: 0.017022935673594475\n",
      "iteration 3205, dc_loss: 0.018579784780740738, tv_loss: 0.017024027183651924\n",
      "iteration 3206, dc_loss: 0.01858181320130825, tv_loss: 0.017021851614117622\n",
      "iteration 3207, dc_loss: 0.018579503521323204, tv_loss: 0.0170239619910717\n",
      "iteration 3208, dc_loss: 0.01858113519847393, tv_loss: 0.01702221855521202\n",
      "iteration 3209, dc_loss: 0.018580449745059013, tv_loss: 0.017022904008626938\n",
      "iteration 3210, dc_loss: 0.01857956126332283, tv_loss: 0.017023801803588867\n",
      "iteration 3211, dc_loss: 0.018581725656986237, tv_loss: 0.017021527513861656\n",
      "iteration 3212, dc_loss: 0.01857868582010269, tv_loss: 0.017024418339133263\n",
      "iteration 3213, dc_loss: 0.01858118176460266, tv_loss: 0.0170217864215374\n",
      "iteration 3214, dc_loss: 0.018579993396997452, tv_loss: 0.01702287793159485\n",
      "iteration 3215, dc_loss: 0.018579410389065742, tv_loss: 0.017023438587784767\n",
      "iteration 3216, dc_loss: 0.018581083044409752, tv_loss: 0.01702178828418255\n",
      "iteration 3217, dc_loss: 0.018578385934233665, tv_loss: 0.017024453729391098\n",
      "iteration 3218, dc_loss: 0.018581319600343704, tv_loss: 0.017021406441926956\n",
      "iteration 3219, dc_loss: 0.01857915334403515, tv_loss: 0.01702342927455902\n",
      "iteration 3220, dc_loss: 0.01857946813106537, tv_loss: 0.017023004591464996\n",
      "iteration 3221, dc_loss: 0.018580205738544464, tv_loss: 0.017022183164954185\n",
      "iteration 3222, dc_loss: 0.018578961491584778, tv_loss: 0.017023367807269096\n",
      "iteration 3223, dc_loss: 0.018580352887511253, tv_loss: 0.017021896317601204\n",
      "iteration 3224, dc_loss: 0.018578659743070602, tv_loss: 0.01702347956597805\n",
      "iteration 3225, dc_loss: 0.018579697236418724, tv_loss: 0.017022350803017616\n",
      "iteration 3226, dc_loss: 0.01857943646609783, tv_loss: 0.017022548243403435\n",
      "iteration 3227, dc_loss: 0.018579185009002686, tv_loss: 0.017022766172885895\n",
      "iteration 3228, dc_loss: 0.018578963354229927, tv_loss: 0.017023010179400444\n",
      "iteration 3229, dc_loss: 0.018579835072159767, tv_loss: 0.017022205516695976\n",
      "iteration 3230, dc_loss: 0.01857777312397957, tv_loss: 0.017024248838424683\n",
      "iteration 3231, dc_loss: 0.018580516800284386, tv_loss: 0.017021309584379196\n",
      "iteration 3232, dc_loss: 0.018578074872493744, tv_loss: 0.017023570835590363\n",
      "iteration 3233, dc_loss: 0.018578799441456795, tv_loss: 0.01702272705733776\n",
      "iteration 3234, dc_loss: 0.01857917197048664, tv_loss: 0.01702227257192135\n",
      "iteration 3235, dc_loss: 0.01857881247997284, tv_loss: 0.01702256314456463\n",
      "iteration 3236, dc_loss: 0.01857837103307247, tv_loss: 0.017022937536239624\n",
      "iteration 3237, dc_loss: 0.018578728660941124, tv_loss: 0.01702253147959709\n",
      "iteration 3238, dc_loss: 0.018578551709651947, tv_loss: 0.017022712156176567\n",
      "iteration 3239, dc_loss: 0.018578769639134407, tv_loss: 0.01702251471579075\n",
      "iteration 3240, dc_loss: 0.018577637150883675, tv_loss: 0.01702360063791275\n",
      "iteration 3241, dc_loss: 0.01857965998351574, tv_loss: 0.017021475359797478\n",
      "iteration 3242, dc_loss: 0.018576761707663536, tv_loss: 0.01702422834932804\n",
      "iteration 3243, dc_loss: 0.01857915148139, tv_loss: 0.01702171377837658\n",
      "iteration 3244, dc_loss: 0.01857871189713478, tv_loss: 0.01702205091714859\n",
      "iteration 3245, dc_loss: 0.018576648086309433, tv_loss: 0.0170240867882967\n",
      "iteration 3246, dc_loss: 0.018579179421067238, tv_loss: 0.017021536827087402\n",
      "iteration 3247, dc_loss: 0.018577756360173225, tv_loss: 0.017022907733917236\n",
      "iteration 3248, dc_loss: 0.01857726275920868, tv_loss: 0.01702331006526947\n",
      "iteration 3249, dc_loss: 0.018578914925456047, tv_loss: 0.01702157035470009\n",
      "iteration 3250, dc_loss: 0.01857629418373108, tv_loss: 0.017024122178554535\n",
      "iteration 3251, dc_loss: 0.018579155206680298, tv_loss: 0.01702118292450905\n",
      "iteration 3252, dc_loss: 0.018576674163341522, tv_loss: 0.01702357642352581\n",
      "iteration 3253, dc_loss: 0.018578017130494118, tv_loss: 0.017022183164954185\n",
      "iteration 3254, dc_loss: 0.018577588722109795, tv_loss: 0.01702257990837097\n",
      "iteration 3255, dc_loss: 0.018577080219984055, tv_loss: 0.01702301576733589\n",
      "iteration 3256, dc_loss: 0.01857754960656166, tv_loss: 0.017022447660565376\n",
      "iteration 3257, dc_loss: 0.01857784390449524, tv_loss: 0.01702205464243889\n",
      "iteration 3258, dc_loss: 0.018575983121991158, tv_loss: 0.017023814842104912\n",
      "iteration 3259, dc_loss: 0.018578842282295227, tv_loss: 0.017020894214510918\n",
      "iteration 3260, dc_loss: 0.01857573539018631, tv_loss: 0.017023945227265358\n",
      "iteration 3261, dc_loss: 0.018577830865979195, tv_loss: 0.017021823674440384\n",
      "iteration 3262, dc_loss: 0.01857677660882473, tv_loss: 0.01702287420630455\n",
      "iteration 3263, dc_loss: 0.018576662987470627, tv_loss: 0.017022930085659027\n",
      "iteration 3264, dc_loss: 0.018577411770820618, tv_loss: 0.017022065818309784\n",
      "iteration 3265, dc_loss: 0.01857662759721279, tv_loss: 0.017022689804434776\n",
      "iteration 3266, dc_loss: 0.01857607066631317, tv_loss: 0.017023129388689995\n",
      "iteration 3267, dc_loss: 0.01857779361307621, tv_loss: 0.01702132262289524\n",
      "iteration 3268, dc_loss: 0.018575439229607582, tv_loss: 0.017023641616106033\n",
      "iteration 3269, dc_loss: 0.018577663227915764, tv_loss: 0.017021382227540016\n",
      "iteration 3270, dc_loss: 0.018575288355350494, tv_loss: 0.017023762688040733\n",
      "iteration 3271, dc_loss: 0.018577322363853455, tv_loss: 0.017021704465150833\n",
      "iteration 3272, dc_loss: 0.018575984984636307, tv_loss: 0.017022905871272087\n",
      "iteration 3273, dc_loss: 0.01857631839811802, tv_loss: 0.01702241599559784\n",
      "iteration 3274, dc_loss: 0.018575891852378845, tv_loss: 0.01702272891998291\n",
      "iteration 3275, dc_loss: 0.018576839938759804, tv_loss: 0.01702168956398964\n",
      "iteration 3276, dc_loss: 0.0185751561075449, tv_loss: 0.017023319378495216\n",
      "iteration 3277, dc_loss: 0.018577121198177338, tv_loss: 0.017021339386701584\n",
      "iteration 3278, dc_loss: 0.018574774265289307, tv_loss: 0.01702372543513775\n",
      "iteration 3279, dc_loss: 0.01857670396566391, tv_loss: 0.017021816223859787\n",
      "iteration 3280, dc_loss: 0.01857563480734825, tv_loss: 0.017022768035531044\n",
      "iteration 3281, dc_loss: 0.018575817346572876, tv_loss: 0.01702241040766239\n",
      "iteration 3282, dc_loss: 0.01857583597302437, tv_loss: 0.017022235319018364\n",
      "iteration 3283, dc_loss: 0.018575286492705345, tv_loss: 0.01702268235385418\n",
      "iteration 3284, dc_loss: 0.01857590116560459, tv_loss: 0.017021991312503815\n",
      "iteration 3285, dc_loss: 0.0185753982514143, tv_loss: 0.017022453248500824\n",
      "iteration 3286, dc_loss: 0.018575085327029228, tv_loss: 0.017022768035531044\n",
      "iteration 3287, dc_loss: 0.0185763631016016, tv_loss: 0.017021525651216507\n",
      "iteration 3288, dc_loss: 0.018573520705103874, tv_loss: 0.01702433079481125\n",
      "iteration 3289, dc_loss: 0.018577611073851585, tv_loss: 0.01702013611793518\n",
      "iteration 3290, dc_loss: 0.018573153764009476, tv_loss: 0.017024431377649307\n",
      "iteration 3291, dc_loss: 0.01857582852244377, tv_loss: 0.017021600157022476\n",
      "iteration 3292, dc_loss: 0.018575914204120636, tv_loss: 0.017021454870700836\n",
      "iteration 3293, dc_loss: 0.018573425710201263, tv_loss: 0.017023911699652672\n",
      "iteration 3294, dc_loss: 0.01857605390250683, tv_loss: 0.017021270468831062\n",
      "iteration 3295, dc_loss: 0.018574614077806473, tv_loss: 0.01702268049120903\n",
      "iteration 3296, dc_loss: 0.01857447624206543, tv_loss: 0.0170227512717247\n",
      "iteration 3297, dc_loss: 0.018575552850961685, tv_loss: 0.017021607607603073\n",
      "iteration 3298, dc_loss: 0.01857319474220276, tv_loss: 0.017023861408233643\n",
      "iteration 3299, dc_loss: 0.018576238304376602, tv_loss: 0.017020689323544502\n",
      "iteration 3300, dc_loss: 0.018573816865682602, tv_loss: 0.017023038119077682\n",
      "iteration 3301, dc_loss: 0.01857423037290573, tv_loss: 0.017022568732500076\n",
      "iteration 3302, dc_loss: 0.01857466623187065, tv_loss: 0.01702207699418068\n",
      "iteration 3303, dc_loss: 0.018574438989162445, tv_loss: 0.01702222414314747\n",
      "iteration 3304, dc_loss: 0.018573706969618797, tv_loss: 0.017022864893078804\n",
      "iteration 3305, dc_loss: 0.018575195223093033, tv_loss: 0.01702130027115345\n",
      "iteration 3306, dc_loss: 0.018572596833109856, tv_loss: 0.017023831605911255\n",
      "iteration 3307, dc_loss: 0.018575694411993027, tv_loss: 0.01702067069709301\n",
      "iteration 3308, dc_loss: 0.01857290230691433, tv_loss: 0.01702341064810753\n",
      "iteration 3309, dc_loss: 0.018574386835098267, tv_loss: 0.017021888867020607\n",
      "iteration 3310, dc_loss: 0.018573906272649765, tv_loss: 0.01702231913805008\n",
      "iteration 3311, dc_loss: 0.018573319539427757, tv_loss: 0.01702282577753067\n",
      "iteration 3312, dc_loss: 0.01857454888522625, tv_loss: 0.017021458595991135\n",
      "iteration 3313, dc_loss: 0.01857292279601097, tv_loss: 0.017022954300045967\n",
      "iteration 3314, dc_loss: 0.018573859706521034, tv_loss: 0.017021900042891502\n",
      "iteration 3315, dc_loss: 0.018573962152004242, tv_loss: 0.01702171377837658\n",
      "iteration 3316, dc_loss: 0.018572621047496796, tv_loss: 0.0170229971408844\n",
      "iteration 3317, dc_loss: 0.018574489280581474, tv_loss: 0.01702113263309002\n",
      "iteration 3318, dc_loss: 0.0185718797147274, tv_loss: 0.017023788765072823\n",
      "iteration 3319, dc_loss: 0.018575262278318405, tv_loss: 0.017020417377352715\n",
      "iteration 3320, dc_loss: 0.018571365624666214, tv_loss: 0.01702415756881237\n",
      "iteration 3321, dc_loss: 0.018574103713035583, tv_loss: 0.017021236941218376\n",
      "iteration 3322, dc_loss: 0.01857316493988037, tv_loss: 0.017022037878632545\n",
      "iteration 3323, dc_loss: 0.0185722466558218, tv_loss: 0.01702285185456276\n",
      "iteration 3324, dc_loss: 0.01857389695942402, tv_loss: 0.017021123319864273\n",
      "iteration 3325, dc_loss: 0.018572255969047546, tv_loss: 0.017022695392370224\n",
      "iteration 3326, dc_loss: 0.018572907894849777, tv_loss: 0.017022008076310158\n",
      "iteration 3327, dc_loss: 0.018572762608528137, tv_loss: 0.017022157087922096\n",
      "iteration 3328, dc_loss: 0.018572576344013214, tv_loss: 0.017022348940372467\n",
      "iteration 3329, dc_loss: 0.018573084846138954, tv_loss: 0.01702178828418255\n",
      "iteration 3330, dc_loss: 0.01857147552073002, tv_loss: 0.017023345455527306\n",
      "iteration 3331, dc_loss: 0.018574519082903862, tv_loss: 0.017020199447870255\n",
      "iteration 3332, dc_loss: 0.01857103407382965, tv_loss: 0.017023572698235512\n",
      "iteration 3333, dc_loss: 0.018572205677628517, tv_loss: 0.017022304236888885\n",
      "iteration 3334, dc_loss: 0.01857326738536358, tv_loss: 0.017021195963025093\n",
      "iteration 3335, dc_loss: 0.01857147552073002, tv_loss: 0.017022909596562386\n",
      "iteration 3336, dc_loss: 0.018572300672531128, tv_loss: 0.017021970823407173\n",
      "iteration 3337, dc_loss: 0.01857258379459381, tv_loss: 0.017021579667925835\n",
      "iteration 3338, dc_loss: 0.018571047112345695, tv_loss: 0.01702304370701313\n",
      "iteration 3339, dc_loss: 0.018573176115751266, tv_loss: 0.01702088862657547\n",
      "iteration 3340, dc_loss: 0.01857040449976921, tv_loss: 0.01702365092933178\n",
      "iteration 3341, dc_loss: 0.018573597073554993, tv_loss: 0.017020441591739655\n",
      "iteration 3342, dc_loss: 0.018570253625512123, tv_loss: 0.01702369935810566\n",
      "iteration 3343, dc_loss: 0.018572676926851273, tv_loss: 0.017021140083670616\n",
      "iteration 3344, dc_loss: 0.018571721389889717, tv_loss: 0.017021939158439636\n",
      "iteration 3345, dc_loss: 0.018570564687252045, tv_loss: 0.017022987827658653\n",
      "iteration 3346, dc_loss: 0.018572315573692322, tv_loss: 0.017021143808960915\n",
      "iteration 3347, dc_loss: 0.018571535125374794, tv_loss: 0.017021842300891876\n",
      "iteration 3348, dc_loss: 0.018570542335510254, tv_loss: 0.01702280156314373\n",
      "iteration 3349, dc_loss: 0.01857243850827217, tv_loss: 0.0170209389179945\n",
      "iteration 3350, dc_loss: 0.01856965944170952, tv_loss: 0.01702377386391163\n",
      "iteration 3351, dc_loss: 0.01857336424291134, tv_loss: 0.01701996847987175\n",
      "iteration 3352, dc_loss: 0.018569206818938255, tv_loss: 0.017023960128426552\n",
      "iteration 3353, dc_loss: 0.018571993336081505, tv_loss: 0.017021017149090767\n",
      "iteration 3354, dc_loss: 0.01857093907892704, tv_loss: 0.01702198013663292\n",
      "iteration 3355, dc_loss: 0.018570492044091225, tv_loss: 0.01702236570417881\n",
      "iteration 3356, dc_loss: 0.018571721389889717, tv_loss: 0.01702113077044487\n",
      "iteration 3357, dc_loss: 0.018569976091384888, tv_loss: 0.0170228723436594\n",
      "iteration 3358, dc_loss: 0.01857093907892704, tv_loss: 0.017021894454956055\n",
      "iteration 3359, dc_loss: 0.01857115514576435, tv_loss: 0.017021600157022476\n",
      "iteration 3360, dc_loss: 0.018569812178611755, tv_loss: 0.017022844403982162\n",
      "iteration 3361, dc_loss: 0.018571868538856506, tv_loss: 0.01702067069709301\n",
      "iteration 3362, dc_loss: 0.018569007515907288, tv_loss: 0.01702343486249447\n",
      "iteration 3363, dc_loss: 0.018571505323052406, tv_loss: 0.01702088862657547\n",
      "iteration 3364, dc_loss: 0.018569836392998695, tv_loss: 0.017022524029016495\n",
      "iteration 3365, dc_loss: 0.018570339307188988, tv_loss: 0.017021922394633293\n",
      "iteration 3366, dc_loss: 0.018569951876997948, tv_loss: 0.01702219434082508\n",
      "iteration 3367, dc_loss: 0.0185711570084095, tv_loss: 0.017020858824253082\n",
      "iteration 3368, dc_loss: 0.018568627536296844, tv_loss: 0.017023341730237007\n",
      "iteration 3369, dc_loss: 0.018571531400084496, tv_loss: 0.017020417377352715\n",
      "iteration 3370, dc_loss: 0.018568502739071846, tv_loss: 0.017023447901010513\n",
      "iteration 3371, dc_loss: 0.01857083849608898, tv_loss: 0.017021048814058304\n",
      "iteration 3372, dc_loss: 0.018570125102996826, tv_loss: 0.017021652311086655\n",
      "iteration 3373, dc_loss: 0.018568800762295723, tv_loss: 0.01702282950282097\n",
      "iteration 3374, dc_loss: 0.018570521846413612, tv_loss: 0.01702098548412323\n",
      "iteration 3375, dc_loss: 0.018569016829133034, tv_loss: 0.017022380605340004\n",
      "iteration 3376, dc_loss: 0.01856975257396698, tv_loss: 0.017021557316184044\n",
      "iteration 3377, dc_loss: 0.018569782376289368, tv_loss: 0.017021460458636284\n",
      "iteration 3378, dc_loss: 0.01856835186481476, tv_loss: 0.017022885382175446\n",
      "iteration 3379, dc_loss: 0.018571127206087112, tv_loss: 0.01702016405761242\n",
      "iteration 3380, dc_loss: 0.01856716349720955, tv_loss: 0.01702415756881237\n",
      "iteration 3381, dc_loss: 0.018571510910987854, tv_loss: 0.017019644379615784\n",
      "iteration 3382, dc_loss: 0.018567649647593498, tv_loss: 0.017023315653204918\n",
      "iteration 3383, dc_loss: 0.018569298088550568, tv_loss: 0.017021536827087402\n",
      "iteration 3384, dc_loss: 0.01857004314661026, tv_loss: 0.017020752653479576\n",
      "iteration 3385, dc_loss: 0.01856725662946701, tv_loss: 0.01702355593442917\n",
      "iteration 3386, dc_loss: 0.01857060007750988, tv_loss: 0.017020221799612045\n",
      "iteration 3387, dc_loss: 0.018568649888038635, tv_loss: 0.01702212728559971\n",
      "iteration 3388, dc_loss: 0.018567906692624092, tv_loss: 0.017022740095853806\n",
      "iteration 3389, dc_loss: 0.0185701884329319, tv_loss: 0.017020342871546745\n",
      "iteration 3390, dc_loss: 0.01856680028140545, tv_loss: 0.01702367328107357\n",
      "iteration 3391, dc_loss: 0.018571073189377785, tv_loss: 0.017019344493746758\n",
      "iteration 3392, dc_loss: 0.01856662519276142, tv_loss: 0.017023732885718346\n",
      "iteration 3393, dc_loss: 0.018568968400359154, tv_loss: 0.017021311447024345\n",
      "iteration 3394, dc_loss: 0.01856905408203602, tv_loss: 0.01702111028134823\n",
      "iteration 3395, dc_loss: 0.018567414954304695, tv_loss: 0.017022620886564255\n",
      "iteration 3396, dc_loss: 0.018568815663456917, tv_loss: 0.01702110283076763\n",
      "iteration 3397, dc_loss: 0.018568305298686028, tv_loss: 0.0170215405523777\n",
      "iteration 3398, dc_loss: 0.018567213788628578, tv_loss: 0.017022620886564255\n",
      "iteration 3399, dc_loss: 0.01856979914009571, tv_loss: 0.017020097002387047\n",
      "iteration 3400, dc_loss: 0.018566418439149857, tv_loss: 0.01702350191771984\n",
      "iteration 3401, dc_loss: 0.0185689739882946, tv_loss: 0.01702079549431801\n",
      "iteration 3402, dc_loss: 0.01856740191578865, tv_loss: 0.017022153362631798\n",
      "iteration 3403, dc_loss: 0.018567869439721107, tv_loss: 0.017021536827087402\n",
      "iteration 3404, dc_loss: 0.018568215891718864, tv_loss: 0.01702111028134823\n",
      "iteration 3405, dc_loss: 0.018567316234111786, tv_loss: 0.01702195033431053\n",
      "iteration 3406, dc_loss: 0.01856737583875656, tv_loss: 0.01702183671295643\n",
      "iteration 3407, dc_loss: 0.018568608909845352, tv_loss: 0.01702054776251316\n",
      "iteration 3408, dc_loss: 0.01856599748134613, tv_loss: 0.017023183405399323\n",
      "iteration 3409, dc_loss: 0.01856926456093788, tv_loss: 0.017019953578710556\n",
      "iteration 3410, dc_loss: 0.01856515370309353, tv_loss: 0.017023934051394463\n",
      "iteration 3411, dc_loss: 0.018569251522421837, tv_loss: 0.017019663006067276\n",
      "iteration 3412, dc_loss: 0.018566573038697243, tv_loss: 0.017022235319018364\n",
      "iteration 3413, dc_loss: 0.01856640912592411, tv_loss: 0.017022348940372467\n",
      "iteration 3414, dc_loss: 0.018568873405456543, tv_loss: 0.01701989583671093\n",
      "iteration 3415, dc_loss: 0.01856517791748047, tv_loss: 0.017023582011461258\n",
      "iteration 3416, dc_loss: 0.018568431958556175, tv_loss: 0.017020219936966896\n",
      "iteration 3417, dc_loss: 0.018566563725471497, tv_loss: 0.017021941021084785\n",
      "iteration 3418, dc_loss: 0.018565911799669266, tv_loss: 0.01702250726521015\n",
      "iteration 3419, dc_loss: 0.01856888458132744, tv_loss: 0.017019519582390785\n",
      "iteration 3420, dc_loss: 0.018564388155937195, tv_loss: 0.017024001106619835\n",
      "iteration 3421, dc_loss: 0.018568698316812515, tv_loss: 0.017019616439938545\n",
      "iteration 3422, dc_loss: 0.018565334379673004, tv_loss: 0.01702284626662731\n",
      "iteration 3423, dc_loss: 0.018566889688372612, tv_loss: 0.017021164298057556\n",
      "iteration 3424, dc_loss: 0.018566658720374107, tv_loss: 0.017021309584379196\n",
      "iteration 3425, dc_loss: 0.018566172569990158, tv_loss: 0.017021702602505684\n",
      "iteration 3426, dc_loss: 0.018566373735666275, tv_loss: 0.017021408304572105\n",
      "iteration 3427, dc_loss: 0.018566718325018883, tv_loss: 0.01702100783586502\n",
      "iteration 3428, dc_loss: 0.018565164878964424, tv_loss: 0.01702256128191948\n",
      "iteration 3429, dc_loss: 0.018567580729722977, tv_loss: 0.01702016219496727\n",
      "iteration 3430, dc_loss: 0.018564220517873764, tv_loss: 0.017023460939526558\n",
      "iteration 3431, dc_loss: 0.018568294122815132, tv_loss: 0.01701926626265049\n",
      "iteration 3432, dc_loss: 0.01856483332812786, tv_loss: 0.017022613435983658\n",
      "iteration 3433, dc_loss: 0.018565380945801735, tv_loss: 0.017021987587213516\n",
      "iteration 3434, dc_loss: 0.018567316234111786, tv_loss: 0.01702001318335533\n",
      "iteration 3435, dc_loss: 0.018564142286777496, tv_loss: 0.017023123800754547\n",
      "iteration 3436, dc_loss: 0.018567102029919624, tv_loss: 0.01702004298567772\n",
      "iteration 3437, dc_loss: 0.018565163016319275, tv_loss: 0.017021864652633667\n",
      "iteration 3438, dc_loss: 0.01856493204832077, tv_loss: 0.017021996900439262\n",
      "iteration 3439, dc_loss: 0.01856672205030918, tv_loss: 0.017020173370838165\n",
      "iteration 3440, dc_loss: 0.018563805148005486, tv_loss: 0.0170231144875288\n",
      "iteration 3441, dc_loss: 0.01856773905456066, tv_loss: 0.017019184306263924\n",
      "iteration 3442, dc_loss: 0.01856301911175251, tv_loss: 0.017023788765072823\n",
      "iteration 3443, dc_loss: 0.01856650598347187, tv_loss: 0.017020121216773987\n",
      "iteration 3444, dc_loss: 0.018565213307738304, tv_loss: 0.01702127791941166\n",
      "iteration 3445, dc_loss: 0.018563952296972275, tv_loss: 0.017022470012307167\n",
      "iteration 3446, dc_loss: 0.01856672391295433, tv_loss: 0.01701967418193817\n",
      "iteration 3447, dc_loss: 0.01856376603245735, tv_loss: 0.01702263019979\n",
      "iteration 3448, dc_loss: 0.018565569072961807, tv_loss: 0.017020845785737038\n",
      "iteration 3449, dc_loss: 0.018565213307738304, tv_loss: 0.017021173611283302\n",
      "iteration 3450, dc_loss: 0.018564077094197273, tv_loss: 0.017022177577018738\n",
      "iteration 3451, dc_loss: 0.018566090613603592, tv_loss: 0.017020057886838913\n",
      "iteration 3452, dc_loss: 0.018562322482466698, tv_loss: 0.017023717984557152\n",
      "iteration 3453, dc_loss: 0.018567372113466263, tv_loss: 0.017018627375364304\n",
      "iteration 3454, dc_loss: 0.018563006073236465, tv_loss: 0.01702295057475567\n",
      "iteration 3455, dc_loss: 0.018564898520708084, tv_loss: 0.017020992934703827\n",
      "iteration 3456, dc_loss: 0.018564345315098763, tv_loss: 0.017021434381604195\n",
      "iteration 3457, dc_loss: 0.0185649786144495, tv_loss: 0.01702066697180271\n",
      "iteration 3458, dc_loss: 0.01856326311826706, tv_loss: 0.01702229119837284\n",
      "iteration 3459, dc_loss: 0.01856527291238308, tv_loss: 0.01702023297548294\n",
      "iteration 3460, dc_loss: 0.01856289803981781, tv_loss: 0.01702260412275791\n",
      "iteration 3461, dc_loss: 0.018565306439995766, tv_loss: 0.017020167782902718\n",
      "iteration 3462, dc_loss: 0.018563877791166306, tv_loss: 0.017021534964442253\n",
      "iteration 3463, dc_loss: 0.018563713878393173, tv_loss: 0.01702158711850643\n",
      "iteration 3464, dc_loss: 0.018564017489552498, tv_loss: 0.01702115125954151\n",
      "iteration 3465, dc_loss: 0.018563702702522278, tv_loss: 0.017021311447024345\n",
      "iteration 3466, dc_loss: 0.01856396533548832, tv_loss: 0.0170209351927042\n",
      "iteration 3467, dc_loss: 0.01856345683336258, tv_loss: 0.017021341249346733\n",
      "iteration 3468, dc_loss: 0.018563536927103996, tv_loss: 0.01702123135328293\n",
      "iteration 3469, dc_loss: 0.018564362078905106, tv_loss: 0.017020385712385178\n",
      "iteration 3470, dc_loss: 0.018562166020274162, tv_loss: 0.01702263578772545\n",
      "iteration 3471, dc_loss: 0.018565785139799118, tv_loss: 0.01701907254755497\n",
      "iteration 3472, dc_loss: 0.01856047473847866, tv_loss: 0.017024295404553413\n",
      "iteration 3473, dc_loss: 0.01856660097837448, tv_loss: 0.017018010839819908\n",
      "iteration 3474, dc_loss: 0.01856096275150776, tv_loss: 0.017023472115397453\n",
      "iteration 3475, dc_loss: 0.01856398768723011, tv_loss: 0.01702033169567585\n",
      "iteration 3476, dc_loss: 0.018563978374004364, tv_loss: 0.01702028326690197\n",
      "iteration 3477, dc_loss: 0.018561579287052155, tv_loss: 0.017022695392370224\n",
      "iteration 3478, dc_loss: 0.01856447011232376, tv_loss: 0.017019793391227722\n",
      "iteration 3479, dc_loss: 0.01856205426156521, tv_loss: 0.017022162675857544\n",
      "iteration 3480, dc_loss: 0.018563605844974518, tv_loss: 0.01702052168548107\n",
      "iteration 3481, dc_loss: 0.018563034012913704, tv_loss: 0.01702096499502659\n",
      "iteration 3482, dc_loss: 0.018561892211437225, tv_loss: 0.01702198199927807\n",
      "iteration 3483, dc_loss: 0.01856420934200287, tv_loss: 0.017019623890519142\n",
      "iteration 3484, dc_loss: 0.018560299649834633, tv_loss: 0.017023544758558273\n",
      "iteration 3485, dc_loss: 0.01856531947851181, tv_loss: 0.017018483951687813\n",
      "iteration 3486, dc_loss: 0.018560322001576424, tv_loss: 0.017023397609591484\n",
      "iteration 3487, dc_loss: 0.018564019352197647, tv_loss: 0.01701955869793892\n",
      "iteration 3488, dc_loss: 0.01856163889169693, tv_loss: 0.0170217864215374\n",
      "iteration 3489, dc_loss: 0.018562596291303635, tv_loss: 0.017020683735609055\n",
      "iteration 3490, dc_loss: 0.01856231316924095, tv_loss: 0.017020899802446365\n",
      "iteration 3491, dc_loss: 0.01856188289821148, tv_loss: 0.017021330073475838\n",
      "iteration 3492, dc_loss: 0.018561864271759987, tv_loss: 0.01702139526605606\n",
      "iteration 3493, dc_loss: 0.01856324076652527, tv_loss: 0.01702001318335533\n",
      "iteration 3494, dc_loss: 0.018560612574219704, tv_loss: 0.017022553831338882\n",
      "iteration 3495, dc_loss: 0.018563199788331985, tv_loss: 0.01701980270445347\n",
      "iteration 3496, dc_loss: 0.018561100587248802, tv_loss: 0.017021745443344116\n",
      "iteration 3497, dc_loss: 0.01856199838221073, tv_loss: 0.017020760104060173\n",
      "iteration 3498, dc_loss: 0.01856188103556633, tv_loss: 0.017020834609866142\n",
      "iteration 3499, dc_loss: 0.01856153830885887, tv_loss: 0.017021145671606064\n",
      "iteration 3500, dc_loss: 0.018560975790023804, tv_loss: 0.017021674662828445\n",
      "iteration 3501, dc_loss: 0.0185637716203928, tv_loss: 0.017018841579556465\n",
      "iteration 3502, dc_loss: 0.018558239564299583, tv_loss: 0.017024336382746696\n",
      "iteration 3503, dc_loss: 0.018565071746706963, tv_loss: 0.017017461359500885\n",
      "iteration 3504, dc_loss: 0.018558481708168983, tv_loss: 0.017023934051394463\n",
      "iteration 3505, dc_loss: 0.018562937155365944, tv_loss: 0.017019324004650116\n",
      "iteration 3506, dc_loss: 0.01856120303273201, tv_loss: 0.017020948231220245\n",
      "iteration 3507, dc_loss: 0.018559707328677177, tv_loss: 0.017022378742694855\n",
      "iteration 3508, dc_loss: 0.018562903627753258, tv_loss: 0.017019150778651237\n",
      "iteration 3509, dc_loss: 0.018559565767645836, tv_loss: 0.01702244207262993\n",
      "iteration 3510, dc_loss: 0.01856231689453125, tv_loss: 0.017019623890519142\n",
      "iteration 3511, dc_loss: 0.018560370430350304, tv_loss: 0.017021464183926582\n",
      "iteration 3512, dc_loss: 0.01856077089905739, tv_loss: 0.01702096313238144\n",
      "iteration 3513, dc_loss: 0.018561651930212975, tv_loss: 0.01702003739774227\n",
      "iteration 3514, dc_loss: 0.018558651208877563, tv_loss: 0.01702304370701313\n",
      "iteration 3515, dc_loss: 0.01856350526213646, tv_loss: 0.017018122598528862\n",
      "iteration 3516, dc_loss: 0.018557405099272728, tv_loss: 0.017024163156747818\n",
      "iteration 3517, dc_loss: 0.018563581630587578, tv_loss: 0.017017889767885208\n",
      "iteration 3518, dc_loss: 0.018558768555521965, tv_loss: 0.017022600397467613\n",
      "iteration 3519, dc_loss: 0.01856072247028351, tv_loss: 0.017020532861351967\n",
      "iteration 3520, dc_loss: 0.018560951575636864, tv_loss: 0.017020203173160553\n",
      "iteration 3521, dc_loss: 0.018559614196419716, tv_loss: 0.017021454870700836\n",
      "iteration 3522, dc_loss: 0.01856023259460926, tv_loss: 0.017020760104060173\n",
      "iteration 3523, dc_loss: 0.0185602568089962, tv_loss: 0.017020689323544502\n",
      "iteration 3524, dc_loss: 0.018559688702225685, tv_loss: 0.017021261155605316\n",
      "iteration 3525, dc_loss: 0.01856088824570179, tv_loss: 0.01702006906270981\n",
      "iteration 3526, dc_loss: 0.018558884039521217, tv_loss: 0.01702203042805195\n",
      "iteration 3527, dc_loss: 0.018561434000730515, tv_loss: 0.01701935939490795\n",
      "iteration 3528, dc_loss: 0.018558375537395477, tv_loss: 0.017022229731082916\n",
      "iteration 3529, dc_loss: 0.018560748547315598, tv_loss: 0.017019709572196007\n",
      "iteration 3530, dc_loss: 0.018559100106358528, tv_loss: 0.017021292820572853\n",
      "iteration 3531, dc_loss: 0.01856006123125553, tv_loss: 0.01702030561864376\n",
      "iteration 3532, dc_loss: 0.018558582291007042, tv_loss: 0.017021771520376205\n",
      "iteration 3533, dc_loss: 0.018561694771051407, tv_loss: 0.017018646001815796\n",
      "iteration 3534, dc_loss: 0.018556494265794754, tv_loss: 0.017023850232362747\n",
      "iteration 3535, dc_loss: 0.018562691286206245, tv_loss: 0.017017601057887077\n",
      "iteration 3536, dc_loss: 0.018556345254182816, tv_loss: 0.017023779451847076\n",
      "iteration 3537, dc_loss: 0.01856120303273201, tv_loss: 0.01701873540878296\n",
      "iteration 3538, dc_loss: 0.018559064716100693, tv_loss: 0.01702079176902771\n",
      "iteration 3539, dc_loss: 0.018557632341980934, tv_loss: 0.017022186890244484\n",
      "iteration 3540, dc_loss: 0.018561258912086487, tv_loss: 0.01701858639717102\n",
      "iteration 3541, dc_loss: 0.018556825816631317, tv_loss: 0.017023012042045593\n",
      "iteration 3542, dc_loss: 0.018560772761702538, tv_loss: 0.017018998041749\n",
      "iteration 3543, dc_loss: 0.018558083102107048, tv_loss: 0.017021551728248596\n",
      "iteration 3544, dc_loss: 0.018558688461780548, tv_loss: 0.017020804807543755\n",
      "iteration 3545, dc_loss: 0.018559640273451805, tv_loss: 0.01701972261071205\n",
      "iteration 3546, dc_loss: 0.018556814640760422, tv_loss: 0.017022520303726196\n",
      "iteration 3547, dc_loss: 0.018561339005827904, tv_loss: 0.017018016427755356\n",
      "iteration 3548, dc_loss: 0.018555309623479843, tv_loss: 0.01702403835952282\n",
      "iteration 3549, dc_loss: 0.01856170780956745, tv_loss: 0.017017576843500137\n",
      "iteration 3550, dc_loss: 0.01855660416185856, tv_loss: 0.01702257990837097\n",
      "iteration 3551, dc_loss: 0.018559155985713005, tv_loss: 0.0170198492705822\n",
      "iteration 3552, dc_loss: 0.01855812966823578, tv_loss: 0.017020704224705696\n",
      "iteration 3553, dc_loss: 0.018558209761977196, tv_loss: 0.017020495608448982\n",
      "iteration 3554, dc_loss: 0.018557924777269363, tv_loss: 0.01702071540057659\n",
      "iteration 3555, dc_loss: 0.018558450043201447, tv_loss: 0.01702018827199936\n",
      "iteration 3556, dc_loss: 0.018557049334049225, tv_loss: 0.017021669074892998\n",
      "iteration 3557, dc_loss: 0.018559450283646584, tv_loss: 0.01701931841671467\n",
      "iteration 3558, dc_loss: 0.018556473776698112, tv_loss: 0.017022181302309036\n",
      "iteration 3559, dc_loss: 0.01855931431055069, tv_loss: 0.01701914519071579\n",
      "iteration 3560, dc_loss: 0.01855716109275818, tv_loss: 0.01702111028134823\n",
      "iteration 3561, dc_loss: 0.01855715923011303, tv_loss: 0.01702100597321987\n",
      "iteration 3562, dc_loss: 0.01855851709842682, tv_loss: 0.017019599676132202\n",
      "iteration 3563, dc_loss: 0.018556872382760048, tv_loss: 0.017021222040057182\n",
      "iteration 3564, dc_loss: 0.018557632341980934, tv_loss: 0.017020434141159058\n",
      "iteration 3565, dc_loss: 0.018558470532298088, tv_loss: 0.017019543796777725\n",
      "iteration 3566, dc_loss: 0.01855485886335373, tv_loss: 0.017023136839270592\n",
      "iteration 3567, dc_loss: 0.018561439588665962, tv_loss: 0.017016593366861343\n",
      "iteration 3568, dc_loss: 0.01855306513607502, tv_loss: 0.017024939879775047\n",
      "iteration 3569, dc_loss: 0.018561072647571564, tv_loss: 0.01701679639518261\n",
      "iteration 3570, dc_loss: 0.018554914742708206, tv_loss: 0.017022786661982536\n",
      "iteration 3571, dc_loss: 0.018557464703917503, tv_loss: 0.0170200876891613\n",
      "iteration 3572, dc_loss: 0.01855831779539585, tv_loss: 0.017019115388393402\n",
      "iteration 3573, dc_loss: 0.018554480746388435, tv_loss: 0.01702285185456276\n",
      "iteration 3574, dc_loss: 0.01855911873281002, tv_loss: 0.017018109560012817\n",
      "iteration 3575, dc_loss: 0.018555384129285812, tv_loss: 0.017021775245666504\n",
      "iteration 3576, dc_loss: 0.0185575932264328, tv_loss: 0.017019590362906456\n",
      "iteration 3577, dc_loss: 0.01855640672147274, tv_loss: 0.01702086254954338\n",
      "iteration 3578, dc_loss: 0.018556984141469002, tv_loss: 0.017020387575030327\n",
      "iteration 3579, dc_loss: 0.018557129427790642, tv_loss: 0.017020143568515778\n",
      "iteration 3580, dc_loss: 0.01855548657476902, tv_loss: 0.01702146977186203\n",
      "iteration 3581, dc_loss: 0.018557574599981308, tv_loss: 0.01701914146542549\n",
      "iteration 3582, dc_loss: 0.018555019050836563, tv_loss: 0.01702161319553852\n",
      "iteration 3583, dc_loss: 0.018557915464043617, tv_loss: 0.01701878011226654\n",
      "iteration 3584, dc_loss: 0.01855444721877575, tv_loss: 0.0170223880559206\n",
      "iteration 3585, dc_loss: 0.01855899579823017, tv_loss: 0.017017900943756104\n",
      "iteration 3586, dc_loss: 0.018552934750914574, tv_loss: 0.01702382229268551\n",
      "iteration 3587, dc_loss: 0.018559984862804413, tv_loss: 0.017016546800732613\n",
      "iteration 3588, dc_loss: 0.01855289563536644, tv_loss: 0.017023446038365364\n",
      "iteration 3589, dc_loss: 0.01855747029185295, tv_loss: 0.017018752172589302\n",
      "iteration 3590, dc_loss: 0.018555892631411552, tv_loss: 0.017020348459482193\n",
      "iteration 3591, dc_loss: 0.01855463534593582, tv_loss: 0.01702161692082882\n",
      "iteration 3592, dc_loss: 0.01855757087469101, tv_loss: 0.017018605023622513\n",
      "iteration 3593, dc_loss: 0.018553636968135834, tv_loss: 0.01702246069908142\n",
      "iteration 3594, dc_loss: 0.018557850271463394, tv_loss: 0.017018135637044907\n",
      "iteration 3595, dc_loss: 0.01855402998626232, tv_loss: 0.017021827399730682\n",
      "iteration 3596, dc_loss: 0.01855654828250408, tv_loss: 0.017019210383296013\n",
      "iteration 3597, dc_loss: 0.01855444349348545, tv_loss: 0.017021246254444122\n",
      "iteration 3598, dc_loss: 0.018555600196123123, tv_loss: 0.017020000144839287\n",
      "iteration 3599, dc_loss: 0.018555814400315285, tv_loss: 0.017019692808389664\n",
      "iteration 3600, dc_loss: 0.018553476780653, tv_loss: 0.017022021114826202\n",
      "iteration 3601, dc_loss: 0.01855853945016861, tv_loss: 0.01701703667640686\n",
      "iteration 3602, dc_loss: 0.018551962450146675, tv_loss: 0.017023306339979172\n",
      "iteration 3603, dc_loss: 0.018555250018835068, tv_loss: 0.01701967604458332\n",
      "iteration 3604, dc_loss: 0.018557876348495483, tv_loss: 0.01701720990240574\n",
      "iteration 3605, dc_loss: 0.018551843240857124, tv_loss: 0.017023326829075813\n",
      "iteration 3606, dc_loss: 0.018555568531155586, tv_loss: 0.017019299790263176\n",
      "iteration 3607, dc_loss: 0.018556583672761917, tv_loss: 0.01701812446117401\n",
      "iteration 3608, dc_loss: 0.01855253428220749, tv_loss: 0.017022229731082916\n",
      "iteration 3609, dc_loss: 0.018555758520960808, tv_loss: 0.017019029706716537\n",
      "iteration 3610, dc_loss: 0.018554912880063057, tv_loss: 0.017019743099808693\n",
      "iteration 3611, dc_loss: 0.018553601577878, tv_loss: 0.017020903527736664\n",
      "iteration 3612, dc_loss: 0.01855575107038021, tv_loss: 0.017018688842654228\n",
      "iteration 3613, dc_loss: 0.018553709611296654, tv_loss: 0.01702071912586689\n",
      "iteration 3614, dc_loss: 0.018554475158452988, tv_loss: 0.01701994426548481\n",
      "iteration 3615, dc_loss: 0.018554920330643654, tv_loss: 0.017019400373101234\n",
      "iteration 3616, dc_loss: 0.01855364814400673, tv_loss: 0.017020530998706818\n",
      "iteration 3617, dc_loss: 0.018554562702775, tv_loss: 0.01701953448355198\n",
      "iteration 3618, dc_loss: 0.018554219976067543, tv_loss: 0.01701985113322735\n",
      "iteration 3619, dc_loss: 0.018553951755166054, tv_loss: 0.017020108178257942\n",
      "iteration 3620, dc_loss: 0.01855435036122799, tv_loss: 0.017019666731357574\n",
      "iteration 3621, dc_loss: 0.01855405792593956, tv_loss: 0.01701987162232399\n",
      "iteration 3622, dc_loss: 0.01855377107858658, tv_loss: 0.017020072788000107\n",
      "iteration 3623, dc_loss: 0.0185546912252903, tv_loss: 0.01701909676194191\n",
      "iteration 3624, dc_loss: 0.01855294406414032, tv_loss: 0.0170208141207695\n",
      "iteration 3625, dc_loss: 0.0185548085719347, tv_loss: 0.017018882557749748\n",
      "iteration 3626, dc_loss: 0.01855342648923397, tv_loss: 0.01702016405761242\n",
      "iteration 3627, dc_loss: 0.01855354942381382, tv_loss: 0.0170199666172266\n",
      "iteration 3628, dc_loss: 0.018554266542196274, tv_loss: 0.017019202932715416\n",
      "iteration 3629, dc_loss: 0.018552882596850395, tv_loss: 0.01702055335044861\n",
      "iteration 3630, dc_loss: 0.01855439692735672, tv_loss: 0.017018986865878105\n",
      "iteration 3631, dc_loss: 0.018553391098976135, tv_loss: 0.01701994054019451\n",
      "iteration 3632, dc_loss: 0.0185529962182045, tv_loss: 0.017020313069224358\n",
      "iteration 3633, dc_loss: 0.0185545664280653, tv_loss: 0.017018752172589302\n",
      "iteration 3634, dc_loss: 0.018552083522081375, tv_loss: 0.01702122576534748\n",
      "iteration 3635, dc_loss: 0.01855453848838806, tv_loss: 0.01701868698000908\n",
      "iteration 3636, dc_loss: 0.018552910536527634, tv_loss: 0.017020193859934807\n",
      "iteration 3637, dc_loss: 0.018552977591753006, tv_loss: 0.017020028084516525\n",
      "iteration 3638, dc_loss: 0.018553780391812325, tv_loss: 0.017019148916006088\n",
      "iteration 3639, dc_loss: 0.018552511930465698, tv_loss: 0.017020368948578835\n",
      "iteration 3640, dc_loss: 0.01855376735329628, tv_loss: 0.017019061371684074\n",
      "iteration 3641, dc_loss: 0.018552735447883606, tv_loss: 0.017020026221871376\n",
      "iteration 3642, dc_loss: 0.018553128466010094, tv_loss: 0.017019586637616158\n",
      "iteration 3643, dc_loss: 0.018553024157881737, tv_loss: 0.01701967976987362\n",
      "iteration 3644, dc_loss: 0.018552321940660477, tv_loss: 0.017020398750901222\n",
      "iteration 3645, dc_loss: 0.01855415105819702, tv_loss: 0.017018551006913185\n",
      "iteration 3646, dc_loss: 0.01855129562318325, tv_loss: 0.017021311447024345\n",
      "iteration 3647, dc_loss: 0.018553810194134712, tv_loss: 0.017018666490912437\n",
      "iteration 3648, dc_loss: 0.018552258610725403, tv_loss: 0.017020102590322495\n",
      "iteration 3649, dc_loss: 0.01855255290865898, tv_loss: 0.017019743099808693\n",
      "iteration 3650, dc_loss: 0.018553340807557106, tv_loss: 0.017018908634781837\n",
      "iteration 3651, dc_loss: 0.018551524728536606, tv_loss: 0.01702066883444786\n",
      "iteration 3652, dc_loss: 0.01855320855975151, tv_loss: 0.01701892912387848\n",
      "iteration 3653, dc_loss: 0.01855243742465973, tv_loss: 0.017019672319293022\n",
      "iteration 3654, dc_loss: 0.018551701679825783, tv_loss: 0.017020422965288162\n",
      "iteration 3655, dc_loss: 0.018553487956523895, tv_loss: 0.017018646001815796\n",
      "iteration 3656, dc_loss: 0.018550843000411987, tv_loss: 0.01702122390270233\n",
      "iteration 3657, dc_loss: 0.01855343207716942, tv_loss: 0.0170185137540102\n",
      "iteration 3658, dc_loss: 0.018551917746663094, tv_loss: 0.01701992191374302\n",
      "iteration 3659, dc_loss: 0.018551630899310112, tv_loss: 0.01702013984322548\n",
      "iteration 3660, dc_loss: 0.018553094938397408, tv_loss: 0.017018629238009453\n",
      "iteration 3661, dc_loss: 0.01855096034705639, tv_loss: 0.017020713537931442\n",
      "iteration 3662, dc_loss: 0.018552707508206367, tv_loss: 0.017018917948007584\n",
      "iteration 3663, dc_loss: 0.018551787361502647, tv_loss: 0.017019804567098618\n",
      "iteration 3664, dc_loss: 0.018551433458924294, tv_loss: 0.017020119354128838\n",
      "iteration 3665, dc_loss: 0.01855255290865898, tv_loss: 0.017018942162394524\n",
      "iteration 3666, dc_loss: 0.018550703302025795, tv_loss: 0.01702074520289898\n",
      "iteration 3667, dc_loss: 0.018553223460912704, tv_loss: 0.01701817288994789\n",
      "iteration 3668, dc_loss: 0.018550219014286995, tv_loss: 0.01702107861638069\n",
      "iteration 3669, dc_loss: 0.018552284687757492, tv_loss: 0.017018932849168777\n",
      "iteration 3670, dc_loss: 0.018551954999566078, tv_loss: 0.017019210383296013\n",
      "iteration 3671, dc_loss: 0.018550626933574677, tv_loss: 0.017020491883158684\n",
      "iteration 3672, dc_loss: 0.018552184104919434, tv_loss: 0.0170188769698143\n",
      "iteration 3673, dc_loss: 0.018551325425505638, tv_loss: 0.017019663006067276\n",
      "iteration 3674, dc_loss: 0.01855071634054184, tv_loss: 0.0170202124863863\n",
      "iteration 3675, dc_loss: 0.01855207420885563, tv_loss: 0.01701880805194378\n",
      "iteration 3676, dc_loss: 0.018550600856542587, tv_loss: 0.01702026091516018\n",
      "iteration 3677, dc_loss: 0.01855209842324257, tv_loss: 0.017018772661685944\n",
      "iteration 3678, dc_loss: 0.01854979433119297, tv_loss: 0.01702103205025196\n",
      "iteration 3679, dc_loss: 0.018551914021372795, tv_loss: 0.017018824815750122\n",
      "iteration 3680, dc_loss: 0.018551524728536606, tv_loss: 0.017019113525748253\n",
      "iteration 3681, dc_loss: 0.018549593165516853, tv_loss: 0.01702094078063965\n",
      "iteration 3682, dc_loss: 0.01855228655040264, tv_loss: 0.017018146812915802\n",
      "iteration 3683, dc_loss: 0.01855066791176796, tv_loss: 0.017019664868712425\n",
      "iteration 3684, dc_loss: 0.01854986697435379, tv_loss: 0.017020409926772118\n",
      "iteration 3685, dc_loss: 0.01855192519724369, tv_loss: 0.01701829954981804\n",
      "iteration 3686, dc_loss: 0.018549833446741104, tv_loss: 0.017020363360643387\n",
      "iteration 3687, dc_loss: 0.018551431596279144, tv_loss: 0.017018763348460197\n",
      "iteration 3688, dc_loss: 0.018549835309386253, tv_loss: 0.017020374536514282\n",
      "iteration 3689, dc_loss: 0.018551120534539223, tv_loss: 0.01701909303665161\n",
      "iteration 3690, dc_loss: 0.018550489097833633, tv_loss: 0.017019595950841904\n",
      "iteration 3691, dc_loss: 0.01855006441473961, tv_loss: 0.017019910737872124\n",
      "iteration 3692, dc_loss: 0.01855103299021721, tv_loss: 0.017018867656588554\n",
      "iteration 3693, dc_loss: 0.0185499656945467, tv_loss: 0.017019854858517647\n",
      "iteration 3694, dc_loss: 0.018550341948866844, tv_loss: 0.017019396647810936\n",
      "iteration 3695, dc_loss: 0.01855066604912281, tv_loss: 0.017019007354974747\n",
      "iteration 3696, dc_loss: 0.018549390137195587, tv_loss: 0.017020313069224358\n",
      "iteration 3697, dc_loss: 0.018551241606473923, tv_loss: 0.017018556594848633\n",
      "iteration 3698, dc_loss: 0.018548693507909775, tv_loss: 0.017021073028445244\n",
      "iteration 3699, dc_loss: 0.018551679328083992, tv_loss: 0.017017904669046402\n",
      "iteration 3700, dc_loss: 0.018549097701907158, tv_loss: 0.01702032797038555\n",
      "iteration 3701, dc_loss: 0.018549814820289612, tv_loss: 0.01701950840651989\n",
      "iteration 3702, dc_loss: 0.01855054497718811, tv_loss: 0.01701870560646057\n",
      "iteration 3703, dc_loss: 0.01854924112558365, tv_loss: 0.017019979655742645\n",
      "iteration 3704, dc_loss: 0.018550170585513115, tv_loss: 0.017019059509038925\n",
      "iteration 3705, dc_loss: 0.018549613654613495, tv_loss: 0.017019668594002724\n",
      "iteration 3706, dc_loss: 0.018549606204032898, tv_loss: 0.01701965741813183\n",
      "iteration 3707, dc_loss: 0.018550124019384384, tv_loss: 0.01701902039349079\n",
      "iteration 3708, dc_loss: 0.018548762425780296, tv_loss: 0.01702026277780533\n",
      "iteration 3709, dc_loss: 0.018550677224993706, tv_loss: 0.01701825112104416\n",
      "iteration 3710, dc_loss: 0.018548306077718735, tv_loss: 0.01702057011425495\n",
      "iteration 3711, dc_loss: 0.018550194799900055, tv_loss: 0.017018670216202736\n",
      "iteration 3712, dc_loss: 0.018549134954810143, tv_loss: 0.017019739374518394\n",
      "iteration 3713, dc_loss: 0.018549330532550812, tv_loss: 0.017019474878907204\n",
      "iteration 3714, dc_loss: 0.018549073487520218, tv_loss: 0.017019616439938545\n",
      "iteration 3715, dc_loss: 0.01854994148015976, tv_loss: 0.017018653452396393\n",
      "iteration 3716, dc_loss: 0.018548114225268364, tv_loss: 0.01702042482793331\n",
      "iteration 3717, dc_loss: 0.01855027861893177, tv_loss: 0.017018232494592667\n",
      "iteration 3718, dc_loss: 0.01854790560901165, tv_loss: 0.017020585015416145\n",
      "iteration 3719, dc_loss: 0.018549799919128418, tv_loss: 0.017018670216202736\n",
      "iteration 3720, dc_loss: 0.018548989668488503, tv_loss: 0.017019445076584816\n",
      "iteration 3721, dc_loss: 0.01854855567216873, tv_loss: 0.01701977103948593\n",
      "iteration 3722, dc_loss: 0.018549133092164993, tv_loss: 0.01701909303665161\n",
      "iteration 3723, dc_loss: 0.018548717722296715, tv_loss: 0.017019422724843025\n",
      "iteration 3724, dc_loss: 0.01854885369539261, tv_loss: 0.017019210383296013\n",
      "iteration 3725, dc_loss: 0.018548976629972458, tv_loss: 0.01701904647052288\n",
      "iteration 3726, dc_loss: 0.018547626212239265, tv_loss: 0.01702040806412697\n",
      "iteration 3727, dc_loss: 0.018550124019384384, tv_loss: 0.017017921432852745\n",
      "iteration 3728, dc_loss: 0.018546966835856438, tv_loss: 0.01702101156115532\n",
      "iteration 3729, dc_loss: 0.018549716100096703, tv_loss: 0.017018137499690056\n",
      "iteration 3730, dc_loss: 0.01854800619184971, tv_loss: 0.017019735649228096\n",
      "iteration 3731, dc_loss: 0.018548183143138885, tv_loss: 0.01701948419213295\n",
      "iteration 3732, dc_loss: 0.018548814579844475, tv_loss: 0.017018793150782585\n",
      "iteration 3733, dc_loss: 0.018548011779785156, tv_loss: 0.01701953448355198\n",
      "iteration 3734, dc_loss: 0.018548209220170975, tv_loss: 0.01701929233968258\n",
      "iteration 3735, dc_loss: 0.018548496067523956, tv_loss: 0.017018985003232956\n",
      "iteration 3736, dc_loss: 0.018547583371400833, tv_loss: 0.01701989583671093\n",
      "iteration 3737, dc_loss: 0.018548883497714996, tv_loss: 0.017018551006913185\n",
      "iteration 3738, dc_loss: 0.01854669488966465, tv_loss: 0.01702066697180271\n",
      "iteration 3739, dc_loss: 0.018549609929323196, tv_loss: 0.0170176662504673\n",
      "iteration 3740, dc_loss: 0.01854671537876129, tv_loss: 0.017020488157868385\n",
      "iteration 3741, dc_loss: 0.01854819431900978, tv_loss: 0.017018944025039673\n",
      "iteration 3742, dc_loss: 0.01854849047958851, tv_loss: 0.01701863296329975\n",
      "iteration 3743, dc_loss: 0.018546830862760544, tv_loss: 0.017020272091031075\n",
      "iteration 3744, dc_loss: 0.01854846253991127, tv_loss: 0.01701858453452587\n",
      "iteration 3745, dc_loss: 0.01854749396443367, tv_loss: 0.017019467428326607\n",
      "iteration 3746, dc_loss: 0.018547121435403824, tv_loss: 0.01701975055038929\n",
      "iteration 3747, dc_loss: 0.018548695370554924, tv_loss: 0.01701812818646431\n",
      "iteration 3748, dc_loss: 0.01854613982141018, tv_loss: 0.01702064648270607\n",
      "iteration 3749, dc_loss: 0.018548889085650444, tv_loss: 0.017017867416143417\n",
      "iteration 3750, dc_loss: 0.018546488136053085, tv_loss: 0.017020219936966896\n",
      "iteration 3751, dc_loss: 0.01854768767952919, tv_loss: 0.017018934711813927\n",
      "iteration 3752, dc_loss: 0.01854725182056427, tv_loss: 0.01701926812529564\n",
      "iteration 3753, dc_loss: 0.01854747161269188, tv_loss: 0.01701894775032997\n",
      "iteration 3754, dc_loss: 0.01854674145579338, tv_loss: 0.017019594088196754\n",
      "iteration 3755, dc_loss: 0.018547525629401207, tv_loss: 0.017018752172589302\n",
      "iteration 3756, dc_loss: 0.01854652911424637, tv_loss: 0.017019741237163544\n",
      "iteration 3757, dc_loss: 0.018547888845205307, tv_loss: 0.017018411308526993\n",
      "iteration 3758, dc_loss: 0.018545983359217644, tv_loss: 0.01702032797038555\n",
      "iteration 3759, dc_loss: 0.018547896295785904, tv_loss: 0.017018329352140427\n",
      "iteration 3760, dc_loss: 0.018546635285019875, tv_loss: 0.017019441351294518\n",
      "iteration 3761, dc_loss: 0.018546530976891518, tv_loss: 0.017019428312778473\n",
      "iteration 3762, dc_loss: 0.018547246232628822, tv_loss: 0.017018629238009453\n",
      "iteration 3763, dc_loss: 0.018546482548117638, tv_loss: 0.01701931282877922\n",
      "iteration 3764, dc_loss: 0.018546534702181816, tv_loss: 0.01701921597123146\n",
      "iteration 3765, dc_loss: 0.01854727230966091, tv_loss: 0.017018448561429977\n",
      "iteration 3766, dc_loss: 0.018545599654316902, tv_loss: 0.017020132392644882\n",
      "iteration 3767, dc_loss: 0.018547873944044113, tv_loss: 0.017017891630530357\n",
      "iteration 3768, dc_loss: 0.018544983118772507, tv_loss: 0.017020782455801964\n",
      "iteration 3769, dc_loss: 0.01854815147817135, tv_loss: 0.017017468810081482\n",
      "iteration 3770, dc_loss: 0.018545063212513924, tv_loss: 0.017020419239997864\n",
      "iteration 3771, dc_loss: 0.018546678125858307, tv_loss: 0.017018716782331467\n",
      "iteration 3772, dc_loss: 0.01854698918759823, tv_loss: 0.017018377780914307\n",
      "iteration 3773, dc_loss: 0.018545018509030342, tv_loss: 0.01702035404741764\n",
      "iteration 3774, dc_loss: 0.018547547981142998, tv_loss: 0.017017798498272896\n",
      "iteration 3775, dc_loss: 0.018545405939221382, tv_loss: 0.017019880935549736\n",
      "iteration 3776, dc_loss: 0.018545595929026604, tv_loss: 0.01701958291232586\n",
      "iteration 3777, dc_loss: 0.018547657877206802, tv_loss: 0.017017459496855736\n",
      "iteration 3778, dc_loss: 0.018543856218457222, tv_loss: 0.01702122949063778\n",
      "iteration 3779, dc_loss: 0.018547698855400085, tv_loss: 0.01701735518872738\n",
      "iteration 3780, dc_loss: 0.018544716760516167, tv_loss: 0.017020275816321373\n",
      "iteration 3781, dc_loss: 0.018546398729085922, tv_loss: 0.017018521204590797\n",
      "iteration 3782, dc_loss: 0.018545987084507942, tv_loss: 0.017018849030137062\n",
      "iteration 3783, dc_loss: 0.01854478195309639, tv_loss: 0.017019951716065407\n",
      "iteration 3784, dc_loss: 0.018546517938375473, tv_loss: 0.017018122598528862\n",
      "iteration 3785, dc_loss: 0.018545450642704964, tv_loss: 0.017019100487232208\n",
      "iteration 3786, dc_loss: 0.018545038998126984, tv_loss: 0.017019474878907204\n",
      "iteration 3787, dc_loss: 0.018546652048826218, tv_loss: 0.017017869278788567\n",
      "iteration 3788, dc_loss: 0.018543897196650505, tv_loss: 0.017020683735609055\n",
      "iteration 3789, dc_loss: 0.01854703202843666, tv_loss: 0.017017528414726257\n",
      "iteration 3790, dc_loss: 0.018544739112257957, tv_loss: 0.017019696533679962\n",
      "iteration 3791, dc_loss: 0.01854470744729042, tv_loss: 0.017019594088196754\n",
      "iteration 3792, dc_loss: 0.01854640617966652, tv_loss: 0.017017794772982597\n",
      "iteration 3793, dc_loss: 0.018544234335422516, tv_loss: 0.017019888386130333\n",
      "iteration 3794, dc_loss: 0.018545785918831825, tv_loss: 0.017018277198076248\n",
      "iteration 3795, dc_loss: 0.01854468695819378, tv_loss: 0.01701933890581131\n",
      "iteration 3796, dc_loss: 0.01854505017399788, tv_loss: 0.017018957063555717\n",
      "iteration 3797, dc_loss: 0.01854553632438183, tv_loss: 0.017018476501107216\n",
      "iteration 3798, dc_loss: 0.01854364573955536, tv_loss: 0.017020314931869507\n",
      "iteration 3799, dc_loss: 0.018547018989920616, tv_loss: 0.01701689139008522\n",
      "iteration 3800, dc_loss: 0.018542729318141937, tv_loss: 0.01702110655605793\n",
      "iteration 3801, dc_loss: 0.018546229228377342, tv_loss: 0.017017528414726257\n",
      "iteration 3802, dc_loss: 0.018544279038906097, tv_loss: 0.017019420862197876\n",
      "iteration 3803, dc_loss: 0.01854432001709938, tv_loss: 0.017019333317875862\n",
      "iteration 3804, dc_loss: 0.018545376136898994, tv_loss: 0.017018212005496025\n",
      "iteration 3805, dc_loss: 0.01854415237903595, tv_loss: 0.01701933890581131\n",
      "iteration 3806, dc_loss: 0.018544264137744904, tv_loss: 0.01701914705336094\n",
      "iteration 3807, dc_loss: 0.01854533515870571, tv_loss: 0.017018038779497147\n",
      "iteration 3808, dc_loss: 0.01854308508336544, tv_loss: 0.01702030375599861\n",
      "iteration 3809, dc_loss: 0.018545962870121002, tv_loss: 0.017017437145113945\n",
      "iteration 3810, dc_loss: 0.018543193116784096, tv_loss: 0.01702013798058033\n",
      "iteration 3811, dc_loss: 0.018544554710388184, tv_loss: 0.017018655315041542\n",
      "iteration 3812, dc_loss: 0.018545035272836685, tv_loss: 0.017018062993884087\n",
      "iteration 3813, dc_loss: 0.018542656674981117, tv_loss: 0.017020344734191895\n",
      "iteration 3814, dc_loss: 0.01854538917541504, tv_loss: 0.017017517238855362\n",
      "iteration 3815, dc_loss: 0.01854354701936245, tv_loss: 0.017019258812069893\n",
      "iteration 3816, dc_loss: 0.018543638288974762, tv_loss: 0.017019107937812805\n",
      "iteration 3817, dc_loss: 0.01854504831135273, tv_loss: 0.017017684876918793\n",
      "iteration 3818, dc_loss: 0.018542293459177017, tv_loss: 0.01702049747109413\n",
      "iteration 3819, dc_loss: 0.018545832484960556, tv_loss: 0.01701706275343895\n",
      "iteration 3820, dc_loss: 0.018542004749178886, tv_loss: 0.0170208178460598\n",
      "iteration 3821, dc_loss: 0.018545014783740044, tv_loss: 0.017017610371112823\n",
      "iteration 3822, dc_loss: 0.018543528392910957, tv_loss: 0.017018938437104225\n",
      "iteration 3823, dc_loss: 0.0185428224503994, tv_loss: 0.017019545659422874\n",
      "iteration 3824, dc_loss: 0.018545027822256088, tv_loss: 0.01701728068292141\n",
      "iteration 3825, dc_loss: 0.018542474135756493, tv_loss: 0.01701980270445347\n",
      "iteration 3826, dc_loss: 0.018543556332588196, tv_loss: 0.017018763348460197\n",
      "iteration 3827, dc_loss: 0.018544070422649384, tv_loss: 0.017018338665366173\n",
      "iteration 3828, dc_loss: 0.018542801961302757, tv_loss: 0.017019562423229218\n",
      "iteration 3829, dc_loss: 0.01854422129690647, tv_loss: 0.017017994076013565\n",
      "iteration 3830, dc_loss: 0.01854199916124344, tv_loss: 0.017020056024193764\n",
      "iteration 3831, dc_loss: 0.01854434423148632, tv_loss: 0.01701764017343521\n",
      "iteration 3832, dc_loss: 0.018542848527431488, tv_loss: 0.017019124701619148\n",
      "iteration 3833, dc_loss: 0.018543051555752754, tv_loss: 0.017018930986523628\n",
      "iteration 3834, dc_loss: 0.018543139100074768, tv_loss: 0.017018795013427734\n",
      "iteration 3835, dc_loss: 0.018543493002653122, tv_loss: 0.01701832190155983\n",
      "iteration 3836, dc_loss: 0.018541868776082993, tv_loss: 0.017019866034388542\n",
      "iteration 3837, dc_loss: 0.018544727936387062, tv_loss: 0.017017005011439323\n",
      "iteration 3838, dc_loss: 0.01854098029434681, tv_loss: 0.017020730301737785\n",
      "iteration 3839, dc_loss: 0.01854396052658558, tv_loss: 0.017017703503370285\n",
      "iteration 3840, dc_loss: 0.018543144688010216, tv_loss: 0.01701841503381729\n",
      "iteration 3841, dc_loss: 0.01854150928556919, tv_loss: 0.017019955441355705\n",
      "iteration 3842, dc_loss: 0.01854383386671543, tv_loss: 0.017017537727952003\n",
      "iteration 3843, dc_loss: 0.018541816622018814, tv_loss: 0.017019454389810562\n",
      "iteration 3844, dc_loss: 0.018543032929301262, tv_loss: 0.017018163576722145\n",
      "iteration 3845, dc_loss: 0.01854225993156433, tv_loss: 0.017018867656588554\n",
      "iteration 3846, dc_loss: 0.018542172387242317, tv_loss: 0.017018917948007584\n",
      "iteration 3847, dc_loss: 0.018543310463428497, tv_loss: 0.017017804086208344\n",
      "iteration 3848, dc_loss: 0.01854081265628338, tv_loss: 0.017020363360643387\n",
      "iteration 3849, dc_loss: 0.018544618040323257, tv_loss: 0.017016513273119926\n",
      "iteration 3850, dc_loss: 0.018539901822805405, tv_loss: 0.017021117731928825\n",
      "iteration 3851, dc_loss: 0.018543994054198265, tv_loss: 0.017016902565956116\n",
      "iteration 3852, dc_loss: 0.018541961908340454, tv_loss: 0.017018837854266167\n",
      "iteration 3853, dc_loss: 0.01854117028415203, tv_loss: 0.017019566148519516\n",
      "iteration 3854, dc_loss: 0.01854310743510723, tv_loss: 0.01701757311820984\n",
      "iteration 3855, dc_loss: 0.018541373312473297, tv_loss: 0.01701926253736019\n",
      "iteration 3856, dc_loss: 0.01854192651808262, tv_loss: 0.017018692567944527\n",
      "iteration 3857, dc_loss: 0.018542299047112465, tv_loss: 0.017018292099237442\n",
      "iteration 3858, dc_loss: 0.01854114606976509, tv_loss: 0.017019443213939667\n",
      "iteration 3859, dc_loss: 0.018542854115366936, tv_loss: 0.017017699778079987\n",
      "iteration 3860, dc_loss: 0.018540609627962112, tv_loss: 0.01701982505619526\n",
      "iteration 3861, dc_loss: 0.01854243315756321, tv_loss: 0.01701788231730461\n",
      "iteration 3862, dc_loss: 0.018541773781180382, tv_loss: 0.017018457874655724\n",
      "iteration 3863, dc_loss: 0.01854090578854084, tv_loss: 0.017019275575876236\n",
      "iteration 3864, dc_loss: 0.018542030826210976, tv_loss: 0.017018092796206474\n",
      "iteration 3865, dc_loss: 0.01854158565402031, tv_loss: 0.017018459737300873\n",
      "iteration 3866, dc_loss: 0.018540406599640846, tv_loss: 0.017019597813487053\n",
      "iteration 3867, dc_loss: 0.018543099984526634, tv_loss: 0.017016872763633728\n",
      "iteration 3868, dc_loss: 0.018539348617196083, tv_loss: 0.01702062040567398\n",
      "iteration 3869, dc_loss: 0.018543139100074768, tv_loss: 0.017016829922795296\n",
      "iteration 3870, dc_loss: 0.018539659678936005, tv_loss: 0.017020218074321747\n",
      "iteration 3871, dc_loss: 0.018542135134339333, tv_loss: 0.017017586156725883\n",
      "iteration 3872, dc_loss: 0.01854122243821621, tv_loss: 0.01701839081943035\n",
      "iteration 3873, dc_loss: 0.0185397882014513, tv_loss: 0.01701975427567959\n",
      "iteration 3874, dc_loss: 0.01854279637336731, tv_loss: 0.017016712576150894\n",
      "iteration 3875, dc_loss: 0.018539849668741226, tv_loss: 0.01701965555548668\n",
      "iteration 3876, dc_loss: 0.01854119263589382, tv_loss: 0.017018292099237442\n",
      "iteration 3877, dc_loss: 0.018541492521762848, tv_loss: 0.017017992213368416\n",
      "iteration 3878, dc_loss: 0.018539540469646454, tv_loss: 0.017019910737872124\n",
      "iteration 3879, dc_loss: 0.018542487174272537, tv_loss: 0.017016833648085594\n",
      "iteration 3880, dc_loss: 0.018538804724812508, tv_loss: 0.017020411789417267\n",
      "iteration 3881, dc_loss: 0.018542220816016197, tv_loss: 0.017016928642988205\n",
      "iteration 3882, dc_loss: 0.018539773300290108, tv_loss: 0.01701934076845646\n",
      "iteration 3883, dc_loss: 0.018541064113378525, tv_loss: 0.017018023878335953\n",
      "iteration 3884, dc_loss: 0.018540244549512863, tv_loss: 0.017018798738718033\n",
      "iteration 3885, dc_loss: 0.018540505319833755, tv_loss: 0.017018437385559082\n",
      "iteration 3886, dc_loss: 0.018539825454354286, tv_loss: 0.017019016668200493\n",
      "iteration 3887, dc_loss: 0.018541507422924042, tv_loss: 0.01701730489730835\n",
      "iteration 3888, dc_loss: 0.01853872276842594, tv_loss: 0.017020102590322495\n",
      "iteration 3889, dc_loss: 0.01854158565402031, tv_loss: 0.017017245292663574\n",
      "iteration 3890, dc_loss: 0.018539490178227425, tv_loss: 0.017019255086779594\n",
      "iteration 3891, dc_loss: 0.018540184944868088, tv_loss: 0.017018413171172142\n",
      "iteration 3892, dc_loss: 0.018540754914283752, tv_loss: 0.01701771281659603\n",
      "iteration 3893, dc_loss: 0.018538877367973328, tv_loss: 0.01701950840651989\n",
      "iteration 3894, dc_loss: 0.018540922552347183, tv_loss: 0.017017392441630363\n",
      "iteration 3895, dc_loss: 0.018539126962423325, tv_loss: 0.01701914332807064\n",
      "iteration 3896, dc_loss: 0.01854036934673786, tv_loss: 0.01701785810291767\n",
      "iteration 3897, dc_loss: 0.01853986829519272, tv_loss: 0.017018340528011322\n",
      "iteration 3898, dc_loss: 0.018538912758231163, tv_loss: 0.017019327729940414\n",
      "iteration 3899, dc_loss: 0.018541401252150536, tv_loss: 0.017016855999827385\n",
      "iteration 3900, dc_loss: 0.018536943942308426, tv_loss: 0.017021292820572853\n",
      "iteration 3901, dc_loss: 0.018543019890785217, tv_loss: 0.017015108838677406\n",
      "iteration 3902, dc_loss: 0.01853768341243267, tv_loss: 0.017020301893353462\n",
      "iteration 3903, dc_loss: 0.018539519980549812, tv_loss: 0.017018359154462814\n",
      "iteration 3904, dc_loss: 0.018540455028414726, tv_loss: 0.01701737381517887\n",
      "iteration 3905, dc_loss: 0.01853824220597744, tv_loss: 0.017019543796777725\n",
      "iteration 3906, dc_loss: 0.018540412187576294, tv_loss: 0.01701732538640499\n",
      "iteration 3907, dc_loss: 0.01853873021900654, tv_loss: 0.01701897755265236\n",
      "iteration 3908, dc_loss: 0.018539510667324066, tv_loss: 0.017018163576722145\n",
      "iteration 3909, dc_loss: 0.01853920705616474, tv_loss: 0.017018401995301247\n",
      "iteration 3910, dc_loss: 0.018538976088166237, tv_loss: 0.017018552869558334\n",
      "iteration 3911, dc_loss: 0.01853993721306324, tv_loss: 0.017017556354403496\n",
      "iteration 3912, dc_loss: 0.01853731833398342, tv_loss: 0.01702013984322548\n",
      "iteration 3913, dc_loss: 0.018541140481829643, tv_loss: 0.017016232013702393\n",
      "iteration 3914, dc_loss: 0.018537305295467377, tv_loss: 0.017020000144839287\n",
      "iteration 3915, dc_loss: 0.018540039658546448, tv_loss: 0.017017191275954247\n",
      "iteration 3916, dc_loss: 0.018538178876042366, tv_loss: 0.01701895147562027\n",
      "iteration 3917, dc_loss: 0.01853928714990616, tv_loss: 0.017017751932144165\n",
      "iteration 3918, dc_loss: 0.01853802613914013, tv_loss: 0.017018981277942657\n",
      "iteration 3919, dc_loss: 0.018539609387516975, tv_loss: 0.017017418518662453\n",
      "iteration 3920, dc_loss: 0.018537577241659164, tv_loss: 0.01701943762600422\n",
      "iteration 3921, dc_loss: 0.01853972114622593, tv_loss: 0.017017221078276634\n",
      "iteration 3922, dc_loss: 0.01853768713772297, tv_loss: 0.017019178718328476\n",
      "iteration 3923, dc_loss: 0.01853889226913452, tv_loss: 0.01701785810291767\n",
      "iteration 3924, dc_loss: 0.01853855326771736, tv_loss: 0.01701812632381916\n",
      "iteration 3925, dc_loss: 0.018537862226366997, tv_loss: 0.0170187596231699\n",
      "iteration 3926, dc_loss: 0.018539076671004295, tv_loss: 0.017017487436532974\n",
      "iteration 3927, dc_loss: 0.018537668511271477, tv_loss: 0.017018817365169525\n",
      "iteration 3928, dc_loss: 0.01853819191455841, tv_loss: 0.017018232494592667\n",
      "iteration 3929, dc_loss: 0.01853896677494049, tv_loss: 0.017017407342791557\n",
      "iteration 3930, dc_loss: 0.018536550924181938, tv_loss: 0.01701981946825981\n",
      "iteration 3931, dc_loss: 0.018540486693382263, tv_loss: 0.01701592467725277\n",
      "iteration 3932, dc_loss: 0.018535250797867775, tv_loss: 0.01702118292450905\n",
      "iteration 3933, dc_loss: 0.01854071393609047, tv_loss: 0.017015593126416206\n",
      "iteration 3934, dc_loss: 0.01853644847869873, tv_loss: 0.01701969839632511\n",
      "iteration 3935, dc_loss: 0.01853800006210804, tv_loss: 0.017018012702465057\n",
      "iteration 3936, dc_loss: 0.01853856071829796, tv_loss: 0.01701737567782402\n",
      "iteration 3937, dc_loss: 0.018536489456892014, tv_loss: 0.01701938919723034\n",
      "iteration 3938, dc_loss: 0.018539102748036385, tv_loss: 0.01701677031815052\n",
      "iteration 3939, dc_loss: 0.018536711111664772, tv_loss: 0.017019163817167282\n",
      "iteration 3940, dc_loss: 0.0185379795730114, tv_loss: 0.017017912119627\n",
      "iteration 3941, dc_loss: 0.01853792369365692, tv_loss: 0.017017947509884834\n",
      "iteration 3942, dc_loss: 0.018536748364567757, tv_loss: 0.017019016668200493\n",
      "iteration 3943, dc_loss: 0.01853889785706997, tv_loss: 0.017016740515828133\n",
      "iteration 3944, dc_loss: 0.018535424023866653, tv_loss: 0.017020128667354584\n",
      "iteration 3945, dc_loss: 0.018539270386099815, tv_loss: 0.017016254365444183\n",
      "iteration 3946, dc_loss: 0.01853584684431553, tv_loss: 0.017019672319293022\n",
      "iteration 3947, dc_loss: 0.018538353964686394, tv_loss: 0.01701713353395462\n",
      "iteration 3948, dc_loss: 0.01853654533624649, tv_loss: 0.017018863931298256\n",
      "iteration 3949, dc_loss: 0.018537968397140503, tv_loss: 0.017017308622598648\n",
      "iteration 3950, dc_loss: 0.018535872921347618, tv_loss: 0.017019325867295265\n",
      "iteration 3951, dc_loss: 0.018538324162364006, tv_loss: 0.017016833648085594\n",
      "iteration 3952, dc_loss: 0.018535614013671875, tv_loss: 0.017019543796777725\n",
      "iteration 3953, dc_loss: 0.018538212403655052, tv_loss: 0.017016906291246414\n",
      "iteration 3954, dc_loss: 0.018536856397986412, tv_loss: 0.01701820269227028\n",
      "iteration 3955, dc_loss: 0.018536027520895004, tv_loss: 0.017018934711813927\n",
      "iteration 3956, dc_loss: 0.018537770956754684, tv_loss: 0.017017096281051636\n",
      "iteration 3957, dc_loss: 0.018535766750574112, tv_loss: 0.017018990591168404\n",
      "iteration 3958, dc_loss: 0.01853743940591812, tv_loss: 0.01701722852885723\n",
      "iteration 3959, dc_loss: 0.018535926938056946, tv_loss: 0.017018644139170647\n",
      "iteration 3960, dc_loss: 0.01853695511817932, tv_loss: 0.017017537727952003\n",
      "iteration 3961, dc_loss: 0.018536744639277458, tv_loss: 0.017017707228660583\n",
      "iteration 3962, dc_loss: 0.018535461276769638, tv_loss: 0.017019016668200493\n",
      "iteration 3963, dc_loss: 0.018538497388362885, tv_loss: 0.017016083002090454\n",
      "iteration 3964, dc_loss: 0.018533365800976753, tv_loss: 0.017021315172314644\n",
      "iteration 3965, dc_loss: 0.01854001171886921, tv_loss: 0.017014523968100548\n",
      "iteration 3966, dc_loss: 0.018533656373620033, tv_loss: 0.017020683735609055\n",
      "iteration 3967, dc_loss: 0.01853722706437111, tv_loss: 0.01701693795621395\n",
      "iteration 3968, dc_loss: 0.018536770716309547, tv_loss: 0.01701730489730835\n",
      "iteration 3969, dc_loss: 0.018534615635871887, tv_loss: 0.01701943762600422\n",
      "iteration 3970, dc_loss: 0.018537577241659164, tv_loss: 0.017016498371958733\n",
      "iteration 3971, dc_loss: 0.018534930422902107, tv_loss: 0.017019135877490044\n",
      "iteration 3972, dc_loss: 0.018536487594246864, tv_loss: 0.017017534002661705\n",
      "iteration 3973, dc_loss: 0.018536102026700974, tv_loss: 0.017017899081110954\n",
      "iteration 3974, dc_loss: 0.018535412847995758, tv_loss: 0.017018558457493782\n",
      "iteration 3975, dc_loss: 0.018536996096372604, tv_loss: 0.017016872763633728\n",
      "iteration 3976, dc_loss: 0.018534399569034576, tv_loss: 0.017019372433423996\n",
      "iteration 3977, dc_loss: 0.018536753952503204, tv_loss: 0.017016908153891563\n",
      "iteration 3978, dc_loss: 0.018534736707806587, tv_loss: 0.01701887510716915\n",
      "iteration 3979, dc_loss: 0.018536632880568504, tv_loss: 0.017016977071762085\n",
      "iteration 3980, dc_loss: 0.0185342226177454, tv_loss: 0.017019348219037056\n",
      "iteration 3981, dc_loss: 0.018537653610110283, tv_loss: 0.017015857622027397\n",
      "iteration 3982, dc_loss: 0.01853298768401146, tv_loss: 0.017020491883158684\n",
      "iteration 3983, dc_loss: 0.01853770576417446, tv_loss: 0.01701570302248001\n",
      "iteration 3984, dc_loss: 0.01853361167013645, tv_loss: 0.017019670456647873\n",
      "iteration 3985, dc_loss: 0.018535781651735306, tv_loss: 0.017017392441630363\n",
      "iteration 3986, dc_loss: 0.018536172807216644, tv_loss: 0.017016975209116936\n",
      "iteration 3987, dc_loss: 0.01853344775736332, tv_loss: 0.017019683495163918\n",
      "iteration 3988, dc_loss: 0.01853722333908081, tv_loss: 0.017015885561704636\n",
      "iteration 3989, dc_loss: 0.0185332503169775, tv_loss: 0.01701980270445347\n",
      "iteration 3990, dc_loss: 0.018536468967795372, tv_loss: 0.01701648160815239\n",
      "iteration 3991, dc_loss: 0.01853441819548607, tv_loss: 0.017018413171172142\n",
      "iteration 3992, dc_loss: 0.0185343399643898, tv_loss: 0.01701841875910759\n",
      "iteration 3993, dc_loss: 0.018536370247602463, tv_loss: 0.01701635867357254\n",
      "iteration 3994, dc_loss: 0.018532291054725647, tv_loss: 0.017020462080836296\n",
      "iteration 3995, dc_loss: 0.018537919968366623, tv_loss: 0.017014823853969574\n",
      "iteration 3996, dc_loss: 0.018532073125243187, tv_loss: 0.017020607367157936\n",
      "iteration 3997, dc_loss: 0.018536267802119255, tv_loss: 0.017016304656863213\n",
      "iteration 3998, dc_loss: 0.018534090369939804, tv_loss: 0.017018429934978485\n",
      "iteration 3999, dc_loss: 0.018534669652581215, tv_loss: 0.01701776683330536\n",
      "iteration 4000, dc_loss: 0.018534764647483826, tv_loss: 0.017017560079693794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e00ecce10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from datalo\n",
    "\n",
    "from ese5934_project.models.FactorFields import DictField, get_coordinates\n",
    "\n",
    "base_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/defaults.yaml\")\n",
    "second_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/image.yaml\")\n",
    "cfg = OmegaConf.merge(\n",
    "    base_conf,\n",
    "    second_conf,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "size = (640, 368)\n",
    "model = DictField(cfg, size, device)\n",
    "coords = get_coordinates(size)\n",
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_4[15]\n",
    "\n",
    "scheduler = lambda t: 0.8 ** (t // 400) * 5e-3\n",
    "optimizer = torchopt.adam(lr=scheduler)\n",
    "\n",
    "params, image_list = reconstruct(\n",
    "    model,\n",
    "    coords,\n",
    "    masked_kspace,\n",
    "    csm,\n",
    "    mask,\n",
    "    alpha=0.005,\n",
    "    optimizer=optimizer,\n",
    "    iterations=4000,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    ")\n",
    "plt.imshow(complex_abs(image_list[-1]), cmap=\"gray\", vmax=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 36.12320252630849\n",
      "SSIM Value mt1: 0.7842791778076317\n",
      "PSNR Value mt1: 38.20538190060181\n",
      "SSIM Value mt1: 0.9346172397449728\n",
      "PSNR Value mt1: 36.12320252630849\n",
      "SSIM Value mt1: 0.7842791778076317\n",
      "PSNR Value mt1: 38.20538190060181\n",
      "SSIM Value mt1: 0.9346172397449728\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"ADAM\": image_list_ADAM[-1],\n",
    "    # \"SIREN\": image_list_SIREN[-1],\n",
    "    \"DF\": image_list[-1],\n",
    "    \"gt\": image_gt.squeeze(),\n",
    "}\n",
    "torch.save(results, \"experiments/best_case_comparision/images_dict.pt\")\n",
    "psnr_dict = {\n",
    "    key: Evaluate_MT1(image_gt, value)[0]\n",
    "    for key, value in results.items()\n",
    "    if key != \"gt\"\n",
    "}\n",
    "torch.save(psnr_dict, \"experiments/best_case_comparision/psnr_dict.pt\")\n",
    "ssim_dict = {\n",
    "    key: Evaluate_MT1(image_gt, value)[1]\n",
    "    for key, value in results.items()\n",
    "    if key != \"gt\"\n",
    "}\n",
    "torch.save(ssim_dict, \"experiments/best_case_comparision/ssim_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ADAM_list = []\n",
    "SIREN_list = []\n",
    "DF_list = []\n",
    "datasets_list = [dataset_gt, dataset_2, dataset_4, dataset_8]\n",
    "# results = pd.DataFrame(\n",
    "#     columns=[\n",
    "#         \"acceration_rate\",\n",
    "#         \"psnr\",\n",
    "#         \"ssim\",\n",
    "#     ],\n",
    "#     index=[\n",
    "#         \"ADAM\",\n",
    "#         \"SIREN\",\n",
    "#         \"DF\",\n",
    "#     ],\n",
    "# )\n",
    "# results.loc[\"ADAM\", \"psnr\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "PSNR Value mt1: 49.782543219055206\n",
      "SSIM Value mt1: 0.9552465639139732\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "PSNR Value mt1: 35.81746788119504\n",
      "SSIM Value mt1: 0.7832948340974734\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "PSNR Value mt1: 36.20641452968033\n",
      "SSIM Value mt1: 0.7780212330974999\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "PSNR Value mt1: 34.87135346651343\n",
      "SSIM Value mt1: 0.7606510936766294\n"
     ]
    }
   ],
   "source": [
    "for dataset, ar in zip(datasets_list, [1, 2, 4, 8]):\n",
    "    scheduler = lambda t: 0.8 ** (t // 400) * 1e-1\n",
    "    optimizer = torchopt.adam(lr=0.1)\n",
    "    kspace, (mean, std), masked_kspace, mask, csm = dataset[15]\n",
    "    field = Grid((640, 368), mean, std)\n",
    "    params, image_list_ADAM = reconstruct(\n",
    "        field,\n",
    "        torch.rand(1, 2),\n",
    "        masked_kspace,\n",
    "        csm,\n",
    "        mask,\n",
    "        alpha=0.005,\n",
    "        optimizer=optimizer,\n",
    "        iterations=4000,\n",
    "        device=torch.device(\"cuda\"),\n",
    "    )\n",
    "    psnr, ssim = Evaluate_MT1(image_gt, image_list_ADAM[-1])\n",
    "    ADAM_list.append((psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(49.782543219055206, 0.9552465639139732), (35.81746788119504, 0.7832948340974734), (36.20641452968033, 0.7780212330974999), (34.87135346651343, 0.7606510936766294)]\n"
     ]
    }
   ],
   "source": [
    "print(ADAM_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = pd.DataFrame(\n",
    "    {\n",
    "        \"Acceleration Rate\": [1, 2, 4, 8],\n",
    "        \"PSNR\": [n[0] for n in ADAM_list],\n",
    "        \"SSIM\": [n[1] for n in ADAM_list],\n",
    "    }\n",
    ")\n",
    "d.to_csv(\"experiments/different_AR/ADAM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 3.18599009513855, tv_loss: 0.0005901351105421782\n",
      "iteration 2, dc_loss: 3.0555496215820312, tv_loss: 0.003078893991187215\n",
      "iteration 3, dc_loss: 2.9740359783172607, tv_loss: 0.005541853606700897\n",
      "iteration 4, dc_loss: 2.9173800945281982, tv_loss: 0.007393249310553074\n",
      "iteration 5, dc_loss: 2.8812050819396973, tv_loss: 0.00861265230923891\n",
      "iteration 6, dc_loss: 2.8570971488952637, tv_loss: 0.009377313777804375\n",
      "iteration 7, dc_loss: 2.8397624492645264, tv_loss: 0.00983405765146017\n",
      "iteration 8, dc_loss: 2.8248579502105713, tv_loss: 0.010136129334568977\n",
      "iteration 9, dc_loss: 2.8094234466552734, tv_loss: 0.010419312864542007\n",
      "iteration 10, dc_loss: 2.793241500854492, tv_loss: 0.010728083550930023\n",
      "iteration 11, dc_loss: 2.7774014472961426, tv_loss: 0.011017663404345512\n",
      "iteration 12, dc_loss: 2.7619478702545166, tv_loss: 0.011285373941063881\n",
      "iteration 13, dc_loss: 2.7462716102600098, tv_loss: 0.01155644841492176\n",
      "iteration 14, dc_loss: 2.7313075065612793, tv_loss: 0.011812151409685612\n",
      "iteration 15, dc_loss: 2.7171573638916016, tv_loss: 0.012041540816426277\n",
      "iteration 16, dc_loss: 2.703204393386841, tv_loss: 0.012238908559083939\n",
      "iteration 17, dc_loss: 2.689452648162842, tv_loss: 0.012408394366502762\n",
      "iteration 18, dc_loss: 2.675945281982422, tv_loss: 0.012566561810672283\n",
      "iteration 19, dc_loss: 2.662456750869751, tv_loss: 0.01271588634699583\n",
      "iteration 20, dc_loss: 2.648873805999756, tv_loss: 0.012841224670410156\n",
      "iteration 21, dc_loss: 2.635471820831299, tv_loss: 0.012928929179906845\n",
      "iteration 22, dc_loss: 2.6223788261413574, tv_loss: 0.012935112230479717\n",
      "iteration 23, dc_loss: 2.6094985008239746, tv_loss: 0.012821940705180168\n",
      "iteration 24, dc_loss: 2.596390962600708, tv_loss: 0.012656895443797112\n",
      "iteration 25, dc_loss: 2.583242654800415, tv_loss: 0.012455599382519722\n",
      "iteration 26, dc_loss: 2.5702950954437256, tv_loss: 0.01213940791785717\n",
      "iteration 27, dc_loss: 2.5572831630706787, tv_loss: 0.011748007498681545\n",
      "iteration 28, dc_loss: 2.544499397277832, tv_loss: 0.011288234032690525\n",
      "iteration 29, dc_loss: 2.531848192214966, tv_loss: 0.010820628143846989\n",
      "iteration 30, dc_loss: 2.519261598587036, tv_loss: 0.010224226862192154\n",
      "iteration 31, dc_loss: 2.5067741870880127, tv_loss: 0.00970772746950388\n",
      "iteration 32, dc_loss: 2.4945995807647705, tv_loss: 0.009157530032098293\n",
      "iteration 33, dc_loss: 2.482656478881836, tv_loss: 0.008974982425570488\n",
      "iteration 34, dc_loss: 2.4727139472961426, tv_loss: 0.008431161753833294\n",
      "iteration 35, dc_loss: 2.464120388031006, tv_loss: 0.00878959521651268\n",
      "iteration 36, dc_loss: 2.449411153793335, tv_loss: 0.0084871556609869\n",
      "iteration 37, dc_loss: 2.4353392124176025, tv_loss: 0.008606518618762493\n",
      "iteration 38, dc_loss: 2.426391363143921, tv_loss: 0.008725350722670555\n",
      "iteration 39, dc_loss: 2.4124033451080322, tv_loss: 0.008704359643161297\n",
      "iteration 40, dc_loss: 2.402641534805298, tv_loss: 0.00866924598813057\n",
      "iteration 41, dc_loss: 2.3907692432403564, tv_loss: 0.008734159171581268\n",
      "iteration 42, dc_loss: 2.378795862197876, tv_loss: 0.00876196101307869\n",
      "iteration 43, dc_loss: 2.3689117431640625, tv_loss: 0.008712631650269032\n",
      "iteration 44, dc_loss: 2.356128692626953, tv_loss: 0.008691265247762203\n",
      "iteration 45, dc_loss: 2.346388101577759, tv_loss: 0.008687683381140232\n",
      "iteration 46, dc_loss: 2.3344805240631104, tv_loss: 0.008660320192575455\n",
      "iteration 47, dc_loss: 2.324181079864502, tv_loss: 0.008651336655020714\n",
      "iteration 48, dc_loss: 2.3133394718170166, tv_loss: 0.008825661614537239\n",
      "iteration 49, dc_loss: 2.3021578788757324, tv_loss: 0.008886387571692467\n",
      "iteration 50, dc_loss: 2.292487859725952, tv_loss: 0.008856762200593948\n",
      "iteration 51, dc_loss: 2.2810161113739014, tv_loss: 0.009036242961883545\n",
      "iteration 52, dc_loss: 2.271014451980591, tv_loss: 0.00914246030151844\n",
      "iteration 53, dc_loss: 2.260601282119751, tv_loss: 0.009143671952188015\n",
      "iteration 54, dc_loss: 2.249986410140991, tv_loss: 0.009265738539397717\n",
      "iteration 55, dc_loss: 2.240107297897339, tv_loss: 0.009402237832546234\n",
      "iteration 56, dc_loss: 2.229515552520752, tv_loss: 0.009450714103877544\n",
      "iteration 57, dc_loss: 2.219741106033325, tv_loss: 0.009526357054710388\n",
      "iteration 58, dc_loss: 2.209629535675049, tv_loss: 0.00966115016490221\n",
      "iteration 59, dc_loss: 2.1994614601135254, tv_loss: 0.009730204939842224\n",
      "iteration 60, dc_loss: 2.189948558807373, tv_loss: 0.00979373138397932\n",
      "iteration 61, dc_loss: 2.1798179149627686, tv_loss: 0.009936897084116936\n",
      "iteration 62, dc_loss: 2.1700687408447266, tv_loss: 0.010028270073235035\n",
      "iteration 63, dc_loss: 2.1606249809265137, tv_loss: 0.010084137320518494\n",
      "iteration 64, dc_loss: 2.150705337524414, tv_loss: 0.010221594013273716\n",
      "iteration 65, dc_loss: 2.1411948204040527, tv_loss: 0.01031937263906002\n",
      "iteration 66, dc_loss: 2.131850004196167, tv_loss: 0.010361842811107635\n",
      "iteration 67, dc_loss: 2.122185707092285, tv_loss: 0.01048495527356863\n",
      "iteration 68, dc_loss: 2.1128194332122803, tv_loss: 0.010578949004411697\n",
      "iteration 69, dc_loss: 2.103624105453491, tv_loss: 0.010631420649588108\n",
      "iteration 70, dc_loss: 2.09420108795166, tv_loss: 0.010756578296422958\n",
      "iteration 71, dc_loss: 2.0849640369415283, tv_loss: 0.010835917666554451\n",
      "iteration 72, dc_loss: 2.075930118560791, tv_loss: 0.010896614752709866\n",
      "iteration 73, dc_loss: 2.066742420196533, tv_loss: 0.011021449230611324\n",
      "iteration 74, dc_loss: 2.0576298236846924, tv_loss: 0.011084775440394878\n",
      "iteration 75, dc_loss: 2.048677682876587, tv_loss: 0.01116675604134798\n",
      "iteration 76, dc_loss: 2.0397539138793945, tv_loss: 0.011275344528257847\n",
      "iteration 77, dc_loss: 2.030834674835205, tv_loss: 0.0113253528252244\n",
      "iteration 78, dc_loss: 2.02192759513855, tv_loss: 0.011422797106206417\n",
      "iteration 79, dc_loss: 2.0131638050079346, tv_loss: 0.01150696724653244\n",
      "iteration 80, dc_loss: 2.00447678565979, tv_loss: 0.011568970046937466\n",
      "iteration 81, dc_loss: 1.9957480430603027, tv_loss: 0.011669335886836052\n",
      "iteration 82, dc_loss: 1.9870761632919312, tv_loss: 0.011732356622815132\n",
      "iteration 83, dc_loss: 1.9784709215164185, tv_loss: 0.011812778189778328\n",
      "iteration 84, dc_loss: 1.9699441194534302, tv_loss: 0.011905764229595661\n",
      "iteration 85, dc_loss: 1.9615230560302734, tv_loss: 0.011953780427575111\n",
      "iteration 86, dc_loss: 1.9530844688415527, tv_loss: 0.012084400281310081\n",
      "iteration 87, dc_loss: 1.9448516368865967, tv_loss: 0.01209881529211998\n",
      "iteration 88, dc_loss: 1.9366753101348877, tv_loss: 0.012265958823263645\n",
      "iteration 89, dc_loss: 1.9289432764053345, tv_loss: 0.012235775589942932\n",
      "iteration 90, dc_loss: 1.9210172891616821, tv_loss: 0.012437449768185616\n",
      "iteration 91, dc_loss: 1.9126032590866089, tv_loss: 0.012387348338961601\n",
      "iteration 92, dc_loss: 1.9035354852676392, tv_loss: 0.012510244734585285\n",
      "iteration 93, dc_loss: 1.8952784538269043, tv_loss: 0.012598011642694473\n",
      "iteration 94, dc_loss: 1.887870192527771, tv_loss: 0.012574722059071064\n",
      "iteration 95, dc_loss: 1.8796862363815308, tv_loss: 0.012776032090187073\n",
      "iteration 96, dc_loss: 1.8711609840393066, tv_loss: 0.012758389115333557\n",
      "iteration 97, dc_loss: 1.863264799118042, tv_loss: 0.012848075479269028\n",
      "iteration 98, dc_loss: 1.8556911945343018, tv_loss: 0.013017207384109497\n",
      "iteration 99, dc_loss: 1.8477120399475098, tv_loss: 0.01303821336477995\n",
      "iteration 100, dc_loss: 1.8395557403564453, tv_loss: 0.013121407479047775\n",
      "iteration 101, dc_loss: 1.8319810628890991, tv_loss: 0.013179175555706024\n",
      "iteration 102, dc_loss: 1.8244222402572632, tv_loss: 0.013217217288911343\n",
      "iteration 103, dc_loss: 1.8165253400802612, tv_loss: 0.01331736333668232\n",
      "iteration 104, dc_loss: 1.8088362216949463, tv_loss: 0.0134229501709342\n",
      "iteration 105, dc_loss: 1.801542043685913, tv_loss: 0.013406112790107727\n",
      "iteration 106, dc_loss: 1.793989658355713, tv_loss: 0.013585359789431095\n",
      "iteration 107, dc_loss: 1.786450982093811, tv_loss: 0.013544431887567043\n",
      "iteration 108, dc_loss: 1.7789920568466187, tv_loss: 0.013656467199325562\n",
      "iteration 109, dc_loss: 1.7716172933578491, tv_loss: 0.013698452152311802\n",
      "iteration 110, dc_loss: 1.7640010118484497, tv_loss: 0.013772054575383663\n",
      "iteration 111, dc_loss: 1.7563732862472534, tv_loss: 0.01378712523728609\n",
      "iteration 112, dc_loss: 1.7488960027694702, tv_loss: 0.013946637511253357\n",
      "iteration 113, dc_loss: 1.7417081594467163, tv_loss: 0.013983284123241901\n",
      "iteration 114, dc_loss: 1.7344584465026855, tv_loss: 0.014066719450056553\n",
      "iteration 115, dc_loss: 1.727284550666809, tv_loss: 0.01414538361132145\n",
      "iteration 116, dc_loss: 1.7203823328018188, tv_loss: 0.014096551574766636\n",
      "iteration 117, dc_loss: 1.7132868766784668, tv_loss: 0.014291960746049881\n",
      "iteration 118, dc_loss: 1.7061842679977417, tv_loss: 0.014236310496926308\n",
      "iteration 119, dc_loss: 1.6988359689712524, tv_loss: 0.014380580745637417\n",
      "iteration 120, dc_loss: 1.6916598081588745, tv_loss: 0.014445919543504715\n",
      "iteration 121, dc_loss: 1.6845752000808716, tv_loss: 0.014472504146397114\n",
      "iteration 122, dc_loss: 1.677569031715393, tv_loss: 0.014558385126292706\n",
      "iteration 123, dc_loss: 1.6708011627197266, tv_loss: 0.014567899517714977\n",
      "iteration 124, dc_loss: 1.6640238761901855, tv_loss: 0.014666017144918442\n",
      "iteration 125, dc_loss: 1.6571866273880005, tv_loss: 0.014673878438770771\n",
      "iteration 126, dc_loss: 1.6501402854919434, tv_loss: 0.014811795204877853\n",
      "iteration 127, dc_loss: 1.6432732343673706, tv_loss: 0.014825805090367794\n",
      "iteration 128, dc_loss: 1.636472463607788, tv_loss: 0.014921065419912338\n",
      "iteration 129, dc_loss: 1.6298017501831055, tv_loss: 0.01497166883200407\n",
      "iteration 130, dc_loss: 1.6231908798217773, tv_loss: 0.014985131099820137\n",
      "iteration 131, dc_loss: 1.6165649890899658, tv_loss: 0.015113763511180878\n",
      "iteration 132, dc_loss: 1.6101094484329224, tv_loss: 0.015068882144987583\n",
      "iteration 133, dc_loss: 1.6034311056137085, tv_loss: 0.015219349414110184\n",
      "iteration 134, dc_loss: 1.5967917442321777, tv_loss: 0.015196215361356735\n",
      "iteration 135, dc_loss: 1.5900801420211792, tv_loss: 0.01529223844408989\n",
      "iteration 136, dc_loss: 1.583535075187683, tv_loss: 0.015359483659267426\n",
      "iteration 137, dc_loss: 1.5771595239639282, tv_loss: 0.015364500693976879\n",
      "iteration 138, dc_loss: 1.5706864595413208, tv_loss: 0.015506413765251637\n",
      "iteration 139, dc_loss: 1.5644383430480957, tv_loss: 0.015457980334758759\n",
      "iteration 140, dc_loss: 1.5580569505691528, tv_loss: 0.015603385865688324\n",
      "iteration 141, dc_loss: 1.5518443584442139, tv_loss: 0.015561561100184917\n",
      "iteration 142, dc_loss: 1.5454453229904175, tv_loss: 0.015677206218242645\n",
      "iteration 143, dc_loss: 1.5391144752502441, tv_loss: 0.015673842281103134\n",
      "iteration 144, dc_loss: 1.5327153205871582, tv_loss: 0.01577039249241352\n",
      "iteration 145, dc_loss: 1.5264338254928589, tv_loss: 0.015814905986189842\n",
      "iteration 146, dc_loss: 1.5201963186264038, tv_loss: 0.0158787053078413\n",
      "iteration 147, dc_loss: 1.5140721797943115, tv_loss: 0.01594649627804756\n",
      "iteration 148, dc_loss: 1.5080676078796387, tv_loss: 0.015956968069076538\n",
      "iteration 149, dc_loss: 1.5020349025726318, tv_loss: 0.0160702895373106\n",
      "iteration 150, dc_loss: 1.4961568117141724, tv_loss: 0.01601698249578476\n",
      "iteration 151, dc_loss: 1.4900859594345093, tv_loss: 0.01622241549193859\n",
      "iteration 152, dc_loss: 1.484229564666748, tv_loss: 0.016065333038568497\n",
      "iteration 153, dc_loss: 1.4779647588729858, tv_loss: 0.01630750671029091\n",
      "iteration 154, dc_loss: 1.471893072128296, tv_loss: 0.016166090965270996\n",
      "iteration 155, dc_loss: 1.4656957387924194, tv_loss: 0.016324535012245178\n",
      "iteration 156, dc_loss: 1.459905982017517, tv_loss: 0.016376931220293045\n",
      "iteration 157, dc_loss: 1.4544010162353516, tv_loss: 0.01638026535511017\n",
      "iteration 158, dc_loss: 1.4486217498779297, tv_loss: 0.016601575538516045\n",
      "iteration 159, dc_loss: 1.4429203271865845, tv_loss: 0.016412461176514626\n",
      "iteration 160, dc_loss: 1.4367741346359253, tv_loss: 0.016622591763734818\n",
      "iteration 161, dc_loss: 1.4309974908828735, tv_loss: 0.01659870147705078\n",
      "iteration 162, dc_loss: 1.4254769086837769, tv_loss: 0.016604993492364883\n",
      "iteration 163, dc_loss: 1.4198423624038696, tv_loss: 0.016815487295389175\n",
      "iteration 164, dc_loss: 1.4141031503677368, tv_loss: 0.016642749309539795\n",
      "iteration 165, dc_loss: 1.4079630374908447, tv_loss: 0.016800977289676666\n",
      "iteration 166, dc_loss: 1.4021738767623901, tv_loss: 0.016833702102303505\n",
      "iteration 167, dc_loss: 1.396748661994934, tv_loss: 0.016859013587236404\n",
      "iteration 168, dc_loss: 1.391204833984375, tv_loss: 0.01707432046532631\n",
      "iteration 169, dc_loss: 1.3856761455535889, tv_loss: 0.016963955014944077\n",
      "iteration 170, dc_loss: 1.3799818754196167, tv_loss: 0.017025591805577278\n",
      "iteration 171, dc_loss: 1.374444603919983, tv_loss: 0.017094045877456665\n",
      "iteration 172, dc_loss: 1.3690592050552368, tv_loss: 0.017161916941404343\n",
      "iteration 173, dc_loss: 1.3637264966964722, tv_loss: 0.017192451283335686\n",
      "iteration 174, dc_loss: 1.3581691980361938, tv_loss: 0.01732293888926506\n",
      "iteration 175, dc_loss: 1.3529331684112549, tv_loss: 0.01716538518667221\n",
      "iteration 176, dc_loss: 1.3472473621368408, tv_loss: 0.017466191202402115\n",
      "iteration 177, dc_loss: 1.3419530391693115, tv_loss: 0.017301570624113083\n",
      "iteration 178, dc_loss: 1.3364142179489136, tv_loss: 0.017464477568864822\n",
      "iteration 179, dc_loss: 1.3311046361923218, tv_loss: 0.017476247623562813\n",
      "iteration 180, dc_loss: 1.3258955478668213, tv_loss: 0.01743529550731182\n",
      "iteration 181, dc_loss: 1.3204518556594849, tv_loss: 0.017597485333681107\n",
      "iteration 182, dc_loss: 1.3152347803115845, tv_loss: 0.01754855550825596\n",
      "iteration 183, dc_loss: 1.3099766969680786, tv_loss: 0.01765955239534378\n",
      "iteration 184, dc_loss: 1.3048434257507324, tv_loss: 0.017725616693496704\n",
      "iteration 185, dc_loss: 1.2998253107070923, tv_loss: 0.017713623121380806\n",
      "iteration 186, dc_loss: 1.2946444749832153, tv_loss: 0.01778450421988964\n",
      "iteration 187, dc_loss: 1.2894352674484253, tv_loss: 0.017853321507573128\n",
      "iteration 188, dc_loss: 1.2844130992889404, tv_loss: 0.017777306959033012\n",
      "iteration 189, dc_loss: 1.2792291641235352, tv_loss: 0.01800837181508541\n",
      "iteration 190, dc_loss: 1.2744545936584473, tv_loss: 0.01779673807322979\n",
      "iteration 191, dc_loss: 1.2691943645477295, tv_loss: 0.0180830005556345\n",
      "iteration 192, dc_loss: 1.2642096281051636, tv_loss: 0.017919505015015602\n",
      "iteration 193, dc_loss: 1.2589458227157593, tv_loss: 0.01809602975845337\n",
      "iteration 194, dc_loss: 1.253904104232788, tv_loss: 0.018138818442821503\n",
      "iteration 195, dc_loss: 1.248986005783081, tv_loss: 0.01813546195626259\n",
      "iteration 196, dc_loss: 1.2439604997634888, tv_loss: 0.018266232684254646\n",
      "iteration 197, dc_loss: 1.2392115592956543, tv_loss: 0.018180236220359802\n",
      "iteration 198, dc_loss: 1.2342329025268555, tv_loss: 0.018369516357779503\n",
      "iteration 199, dc_loss: 1.2295875549316406, tv_loss: 0.018253248184919357\n",
      "iteration 200, dc_loss: 1.2246339321136475, tv_loss: 0.018480857834219933\n",
      "iteration 201, dc_loss: 1.2201080322265625, tv_loss: 0.018271295353770256\n",
      "iteration 202, dc_loss: 1.2151570320129395, tv_loss: 0.018587272614240646\n",
      "iteration 203, dc_loss: 1.2104754447937012, tv_loss: 0.018340101465582848\n",
      "iteration 204, dc_loss: 1.2052654027938843, tv_loss: 0.01862766221165657\n",
      "iteration 205, dc_loss: 1.2004066705703735, tv_loss: 0.01852111518383026\n",
      "iteration 206, dc_loss: 1.1956145763397217, tv_loss: 0.018603196367621422\n",
      "iteration 207, dc_loss: 1.191036343574524, tv_loss: 0.018715640529990196\n",
      "iteration 208, dc_loss: 1.1866257190704346, tv_loss: 0.018604643642902374\n",
      "iteration 209, dc_loss: 1.1818013191223145, tv_loss: 0.0188087597489357\n",
      "iteration 210, dc_loss: 1.1770379543304443, tv_loss: 0.018707619979977608\n",
      "iteration 211, dc_loss: 1.172160267829895, tv_loss: 0.018786808475852013\n",
      "iteration 212, dc_loss: 1.1673980951309204, tv_loss: 0.01889842003583908\n",
      "iteration 213, dc_loss: 1.162993311882019, tv_loss: 0.018792346119880676\n",
      "iteration 214, dc_loss: 1.158333659172058, tv_loss: 0.01906454935669899\n",
      "iteration 215, dc_loss: 1.1540724039077759, tv_loss: 0.01884288340806961\n",
      "iteration 216, dc_loss: 1.1493518352508545, tv_loss: 0.01911001279950142\n",
      "iteration 217, dc_loss: 1.1448590755462646, tv_loss: 0.018962843343615532\n",
      "iteration 218, dc_loss: 1.1400279998779297, tv_loss: 0.019120212644338608\n",
      "iteration 219, dc_loss: 1.1355844736099243, tv_loss: 0.019080093130469322\n",
      "iteration 220, dc_loss: 1.1310938596725464, tv_loss: 0.01918044313788414\n",
      "iteration 221, dc_loss: 1.1267169713974, tv_loss: 0.019188718870282173\n",
      "iteration 222, dc_loss: 1.1221808195114136, tv_loss: 0.01921243779361248\n",
      "iteration 223, dc_loss: 1.1176177263259888, tv_loss: 0.019269920885562897\n",
      "iteration 224, dc_loss: 1.1132534742355347, tv_loss: 0.01923835277557373\n",
      "iteration 225, dc_loss: 1.1088237762451172, tv_loss: 0.019376149401068687\n",
      "iteration 226, dc_loss: 1.1045641899108887, tv_loss: 0.019318468868732452\n",
      "iteration 227, dc_loss: 1.1000324487686157, tv_loss: 0.019450703635811806\n",
      "iteration 228, dc_loss: 1.0956367254257202, tv_loss: 0.019418345764279366\n",
      "iteration 229, dc_loss: 1.0912892818450928, tv_loss: 0.019443226978182793\n",
      "iteration 230, dc_loss: 1.0869678258895874, tv_loss: 0.019568422809243202\n",
      "iteration 231, dc_loss: 1.0829375982284546, tv_loss: 0.01948818564414978\n",
      "iteration 232, dc_loss: 1.0785804986953735, tv_loss: 0.019712276756763458\n",
      "iteration 233, dc_loss: 1.0746556520462036, tv_loss: 0.019495446234941483\n",
      "iteration 234, dc_loss: 1.0702351331710815, tv_loss: 0.01981062814593315\n",
      "iteration 235, dc_loss: 1.066327691078186, tv_loss: 0.019549204036593437\n",
      "iteration 236, dc_loss: 1.0619728565216064, tv_loss: 0.01988503336906433\n",
      "iteration 237, dc_loss: 1.0581200122833252, tv_loss: 0.019607847556471825\n",
      "iteration 238, dc_loss: 1.053884744644165, tv_loss: 0.019922683015465736\n",
      "iteration 239, dc_loss: 1.0500324964523315, tv_loss: 0.019658641889691353\n",
      "iteration 240, dc_loss: 1.0456465482711792, tv_loss: 0.01992131397128105\n",
      "iteration 241, dc_loss: 1.0415654182434082, tv_loss: 0.019777053967118263\n",
      "iteration 242, dc_loss: 1.0373364686965942, tv_loss: 0.0198679082095623\n",
      "iteration 243, dc_loss: 1.033407211303711, tv_loss: 0.020016519352793694\n",
      "iteration 244, dc_loss: 1.029555082321167, tv_loss: 0.019828995689749718\n",
      "iteration 245, dc_loss: 1.0252794027328491, tv_loss: 0.020201368257403374\n",
      "iteration 246, dc_loss: 1.0211827754974365, tv_loss: 0.01988409273326397\n",
      "iteration 247, dc_loss: 1.0167226791381836, tv_loss: 0.020112786442041397\n",
      "iteration 248, dc_loss: 1.0128536224365234, tv_loss: 0.020106641575694084\n",
      "iteration 249, dc_loss: 1.009149193763733, tv_loss: 0.020095497369766235\n",
      "iteration 250, dc_loss: 1.005210280418396, tv_loss: 0.020321261137723923\n",
      "iteration 251, dc_loss: 1.0010673999786377, tv_loss: 0.02006869949400425\n",
      "iteration 252, dc_loss: 0.9967174530029297, tv_loss: 0.02030966989696026\n",
      "iteration 253, dc_loss: 0.9931604266166687, tv_loss: 0.020188435912132263\n",
      "iteration 254, dc_loss: 0.9891746640205383, tv_loss: 0.020380552858114243\n",
      "iteration 255, dc_loss: 0.9851621389389038, tv_loss: 0.020375775173306465\n",
      "iteration 256, dc_loss: 0.9810680150985718, tv_loss: 0.02033715695142746\n",
      "iteration 257, dc_loss: 0.9772002100944519, tv_loss: 0.020446691662073135\n",
      "iteration 258, dc_loss: 0.9736632704734802, tv_loss: 0.020332269370555878\n",
      "iteration 259, dc_loss: 0.9695236086845398, tv_loss: 0.020602775737643242\n",
      "iteration 260, dc_loss: 0.9657140970230103, tv_loss: 0.020419664680957794\n",
      "iteration 261, dc_loss: 0.9618518948554993, tv_loss: 0.0205619465559721\n",
      "iteration 262, dc_loss: 0.9581822752952576, tv_loss: 0.02056564949452877\n",
      "iteration 263, dc_loss: 0.9544469714164734, tv_loss: 0.020590607076883316\n",
      "iteration 264, dc_loss: 0.9505032300949097, tv_loss: 0.02074998803436756\n",
      "iteration 265, dc_loss: 0.9470304846763611, tv_loss: 0.020595449954271317\n",
      "iteration 266, dc_loss: 0.9432939291000366, tv_loss: 0.02083667740225792\n",
      "iteration 267, dc_loss: 0.9399418830871582, tv_loss: 0.020580563694238663\n",
      "iteration 268, dc_loss: 0.9361394047737122, tv_loss: 0.020949533209204674\n",
      "iteration 269, dc_loss: 0.932891845703125, tv_loss: 0.020659931004047394\n",
      "iteration 270, dc_loss: 0.9291087985038757, tv_loss: 0.02101716212928295\n",
      "iteration 271, dc_loss: 0.9255600571632385, tv_loss: 0.020720714703202248\n",
      "iteration 272, dc_loss: 0.9214401841163635, tv_loss: 0.021031267940998077\n",
      "iteration 273, dc_loss: 0.9177759289741516, tv_loss: 0.020902229472994804\n",
      "iteration 274, dc_loss: 0.9141497611999512, tv_loss: 0.020958933979272842\n",
      "iteration 275, dc_loss: 0.9106592535972595, tv_loss: 0.021045813336968422\n",
      "iteration 276, dc_loss: 0.9071837663650513, tv_loss: 0.020914226770401\n",
      "iteration 277, dc_loss: 0.9034729599952698, tv_loss: 0.02117510326206684\n",
      "iteration 278, dc_loss: 0.9002509117126465, tv_loss: 0.020975181832909584\n",
      "iteration 279, dc_loss: 0.8965209126472473, tv_loss: 0.02126873843371868\n",
      "iteration 280, dc_loss: 0.8930468559265137, tv_loss: 0.021094929426908493\n",
      "iteration 281, dc_loss: 0.8894026875495911, tv_loss: 0.02120727300643921\n",
      "iteration 282, dc_loss: 0.8858881592750549, tv_loss: 0.02123899571597576\n",
      "iteration 283, dc_loss: 0.8825388550758362, tv_loss: 0.021208373829722404\n",
      "iteration 284, dc_loss: 0.8790128827095032, tv_loss: 0.021383950486779213\n",
      "iteration 285, dc_loss: 0.8757839202880859, tv_loss: 0.021249152719974518\n",
      "iteration 286, dc_loss: 0.872248113155365, tv_loss: 0.02146000787615776\n",
      "iteration 287, dc_loss: 0.8691940307617188, tv_loss: 0.021258540451526642\n",
      "iteration 288, dc_loss: 0.8657863736152649, tv_loss: 0.021580899134278297\n",
      "iteration 289, dc_loss: 0.8627762198448181, tv_loss: 0.021286342293024063\n",
      "iteration 290, dc_loss: 0.8591707944869995, tv_loss: 0.021683773025870323\n",
      "iteration 291, dc_loss: 0.8561678528785706, tv_loss: 0.021295446902513504\n",
      "iteration 292, dc_loss: 0.8525043725967407, tv_loss: 0.021730341017246246\n",
      "iteration 293, dc_loss: 0.8492022752761841, tv_loss: 0.021403035148978233\n",
      "iteration 294, dc_loss: 0.8454422354698181, tv_loss: 0.021699240431189537\n",
      "iteration 295, dc_loss: 0.8422096967697144, tv_loss: 0.021661950275301933\n",
      "iteration 296, dc_loss: 0.8390628695487976, tv_loss: 0.021586239337921143\n",
      "iteration 297, dc_loss: 0.8357544541358948, tv_loss: 0.021837268024683\n",
      "iteration 298, dc_loss: 0.8330131769180298, tv_loss: 0.021544795483350754\n",
      "iteration 299, dc_loss: 0.8296746015548706, tv_loss: 0.02196982130408287\n",
      "iteration 300, dc_loss: 0.8267547488212585, tv_loss: 0.02162344940006733\n",
      "iteration 301, dc_loss: 0.8233773112297058, tv_loss: 0.021951623260974884\n",
      "iteration 302, dc_loss: 0.820175290107727, tv_loss: 0.021760055795311928\n",
      "iteration 303, dc_loss: 0.8166463375091553, tv_loss: 0.021930484101176262\n",
      "iteration 304, dc_loss: 0.8132758140563965, tv_loss: 0.021923212334513664\n",
      "iteration 305, dc_loss: 0.8102344274520874, tv_loss: 0.02189113385975361\n",
      "iteration 306, dc_loss: 0.8071534037590027, tv_loss: 0.022089924663305283\n",
      "iteration 307, dc_loss: 0.8044774532318115, tv_loss: 0.021857041865587234\n",
      "iteration 308, dc_loss: 0.8013194799423218, tv_loss: 0.022246049717068672\n",
      "iteration 309, dc_loss: 0.7984391450881958, tv_loss: 0.021861758083105087\n",
      "iteration 310, dc_loss: 0.7949236035346985, tv_loss: 0.0222945399582386\n",
      "iteration 311, dc_loss: 0.7919192910194397, tv_loss: 0.02196686528623104\n",
      "iteration 312, dc_loss: 0.7884936332702637, tv_loss: 0.022198054939508438\n",
      "iteration 313, dc_loss: 0.7853638529777527, tv_loss: 0.022195708006620407\n",
      "iteration 314, dc_loss: 0.7824749946594238, tv_loss: 0.022122494876384735\n",
      "iteration 315, dc_loss: 0.7794017195701599, tv_loss: 0.02236281894147396\n",
      "iteration 316, dc_loss: 0.7767702341079712, tv_loss: 0.022079341113567352\n",
      "iteration 317, dc_loss: 0.7736494541168213, tv_loss: 0.02246195264160633\n",
      "iteration 318, dc_loss: 0.7706935405731201, tv_loss: 0.0221202801913023\n",
      "iteration 319, dc_loss: 0.767342746257782, tv_loss: 0.02240651845932007\n",
      "iteration 320, dc_loss: 0.7641868591308594, tv_loss: 0.022342834621667862\n",
      "iteration 321, dc_loss: 0.7613245248794556, tv_loss: 0.022358587011694908\n",
      "iteration 322, dc_loss: 0.7582551836967468, tv_loss: 0.02255033515393734\n",
      "iteration 323, dc_loss: 0.7559319734573364, tv_loss: 0.022258754819631577\n",
      "iteration 324, dc_loss: 0.7532669305801392, tv_loss: 0.02274165488779545\n",
      "iteration 325, dc_loss: 0.7509987950325012, tv_loss: 0.022321568801999092\n",
      "iteration 326, dc_loss: 0.7474086880683899, tv_loss: 0.02269362471997738\n",
      "iteration 327, dc_loss: 0.743995726108551, tv_loss: 0.022383634001016617\n",
      "iteration 328, dc_loss: 0.74085932970047, tv_loss: 0.022599415853619576\n",
      "iteration 329, dc_loss: 0.7382611036300659, tv_loss: 0.022654356434941292\n",
      "iteration 330, dc_loss: 0.7355563044548035, tv_loss: 0.02250712364912033\n",
      "iteration 331, dc_loss: 0.7322065830230713, tv_loss: 0.022708777338266373\n",
      "iteration 332, dc_loss: 0.7294002771377563, tv_loss: 0.022544868290424347\n",
      "iteration 333, dc_loss: 0.7263867855072021, tv_loss: 0.022766847163438797\n",
      "iteration 334, dc_loss: 0.7234390377998352, tv_loss: 0.022764327004551888\n",
      "iteration 335, dc_loss: 0.7206114530563354, tv_loss: 0.02277022786438465\n",
      "iteration 336, dc_loss: 0.7178122997283936, tv_loss: 0.02284143678843975\n",
      "iteration 337, dc_loss: 0.7151510715484619, tv_loss: 0.022675255313515663\n",
      "iteration 338, dc_loss: 0.7120075821876526, tv_loss: 0.02286490425467491\n",
      "iteration 339, dc_loss: 0.7090892195701599, tv_loss: 0.022878844290971756\n",
      "iteration 340, dc_loss: 0.7063602209091187, tv_loss: 0.022913746535778046\n",
      "iteration 341, dc_loss: 0.7036901116371155, tv_loss: 0.022990722209215164\n",
      "iteration 342, dc_loss: 0.7010469436645508, tv_loss: 0.022896889597177505\n",
      "iteration 343, dc_loss: 0.6981642842292786, tv_loss: 0.022997455671429634\n",
      "iteration 344, dc_loss: 0.695361852645874, tv_loss: 0.023038510233163834\n",
      "iteration 345, dc_loss: 0.6928558945655823, tv_loss: 0.022969800978899002\n",
      "iteration 346, dc_loss: 0.6901815533638, tv_loss: 0.023215917870402336\n",
      "iteration 347, dc_loss: 0.6881518363952637, tv_loss: 0.02287323959171772\n",
      "iteration 348, dc_loss: 0.6859006285667419, tv_loss: 0.023392723873257637\n",
      "iteration 349, dc_loss: 0.6844744682312012, tv_loss: 0.022826623171567917\n",
      "iteration 350, dc_loss: 0.6831427812576294, tv_loss: 0.02358117513358593\n",
      "iteration 351, dc_loss: 0.6795827150344849, tv_loss: 0.022841745987534523\n",
      "iteration 352, dc_loss: 0.6753624081611633, tv_loss: 0.02332795411348343\n",
      "iteration 353, dc_loss: 0.6722946763038635, tv_loss: 0.02320433035492897\n",
      "iteration 354, dc_loss: 0.6707560420036316, tv_loss: 0.022999728098511696\n",
      "iteration 355, dc_loss: 0.6685587763786316, tv_loss: 0.023612894117832184\n",
      "iteration 356, dc_loss: 0.6650803089141846, tv_loss: 0.023270418867468834\n",
      "iteration 357, dc_loss: 0.6626523733139038, tv_loss: 0.023355143144726753\n",
      "iteration 358, dc_loss: 0.6602535247802734, tv_loss: 0.023509308695793152\n",
      "iteration 359, dc_loss: 0.6572660803794861, tv_loss: 0.023159759119153023\n",
      "iteration 360, dc_loss: 0.6548042297363281, tv_loss: 0.023561468347907066\n",
      "iteration 361, dc_loss: 0.652418315410614, tv_loss: 0.02345944568514824\n",
      "iteration 362, dc_loss: 0.6494928002357483, tv_loss: 0.023440824821591377\n",
      "iteration 363, dc_loss: 0.6469306349754333, tv_loss: 0.023686207830905914\n",
      "iteration 364, dc_loss: 0.6448554396629333, tv_loss: 0.023443272337317467\n",
      "iteration 365, dc_loss: 0.6420334577560425, tv_loss: 0.023530030623078346\n",
      "iteration 366, dc_loss: 0.6393674612045288, tv_loss: 0.023687412962317467\n",
      "iteration 367, dc_loss: 0.6370289325714111, tv_loss: 0.023652518168091774\n",
      "iteration 368, dc_loss: 0.6346243619918823, tv_loss: 0.023617984727025032\n",
      "iteration 369, dc_loss: 0.6320062875747681, tv_loss: 0.0236834567040205\n",
      "iteration 370, dc_loss: 0.6295216083526611, tv_loss: 0.023717543110251427\n",
      "iteration 371, dc_loss: 0.6273518800735474, tv_loss: 0.023693544790148735\n",
      "iteration 372, dc_loss: 0.6246564984321594, tv_loss: 0.02383062057197094\n",
      "iteration 373, dc_loss: 0.6222525835037231, tv_loss: 0.023765966296195984\n",
      "iteration 374, dc_loss: 0.6200088858604431, tv_loss: 0.023745786398649216\n",
      "iteration 375, dc_loss: 0.6174132823944092, tv_loss: 0.023866308853030205\n",
      "iteration 376, dc_loss: 0.615190327167511, tv_loss: 0.023835381492972374\n",
      "iteration 377, dc_loss: 0.6127755045890808, tv_loss: 0.023921430110931396\n",
      "iteration 378, dc_loss: 0.6103321313858032, tv_loss: 0.0238726157695055\n",
      "iteration 379, dc_loss: 0.6080988049507141, tv_loss: 0.023887353017926216\n",
      "iteration 380, dc_loss: 0.6057027578353882, tv_loss: 0.02399822138249874\n",
      "iteration 381, dc_loss: 0.6033097505569458, tv_loss: 0.023988332599401474\n",
      "iteration 382, dc_loss: 0.6010500192642212, tv_loss: 0.024005630984902382\n",
      "iteration 383, dc_loss: 0.5988249182701111, tv_loss: 0.023977002128958702\n",
      "iteration 384, dc_loss: 0.5964510440826416, tv_loss: 0.02407977543771267\n",
      "iteration 385, dc_loss: 0.594415009021759, tv_loss: 0.023999741300940514\n",
      "iteration 386, dc_loss: 0.5921684503555298, tv_loss: 0.0242209043353796\n",
      "iteration 387, dc_loss: 0.5903549790382385, tv_loss: 0.02397458255290985\n",
      "iteration 388, dc_loss: 0.5879257321357727, tv_loss: 0.02432679757475853\n",
      "iteration 389, dc_loss: 0.585700273513794, tv_loss: 0.023991841822862625\n",
      "iteration 390, dc_loss: 0.5829538702964783, tv_loss: 0.02431895211338997\n",
      "iteration 391, dc_loss: 0.580948531627655, tv_loss: 0.024119427427649498\n",
      "iteration 392, dc_loss: 0.578997790813446, tv_loss: 0.024275358766317368\n",
      "iteration 393, dc_loss: 0.5770914554595947, tv_loss: 0.024154918268322945\n",
      "iteration 394, dc_loss: 0.574722409248352, tv_loss: 0.024507369846105576\n",
      "iteration 395, dc_loss: 0.5727521777153015, tv_loss: 0.024078916758298874\n",
      "iteration 396, dc_loss: 0.5704064965248108, tv_loss: 0.02457934059202671\n",
      "iteration 397, dc_loss: 0.5686977505683899, tv_loss: 0.0240677110850811\n",
      "iteration 398, dc_loss: 0.5662026405334473, tv_loss: 0.024543587118387222\n",
      "iteration 399, dc_loss: 0.5639340877532959, tv_loss: 0.024199748411774635\n",
      "iteration 400, dc_loss: 0.5615569353103638, tv_loss: 0.024449586868286133\n",
      "iteration 401, dc_loss: 0.5594390034675598, tv_loss: 0.02443559095263481\n",
      "iteration 402, dc_loss: 0.5573612451553345, tv_loss: 0.024421194568276405\n",
      "iteration 403, dc_loss: 0.5557562708854675, tv_loss: 0.024401487782597542\n",
      "iteration 404, dc_loss: 0.5540205240249634, tv_loss: 0.02437683939933777\n",
      "iteration 405, dc_loss: 0.5520751476287842, tv_loss: 0.024484261870384216\n",
      "iteration 406, dc_loss: 0.5505361557006836, tv_loss: 0.024508658796548843\n",
      "iteration 407, dc_loss: 0.5487973093986511, tv_loss: 0.02448539435863495\n",
      "iteration 408, dc_loss: 0.5470460057258606, tv_loss: 0.02455105446279049\n",
      "iteration 409, dc_loss: 0.5453599095344543, tv_loss: 0.024530494585633278\n",
      "iteration 410, dc_loss: 0.5436526536941528, tv_loss: 0.024482574313879013\n",
      "iteration 411, dc_loss: 0.5419387221336365, tv_loss: 0.02457248792052269\n",
      "iteration 412, dc_loss: 0.5402711629867554, tv_loss: 0.024575263261795044\n",
      "iteration 413, dc_loss: 0.5386322140693665, tv_loss: 0.024606553837656975\n",
      "iteration 414, dc_loss: 0.5369148254394531, tv_loss: 0.024716142565011978\n",
      "iteration 415, dc_loss: 0.5352442860603333, tv_loss: 0.024642979726195335\n",
      "iteration 416, dc_loss: 0.5336154699325562, tv_loss: 0.024628274142742157\n",
      "iteration 417, dc_loss: 0.531910240650177, tv_loss: 0.024694764986634254\n",
      "iteration 418, dc_loss: 0.530272364616394, tv_loss: 0.024755343794822693\n",
      "iteration 419, dc_loss: 0.5286591649055481, tv_loss: 0.02474386617541313\n",
      "iteration 420, dc_loss: 0.5269725918769836, tv_loss: 0.024792661890387535\n",
      "iteration 421, dc_loss: 0.5253309607505798, tv_loss: 0.024751782417297363\n",
      "iteration 422, dc_loss: 0.5237610340118408, tv_loss: 0.02473381720483303\n",
      "iteration 423, dc_loss: 0.5220569372177124, tv_loss: 0.024897290393710136\n",
      "iteration 424, dc_loss: 0.5204954147338867, tv_loss: 0.024845771491527557\n",
      "iteration 425, dc_loss: 0.5188683867454529, tv_loss: 0.024827968329191208\n",
      "iteration 426, dc_loss: 0.5172113180160522, tv_loss: 0.024923821911215782\n",
      "iteration 427, dc_loss: 0.515729546546936, tv_loss: 0.024813393130898476\n",
      "iteration 428, dc_loss: 0.5140496492385864, tv_loss: 0.024946123361587524\n",
      "iteration 429, dc_loss: 0.5125876665115356, tv_loss: 0.02495153620839119\n",
      "iteration 430, dc_loss: 0.5111632943153381, tv_loss: 0.024994244799017906\n",
      "iteration 431, dc_loss: 0.5098584294319153, tv_loss: 0.024876978248357773\n",
      "iteration 432, dc_loss: 0.5087711215019226, tv_loss: 0.025126637890934944\n",
      "iteration 433, dc_loss: 0.5071781873703003, tv_loss: 0.02478197030723095\n",
      "iteration 434, dc_loss: 0.5053675770759583, tv_loss: 0.025225616991519928\n",
      "iteration 435, dc_loss: 0.5036640167236328, tv_loss: 0.024934150278568268\n",
      "iteration 436, dc_loss: 0.5019382834434509, tv_loss: 0.02501324750483036\n",
      "iteration 437, dc_loss: 0.5007249116897583, tv_loss: 0.025315668433904648\n",
      "iteration 438, dc_loss: 0.4999065101146698, tv_loss: 0.024813691154122353\n",
      "iteration 439, dc_loss: 0.49821731448173523, tv_loss: 0.025353925302624702\n",
      "iteration 440, dc_loss: 0.4961964190006256, tv_loss: 0.025069693103432655\n",
      "iteration 441, dc_loss: 0.494734525680542, tv_loss: 0.025059925392270088\n",
      "iteration 442, dc_loss: 0.49284934997558594, tv_loss: 0.025326775386929512\n",
      "iteration 443, dc_loss: 0.49167293310165405, tv_loss: 0.025087980553507805\n",
      "iteration 444, dc_loss: 0.49069759249687195, tv_loss: 0.025286270305514336\n",
      "iteration 445, dc_loss: 0.4885529577732086, tv_loss: 0.025169270113110542\n",
      "iteration 446, dc_loss: 0.48679840564727783, tv_loss: 0.025248944759368896\n",
      "iteration 447, dc_loss: 0.48555174469947815, tv_loss: 0.02534548193216324\n",
      "iteration 448, dc_loss: 0.48435524106025696, tv_loss: 0.02522108517587185\n",
      "iteration 449, dc_loss: 0.48272162675857544, tv_loss: 0.02537410520017147\n",
      "iteration 450, dc_loss: 0.4812009334564209, tv_loss: 0.025196988135576248\n",
      "iteration 451, dc_loss: 0.47959044575691223, tv_loss: 0.0254212636500597\n",
      "iteration 452, dc_loss: 0.4781797528266907, tv_loss: 0.025341464206576347\n",
      "iteration 453, dc_loss: 0.47698870301246643, tv_loss: 0.02530580386519432\n",
      "iteration 454, dc_loss: 0.4753613770008087, tv_loss: 0.025608714669942856\n",
      "iteration 455, dc_loss: 0.4740404784679413, tv_loss: 0.025207890197634697\n",
      "iteration 456, dc_loss: 0.47224748134613037, tv_loss: 0.025404030457139015\n",
      "iteration 457, dc_loss: 0.47078263759613037, tv_loss: 0.02556648477911949\n",
      "iteration 458, dc_loss: 0.46973878145217896, tv_loss: 0.02537386864423752\n",
      "iteration 459, dc_loss: 0.46812334656715393, tv_loss: 0.025582650676369667\n",
      "iteration 460, dc_loss: 0.46678927540779114, tv_loss: 0.025434298440814018\n",
      "iteration 461, dc_loss: 0.4654828906059265, tv_loss: 0.02548847533762455\n",
      "iteration 462, dc_loss: 0.4639185667037964, tv_loss: 0.02557677961885929\n",
      "iteration 463, dc_loss: 0.46243059635162354, tv_loss: 0.02557685226202011\n",
      "iteration 464, dc_loss: 0.46123206615448, tv_loss: 0.025554358959197998\n",
      "iteration 465, dc_loss: 0.45993855595588684, tv_loss: 0.02560282126069069\n",
      "iteration 466, dc_loss: 0.45844918489456177, tv_loss: 0.025570891797542572\n",
      "iteration 467, dc_loss: 0.45712605118751526, tv_loss: 0.025571826845407486\n",
      "iteration 468, dc_loss: 0.45574527978897095, tv_loss: 0.02582414820790291\n",
      "iteration 469, dc_loss: 0.4546239376068115, tv_loss: 0.025534771382808685\n",
      "iteration 470, dc_loss: 0.4531856179237366, tv_loss: 0.025621432811021805\n",
      "iteration 471, dc_loss: 0.45165151357650757, tv_loss: 0.025837456807494164\n",
      "iteration 472, dc_loss: 0.45068901777267456, tv_loss: 0.02556915208697319\n",
      "iteration 473, dc_loss: 0.4492710828781128, tv_loss: 0.025817297399044037\n",
      "iteration 474, dc_loss: 0.4479994475841522, tv_loss: 0.025780782103538513\n",
      "iteration 475, dc_loss: 0.44685661792755127, tv_loss: 0.025662101805210114\n",
      "iteration 476, dc_loss: 0.44506752490997314, tv_loss: 0.02578703872859478\n",
      "iteration 477, dc_loss: 0.4437832534313202, tv_loss: 0.025870302692055702\n",
      "iteration 478, dc_loss: 0.4429750144481659, tv_loss: 0.025775684043765068\n",
      "iteration 479, dc_loss: 0.44137948751449585, tv_loss: 0.025821562856435776\n",
      "iteration 480, dc_loss: 0.43979352712631226, tv_loss: 0.02583417296409607\n",
      "iteration 481, dc_loss: 0.43884438276290894, tv_loss: 0.025842927396297455\n",
      "iteration 482, dc_loss: 0.43762609362602234, tv_loss: 0.02598915994167328\n",
      "iteration 483, dc_loss: 0.4361315071582794, tv_loss: 0.025848383083939552\n",
      "iteration 484, dc_loss: 0.4348294138908386, tv_loss: 0.02583857998251915\n",
      "iteration 485, dc_loss: 0.4336276054382324, tv_loss: 0.02600284479558468\n",
      "iteration 486, dc_loss: 0.432583212852478, tv_loss: 0.025877758860588074\n",
      "iteration 487, dc_loss: 0.43096354603767395, tv_loss: 0.02600720338523388\n",
      "iteration 488, dc_loss: 0.42986980080604553, tv_loss: 0.026024233549833298\n",
      "iteration 489, dc_loss: 0.428681880235672, tv_loss: 0.025992415845394135\n",
      "iteration 490, dc_loss: 0.42728742957115173, tv_loss: 0.02593926154077053\n",
      "iteration 491, dc_loss: 0.42596715688705444, tv_loss: 0.02611985057592392\n",
      "iteration 492, dc_loss: 0.42474570870399475, tv_loss: 0.026016144081950188\n",
      "iteration 493, dc_loss: 0.4235096275806427, tv_loss: 0.026059946045279503\n",
      "iteration 494, dc_loss: 0.4221755266189575, tv_loss: 0.02610187791287899\n",
      "iteration 495, dc_loss: 0.4210221767425537, tv_loss: 0.026096977293491364\n",
      "iteration 496, dc_loss: 0.4197004437446594, tv_loss: 0.0261483546346426\n",
      "iteration 497, dc_loss: 0.41848114132881165, tv_loss: 0.026110000908374786\n",
      "iteration 498, dc_loss: 0.4173336923122406, tv_loss: 0.026137065142393112\n",
      "iteration 499, dc_loss: 0.4159848093986511, tv_loss: 0.026177993044257164\n",
      "iteration 500, dc_loss: 0.41482993960380554, tv_loss: 0.0261547788977623\n",
      "iteration 501, dc_loss: 0.4136413335800171, tv_loss: 0.02621578797698021\n",
      "iteration 502, dc_loss: 0.41241735219955444, tv_loss: 0.02626957558095455\n",
      "iteration 503, dc_loss: 0.41119110584259033, tv_loss: 0.0261943768709898\n",
      "iteration 504, dc_loss: 0.40999478101730347, tv_loss: 0.026196632534265518\n",
      "iteration 505, dc_loss: 0.40881070494651794, tv_loss: 0.026331176981329918\n",
      "iteration 506, dc_loss: 0.4077085852622986, tv_loss: 0.026213111355900764\n",
      "iteration 507, dc_loss: 0.40642791986465454, tv_loss: 0.02636166475713253\n",
      "iteration 508, dc_loss: 0.4055066406726837, tv_loss: 0.026248132809996605\n",
      "iteration 509, dc_loss: 0.40424612164497375, tv_loss: 0.026440707966685295\n",
      "iteration 510, dc_loss: 0.4035053849220276, tv_loss: 0.02617906779050827\n",
      "iteration 511, dc_loss: 0.4023963212966919, tv_loss: 0.026594622060656548\n",
      "iteration 512, dc_loss: 0.40203526616096497, tv_loss: 0.026099154725670815\n",
      "iteration 513, dc_loss: 0.4009826183319092, tv_loss: 0.026675837114453316\n",
      "iteration 514, dc_loss: 0.399882435798645, tv_loss: 0.026063749566674232\n",
      "iteration 515, dc_loss: 0.3978135287761688, tv_loss: 0.02655290625989437\n",
      "iteration 516, dc_loss: 0.3963438868522644, tv_loss: 0.026280602440238\n",
      "iteration 517, dc_loss: 0.39519065618515015, tv_loss: 0.02634504809975624\n",
      "iteration 518, dc_loss: 0.3941737115383148, tv_loss: 0.0266144797205925\n",
      "iteration 519, dc_loss: 0.3936721980571747, tv_loss: 0.026394840329885483\n",
      "iteration 520, dc_loss: 0.3922503590583801, tv_loss: 0.02672063745558262\n",
      "iteration 521, dc_loss: 0.3911909759044647, tv_loss: 0.02629079669713974\n",
      "iteration 522, dc_loss: 0.3897171914577484, tv_loss: 0.026491042226552963\n",
      "iteration 523, dc_loss: 0.38863202929496765, tv_loss: 0.0266355462372303\n",
      "iteration 524, dc_loss: 0.3878951668739319, tv_loss: 0.026482634246349335\n",
      "iteration 525, dc_loss: 0.3863832354545593, tv_loss: 0.026720987632870674\n",
      "iteration 526, dc_loss: 0.3854460120201111, tv_loss: 0.02649090439081192\n",
      "iteration 527, dc_loss: 0.3844231963157654, tv_loss: 0.02655130997300148\n",
      "iteration 528, dc_loss: 0.3831233084201813, tv_loss: 0.02666938304901123\n",
      "iteration 529, dc_loss: 0.38221147656440735, tv_loss: 0.026630694046616554\n",
      "iteration 530, dc_loss: 0.3809702396392822, tv_loss: 0.026793764904141426\n",
      "iteration 531, dc_loss: 0.380215048789978, tv_loss: 0.02660899981856346\n",
      "iteration 532, dc_loss: 0.37953296303749084, tv_loss: 0.026713307946920395\n",
      "iteration 533, dc_loss: 0.3782319128513336, tv_loss: 0.026677383109927177\n",
      "iteration 534, dc_loss: 0.377165824174881, tv_loss: 0.026775602251291275\n",
      "iteration 535, dc_loss: 0.3760702610015869, tv_loss: 0.0267926137894392\n",
      "iteration 536, dc_loss: 0.37494492530822754, tv_loss: 0.026647470891475677\n",
      "iteration 537, dc_loss: 0.3736371099948883, tv_loss: 0.026793407276272774\n",
      "iteration 538, dc_loss: 0.3726887106895447, tv_loss: 0.026754392310976982\n",
      "iteration 539, dc_loss: 0.37164393067359924, tv_loss: 0.026880452409386635\n",
      "iteration 540, dc_loss: 0.37044665217399597, tv_loss: 0.026872264221310616\n",
      "iteration 541, dc_loss: 0.3694608509540558, tv_loss: 0.026729673147201538\n",
      "iteration 542, dc_loss: 0.36840882897377014, tv_loss: 0.02688797563314438\n",
      "iteration 543, dc_loss: 0.36746782064437866, tv_loss: 0.026825198903679848\n",
      "iteration 544, dc_loss: 0.3662552237510681, tv_loss: 0.02700350061058998\n",
      "iteration 545, dc_loss: 0.3652665615081787, tv_loss: 0.026838399469852448\n",
      "iteration 546, dc_loss: 0.3642449975013733, tv_loss: 0.026861025020480156\n",
      "iteration 547, dc_loss: 0.3632308542728424, tv_loss: 0.026854123920202255\n",
      "iteration 548, dc_loss: 0.3621310889720917, tv_loss: 0.02695683017373085\n",
      "iteration 549, dc_loss: 0.36111772060394287, tv_loss: 0.027092916890978813\n",
      "iteration 550, dc_loss: 0.36027243733406067, tv_loss: 0.026870297268033028\n",
      "iteration 551, dc_loss: 0.359116792678833, tv_loss: 0.027009539306163788\n",
      "iteration 552, dc_loss: 0.35822391510009766, tv_loss: 0.026950618252158165\n",
      "iteration 553, dc_loss: 0.3571333587169647, tv_loss: 0.027175256982445717\n",
      "iteration 554, dc_loss: 0.3561722934246063, tv_loss: 0.02698471024632454\n",
      "iteration 555, dc_loss: 0.35515859723091125, tv_loss: 0.027034439146518707\n",
      "iteration 556, dc_loss: 0.3542289137840271, tv_loss: 0.027184708043932915\n",
      "iteration 557, dc_loss: 0.3531634509563446, tv_loss: 0.027081839740276337\n",
      "iteration 558, dc_loss: 0.3522111177444458, tv_loss: 0.02703924849629402\n",
      "iteration 559, dc_loss: 0.35124480724334717, tv_loss: 0.02717350609600544\n",
      "iteration 560, dc_loss: 0.3503095507621765, tv_loss: 0.027175139635801315\n",
      "iteration 561, dc_loss: 0.3493741452693939, tv_loss: 0.02707969769835472\n",
      "iteration 562, dc_loss: 0.3483388423919678, tv_loss: 0.027124697342514992\n",
      "iteration 563, dc_loss: 0.3474823534488678, tv_loss: 0.027166780084371567\n",
      "iteration 564, dc_loss: 0.3464537262916565, tv_loss: 0.027308296412229538\n",
      "iteration 565, dc_loss: 0.3456484377384186, tv_loss: 0.02710050716996193\n",
      "iteration 566, dc_loss: 0.34464070200920105, tv_loss: 0.027258917689323425\n",
      "iteration 567, dc_loss: 0.34401801228523254, tv_loss: 0.02720453590154648\n",
      "iteration 568, dc_loss: 0.3430345952510834, tv_loss: 0.027454394847154617\n",
      "iteration 569, dc_loss: 0.3427124321460724, tv_loss: 0.02696683257818222\n",
      "iteration 570, dc_loss: 0.341776043176651, tv_loss: 0.02754131704568863\n",
      "iteration 571, dc_loss: 0.34162190556526184, tv_loss: 0.02697799913585186\n",
      "iteration 572, dc_loss: 0.3404136002063751, tv_loss: 0.027654819190502167\n",
      "iteration 573, dc_loss: 0.3392044007778168, tv_loss: 0.027047857642173767\n",
      "iteration 574, dc_loss: 0.33730626106262207, tv_loss: 0.02738766185939312\n",
      "iteration 575, dc_loss: 0.3363199830055237, tv_loss: 0.02737528458237648\n",
      "iteration 576, dc_loss: 0.3359260857105255, tv_loss: 0.02734256722033024\n",
      "iteration 577, dc_loss: 0.3349936604499817, tv_loss: 0.02753172628581524\n",
      "iteration 578, dc_loss: 0.33441346883773804, tv_loss: 0.02721365913748741\n",
      "iteration 579, dc_loss: 0.3330906629562378, tv_loss: 0.027748936787247658\n",
      "iteration 580, dc_loss: 0.33213385939598083, tv_loss: 0.027255438268184662\n",
      "iteration 581, dc_loss: 0.3311563730239868, tv_loss: 0.027495624497532845\n",
      "iteration 582, dc_loss: 0.3303031325340271, tv_loss: 0.027738995850086212\n",
      "iteration 583, dc_loss: 0.32991451025009155, tv_loss: 0.02724388614296913\n",
      "iteration 584, dc_loss: 0.3286750018596649, tv_loss: 0.02786232717335224\n",
      "iteration 585, dc_loss: 0.32764583826065063, tv_loss: 0.027487091720104218\n",
      "iteration 586, dc_loss: 0.32688087224960327, tv_loss: 0.027589675039052963\n",
      "iteration 587, dc_loss: 0.32590022683143616, tv_loss: 0.02762719802558422\n",
      "iteration 588, dc_loss: 0.32531291246414185, tv_loss: 0.027493786066770554\n",
      "iteration 589, dc_loss: 0.32433491945266724, tv_loss: 0.02794484980404377\n",
      "iteration 590, dc_loss: 0.3235139846801758, tv_loss: 0.02754129283130169\n",
      "iteration 591, dc_loss: 0.3226240277290344, tv_loss: 0.028034457936882973\n",
      "iteration 592, dc_loss: 0.3216607868671417, tv_loss: 0.027711614966392517\n",
      "iteration 593, dc_loss: 0.3210507035255432, tv_loss: 0.027897652238607407\n",
      "iteration 594, dc_loss: 0.3198989927768707, tv_loss: 0.027940580621361732\n",
      "iteration 595, dc_loss: 0.31941965222358704, tv_loss: 0.027849823236465454\n",
      "iteration 596, dc_loss: 0.3183761239051819, tv_loss: 0.02788774110376835\n",
      "iteration 597, dc_loss: 0.3178788721561432, tv_loss: 0.027825944125652313\n",
      "iteration 598, dc_loss: 0.31679192185401917, tv_loss: 0.027884406968951225\n",
      "iteration 599, dc_loss: 0.3159521222114563, tv_loss: 0.028045691549777985\n",
      "iteration 600, dc_loss: 0.31511837244033813, tv_loss: 0.02782599627971649\n",
      "iteration 601, dc_loss: 0.3142869472503662, tv_loss: 0.028010189533233643\n",
      "iteration 602, dc_loss: 0.3135391175746918, tv_loss: 0.02788301184773445\n",
      "iteration 603, dc_loss: 0.312786728143692, tv_loss: 0.028010640293359756\n",
      "iteration 604, dc_loss: 0.3119175136089325, tv_loss: 0.027834340929985046\n",
      "iteration 605, dc_loss: 0.31099629402160645, tv_loss: 0.028127970173954964\n",
      "iteration 606, dc_loss: 0.31030046939849854, tv_loss: 0.027881938964128494\n",
      "iteration 607, dc_loss: 0.3093760907649994, tv_loss: 0.02807587757706642\n",
      "iteration 608, dc_loss: 0.3086232841014862, tv_loss: 0.027897251769900322\n",
      "iteration 609, dc_loss: 0.3077886402606964, tv_loss: 0.028027448803186417\n",
      "iteration 610, dc_loss: 0.3069293200969696, tv_loss: 0.0279841385781765\n",
      "iteration 611, dc_loss: 0.30623993277549744, tv_loss: 0.028027644380927086\n",
      "iteration 612, dc_loss: 0.30535176396369934, tv_loss: 0.028032401576638222\n",
      "iteration 613, dc_loss: 0.3047749400138855, tv_loss: 0.02803060971200466\n",
      "iteration 614, dc_loss: 0.30384963750839233, tv_loss: 0.02809702418744564\n",
      "iteration 615, dc_loss: 0.3032224774360657, tv_loss: 0.028065921738743782\n",
      "iteration 616, dc_loss: 0.3023342490196228, tv_loss: 0.028112957254052162\n",
      "iteration 617, dc_loss: 0.3019043505191803, tv_loss: 0.027982940897345543\n",
      "iteration 618, dc_loss: 0.3009311258792877, tv_loss: 0.02822553552687168\n",
      "iteration 619, dc_loss: 0.30045196413993835, tv_loss: 0.028019951656460762\n",
      "iteration 620, dc_loss: 0.2993985116481781, tv_loss: 0.0282141100615263\n",
      "iteration 621, dc_loss: 0.2989117503166199, tv_loss: 0.028008930385112762\n",
      "iteration 622, dc_loss: 0.29787036776542664, tv_loss: 0.028215626254677773\n",
      "iteration 623, dc_loss: 0.29722875356674194, tv_loss: 0.028136886656284332\n",
      "iteration 624, dc_loss: 0.2963017225265503, tv_loss: 0.028158998116850853\n",
      "iteration 625, dc_loss: 0.2956126928329468, tv_loss: 0.028140773996710777\n",
      "iteration 626, dc_loss: 0.29471728205680847, tv_loss: 0.028198126703500748\n",
      "iteration 627, dc_loss: 0.2940222918987274, tv_loss: 0.028253108263015747\n",
      "iteration 628, dc_loss: 0.2933528423309326, tv_loss: 0.028131328523159027\n",
      "iteration 629, dc_loss: 0.2925567626953125, tv_loss: 0.02830437570810318\n",
      "iteration 630, dc_loss: 0.29196271300315857, tv_loss: 0.028117679059505463\n",
      "iteration 631, dc_loss: 0.2910870909690857, tv_loss: 0.028394171968102455\n",
      "iteration 632, dc_loss: 0.29059433937072754, tv_loss: 0.028103601187467575\n",
      "iteration 633, dc_loss: 0.28974711894989014, tv_loss: 0.028402676805853844\n",
      "iteration 634, dc_loss: 0.2893278896808624, tv_loss: 0.0281192846596241\n",
      "iteration 635, dc_loss: 0.28847944736480713, tv_loss: 0.02847752906382084\n",
      "iteration 636, dc_loss: 0.2880508303642273, tv_loss: 0.028095491230487823\n",
      "iteration 637, dc_loss: 0.2871134579181671, tv_loss: 0.028520073741674423\n",
      "iteration 638, dc_loss: 0.28674933314323425, tv_loss: 0.028100239112973213\n",
      "iteration 639, dc_loss: 0.28571248054504395, tv_loss: 0.02857161872088909\n",
      "iteration 640, dc_loss: 0.28526389598846436, tv_loss: 0.028134092688560486\n",
      "iteration 641, dc_loss: 0.2841964364051819, tv_loss: 0.028504984453320503\n",
      "iteration 642, dc_loss: 0.2836056351661682, tv_loss: 0.0282374806702137\n",
      "iteration 643, dc_loss: 0.28269556164741516, tv_loss: 0.028469445183873177\n",
      "iteration 644, dc_loss: 0.28204378485679626, tv_loss: 0.028327230364084244\n",
      "iteration 645, dc_loss: 0.2813778221607208, tv_loss: 0.028408421203494072\n",
      "iteration 646, dc_loss: 0.2806370258331299, tv_loss: 0.028407558798789978\n",
      "iteration 647, dc_loss: 0.28009486198425293, tv_loss: 0.028408624231815338\n",
      "iteration 648, dc_loss: 0.27931392192840576, tv_loss: 0.028463544324040413\n",
      "iteration 649, dc_loss: 0.27887141704559326, tv_loss: 0.02837917022407055\n",
      "iteration 650, dc_loss: 0.27800747752189636, tv_loss: 0.028524965047836304\n",
      "iteration 651, dc_loss: 0.277576208114624, tv_loss: 0.02841058000922203\n",
      "iteration 652, dc_loss: 0.27672672271728516, tv_loss: 0.028541522100567818\n",
      "iteration 653, dc_loss: 0.27625349164009094, tv_loss: 0.0284101739525795\n",
      "iteration 654, dc_loss: 0.2753753364086151, tv_loss: 0.028563618659973145\n",
      "iteration 655, dc_loss: 0.27484264969825745, tv_loss: 0.028485868126153946\n",
      "iteration 656, dc_loss: 0.27407360076904297, tv_loss: 0.028517715632915497\n",
      "iteration 657, dc_loss: 0.2735047936439514, tv_loss: 0.028514351695775986\n",
      "iteration 658, dc_loss: 0.2727492153644562, tv_loss: 0.02854406274855137\n",
      "iteration 659, dc_loss: 0.27211639285087585, tv_loss: 0.028575357049703598\n",
      "iteration 660, dc_loss: 0.27143752574920654, tv_loss: 0.0285059604793787\n",
      "iteration 661, dc_loss: 0.2707920968532562, tv_loss: 0.028625447303056717\n",
      "iteration 662, dc_loss: 0.27023300528526306, tv_loss: 0.02850424312055111\n",
      "iteration 663, dc_loss: 0.26953497529029846, tv_loss: 0.028705326840281487\n",
      "iteration 664, dc_loss: 0.2690806984901428, tv_loss: 0.02846376784145832\n",
      "iteration 665, dc_loss: 0.268292099237442, tv_loss: 0.028747908771038055\n",
      "iteration 666, dc_loss: 0.26792722940444946, tv_loss: 0.02847021073102951\n",
      "iteration 667, dc_loss: 0.2671424150466919, tv_loss: 0.028824135661125183\n",
      "iteration 668, dc_loss: 0.26700881123542786, tv_loss: 0.02841958776116371\n",
      "iteration 669, dc_loss: 0.26625627279281616, tv_loss: 0.02892334572970867\n",
      "iteration 670, dc_loss: 0.26641547679901123, tv_loss: 0.0283778328448534\n",
      "iteration 671, dc_loss: 0.2655583322048187, tv_loss: 0.029039563611149788\n",
      "iteration 672, dc_loss: 0.26532572507858276, tv_loss: 0.028376542031764984\n",
      "iteration 673, dc_loss: 0.26386505365371704, tv_loss: 0.028949396684765816\n",
      "iteration 674, dc_loss: 0.26333341002464294, tv_loss: 0.028544481843709946\n",
      "iteration 675, dc_loss: 0.2623281478881836, tv_loss: 0.028798338025808334\n",
      "iteration 676, dc_loss: 0.2618007957935333, tv_loss: 0.028694503009319305\n",
      "iteration 677, dc_loss: 0.2613445222377777, tv_loss: 0.028672978281974792\n",
      "iteration 678, dc_loss: 0.2606865465641022, tv_loss: 0.028876645490527153\n",
      "iteration 679, dc_loss: 0.2603514492511749, tv_loss: 0.028609471395611763\n",
      "iteration 680, dc_loss: 0.25944480299949646, tv_loss: 0.02885722555220127\n",
      "iteration 681, dc_loss: 0.2588151693344116, tv_loss: 0.028743872418999672\n",
      "iteration 682, dc_loss: 0.2581588923931122, tv_loss: 0.028741557151079178\n",
      "iteration 683, dc_loss: 0.2574392259120941, tv_loss: 0.028967928141355515\n",
      "iteration 684, dc_loss: 0.2573372721672058, tv_loss: 0.028582092374563217\n",
      "iteration 685, dc_loss: 0.2564304769039154, tv_loss: 0.028994828462600708\n",
      "iteration 686, dc_loss: 0.2560707628726959, tv_loss: 0.02867402881383896\n",
      "iteration 687, dc_loss: 0.25517329573631287, tv_loss: 0.028909046202898026\n",
      "iteration 688, dc_loss: 0.2546986937522888, tv_loss: 0.02871681936085224\n",
      "iteration 689, dc_loss: 0.2540479600429535, tv_loss: 0.028821248561143875\n",
      "iteration 690, dc_loss: 0.25352713465690613, tv_loss: 0.02887803316116333\n",
      "iteration 691, dc_loss: 0.2530914843082428, tv_loss: 0.028725609183311462\n",
      "iteration 692, dc_loss: 0.25246062874794006, tv_loss: 0.02890082076191902\n",
      "iteration 693, dc_loss: 0.25188854336738586, tv_loss: 0.02871660701930523\n",
      "iteration 694, dc_loss: 0.25121039152145386, tv_loss: 0.028941968455910683\n",
      "iteration 695, dc_loss: 0.25060829520225525, tv_loss: 0.028793832287192345\n",
      "iteration 696, dc_loss: 0.2501840889453888, tv_loss: 0.028794238343834877\n",
      "iteration 697, dc_loss: 0.2495085746049881, tv_loss: 0.028917884454131126\n",
      "iteration 698, dc_loss: 0.24914711713790894, tv_loss: 0.02878553606569767\n",
      "iteration 699, dc_loss: 0.24845106899738312, tv_loss: 0.028937174007296562\n",
      "iteration 700, dc_loss: 0.2480294555425644, tv_loss: 0.028821343556046486\n",
      "iteration 701, dc_loss: 0.24730537831783295, tv_loss: 0.028945783153176308\n",
      "iteration 702, dc_loss: 0.2469004988670349, tv_loss: 0.02887219563126564\n",
      "iteration 703, dc_loss: 0.24627700448036194, tv_loss: 0.028927216306328773\n",
      "iteration 704, dc_loss: 0.24580855667591095, tv_loss: 0.028894972056150436\n",
      "iteration 705, dc_loss: 0.2452470064163208, tv_loss: 0.028951212763786316\n",
      "iteration 706, dc_loss: 0.24474841356277466, tv_loss: 0.0288968775421381\n",
      "iteration 707, dc_loss: 0.24416673183441162, tv_loss: 0.028977351263165474\n",
      "iteration 708, dc_loss: 0.24371251463890076, tv_loss: 0.028910892084240913\n",
      "iteration 709, dc_loss: 0.2431574910879135, tv_loss: 0.028973422944545746\n",
      "iteration 710, dc_loss: 0.24269956350326538, tv_loss: 0.028916245326399803\n",
      "iteration 711, dc_loss: 0.24216750264167786, tv_loss: 0.029012111946940422\n",
      "iteration 712, dc_loss: 0.24174143373966217, tv_loss: 0.028931157663464546\n",
      "iteration 713, dc_loss: 0.24112056195735931, tv_loss: 0.029059557244181633\n",
      "iteration 714, dc_loss: 0.24080482125282288, tv_loss: 0.028884677216410637\n",
      "iteration 715, dc_loss: 0.24011605978012085, tv_loss: 0.029146049171686172\n",
      "iteration 716, dc_loss: 0.24001866579055786, tv_loss: 0.028855640441179276\n",
      "iteration 717, dc_loss: 0.23937909305095673, tv_loss: 0.02924511209130287\n",
      "iteration 718, dc_loss: 0.2395399808883667, tv_loss: 0.02880382165312767\n",
      "iteration 719, dc_loss: 0.2389449179172516, tv_loss: 0.029359400272369385\n",
      "iteration 720, dc_loss: 0.23917639255523682, tv_loss: 0.028736619278788567\n",
      "iteration 721, dc_loss: 0.2380785346031189, tv_loss: 0.02939334325492382\n",
      "iteration 722, dc_loss: 0.2375960350036621, tv_loss: 0.02883005142211914\n",
      "iteration 723, dc_loss: 0.23627693951129913, tv_loss: 0.02923290617763996\n",
      "iteration 724, dc_loss: 0.23575036227703094, tv_loss: 0.029081720858812332\n",
      "iteration 725, dc_loss: 0.2355266809463501, tv_loss: 0.02897581458091736\n",
      "iteration 726, dc_loss: 0.23496145009994507, tv_loss: 0.029327496886253357\n",
      "iteration 727, dc_loss: 0.23492582142353058, tv_loss: 0.028887949883937836\n",
      "iteration 728, dc_loss: 0.2338765561580658, tv_loss: 0.0293401088565588\n",
      "iteration 729, dc_loss: 0.23349477350711823, tv_loss: 0.029035938903689384\n",
      "iteration 730, dc_loss: 0.23285160958766937, tv_loss: 0.029162172228097916\n",
      "iteration 731, dc_loss: 0.23235978186130524, tv_loss: 0.02927195280790329\n",
      "iteration 732, dc_loss: 0.23219981789588928, tv_loss: 0.029049133881926537\n",
      "iteration 733, dc_loss: 0.23139537870883942, tv_loss: 0.029339630156755447\n",
      "iteration 734, dc_loss: 0.2310853898525238, tv_loss: 0.02911602519452572\n",
      "iteration 735, dc_loss: 0.2304777204990387, tv_loss: 0.029238266870379448\n",
      "iteration 736, dc_loss: 0.22999419271945953, tv_loss: 0.029293213039636612\n",
      "iteration 737, dc_loss: 0.2296541929244995, tv_loss: 0.029207386076450348\n",
      "iteration 738, dc_loss: 0.2291007936000824, tv_loss: 0.029321417212486267\n",
      "iteration 739, dc_loss: 0.22877100110054016, tv_loss: 0.029186412692070007\n",
      "iteration 740, dc_loss: 0.22814984619617462, tv_loss: 0.029311111196875572\n",
      "iteration 741, dc_loss: 0.2277219444513321, tv_loss: 0.029236797243356705\n",
      "iteration 742, dc_loss: 0.22721271216869354, tv_loss: 0.02933989278972149\n",
      "iteration 743, dc_loss: 0.22677619755268097, tv_loss: 0.02933092787861824\n",
      "iteration 744, dc_loss: 0.2264605164527893, tv_loss: 0.02925034984946251\n",
      "iteration 745, dc_loss: 0.22586660087108612, tv_loss: 0.029387662187218666\n",
      "iteration 746, dc_loss: 0.2255602777004242, tv_loss: 0.029280206188559532\n",
      "iteration 747, dc_loss: 0.224956676363945, tv_loss: 0.02940966747701168\n",
      "iteration 748, dc_loss: 0.22460444271564484, tv_loss: 0.029333049431443214\n",
      "iteration 749, dc_loss: 0.2241530865430832, tv_loss: 0.02934662438929081\n",
      "iteration 750, dc_loss: 0.22370870411396027, tv_loss: 0.029411710798740387\n",
      "iteration 751, dc_loss: 0.2233058661222458, tv_loss: 0.02933981828391552\n",
      "iteration 752, dc_loss: 0.22277912497520447, tv_loss: 0.02944408915936947\n",
      "iteration 753, dc_loss: 0.2224293351173401, tv_loss: 0.029358556494116783\n",
      "iteration 754, dc_loss: 0.2219908982515335, tv_loss: 0.029418103396892548\n",
      "iteration 755, dc_loss: 0.22158131003379822, tv_loss: 0.029392775148153305\n",
      "iteration 756, dc_loss: 0.22113168239593506, tv_loss: 0.029416879639029503\n",
      "iteration 757, dc_loss: 0.22071871161460876, tv_loss: 0.02945004589855671\n",
      "iteration 758, dc_loss: 0.22031445801258087, tv_loss: 0.02945994772017002\n",
      "iteration 759, dc_loss: 0.21987448632717133, tv_loss: 0.029485784471035004\n",
      "iteration 760, dc_loss: 0.21954578161239624, tv_loss: 0.029399218037724495\n",
      "iteration 761, dc_loss: 0.21909423172473907, tv_loss: 0.029495103284716606\n",
      "iteration 762, dc_loss: 0.21883293986320496, tv_loss: 0.02941063605248928\n",
      "iteration 763, dc_loss: 0.21831241250038147, tv_loss: 0.029614130035042763\n",
      "iteration 764, dc_loss: 0.2181306630373001, tv_loss: 0.029384354129433632\n",
      "iteration 765, dc_loss: 0.21748070418834686, tv_loss: 0.02963941916823387\n",
      "iteration 766, dc_loss: 0.21735379099845886, tv_loss: 0.029360605403780937\n",
      "iteration 767, dc_loss: 0.2167094647884369, tv_loss: 0.02966468222439289\n",
      "iteration 768, dc_loss: 0.2167392075061798, tv_loss: 0.029340840876102448\n",
      "iteration 769, dc_loss: 0.21620367467403412, tv_loss: 0.029778432101011276\n",
      "iteration 770, dc_loss: 0.2162625640630722, tv_loss: 0.029361432418227196\n",
      "iteration 771, dc_loss: 0.21551963686943054, tv_loss: 0.029783129692077637\n",
      "iteration 772, dc_loss: 0.2152750939130783, tv_loss: 0.029357071965932846\n",
      "iteration 773, dc_loss: 0.21437977254390717, tv_loss: 0.02971513196825981\n",
      "iteration 774, dc_loss: 0.21412600576877594, tv_loss: 0.029495136812329292\n",
      "iteration 775, dc_loss: 0.21356794238090515, tv_loss: 0.02967221476137638\n",
      "iteration 776, dc_loss: 0.2132166475057602, tv_loss: 0.029603848233819008\n",
      "iteration 777, dc_loss: 0.21277455985546112, tv_loss: 0.029563290998339653\n",
      "iteration 778, dc_loss: 0.21228493750095367, tv_loss: 0.029661990702152252\n",
      "iteration 779, dc_loss: 0.2120824009180069, tv_loss: 0.029529908671975136\n",
      "iteration 780, dc_loss: 0.21153146028518677, tv_loss: 0.02974162995815277\n",
      "iteration 781, dc_loss: 0.21134357154369354, tv_loss: 0.02950403466820717\n",
      "iteration 782, dc_loss: 0.21068644523620605, tv_loss: 0.02975919470191002\n",
      "iteration 783, dc_loss: 0.21052560210227966, tv_loss: 0.02953881397843361\n",
      "iteration 784, dc_loss: 0.21001151204109192, tv_loss: 0.02971985749900341\n",
      "iteration 785, dc_loss: 0.20977555215358734, tv_loss: 0.029595257714390755\n",
      "iteration 786, dc_loss: 0.2092641144990921, tv_loss: 0.029699435457587242\n",
      "iteration 787, dc_loss: 0.208860844373703, tv_loss: 0.029698368161916733\n",
      "iteration 788, dc_loss: 0.2084457278251648, tv_loss: 0.029759466648101807\n",
      "iteration 789, dc_loss: 0.20808054506778717, tv_loss: 0.02972257509827614\n",
      "iteration 790, dc_loss: 0.20775164663791656, tv_loss: 0.02967754378914833\n",
      "iteration 791, dc_loss: 0.20733146369457245, tv_loss: 0.029737962409853935\n",
      "iteration 792, dc_loss: 0.20706132054328918, tv_loss: 0.029673507437109947\n",
      "iteration 793, dc_loss: 0.2065763622522354, tv_loss: 0.029832474887371063\n",
      "iteration 794, dc_loss: 0.2064495086669922, tv_loss: 0.029639223590493202\n",
      "iteration 795, dc_loss: 0.20596536993980408, tv_loss: 0.02985144779086113\n",
      "iteration 796, dc_loss: 0.20593738555908203, tv_loss: 0.029599348083138466\n",
      "iteration 797, dc_loss: 0.20538417994976044, tv_loss: 0.029920315369963646\n",
      "iteration 798, dc_loss: 0.20545241236686707, tv_loss: 0.029600799083709717\n",
      "iteration 799, dc_loss: 0.20482099056243896, tv_loss: 0.03005344048142433\n",
      "iteration 800, dc_loss: 0.2048979252576828, tv_loss: 0.029576243832707405\n",
      "iteration 801, dc_loss: 0.2040814906358719, tv_loss: 0.030026236549019814\n",
      "iteration 802, dc_loss: 0.20365101099014282, tv_loss: 0.029690949246287346\n",
      "iteration 803, dc_loss: 0.20339736342430115, tv_loss: 0.02970431186258793\n",
      "iteration 804, dc_loss: 0.2031193971633911, tv_loss: 0.029990587383508682\n",
      "iteration 805, dc_loss: 0.2028283029794693, tv_loss: 0.029709987342357635\n",
      "iteration 806, dc_loss: 0.20246946811676025, tv_loss: 0.029770467430353165\n",
      "iteration 807, dc_loss: 0.2022000104188919, tv_loss: 0.029953403398394585\n",
      "iteration 808, dc_loss: 0.20201274752616882, tv_loss: 0.0297379270195961\n",
      "iteration 809, dc_loss: 0.2015708088874817, tv_loss: 0.029830103740096092\n",
      "iteration 810, dc_loss: 0.20130741596221924, tv_loss: 0.02993830293416977\n",
      "iteration 811, dc_loss: 0.20117270946502686, tv_loss: 0.029777392745018005\n",
      "iteration 812, dc_loss: 0.2007245570421219, tv_loss: 0.02986142411828041\n",
      "iteration 813, dc_loss: 0.20047079026699066, tv_loss: 0.02991037629544735\n",
      "iteration 814, dc_loss: 0.2003544569015503, tv_loss: 0.02978767268359661\n",
      "iteration 815, dc_loss: 0.19988736510276794, tv_loss: 0.02993014268577099\n",
      "iteration 816, dc_loss: 0.19965408742427826, tv_loss: 0.029929302632808685\n",
      "iteration 817, dc_loss: 0.19950510561466217, tv_loss: 0.029835674911737442\n",
      "iteration 818, dc_loss: 0.1990990787744522, tv_loss: 0.029956011101603508\n",
      "iteration 819, dc_loss: 0.1988617330789566, tv_loss: 0.029918024316430092\n",
      "iteration 820, dc_loss: 0.19868703186511993, tv_loss: 0.02984943985939026\n",
      "iteration 821, dc_loss: 0.19829247891902924, tv_loss: 0.030015863478183746\n",
      "iteration 822, dc_loss: 0.19810166954994202, tv_loss: 0.02992040477693081\n",
      "iteration 823, dc_loss: 0.19786182045936584, tv_loss: 0.029911333695054054\n",
      "iteration 824, dc_loss: 0.19752010703086853, tv_loss: 0.029983634129166603\n",
      "iteration 825, dc_loss: 0.19732746481895447, tv_loss: 0.029934758320450783\n",
      "iteration 826, dc_loss: 0.19704194366931915, tv_loss: 0.029917405918240547\n",
      "iteration 827, dc_loss: 0.1967596560716629, tv_loss: 0.02997581474483013\n",
      "iteration 828, dc_loss: 0.19657982885837555, tv_loss: 0.02992687188088894\n",
      "iteration 829, dc_loss: 0.19623418152332306, tv_loss: 0.02998100221157074\n",
      "iteration 830, dc_loss: 0.19599859416484833, tv_loss: 0.02997584268450737\n",
      "iteration 831, dc_loss: 0.19580262899398804, tv_loss: 0.02994040586054325\n",
      "iteration 832, dc_loss: 0.19547809660434723, tv_loss: 0.03000485524535179\n",
      "iteration 833, dc_loss: 0.19526490569114685, tv_loss: 0.02997981198132038\n",
      "iteration 834, dc_loss: 0.19503487646579742, tv_loss: 0.030024634674191475\n",
      "iteration 835, dc_loss: 0.19471819698810577, tv_loss: 0.030089808627963066\n",
      "iteration 836, dc_loss: 0.1945178359746933, tv_loss: 0.029975280165672302\n",
      "iteration 837, dc_loss: 0.19425886869430542, tv_loss: 0.029995888471603394\n",
      "iteration 838, dc_loss: 0.19397424161434174, tv_loss: 0.030085546895861626\n",
      "iteration 839, dc_loss: 0.19379326701164246, tv_loss: 0.030024459585547447\n",
      "iteration 840, dc_loss: 0.1935022920370102, tv_loss: 0.030065009370446205\n",
      "iteration 841, dc_loss: 0.1932675689458847, tv_loss: 0.030035926029086113\n",
      "iteration 842, dc_loss: 0.19302316009998322, tv_loss: 0.03004472889006138\n",
      "iteration 843, dc_loss: 0.19276180863380432, tv_loss: 0.030074890702962875\n",
      "iteration 844, dc_loss: 0.1925458461046219, tv_loss: 0.03003392182290554\n",
      "iteration 845, dc_loss: 0.1923009753227234, tv_loss: 0.030038785189390182\n",
      "iteration 846, dc_loss: 0.19202634692192078, tv_loss: 0.03007683716714382\n",
      "iteration 847, dc_loss: 0.1918320655822754, tv_loss: 0.0300621185451746\n",
      "iteration 848, dc_loss: 0.19154883921146393, tv_loss: 0.030140673741698265\n",
      "iteration 849, dc_loss: 0.1913410723209381, tv_loss: 0.030115459114313126\n",
      "iteration 850, dc_loss: 0.19109855592250824, tv_loss: 0.030081847682595253\n",
      "iteration 851, dc_loss: 0.19083279371261597, tv_loss: 0.030109860002994537\n",
      "iteration 852, dc_loss: 0.19062770903110504, tv_loss: 0.03008853644132614\n",
      "iteration 853, dc_loss: 0.1903679072856903, tv_loss: 0.030105585232377052\n",
      "iteration 854, dc_loss: 0.19013965129852295, tv_loss: 0.03011184185743332\n",
      "iteration 855, dc_loss: 0.18990856409072876, tv_loss: 0.030146632343530655\n",
      "iteration 856, dc_loss: 0.18968968093395233, tv_loss: 0.0301821231842041\n",
      "iteration 857, dc_loss: 0.18944096565246582, tv_loss: 0.030150560662150383\n",
      "iteration 858, dc_loss: 0.18921704590320587, tv_loss: 0.03012208640575409\n",
      "iteration 859, dc_loss: 0.1889617145061493, tv_loss: 0.03015952929854393\n",
      "iteration 860, dc_loss: 0.18875111639499664, tv_loss: 0.030143311247229576\n",
      "iteration 861, dc_loss: 0.18851354718208313, tv_loss: 0.030149297788739204\n",
      "iteration 862, dc_loss: 0.18828652799129486, tv_loss: 0.030162585899233818\n",
      "iteration 863, dc_loss: 0.18806901574134827, tv_loss: 0.03020314872264862\n",
      "iteration 864, dc_loss: 0.18781903386116028, tv_loss: 0.030229708179831505\n",
      "iteration 865, dc_loss: 0.18761219084262848, tv_loss: 0.030175942927598953\n",
      "iteration 866, dc_loss: 0.18736723065376282, tv_loss: 0.03019336611032486\n",
      "iteration 867, dc_loss: 0.18716168403625488, tv_loss: 0.03017488121986389\n",
      "iteration 868, dc_loss: 0.18694353103637695, tv_loss: 0.030181128531694412\n",
      "iteration 869, dc_loss: 0.1866801530122757, tv_loss: 0.03030143864452839\n",
      "iteration 870, dc_loss: 0.1865013688802719, tv_loss: 0.03021552786231041\n",
      "iteration 871, dc_loss: 0.18624833226203918, tv_loss: 0.030226968228816986\n",
      "iteration 872, dc_loss: 0.18605421483516693, tv_loss: 0.030231870710849762\n",
      "iteration 873, dc_loss: 0.18581977486610413, tv_loss: 0.030306361615657806\n",
      "iteration 874, dc_loss: 0.18556062877178192, tv_loss: 0.03026478923857212\n",
      "iteration 875, dc_loss: 0.1853940337896347, tv_loss: 0.030232343822717667\n",
      "iteration 876, dc_loss: 0.18515343964099884, tv_loss: 0.030287226662039757\n",
      "iteration 877, dc_loss: 0.18493276834487915, tv_loss: 0.030323738232254982\n",
      "iteration 878, dc_loss: 0.18473504483699799, tv_loss: 0.030232425779104233\n",
      "iteration 879, dc_loss: 0.18449734151363373, tv_loss: 0.030334601178765297\n",
      "iteration 880, dc_loss: 0.1842873990535736, tv_loss: 0.030280444771051407\n",
      "iteration 881, dc_loss: 0.18407317996025085, tv_loss: 0.03031039610505104\n",
      "iteration 882, dc_loss: 0.18385276198387146, tv_loss: 0.03030042164027691\n",
      "iteration 883, dc_loss: 0.18365919589996338, tv_loss: 0.030371075496077538\n",
      "iteration 884, dc_loss: 0.1834266483783722, tv_loss: 0.030329624190926552\n",
      "iteration 885, dc_loss: 0.18323595821857452, tv_loss: 0.03033045493066311\n",
      "iteration 886, dc_loss: 0.18300320208072662, tv_loss: 0.030400477349758148\n",
      "iteration 887, dc_loss: 0.1828029900789261, tv_loss: 0.030288904905319214\n",
      "iteration 888, dc_loss: 0.18258284032344818, tv_loss: 0.030430955812335014\n",
      "iteration 889, dc_loss: 0.1823524385690689, tv_loss: 0.030374042689800262\n",
      "iteration 890, dc_loss: 0.18218322098255157, tv_loss: 0.030403079465031624\n",
      "iteration 891, dc_loss: 0.18194548785686493, tv_loss: 0.030410796403884888\n",
      "iteration 892, dc_loss: 0.18178535997867584, tv_loss: 0.03036242164671421\n",
      "iteration 893, dc_loss: 0.18153482675552368, tv_loss: 0.030395036563277245\n",
      "iteration 894, dc_loss: 0.1813146024942398, tv_loss: 0.030428601428866386\n",
      "iteration 895, dc_loss: 0.18112340569496155, tv_loss: 0.03041602484881878\n",
      "iteration 896, dc_loss: 0.18094412982463837, tv_loss: 0.030458111315965652\n",
      "iteration 897, dc_loss: 0.18074004352092743, tv_loss: 0.03035796619951725\n",
      "iteration 898, dc_loss: 0.1805051565170288, tv_loss: 0.030504172667860985\n",
      "iteration 899, dc_loss: 0.18028096854686737, tv_loss: 0.03047833777964115\n",
      "iteration 900, dc_loss: 0.18014980852603912, tv_loss: 0.03042491525411606\n",
      "iteration 901, dc_loss: 0.17993633449077606, tv_loss: 0.030401980504393578\n",
      "iteration 902, dc_loss: 0.17970241606235504, tv_loss: 0.03047172538936138\n",
      "iteration 903, dc_loss: 0.17947816848754883, tv_loss: 0.030446652323007584\n",
      "iteration 904, dc_loss: 0.17934276163578033, tv_loss: 0.030470220372080803\n",
      "iteration 905, dc_loss: 0.17910456657409668, tv_loss: 0.03043762780725956\n",
      "iteration 906, dc_loss: 0.17889505624771118, tv_loss: 0.030542641878128052\n",
      "iteration 907, dc_loss: 0.17868271470069885, tv_loss: 0.03048553876578808\n",
      "iteration 908, dc_loss: 0.17855946719646454, tv_loss: 0.030502278357744217\n",
      "iteration 909, dc_loss: 0.178331658244133, tv_loss: 0.03047262318432331\n",
      "iteration 910, dc_loss: 0.17809134721755981, tv_loss: 0.03057103604078293\n",
      "iteration 911, dc_loss: 0.177887961268425, tv_loss: 0.0305547546595335\n",
      "iteration 912, dc_loss: 0.17775565385818481, tv_loss: 0.030482428148388863\n",
      "iteration 913, dc_loss: 0.17756353318691254, tv_loss: 0.03049699030816555\n",
      "iteration 914, dc_loss: 0.17733106017112732, tv_loss: 0.030490491539239883\n",
      "iteration 915, dc_loss: 0.1771192103624344, tv_loss: 0.03054964914917946\n",
      "iteration 916, dc_loss: 0.17693285644054413, tv_loss: 0.030475899577140808\n",
      "iteration 917, dc_loss: 0.1767749935388565, tv_loss: 0.030524391680955887\n",
      "iteration 918, dc_loss: 0.17658551037311554, tv_loss: 0.03046496957540512\n",
      "iteration 919, dc_loss: 0.17633600533008575, tv_loss: 0.030506525188684464\n",
      "iteration 920, dc_loss: 0.17618682980537415, tv_loss: 0.03050791658461094\n",
      "iteration 921, dc_loss: 0.17598223686218262, tv_loss: 0.03052385523915291\n",
      "iteration 922, dc_loss: 0.17579978704452515, tv_loss: 0.030504239723086357\n",
      "iteration 923, dc_loss: 0.17560110986232758, tv_loss: 0.030503107234835625\n",
      "iteration 924, dc_loss: 0.17543604969978333, tv_loss: 0.030559992417693138\n",
      "iteration 925, dc_loss: 0.17522743344306946, tv_loss: 0.030533811077475548\n",
      "iteration 926, dc_loss: 0.17503944039344788, tv_loss: 0.030542880296707153\n",
      "iteration 927, dc_loss: 0.17482781410217285, tv_loss: 0.030586205422878265\n",
      "iteration 928, dc_loss: 0.17470811307430267, tv_loss: 0.030543534085154533\n",
      "iteration 929, dc_loss: 0.17449133098125458, tv_loss: 0.030512915924191475\n",
      "iteration 930, dc_loss: 0.17430272698402405, tv_loss: 0.030605128034949303\n",
      "iteration 931, dc_loss: 0.1741146445274353, tv_loss: 0.030587784945964813\n",
      "iteration 932, dc_loss: 0.17400434613227844, tv_loss: 0.030540119856595993\n",
      "iteration 933, dc_loss: 0.17384101450443268, tv_loss: 0.030572550371289253\n",
      "iteration 934, dc_loss: 0.17372450232505798, tv_loss: 0.030650269240140915\n",
      "iteration 935, dc_loss: 0.17368605732917786, tv_loss: 0.030565669760107994\n",
      "iteration 936, dc_loss: 0.17358362674713135, tv_loss: 0.030645102262496948\n",
      "iteration 937, dc_loss: 0.17353036999702454, tv_loss: 0.03053668513894081\n",
      "iteration 938, dc_loss: 0.1731431782245636, tv_loss: 0.03071954846382141\n",
      "iteration 939, dc_loss: 0.17316897213459015, tv_loss: 0.030488746240735054\n",
      "iteration 940, dc_loss: 0.17281058430671692, tv_loss: 0.03078582137823105\n",
      "iteration 941, dc_loss: 0.17298728227615356, tv_loss: 0.03040725365281105\n",
      "iteration 942, dc_loss: 0.1723974496126175, tv_loss: 0.03082290105521679\n",
      "iteration 943, dc_loss: 0.1723187267780304, tv_loss: 0.030517973005771637\n",
      "iteration 944, dc_loss: 0.17200183868408203, tv_loss: 0.030640024691820145\n",
      "iteration 945, dc_loss: 0.17196355760097504, tv_loss: 0.030659442767500877\n",
      "iteration 946, dc_loss: 0.17206203937530518, tv_loss: 0.030548086389899254\n",
      "iteration 947, dc_loss: 0.17199859023094177, tv_loss: 0.030755897983908653\n",
      "iteration 948, dc_loss: 0.171782985329628, tv_loss: 0.03060351312160492\n",
      "iteration 949, dc_loss: 0.17152024805545807, tv_loss: 0.030670100823044777\n",
      "iteration 950, dc_loss: 0.17134439945220947, tv_loss: 0.030726054683327675\n",
      "iteration 951, dc_loss: 0.1711716502904892, tv_loss: 0.030661454424262047\n",
      "iteration 952, dc_loss: 0.17099477350711823, tv_loss: 0.030601253733038902\n",
      "iteration 953, dc_loss: 0.1705760955810547, tv_loss: 0.030738992616534233\n",
      "iteration 954, dc_loss: 0.17049485445022583, tv_loss: 0.030659455806016922\n",
      "iteration 955, dc_loss: 0.170259490609169, tv_loss: 0.030691616237163544\n",
      "iteration 956, dc_loss: 0.17005375027656555, tv_loss: 0.030685847625136375\n",
      "iteration 957, dc_loss: 0.16990944743156433, tv_loss: 0.030601758509874344\n",
      "iteration 958, dc_loss: 0.16962414979934692, tv_loss: 0.030773531645536423\n",
      "iteration 959, dc_loss: 0.16957536339759827, tv_loss: 0.0305885449051857\n",
      "iteration 960, dc_loss: 0.16923777759075165, tv_loss: 0.030752386897802353\n",
      "iteration 961, dc_loss: 0.16911832988262177, tv_loss: 0.030646774917840958\n",
      "iteration 962, dc_loss: 0.16889874637126923, tv_loss: 0.03072192147374153\n",
      "iteration 963, dc_loss: 0.16871480643749237, tv_loss: 0.030700240284204483\n",
      "iteration 964, dc_loss: 0.16856692731380463, tv_loss: 0.030663853511214256\n",
      "iteration 965, dc_loss: 0.1683378964662552, tv_loss: 0.030679475516080856\n",
      "iteration 966, dc_loss: 0.1681816428899765, tv_loss: 0.030735492706298828\n",
      "iteration 967, dc_loss: 0.16797704994678497, tv_loss: 0.03073381818830967\n",
      "iteration 968, dc_loss: 0.16786184906959534, tv_loss: 0.030698981136083603\n",
      "iteration 969, dc_loss: 0.16761845350265503, tv_loss: 0.03075418807566166\n",
      "iteration 970, dc_loss: 0.16757480800151825, tv_loss: 0.030686968937516212\n",
      "iteration 971, dc_loss: 0.16725821793079376, tv_loss: 0.030796557664871216\n",
      "iteration 972, dc_loss: 0.16722825169563293, tv_loss: 0.03067593090236187\n",
      "iteration 973, dc_loss: 0.1669534295797348, tv_loss: 0.030747950077056885\n",
      "iteration 974, dc_loss: 0.16681614518165588, tv_loss: 0.030760271474719048\n",
      "iteration 975, dc_loss: 0.16661134362220764, tv_loss: 0.030764490365982056\n",
      "iteration 976, dc_loss: 0.16651411354541779, tv_loss: 0.030714591965079308\n",
      "iteration 977, dc_loss: 0.16627639532089233, tv_loss: 0.030758563429117203\n",
      "iteration 978, dc_loss: 0.1661357581615448, tv_loss: 0.030774027109146118\n",
      "iteration 979, dc_loss: 0.1659543514251709, tv_loss: 0.030766863375902176\n",
      "iteration 980, dc_loss: 0.1658395677804947, tv_loss: 0.0307467021048069\n",
      "iteration 981, dc_loss: 0.16566136479377747, tv_loss: 0.030737657099962234\n",
      "iteration 982, dc_loss: 0.16544613242149353, tv_loss: 0.03083585388958454\n",
      "iteration 983, dc_loss: 0.16535653173923492, tv_loss: 0.030747583135962486\n",
      "iteration 984, dc_loss: 0.1651555299758911, tv_loss: 0.030806677415966988\n",
      "iteration 985, dc_loss: 0.16506914794445038, tv_loss: 0.03072272427380085\n",
      "iteration 986, dc_loss: 0.16481183469295502, tv_loss: 0.030849065631628036\n",
      "iteration 987, dc_loss: 0.16476154327392578, tv_loss: 0.030738288536667824\n",
      "iteration 988, dc_loss: 0.1645536720752716, tv_loss: 0.030830882489681244\n",
      "iteration 989, dc_loss: 0.16451044380664825, tv_loss: 0.03071841225028038\n",
      "iteration 990, dc_loss: 0.164220929145813, tv_loss: 0.030927814543247223\n",
      "iteration 991, dc_loss: 0.16434024274349213, tv_loss: 0.030707022175192833\n",
      "iteration 992, dc_loss: 0.16404975950717926, tv_loss: 0.03097079135477543\n",
      "iteration 993, dc_loss: 0.16430747509002686, tv_loss: 0.030689924955368042\n",
      "iteration 994, dc_loss: 0.16391536593437195, tv_loss: 0.03103034943342209\n",
      "iteration 995, dc_loss: 0.1642693728208542, tv_loss: 0.03061722032725811\n",
      "iteration 996, dc_loss: 0.163801372051239, tv_loss: 0.03108050674200058\n",
      "iteration 997, dc_loss: 0.1639711707830429, tv_loss: 0.030645912513136864\n",
      "iteration 998, dc_loss: 0.16324861347675323, tv_loss: 0.031053299084305763\n",
      "iteration 999, dc_loss: 0.16310854256153107, tv_loss: 0.030737027525901794\n",
      "iteration 1000, dc_loss: 0.16272439062595367, tv_loss: 0.030852889642119408\n",
      "iteration 1001, dc_loss: 0.1625576615333557, tv_loss: 0.030885230749845505\n",
      "iteration 1002, dc_loss: 0.1626649796962738, tv_loss: 0.0307364072650671\n",
      "iteration 1003, dc_loss: 0.16241800785064697, tv_loss: 0.03098905459046364\n",
      "iteration 1004, dc_loss: 0.1625223308801651, tv_loss: 0.03076285310089588\n",
      "iteration 1005, dc_loss: 0.1620520055294037, tv_loss: 0.03099961206316948\n",
      "iteration 1006, dc_loss: 0.161963552236557, tv_loss: 0.030805818736553192\n",
      "iteration 1007, dc_loss: 0.16169202327728271, tv_loss: 0.03087960183620453\n",
      "iteration 1008, dc_loss: 0.16150765120983124, tv_loss: 0.030969342216849327\n",
      "iteration 1009, dc_loss: 0.16159428656101227, tv_loss: 0.03078043833374977\n",
      "iteration 1010, dc_loss: 0.16131766140460968, tv_loss: 0.03102514147758484\n",
      "iteration 1011, dc_loss: 0.16143196821212769, tv_loss: 0.03072914108633995\n",
      "iteration 1012, dc_loss: 0.16096574068069458, tv_loss: 0.03099946677684784\n",
      "iteration 1013, dc_loss: 0.16086633503437042, tv_loss: 0.030943602323532104\n",
      "iteration 1014, dc_loss: 0.160663902759552, tv_loss: 0.0309520922601223\n",
      "iteration 1015, dc_loss: 0.16049370169639587, tv_loss: 0.03094896674156189\n",
      "iteration 1016, dc_loss: 0.16056112945079803, tv_loss: 0.03092622198164463\n",
      "iteration 1017, dc_loss: 0.16023440659046173, tv_loss: 0.031041275709867477\n",
      "iteration 1018, dc_loss: 0.1602049171924591, tv_loss: 0.030905110761523247\n",
      "iteration 1019, dc_loss: 0.1599491536617279, tv_loss: 0.031068699434399605\n",
      "iteration 1020, dc_loss: 0.15982089936733246, tv_loss: 0.0309299249202013\n",
      "iteration 1021, dc_loss: 0.15968088805675507, tv_loss: 0.030952293425798416\n",
      "iteration 1022, dc_loss: 0.15948019921779633, tv_loss: 0.031117353588342667\n",
      "iteration 1023, dc_loss: 0.15947365760803223, tv_loss: 0.030889842659235\n",
      "iteration 1024, dc_loss: 0.15923190116882324, tv_loss: 0.031102461740374565\n",
      "iteration 1025, dc_loss: 0.15921509265899658, tv_loss: 0.03094968944787979\n",
      "iteration 1026, dc_loss: 0.15894533693790436, tv_loss: 0.031018376350402832\n",
      "iteration 1027, dc_loss: 0.1588490605354309, tv_loss: 0.031085427850484848\n",
      "iteration 1028, dc_loss: 0.15865828096866608, tv_loss: 0.031009087339043617\n",
      "iteration 1029, dc_loss: 0.15856443345546722, tv_loss: 0.03110821731388569\n",
      "iteration 1030, dc_loss: 0.15846037864685059, tv_loss: 0.03095887042582035\n",
      "iteration 1031, dc_loss: 0.1582178771495819, tv_loss: 0.031171578913927078\n",
      "iteration 1032, dc_loss: 0.15815624594688416, tv_loss: 0.03105224296450615\n",
      "iteration 1033, dc_loss: 0.157980814576149, tv_loss: 0.03108675591647625\n",
      "iteration 1034, dc_loss: 0.15797100961208344, tv_loss: 0.031063558533787727\n",
      "iteration 1035, dc_loss: 0.1577185094356537, tv_loss: 0.031080586835741997\n",
      "iteration 1036, dc_loss: 0.15762804448604584, tv_loss: 0.031123215332627296\n",
      "iteration 1037, dc_loss: 0.15742915868759155, tv_loss: 0.031116699799895287\n",
      "iteration 1038, dc_loss: 0.15735812485218048, tv_loss: 0.031119292601943016\n",
      "iteration 1039, dc_loss: 0.1572079211473465, tv_loss: 0.0310894176363945\n",
      "iteration 1040, dc_loss: 0.15710437297821045, tv_loss: 0.031065842136740685\n",
      "iteration 1041, dc_loss: 0.1569504290819168, tv_loss: 0.031156696379184723\n",
      "iteration 1042, dc_loss: 0.15679579973220825, tv_loss: 0.031101422384381294\n",
      "iteration 1043, dc_loss: 0.15671944618225098, tv_loss: 0.031143758445978165\n",
      "iteration 1044, dc_loss: 0.15662391483783722, tv_loss: 0.03103303536772728\n",
      "iteration 1045, dc_loss: 0.15641680359840393, tv_loss: 0.03116917796432972\n",
      "iteration 1046, dc_loss: 0.15632610023021698, tv_loss: 0.031120648607611656\n",
      "iteration 1047, dc_loss: 0.15611740946769714, tv_loss: 0.031169166788458824\n",
      "iteration 1048, dc_loss: 0.15618067979812622, tv_loss: 0.03107469715178013\n",
      "iteration 1049, dc_loss: 0.1558753401041031, tv_loss: 0.03120231255888939\n",
      "iteration 1050, dc_loss: 0.15596479177474976, tv_loss: 0.031076373532414436\n",
      "iteration 1051, dc_loss: 0.1556311547756195, tv_loss: 0.031270187348127365\n",
      "iteration 1052, dc_loss: 0.1558283418416977, tv_loss: 0.031029097735881805\n",
      "iteration 1053, dc_loss: 0.15549704432487488, tv_loss: 0.0312645249068737\n",
      "iteration 1054, dc_loss: 0.15568728744983673, tv_loss: 0.030945276841521263\n",
      "iteration 1055, dc_loss: 0.1552484929561615, tv_loss: 0.03132934123277664\n",
      "iteration 1056, dc_loss: 0.1553374081850052, tv_loss: 0.030952302739024162\n",
      "iteration 1057, dc_loss: 0.15495184063911438, tv_loss: 0.03130418062210083\n",
      "iteration 1058, dc_loss: 0.15492703020572662, tv_loss: 0.03103032521903515\n",
      "iteration 1059, dc_loss: 0.15465444326400757, tv_loss: 0.03116236813366413\n",
      "iteration 1060, dc_loss: 0.15451103448867798, tv_loss: 0.03119230829179287\n",
      "iteration 1061, dc_loss: 0.1544765830039978, tv_loss: 0.031065139919519424\n",
      "iteration 1062, dc_loss: 0.15426042675971985, tv_loss: 0.03134847432374954\n",
      "iteration 1063, dc_loss: 0.15439411997795105, tv_loss: 0.031014492735266685\n",
      "iteration 1064, dc_loss: 0.15406493842601776, tv_loss: 0.03140341490507126\n",
      "iteration 1065, dc_loss: 0.1541437953710556, tv_loss: 0.03107628785073757\n",
      "iteration 1066, dc_loss: 0.15374897420406342, tv_loss: 0.03141644597053528\n",
      "iteration 1067, dc_loss: 0.15386736392974854, tv_loss: 0.03099382109940052\n",
      "iteration 1068, dc_loss: 0.15352651476860046, tv_loss: 0.03131292015314102\n",
      "iteration 1069, dc_loss: 0.15345507860183716, tv_loss: 0.031108934432268143\n",
      "iteration 1070, dc_loss: 0.15324105322360992, tv_loss: 0.031246325001120567\n",
      "iteration 1071, dc_loss: 0.15317940711975098, tv_loss: 0.031181594356894493\n",
      "iteration 1072, dc_loss: 0.15304400026798248, tv_loss: 0.031113652512431145\n",
      "iteration 1073, dc_loss: 0.15285871922969818, tv_loss: 0.031355198472738266\n",
      "iteration 1074, dc_loss: 0.15279506146907806, tv_loss: 0.031155480071902275\n",
      "iteration 1075, dc_loss: 0.15265120565891266, tv_loss: 0.031336650252342224\n",
      "iteration 1076, dc_loss: 0.15254636108875275, tv_loss: 0.031161010265350342\n",
      "iteration 1077, dc_loss: 0.15236589312553406, tv_loss: 0.031370725482702255\n",
      "iteration 1078, dc_loss: 0.1522616147994995, tv_loss: 0.031166374683380127\n",
      "iteration 1079, dc_loss: 0.15213386714458466, tv_loss: 0.03136064112186432\n",
      "iteration 1080, dc_loss: 0.15203014016151428, tv_loss: 0.03115462325513363\n",
      "iteration 1081, dc_loss: 0.15192654728889465, tv_loss: 0.0314064621925354\n",
      "iteration 1082, dc_loss: 0.15178102254867554, tv_loss: 0.0312584713101387\n",
      "iteration 1083, dc_loss: 0.15168240666389465, tv_loss: 0.031450022011995316\n",
      "iteration 1084, dc_loss: 0.15154466032981873, tv_loss: 0.03140915557742119\n",
      "iteration 1085, dc_loss: 0.1514217108488083, tv_loss: 0.031309738755226135\n",
      "iteration 1086, dc_loss: 0.15131157636642456, tv_loss: 0.03136640787124634\n",
      "iteration 1087, dc_loss: 0.15120382606983185, tv_loss: 0.03126600384712219\n",
      "iteration 1088, dc_loss: 0.15113984048366547, tv_loss: 0.03122679702937603\n",
      "iteration 1089, dc_loss: 0.1509646475315094, tv_loss: 0.03136886656284332\n",
      "iteration 1090, dc_loss: 0.15097017586231232, tv_loss: 0.03116978518664837\n",
      "iteration 1091, dc_loss: 0.1507931649684906, tv_loss: 0.031496401876211166\n",
      "iteration 1092, dc_loss: 0.15098553895950317, tv_loss: 0.03112747333943844\n",
      "iteration 1093, dc_loss: 0.15075358748435974, tv_loss: 0.03165208175778389\n",
      "iteration 1094, dc_loss: 0.15109996497631073, tv_loss: 0.031250037252902985\n",
      "iteration 1095, dc_loss: 0.15076856315135956, tv_loss: 0.03153255209326744\n",
      "iteration 1096, dc_loss: 0.15106335282325745, tv_loss: 0.031092574819922447\n",
      "iteration 1097, dc_loss: 0.15042880177497864, tv_loss: 0.031533561646938324\n",
      "iteration 1098, dc_loss: 0.15042607486248016, tv_loss: 0.03117148019373417\n",
      "iteration 1099, dc_loss: 0.14990945160388947, tv_loss: 0.031412530690431595\n",
      "iteration 1100, dc_loss: 0.1498367339372635, tv_loss: 0.03122694417834282\n",
      "iteration 1101, dc_loss: 0.1497853547334671, tv_loss: 0.03133562579751015\n",
      "iteration 1102, dc_loss: 0.14968042075634003, tv_loss: 0.031379181891679764\n",
      "iteration 1103, dc_loss: 0.14981292188167572, tv_loss: 0.03127739951014519\n",
      "iteration 1104, dc_loss: 0.14937177300453186, tv_loss: 0.03136400878429413\n",
      "iteration 1105, dc_loss: 0.14929735660552979, tv_loss: 0.03140350058674812\n",
      "iteration 1106, dc_loss: 0.14906656742095947, tv_loss: 0.03142350912094116\n",
      "iteration 1107, dc_loss: 0.14899922907352448, tv_loss: 0.03146069124341011\n",
      "iteration 1108, dc_loss: 0.14904174208641052, tv_loss: 0.03134845942258835\n",
      "iteration 1109, dc_loss: 0.14881935715675354, tv_loss: 0.03143511340022087\n",
      "iteration 1110, dc_loss: 0.14883466064929962, tv_loss: 0.03133438900113106\n",
      "iteration 1111, dc_loss: 0.14849336445331573, tv_loss: 0.03141823410987854\n",
      "iteration 1112, dc_loss: 0.14858274161815643, tv_loss: 0.031245065852999687\n",
      "iteration 1113, dc_loss: 0.14831110835075378, tv_loss: 0.031446103006601334\n",
      "iteration 1114, dc_loss: 0.14827342331409454, tv_loss: 0.031351689249277115\n",
      "iteration 1115, dc_loss: 0.14816135168075562, tv_loss: 0.03142254427075386\n",
      "iteration 1116, dc_loss: 0.14800001680850983, tv_loss: 0.03134724497795105\n",
      "iteration 1117, dc_loss: 0.14791251718997955, tv_loss: 0.03146883100271225\n",
      "iteration 1118, dc_loss: 0.14774397015571594, tv_loss: 0.03148747235536575\n",
      "iteration 1119, dc_loss: 0.1477290838956833, tv_loss: 0.031375668942928314\n",
      "iteration 1120, dc_loss: 0.14755551517009735, tv_loss: 0.03137483820319176\n",
      "iteration 1121, dc_loss: 0.14745159447193146, tv_loss: 0.03138197958469391\n",
      "iteration 1122, dc_loss: 0.14725564420223236, tv_loss: 0.03137950226664543\n",
      "iteration 1123, dc_loss: 0.14721183478832245, tv_loss: 0.03141419589519501\n",
      "iteration 1124, dc_loss: 0.14706255495548248, tv_loss: 0.031343527138233185\n",
      "iteration 1125, dc_loss: 0.14700716733932495, tv_loss: 0.031476251780986786\n",
      "iteration 1126, dc_loss: 0.14682017266750336, tv_loss: 0.03143729642033577\n",
      "iteration 1127, dc_loss: 0.14674925804138184, tv_loss: 0.03147055208683014\n",
      "iteration 1128, dc_loss: 0.14664390683174133, tv_loss: 0.031419988721609116\n",
      "iteration 1129, dc_loss: 0.14649458229541779, tv_loss: 0.031454410403966904\n",
      "iteration 1130, dc_loss: 0.14642898738384247, tv_loss: 0.03137516602873802\n",
      "iteration 1131, dc_loss: 0.14629815518856049, tv_loss: 0.03144608065485954\n",
      "iteration 1132, dc_loss: 0.14615552127361298, tv_loss: 0.0314217135310173\n",
      "iteration 1133, dc_loss: 0.14609451591968536, tv_loss: 0.031439948827028275\n",
      "iteration 1134, dc_loss: 0.14592254161834717, tv_loss: 0.031437940895557404\n",
      "iteration 1135, dc_loss: 0.14594390988349915, tv_loss: 0.03139522299170494\n",
      "iteration 1136, dc_loss: 0.14570127427577972, tv_loss: 0.031453948467969894\n",
      "iteration 1137, dc_loss: 0.14569959044456482, tv_loss: 0.03146389499306679\n",
      "iteration 1138, dc_loss: 0.1454974114894867, tv_loss: 0.03148939460515976\n",
      "iteration 1139, dc_loss: 0.14554759860038757, tv_loss: 0.03141595423221588\n",
      "iteration 1140, dc_loss: 0.14532458782196045, tv_loss: 0.03148488327860832\n",
      "iteration 1141, dc_loss: 0.14538022875785828, tv_loss: 0.031418364495038986\n",
      "iteration 1142, dc_loss: 0.14513561129570007, tv_loss: 0.031544044613838196\n",
      "iteration 1143, dc_loss: 0.14528509974479675, tv_loss: 0.03137921541929245\n",
      "iteration 1144, dc_loss: 0.14500659704208374, tv_loss: 0.03156397491693497\n",
      "iteration 1145, dc_loss: 0.1451699286699295, tv_loss: 0.031357377767562866\n",
      "iteration 1146, dc_loss: 0.1448136568069458, tv_loss: 0.031610991805791855\n",
      "iteration 1147, dc_loss: 0.14503520727157593, tv_loss: 0.03132438659667969\n",
      "iteration 1148, dc_loss: 0.14461737871170044, tv_loss: 0.03157750889658928\n",
      "iteration 1149, dc_loss: 0.14478017389774323, tv_loss: 0.03138306364417076\n",
      "iteration 1150, dc_loss: 0.14442117512226105, tv_loss: 0.03162486106157303\n",
      "iteration 1151, dc_loss: 0.14462536573410034, tv_loss: 0.031375277787446976\n",
      "iteration 1152, dc_loss: 0.14433683454990387, tv_loss: 0.03159772604703903\n",
      "iteration 1153, dc_loss: 0.14441010355949402, tv_loss: 0.03139946609735489\n",
      "iteration 1154, dc_loss: 0.1440735012292862, tv_loss: 0.031560514122247696\n",
      "iteration 1155, dc_loss: 0.14411082863807678, tv_loss: 0.03140292689204216\n",
      "iteration 1156, dc_loss: 0.14377547800540924, tv_loss: 0.03155064955353737\n",
      "iteration 1157, dc_loss: 0.14377890527248383, tv_loss: 0.031495895236730576\n",
      "iteration 1158, dc_loss: 0.1435651034116745, tv_loss: 0.031524792313575745\n",
      "iteration 1159, dc_loss: 0.14349091053009033, tv_loss: 0.03149857372045517\n",
      "iteration 1160, dc_loss: 0.14337879419326782, tv_loss: 0.0314534567296505\n",
      "iteration 1161, dc_loss: 0.1432318091392517, tv_loss: 0.03157676011323929\n",
      "iteration 1162, dc_loss: 0.14324186742305756, tv_loss: 0.03143705427646637\n",
      "iteration 1163, dc_loss: 0.14306679368019104, tv_loss: 0.03159071505069733\n",
      "iteration 1164, dc_loss: 0.14308175444602966, tv_loss: 0.031426336616277695\n",
      "iteration 1165, dc_loss: 0.14285317063331604, tv_loss: 0.03159865736961365\n",
      "iteration 1166, dc_loss: 0.14283490180969238, tv_loss: 0.031464267522096634\n",
      "iteration 1167, dc_loss: 0.14265504479408264, tv_loss: 0.03157367929816246\n",
      "iteration 1168, dc_loss: 0.142637699842453, tv_loss: 0.0314481146633625\n",
      "iteration 1169, dc_loss: 0.14245298504829407, tv_loss: 0.03158634528517723\n",
      "iteration 1170, dc_loss: 0.14244309067726135, tv_loss: 0.031466610729694366\n",
      "iteration 1171, dc_loss: 0.14227569103240967, tv_loss: 0.03159172087907791\n",
      "iteration 1172, dc_loss: 0.1422928422689438, tv_loss: 0.03143622726202011\n",
      "iteration 1173, dc_loss: 0.14205493032932281, tv_loss: 0.03164249658584595\n",
      "iteration 1174, dc_loss: 0.1421002894639969, tv_loss: 0.03146300092339516\n",
      "iteration 1175, dc_loss: 0.14187774062156677, tv_loss: 0.03161320090293884\n",
      "iteration 1176, dc_loss: 0.14195115864276886, tv_loss: 0.03142965957522392\n",
      "iteration 1177, dc_loss: 0.14170153439044952, tv_loss: 0.031647831201553345\n",
      "iteration 1178, dc_loss: 0.14181852340698242, tv_loss: 0.03142917528748512\n",
      "iteration 1179, dc_loss: 0.1415843963623047, tv_loss: 0.031652119010686874\n",
      "iteration 1180, dc_loss: 0.14174333214759827, tv_loss: 0.031390778720378876\n",
      "iteration 1181, dc_loss: 0.14145462214946747, tv_loss: 0.031725089997053146\n",
      "iteration 1182, dc_loss: 0.14169032871723175, tv_loss: 0.03140058368444443\n",
      "iteration 1183, dc_loss: 0.14133796095848083, tv_loss: 0.03171933442354202\n",
      "iteration 1184, dc_loss: 0.14159585535526276, tv_loss: 0.0313459075987339\n",
      "iteration 1185, dc_loss: 0.14111457765102386, tv_loss: 0.03175961971282959\n",
      "iteration 1186, dc_loss: 0.14125755429267883, tv_loss: 0.03138855844736099\n",
      "iteration 1187, dc_loss: 0.1408623456954956, tv_loss: 0.031678128987550735\n",
      "iteration 1188, dc_loss: 0.14086434245109558, tv_loss: 0.03144736588001251\n",
      "iteration 1189, dc_loss: 0.14060421288013458, tv_loss: 0.03163071721792221\n",
      "iteration 1190, dc_loss: 0.14056400954723358, tv_loss: 0.031555354595184326\n",
      "iteration 1191, dc_loss: 0.14055900275707245, tv_loss: 0.03153444081544876\n",
      "iteration 1192, dc_loss: 0.14039310812950134, tv_loss: 0.031602442264556885\n",
      "iteration 1193, dc_loss: 0.14041200280189514, tv_loss: 0.03155761584639549\n",
      "iteration 1194, dc_loss: 0.14022620022296906, tv_loss: 0.03162180259823799\n",
      "iteration 1195, dc_loss: 0.1401900053024292, tv_loss: 0.031553253531455994\n",
      "iteration 1196, dc_loss: 0.14001351594924927, tv_loss: 0.031576383858919144\n",
      "iteration 1197, dc_loss: 0.13992471992969513, tv_loss: 0.031579408794641495\n",
      "iteration 1198, dc_loss: 0.13977931439876556, tv_loss: 0.03159528970718384\n",
      "iteration 1199, dc_loss: 0.13972146809101105, tv_loss: 0.031598154455423355\n",
      "iteration 1200, dc_loss: 0.13964217901229858, tv_loss: 0.031557343900203705\n",
      "iteration 1201, dc_loss: 0.1395302563905716, tv_loss: 0.031631167978048325\n",
      "iteration 1202, dc_loss: 0.1394556611776352, tv_loss: 0.03158780559897423\n",
      "iteration 1203, dc_loss: 0.13936688005924225, tv_loss: 0.031545307487249374\n",
      "iteration 1204, dc_loss: 0.13932423293590546, tv_loss: 0.031566694378852844\n",
      "iteration 1205, dc_loss: 0.1392294019460678, tv_loss: 0.031520549207925797\n",
      "iteration 1206, dc_loss: 0.1391293704509735, tv_loss: 0.03157833591103554\n",
      "iteration 1207, dc_loss: 0.13909263908863068, tv_loss: 0.03150807321071625\n",
      "iteration 1208, dc_loss: 0.13903792202472687, tv_loss: 0.031509313732385635\n",
      "iteration 1209, dc_loss: 0.13892462849617004, tv_loss: 0.03154057264328003\n",
      "iteration 1210, dc_loss: 0.13885508477687836, tv_loss: 0.03152410313487053\n",
      "iteration 1211, dc_loss: 0.13880908489227295, tv_loss: 0.03153283894062042\n",
      "iteration 1212, dc_loss: 0.13873210549354553, tv_loss: 0.03149786219000816\n",
      "iteration 1213, dc_loss: 0.13867348432540894, tv_loss: 0.03153040632605553\n",
      "iteration 1214, dc_loss: 0.1385788768529892, tv_loss: 0.031514864414930344\n",
      "iteration 1215, dc_loss: 0.13851532340049744, tv_loss: 0.03153971955180168\n",
      "iteration 1216, dc_loss: 0.1384652853012085, tv_loss: 0.031504109501838684\n",
      "iteration 1217, dc_loss: 0.13836202025413513, tv_loss: 0.03153379261493683\n",
      "iteration 1218, dc_loss: 0.13829369843006134, tv_loss: 0.031556136906147\n",
      "iteration 1219, dc_loss: 0.13826459646224976, tv_loss: 0.03148828446865082\n",
      "iteration 1220, dc_loss: 0.138168603181839, tv_loss: 0.031539853662252426\n",
      "iteration 1221, dc_loss: 0.13809223473072052, tv_loss: 0.031534116715192795\n",
      "iteration 1222, dc_loss: 0.13804705440998077, tv_loss: 0.03150977939367294\n",
      "iteration 1223, dc_loss: 0.1379612386226654, tv_loss: 0.03154486417770386\n",
      "iteration 1224, dc_loss: 0.13788796961307526, tv_loss: 0.031531721353530884\n",
      "iteration 1225, dc_loss: 0.13783195614814758, tv_loss: 0.031531501561403275\n",
      "iteration 1226, dc_loss: 0.13776662945747375, tv_loss: 0.03153209760785103\n",
      "iteration 1227, dc_loss: 0.13769160211086273, tv_loss: 0.03153117746114731\n",
      "iteration 1228, dc_loss: 0.13762161135673523, tv_loss: 0.03153924271464348\n",
      "iteration 1229, dc_loss: 0.13756366074085236, tv_loss: 0.03153849393129349\n",
      "iteration 1230, dc_loss: 0.13748909533023834, tv_loss: 0.031530968844890594\n",
      "iteration 1231, dc_loss: 0.13741955161094666, tv_loss: 0.031542059034109116\n",
      "iteration 1232, dc_loss: 0.13736280798912048, tv_loss: 0.03153826668858528\n",
      "iteration 1233, dc_loss: 0.13728344440460205, tv_loss: 0.031540803611278534\n",
      "iteration 1234, dc_loss: 0.13722051680088043, tv_loss: 0.031540095806121826\n",
      "iteration 1235, dc_loss: 0.13716645538806915, tv_loss: 0.03153783082962036\n",
      "iteration 1236, dc_loss: 0.13707943260669708, tv_loss: 0.03155062347650528\n",
      "iteration 1237, dc_loss: 0.13701894879341125, tv_loss: 0.03154336288571358\n",
      "iteration 1238, dc_loss: 0.13696320354938507, tv_loss: 0.03154733404517174\n",
      "iteration 1239, dc_loss: 0.13688139617443085, tv_loss: 0.03155475854873657\n",
      "iteration 1240, dc_loss: 0.13683375716209412, tv_loss: 0.03152993693947792\n",
      "iteration 1241, dc_loss: 0.136759951710701, tv_loss: 0.031551893800497055\n",
      "iteration 1242, dc_loss: 0.13667501509189606, tv_loss: 0.03157094493508339\n",
      "iteration 1243, dc_loss: 0.1366395354270935, tv_loss: 0.031535275280475616\n",
      "iteration 1244, dc_loss: 0.13655857741832733, tv_loss: 0.03155054897069931\n",
      "iteration 1245, dc_loss: 0.13648945093154907, tv_loss: 0.031558338552713394\n",
      "iteration 1246, dc_loss: 0.13643980026245117, tv_loss: 0.03154619038105011\n",
      "iteration 1247, dc_loss: 0.13635388016700745, tv_loss: 0.031566448509693146\n",
      "iteration 1248, dc_loss: 0.13630318641662598, tv_loss: 0.031552109867334366\n",
      "iteration 1249, dc_loss: 0.13623575866222382, tv_loss: 0.03155643492937088\n",
      "iteration 1250, dc_loss: 0.13615906238555908, tv_loss: 0.031573906540870667\n",
      "iteration 1251, dc_loss: 0.13612115383148193, tv_loss: 0.03155166283249855\n",
      "iteration 1252, dc_loss: 0.13602380454540253, tv_loss: 0.03157968074083328\n",
      "iteration 1253, dc_loss: 0.13598878681659698, tv_loss: 0.03155042231082916\n",
      "iteration 1254, dc_loss: 0.13590089976787567, tv_loss: 0.03157541900873184\n",
      "iteration 1255, dc_loss: 0.13585031032562256, tv_loss: 0.031563784927129745\n",
      "iteration 1256, dc_loss: 0.1357806921005249, tv_loss: 0.031571291387081146\n",
      "iteration 1257, dc_loss: 0.13571743667125702, tv_loss: 0.03157351538538933\n",
      "iteration 1258, dc_loss: 0.13564811646938324, tv_loss: 0.031581901013851166\n",
      "iteration 1259, dc_loss: 0.1356021761894226, tv_loss: 0.03156613931059837\n",
      "iteration 1260, dc_loss: 0.13550959527492523, tv_loss: 0.03159608319401741\n",
      "iteration 1261, dc_loss: 0.13547293841838837, tv_loss: 0.031570736318826675\n",
      "iteration 1262, dc_loss: 0.13539253175258636, tv_loss: 0.03158898651599884\n",
      "iteration 1263, dc_loss: 0.13533949851989746, tv_loss: 0.03158232197165489\n",
      "iteration 1264, dc_loss: 0.1352708786725998, tv_loss: 0.03159358352422714\n",
      "iteration 1265, dc_loss: 0.13520507514476776, tv_loss: 0.031599145382642746\n",
      "iteration 1266, dc_loss: 0.13514791429042816, tv_loss: 0.0315948948264122\n",
      "iteration 1267, dc_loss: 0.13507001101970673, tv_loss: 0.03160751238465309\n",
      "iteration 1268, dc_loss: 0.13503246009349823, tv_loss: 0.03158370777964592\n",
      "iteration 1269, dc_loss: 0.13494521379470825, tv_loss: 0.03161131963133812\n",
      "iteration 1270, dc_loss: 0.13490059971809387, tv_loss: 0.031596217304468155\n",
      "iteration 1271, dc_loss: 0.13482125103473663, tv_loss: 0.03161276504397392\n",
      "iteration 1272, dc_loss: 0.13477720320224762, tv_loss: 0.03159669414162636\n",
      "iteration 1273, dc_loss: 0.1346973031759262, tv_loss: 0.03161453828215599\n",
      "iteration 1274, dc_loss: 0.13464853167533875, tv_loss: 0.031606048345565796\n",
      "iteration 1275, dc_loss: 0.1345723569393158, tv_loss: 0.03162367641925812\n",
      "iteration 1276, dc_loss: 0.13452281057834625, tv_loss: 0.031610891222953796\n",
      "iteration 1277, dc_loss: 0.13444945216178894, tv_loss: 0.031623244285583496\n",
      "iteration 1278, dc_loss: 0.13439908623695374, tv_loss: 0.03160737082362175\n",
      "iteration 1279, dc_loss: 0.13432903587818146, tv_loss: 0.03161712735891342\n",
      "iteration 1280, dc_loss: 0.13427335023880005, tv_loss: 0.03161788359284401\n",
      "iteration 1281, dc_loss: 0.13419897854328156, tv_loss: 0.03163870424032211\n",
      "iteration 1282, dc_loss: 0.1341531127691269, tv_loss: 0.03162362053990364\n",
      "iteration 1283, dc_loss: 0.1340802013874054, tv_loss: 0.03162772208452225\n",
      "iteration 1284, dc_loss: 0.13402923941612244, tv_loss: 0.031615495681762695\n",
      "iteration 1285, dc_loss: 0.13395152986049652, tv_loss: 0.03163357824087143\n",
      "iteration 1286, dc_loss: 0.1339104175567627, tv_loss: 0.031621839851140976\n",
      "iteration 1287, dc_loss: 0.13382723927497864, tv_loss: 0.03165290132164955\n",
      "iteration 1288, dc_loss: 0.13378718495368958, tv_loss: 0.031630512326955795\n",
      "iteration 1289, dc_loss: 0.1337069272994995, tv_loss: 0.0316447839140892\n",
      "iteration 1290, dc_loss: 0.13367141783237457, tv_loss: 0.03161798045039177\n",
      "iteration 1291, dc_loss: 0.13358213007450104, tv_loss: 0.03164883702993393\n",
      "iteration 1292, dc_loss: 0.13354536890983582, tv_loss: 0.0316307507455349\n",
      "iteration 1293, dc_loss: 0.13346178829669952, tv_loss: 0.031659726053476334\n",
      "iteration 1294, dc_loss: 0.13342900574207306, tv_loss: 0.03163589909672737\n",
      "iteration 1295, dc_loss: 0.13334280252456665, tv_loss: 0.03166057914495468\n",
      "iteration 1296, dc_loss: 0.13331197202205658, tv_loss: 0.03163012117147446\n",
      "iteration 1297, dc_loss: 0.13322219252586365, tv_loss: 0.03166550025343895\n",
      "iteration 1298, dc_loss: 0.13320095837116241, tv_loss: 0.03163450211286545\n",
      "iteration 1299, dc_loss: 0.13310091197490692, tv_loss: 0.03168407082557678\n",
      "iteration 1300, dc_loss: 0.13309268653392792, tv_loss: 0.03162793815135956\n",
      "iteration 1301, dc_loss: 0.13299089670181274, tv_loss: 0.031678877770900726\n",
      "iteration 1302, dc_loss: 0.1329893171787262, tv_loss: 0.031622085720300674\n",
      "iteration 1303, dc_loss: 0.13285814225673676, tv_loss: 0.03170594945549965\n",
      "iteration 1304, dc_loss: 0.13287432491779327, tv_loss: 0.0316283255815506\n",
      "iteration 1305, dc_loss: 0.13273698091506958, tv_loss: 0.031699299812316895\n",
      "iteration 1306, dc_loss: 0.1327345073223114, tv_loss: 0.03163254261016846\n",
      "iteration 1307, dc_loss: 0.13262443244457245, tv_loss: 0.03168574348092079\n",
      "iteration 1308, dc_loss: 0.13260428607463837, tv_loss: 0.031652405858039856\n",
      "iteration 1309, dc_loss: 0.13250522315502167, tv_loss: 0.031692806631326675\n",
      "iteration 1310, dc_loss: 0.13247299194335938, tv_loss: 0.03166402131319046\n",
      "iteration 1311, dc_loss: 0.1323908418416977, tv_loss: 0.03168368712067604\n",
      "iteration 1312, dc_loss: 0.13235490024089813, tv_loss: 0.03166040778160095\n",
      "iteration 1313, dc_loss: 0.13227230310440063, tv_loss: 0.03168150410056114\n",
      "iteration 1314, dc_loss: 0.13222813606262207, tv_loss: 0.03167420253157616\n",
      "iteration 1315, dc_loss: 0.13216102123260498, tv_loss: 0.031694553792476654\n",
      "iteration 1316, dc_loss: 0.13212038576602936, tv_loss: 0.031676966696977615\n",
      "iteration 1317, dc_loss: 0.1320469230413437, tv_loss: 0.03168432042002678\n",
      "iteration 1318, dc_loss: 0.13199393451213837, tv_loss: 0.031679145991802216\n",
      "iteration 1319, dc_loss: 0.1319354623556137, tv_loss: 0.03168559819459915\n",
      "iteration 1320, dc_loss: 0.13187623023986816, tv_loss: 0.031691208481788635\n",
      "iteration 1321, dc_loss: 0.13183896243572235, tv_loss: 0.031684309244155884\n",
      "iteration 1322, dc_loss: 0.13176028430461884, tv_loss: 0.03171626478433609\n",
      "iteration 1323, dc_loss: 0.13174766302108765, tv_loss: 0.03167225793004036\n",
      "iteration 1324, dc_loss: 0.1316504180431366, tv_loss: 0.0317254401743412\n",
      "iteration 1325, dc_loss: 0.1316564381122589, tv_loss: 0.031660642474889755\n",
      "iteration 1326, dc_loss: 0.13153886795043945, tv_loss: 0.03174285963177681\n",
      "iteration 1327, dc_loss: 0.13157488405704498, tv_loss: 0.03164875507354736\n",
      "iteration 1328, dc_loss: 0.13141269981861115, tv_loss: 0.031765833497047424\n",
      "iteration 1329, dc_loss: 0.1314815729856491, tv_loss: 0.031635582447052\n",
      "iteration 1330, dc_loss: 0.1312911957502365, tv_loss: 0.03179062530398369\n",
      "iteration 1331, dc_loss: 0.13138827681541443, tv_loss: 0.031626101583242416\n",
      "iteration 1332, dc_loss: 0.1311747133731842, tv_loss: 0.03177982568740845\n",
      "iteration 1333, dc_loss: 0.13126584887504578, tv_loss: 0.03162171319127083\n",
      "iteration 1334, dc_loss: 0.13105782866477966, tv_loss: 0.031779706478118896\n",
      "iteration 1335, dc_loss: 0.13111986219882965, tv_loss: 0.031661953777074814\n",
      "iteration 1336, dc_loss: 0.13094472885131836, tv_loss: 0.03176609054207802\n",
      "iteration 1337, dc_loss: 0.13096974790096283, tv_loss: 0.03166915476322174\n",
      "iteration 1338, dc_loss: 0.13084061443805695, tv_loss: 0.031741201877593994\n",
      "iteration 1339, dc_loss: 0.13084492087364197, tv_loss: 0.031684406101703644\n",
      "iteration 1340, dc_loss: 0.13072672486305237, tv_loss: 0.031753089278936386\n",
      "iteration 1341, dc_loss: 0.13070929050445557, tv_loss: 0.03170308843255043\n",
      "iteration 1342, dc_loss: 0.13061396777629852, tv_loss: 0.03173868730664253\n",
      "iteration 1343, dc_loss: 0.13061173260211945, tv_loss: 0.03168906271457672\n",
      "iteration 1344, dc_loss: 0.13049939274787903, tv_loss: 0.031753309071063995\n",
      "iteration 1345, dc_loss: 0.13049177825450897, tv_loss: 0.031706374138593674\n",
      "iteration 1346, dc_loss: 0.13038982450962067, tv_loss: 0.03175776079297066\n",
      "iteration 1347, dc_loss: 0.13038718700408936, tv_loss: 0.03169848397374153\n",
      "iteration 1348, dc_loss: 0.130281463265419, tv_loss: 0.0317554734647274\n",
      "iteration 1349, dc_loss: 0.1302730292081833, tv_loss: 0.031714845448732376\n",
      "iteration 1350, dc_loss: 0.13018381595611572, tv_loss: 0.03176445886492729\n",
      "iteration 1351, dc_loss: 0.1301804929971695, tv_loss: 0.031714167445898056\n",
      "iteration 1352, dc_loss: 0.1300724595785141, tv_loss: 0.03177587315440178\n",
      "iteration 1353, dc_loss: 0.13008618354797363, tv_loss: 0.03171071037650108\n",
      "iteration 1354, dc_loss: 0.1299680769443512, tv_loss: 0.031789589673280716\n",
      "iteration 1355, dc_loss: 0.12998777627944946, tv_loss: 0.03171490505337715\n",
      "iteration 1356, dc_loss: 0.12985797226428986, tv_loss: 0.03179246187210083\n",
      "iteration 1357, dc_loss: 0.12987610697746277, tv_loss: 0.03171126917004585\n",
      "iteration 1358, dc_loss: 0.12974931299686432, tv_loss: 0.03178945928812027\n",
      "iteration 1359, dc_loss: 0.12978968024253845, tv_loss: 0.03168606385588646\n",
      "iteration 1360, dc_loss: 0.12963458895683289, tv_loss: 0.03179730474948883\n",
      "iteration 1361, dc_loss: 0.1296779215335846, tv_loss: 0.03170895203948021\n",
      "iteration 1362, dc_loss: 0.12952548265457153, tv_loss: 0.03182642534375191\n",
      "iteration 1363, dc_loss: 0.1295861005783081, tv_loss: 0.0316927544772625\n",
      "iteration 1364, dc_loss: 0.12941999733448029, tv_loss: 0.03180255368351936\n",
      "iteration 1365, dc_loss: 0.12946076691150665, tv_loss: 0.03170478716492653\n",
      "iteration 1366, dc_loss: 0.12931610643863678, tv_loss: 0.03180946037173271\n",
      "iteration 1367, dc_loss: 0.129372239112854, tv_loss: 0.03169927000999451\n",
      "iteration 1368, dc_loss: 0.12920428812503815, tv_loss: 0.0318312793970108\n",
      "iteration 1369, dc_loss: 0.1292569786310196, tv_loss: 0.031712163239717484\n",
      "iteration 1370, dc_loss: 0.12910692393779755, tv_loss: 0.031819868832826614\n",
      "iteration 1371, dc_loss: 0.1291780173778534, tv_loss: 0.0316917784512043\n",
      "iteration 1372, dc_loss: 0.12899470329284668, tv_loss: 0.031840503215789795\n",
      "iteration 1373, dc_loss: 0.1290818750858307, tv_loss: 0.031704094260931015\n",
      "iteration 1374, dc_loss: 0.1288980394601822, tv_loss: 0.03186464309692383\n",
      "iteration 1375, dc_loss: 0.1290104240179062, tv_loss: 0.03170722350478172\n",
      "iteration 1376, dc_loss: 0.12879164516925812, tv_loss: 0.03186938539147377\n",
      "iteration 1377, dc_loss: 0.12889336049556732, tv_loss: 0.03170839324593544\n",
      "iteration 1378, dc_loss: 0.12871867418289185, tv_loss: 0.031849075108766556\n",
      "iteration 1379, dc_loss: 0.12883524596691132, tv_loss: 0.03170052543282509\n",
      "iteration 1380, dc_loss: 0.12861180305480957, tv_loss: 0.03186332806944847\n",
      "iteration 1381, dc_loss: 0.12879468500614166, tv_loss: 0.03170716390013695\n",
      "iteration 1382, dc_loss: 0.12863443791866302, tv_loss: 0.031876422464847565\n",
      "iteration 1383, dc_loss: 0.12890945374965668, tv_loss: 0.03167429566383362\n",
      "iteration 1384, dc_loss: 0.1286364644765854, tv_loss: 0.031922414898872375\n",
      "iteration 1385, dc_loss: 0.12885668873786926, tv_loss: 0.031655147671699524\n",
      "iteration 1386, dc_loss: 0.12844489514827728, tv_loss: 0.031945474445819855\n",
      "iteration 1387, dc_loss: 0.12862426042556763, tv_loss: 0.031673725694417953\n",
      "iteration 1388, dc_loss: 0.12828287482261658, tv_loss: 0.03189477697014809\n",
      "iteration 1389, dc_loss: 0.1282796710729599, tv_loss: 0.03176886588335037\n",
      "iteration 1390, dc_loss: 0.12818893790245056, tv_loss: 0.031798105686903\n",
      "iteration 1391, dc_loss: 0.12811151146888733, tv_loss: 0.031814273446798325\n",
      "iteration 1392, dc_loss: 0.12814611196517944, tv_loss: 0.03175297752022743\n",
      "iteration 1393, dc_loss: 0.12796543538570404, tv_loss: 0.03189406916499138\n",
      "iteration 1394, dc_loss: 0.12810485064983368, tv_loss: 0.031711723655462265\n",
      "iteration 1395, dc_loss: 0.1278647929430008, tv_loss: 0.0318942666053772\n",
      "iteration 1396, dc_loss: 0.1279231309890747, tv_loss: 0.0317465141415596\n",
      "iteration 1397, dc_loss: 0.12776675820350647, tv_loss: 0.03182891756296158\n",
      "iteration 1398, dc_loss: 0.12771625816822052, tv_loss: 0.03180159628391266\n",
      "iteration 1399, dc_loss: 0.1276703029870987, tv_loss: 0.03180483356118202\n",
      "iteration 1400, dc_loss: 0.12759508192539215, tv_loss: 0.03182678297162056\n",
      "iteration 1401, dc_loss: 0.12762577831745148, tv_loss: 0.03175465017557144\n",
      "iteration 1402, dc_loss: 0.12746810913085938, tv_loss: 0.03187352418899536\n",
      "iteration 1403, dc_loss: 0.12751330435276031, tv_loss: 0.03177435323596001\n",
      "iteration 1404, dc_loss: 0.12735851109027863, tv_loss: 0.031874023377895355\n",
      "iteration 1405, dc_loss: 0.12738534808158875, tv_loss: 0.03177710622549057\n",
      "iteration 1406, dc_loss: 0.12726373970508575, tv_loss: 0.03184002265334129\n",
      "iteration 1407, dc_loss: 0.12724138796329498, tv_loss: 0.03181082382798195\n",
      "iteration 1408, dc_loss: 0.1271623969078064, tv_loss: 0.03184245899319649\n",
      "iteration 1409, dc_loss: 0.1271117776632309, tv_loss: 0.03184103965759277\n",
      "iteration 1410, dc_loss: 0.1270957589149475, tv_loss: 0.031807903200387955\n",
      "iteration 1411, dc_loss: 0.127004012465477, tv_loss: 0.03184875473380089\n",
      "iteration 1412, dc_loss: 0.12700004875659943, tv_loss: 0.03180130571126938\n",
      "iteration 1413, dc_loss: 0.1268882155418396, tv_loss: 0.03186136111617088\n",
      "iteration 1414, dc_loss: 0.12691256403923035, tv_loss: 0.03179585933685303\n",
      "iteration 1415, dc_loss: 0.1267888993024826, tv_loss: 0.03187514469027519\n",
      "iteration 1416, dc_loss: 0.1268114596605301, tv_loss: 0.03179076686501503\n",
      "iteration 1417, dc_loss: 0.12668301165103912, tv_loss: 0.03187655285000801\n",
      "iteration 1418, dc_loss: 0.12672457098960876, tv_loss: 0.03179209679365158\n",
      "iteration 1419, dc_loss: 0.12658417224884033, tv_loss: 0.03190147504210472\n",
      "iteration 1420, dc_loss: 0.12662561237812042, tv_loss: 0.031799253076314926\n",
      "iteration 1421, dc_loss: 0.12650041282176971, tv_loss: 0.03188157081604004\n",
      "iteration 1422, dc_loss: 0.12651818990707397, tv_loss: 0.03181980922818184\n",
      "iteration 1423, dc_loss: 0.1263931542634964, tv_loss: 0.031897444278001785\n",
      "iteration 1424, dc_loss: 0.126456156373024, tv_loss: 0.03178989887237549\n",
      "iteration 1425, dc_loss: 0.12628905475139618, tv_loss: 0.03192108869552612\n",
      "iteration 1426, dc_loss: 0.12635931372642517, tv_loss: 0.03179607167840004\n",
      "iteration 1427, dc_loss: 0.12619778513908386, tv_loss: 0.03191033750772476\n",
      "iteration 1428, dc_loss: 0.12626172602176666, tv_loss: 0.031803395599126816\n",
      "iteration 1429, dc_loss: 0.1261134296655655, tv_loss: 0.03190293163061142\n",
      "iteration 1430, dc_loss: 0.126181498169899, tv_loss: 0.0317930206656456\n",
      "iteration 1431, dc_loss: 0.12599022686481476, tv_loss: 0.03195470944046974\n",
      "iteration 1432, dc_loss: 0.1261049211025238, tv_loss: 0.03178888559341431\n",
      "iteration 1433, dc_loss: 0.12592047452926636, tv_loss: 0.03193584829568863\n",
      "iteration 1434, dc_loss: 0.12602223455905914, tv_loss: 0.03177877888083458\n",
      "iteration 1435, dc_loss: 0.125822514295578, tv_loss: 0.03193755820393562\n",
      "iteration 1436, dc_loss: 0.12592938542366028, tv_loss: 0.03179053217172623\n",
      "iteration 1437, dc_loss: 0.12572842836380005, tv_loss: 0.03196470066905022\n",
      "iteration 1438, dc_loss: 0.125871941447258, tv_loss: 0.03177798539400101\n",
      "iteration 1439, dc_loss: 0.12565740942955017, tv_loss: 0.03195436671376228\n",
      "iteration 1440, dc_loss: 0.1257733404636383, tv_loss: 0.03179435804486275\n",
      "iteration 1441, dc_loss: 0.1255834549665451, tv_loss: 0.031959351152181625\n",
      "iteration 1442, dc_loss: 0.1256815642118454, tv_loss: 0.03179698437452316\n",
      "iteration 1443, dc_loss: 0.12547896802425385, tv_loss: 0.031954631209373474\n",
      "iteration 1444, dc_loss: 0.12555930018424988, tv_loss: 0.03180622681975365\n",
      "iteration 1445, dc_loss: 0.12536479532718658, tv_loss: 0.03195101395249367\n",
      "iteration 1446, dc_loss: 0.12542608380317688, tv_loss: 0.03182671591639519\n",
      "iteration 1447, dc_loss: 0.12525032460689545, tv_loss: 0.03193572163581848\n",
      "iteration 1448, dc_loss: 0.12531110644340515, tv_loss: 0.0318414568901062\n",
      "iteration 1449, dc_loss: 0.1251554787158966, tv_loss: 0.03194154426455498\n",
      "iteration 1450, dc_loss: 0.12520600855350494, tv_loss: 0.03184638172388077\n",
      "iteration 1451, dc_loss: 0.12505482137203217, tv_loss: 0.03192567452788353\n",
      "iteration 1452, dc_loss: 0.12511524558067322, tv_loss: 0.031855978071689606\n",
      "iteration 1453, dc_loss: 0.12497968226671219, tv_loss: 0.03193489462137222\n",
      "iteration 1454, dc_loss: 0.1250612735748291, tv_loss: 0.03184223175048828\n",
      "iteration 1455, dc_loss: 0.12488792836666107, tv_loss: 0.03197381645441055\n",
      "iteration 1456, dc_loss: 0.12501858174800873, tv_loss: 0.03183463215827942\n",
      "iteration 1457, dc_loss: 0.12483371049165726, tv_loss: 0.031984277069568634\n",
      "iteration 1458, dc_loss: 0.12499606609344482, tv_loss: 0.031796153634786606\n",
      "iteration 1459, dc_loss: 0.12473795562982559, tv_loss: 0.03201449289917946\n",
      "iteration 1460, dc_loss: 0.12493179738521576, tv_loss: 0.03178107738494873\n",
      "iteration 1461, dc_loss: 0.12464062869548798, tv_loss: 0.03202851116657257\n",
      "iteration 1462, dc_loss: 0.12481953948736191, tv_loss: 0.03178649768233299\n",
      "iteration 1463, dc_loss: 0.12452532351016998, tv_loss: 0.03202633932232857\n",
      "iteration 1464, dc_loss: 0.12466287612915039, tv_loss: 0.03181827813386917\n",
      "iteration 1465, dc_loss: 0.12442762404680252, tv_loss: 0.03198336064815521\n",
      "iteration 1466, dc_loss: 0.12448287755250931, tv_loss: 0.031859420239925385\n",
      "iteration 1467, dc_loss: 0.12431301921606064, tv_loss: 0.03196226432919502\n",
      "iteration 1468, dc_loss: 0.12433207780122757, tv_loss: 0.03188510984182358\n",
      "iteration 1469, dc_loss: 0.12424244731664658, tv_loss: 0.031937263906002045\n",
      "iteration 1470, dc_loss: 0.1242070123553276, tv_loss: 0.031913332641124725\n",
      "iteration 1471, dc_loss: 0.12418998032808304, tv_loss: 0.03188407048583031\n",
      "iteration 1472, dc_loss: 0.12407650798559189, tv_loss: 0.03197290375828743\n",
      "iteration 1473, dc_loss: 0.12412811070680618, tv_loss: 0.031883303076028824\n",
      "iteration 1474, dc_loss: 0.12397797405719757, tv_loss: 0.031976085156202316\n",
      "iteration 1475, dc_loss: 0.1240740641951561, tv_loss: 0.03185863420367241\n",
      "iteration 1476, dc_loss: 0.12388158589601517, tv_loss: 0.03199344873428345\n",
      "iteration 1477, dc_loss: 0.12397141754627228, tv_loss: 0.03185562416911125\n",
      "iteration 1478, dc_loss: 0.12381193041801453, tv_loss: 0.03198520466685295\n",
      "iteration 1479, dc_loss: 0.123845174908638, tv_loss: 0.03189903125166893\n",
      "iteration 1480, dc_loss: 0.12371253222227097, tv_loss: 0.03196662664413452\n",
      "iteration 1481, dc_loss: 0.12377164512872696, tv_loss: 0.031883951276540756\n",
      "iteration 1482, dc_loss: 0.12364384531974792, tv_loss: 0.03197106346487999\n",
      "iteration 1483, dc_loss: 0.12366168946027756, tv_loss: 0.03190502151846886\n",
      "iteration 1484, dc_loss: 0.1235794872045517, tv_loss: 0.03195277974009514\n",
      "iteration 1485, dc_loss: 0.12359212338924408, tv_loss: 0.03190737962722778\n",
      "iteration 1486, dc_loss: 0.1234765499830246, tv_loss: 0.03200157731771469\n",
      "iteration 1487, dc_loss: 0.12355928122997284, tv_loss: 0.03187768906354904\n",
      "iteration 1488, dc_loss: 0.12341102957725525, tv_loss: 0.03199271112680435\n",
      "iteration 1489, dc_loss: 0.12349660694599152, tv_loss: 0.03187426179647446\n",
      "iteration 1490, dc_loss: 0.12336255609989166, tv_loss: 0.032003723084926605\n",
      "iteration 1491, dc_loss: 0.12344317883253098, tv_loss: 0.03187686949968338\n",
      "iteration 1492, dc_loss: 0.1232498437166214, tv_loss: 0.03204498440027237\n",
      "iteration 1493, dc_loss: 0.12342014163732529, tv_loss: 0.0318392850458622\n",
      "iteration 1494, dc_loss: 0.12318317592144012, tv_loss: 0.03204972296953201\n",
      "iteration 1495, dc_loss: 0.1233559399843216, tv_loss: 0.031835153698921204\n",
      "iteration 1496, dc_loss: 0.12306243926286697, tv_loss: 0.032087355852127075\n",
      "iteration 1497, dc_loss: 0.12328193336725235, tv_loss: 0.031820014119148254\n",
      "iteration 1498, dc_loss: 0.12297580391168594, tv_loss: 0.03208240866661072\n",
      "iteration 1499, dc_loss: 0.12318602204322815, tv_loss: 0.03184878081083298\n",
      "iteration 1500, dc_loss: 0.12288009375333786, tv_loss: 0.032051634043455124\n",
      "iteration 1501, dc_loss: 0.12299875169992447, tv_loss: 0.031872011721134186\n",
      "iteration 1502, dc_loss: 0.12281794100999832, tv_loss: 0.03200226649641991\n",
      "iteration 1503, dc_loss: 0.1228509396314621, tv_loss: 0.031920868903398514\n",
      "iteration 1504, dc_loss: 0.12274445593357086, tv_loss: 0.032009683549404144\n",
      "iteration 1505, dc_loss: 0.12275506556034088, tv_loss: 0.031950850039720535\n",
      "iteration 1506, dc_loss: 0.12265907227993011, tv_loss: 0.03199543058872223\n",
      "iteration 1507, dc_loss: 0.12261123210191727, tv_loss: 0.03195340186357498\n",
      "iteration 1508, dc_loss: 0.122556172311306, tv_loss: 0.03195852041244507\n",
      "iteration 1509, dc_loss: 0.12248099595308304, tv_loss: 0.031985726207494736\n",
      "iteration 1510, dc_loss: 0.12248940020799637, tv_loss: 0.03193933516740799\n",
      "iteration 1511, dc_loss: 0.12237673997879028, tv_loss: 0.032034698873758316\n",
      "iteration 1512, dc_loss: 0.12244842946529388, tv_loss: 0.03191768750548363\n",
      "iteration 1513, dc_loss: 0.12227676808834076, tv_loss: 0.032060109078884125\n",
      "iteration 1514, dc_loss: 0.12239498645067215, tv_loss: 0.031900156289339066\n",
      "iteration 1515, dc_loss: 0.12222356349229813, tv_loss: 0.032059766352176666\n",
      "iteration 1516, dc_loss: 0.12227418273687363, tv_loss: 0.03194069862365723\n",
      "iteration 1517, dc_loss: 0.12212061882019043, tv_loss: 0.03202047199010849\n",
      "iteration 1518, dc_loss: 0.12214765697717667, tv_loss: 0.03194978088140488\n",
      "iteration 1519, dc_loss: 0.1220279186964035, tv_loss: 0.032028887420892715\n",
      "iteration 1520, dc_loss: 0.12204264104366302, tv_loss: 0.03197984769940376\n",
      "iteration 1521, dc_loss: 0.12195570766925812, tv_loss: 0.03200218454003334\n",
      "iteration 1522, dc_loss: 0.12194162607192993, tv_loss: 0.03197092562913895\n",
      "iteration 1523, dc_loss: 0.12186678498983383, tv_loss: 0.03200671449303627\n",
      "iteration 1524, dc_loss: 0.12185637652873993, tv_loss: 0.031979359686374664\n",
      "iteration 1525, dc_loss: 0.12178909778594971, tv_loss: 0.03200848400592804\n",
      "iteration 1526, dc_loss: 0.12177661061286926, tv_loss: 0.03197575360536575\n",
      "iteration 1527, dc_loss: 0.12168861925601959, tv_loss: 0.03200765699148178\n",
      "iteration 1528, dc_loss: 0.12167444825172424, tv_loss: 0.03197851404547691\n",
      "iteration 1529, dc_loss: 0.12160276621580124, tv_loss: 0.032005708664655685\n",
      "iteration 1530, dc_loss: 0.12160413712263107, tv_loss: 0.031976744532585144\n",
      "iteration 1531, dc_loss: 0.12151020765304565, tv_loss: 0.03202510625123978\n",
      "iteration 1532, dc_loss: 0.12155834585428238, tv_loss: 0.03194769471883774\n",
      "iteration 1533, dc_loss: 0.12142205983400345, tv_loss: 0.03206484392285347\n",
      "iteration 1534, dc_loss: 0.1214994564652443, tv_loss: 0.03195464611053467\n",
      "iteration 1535, dc_loss: 0.12136059999465942, tv_loss: 0.0320645235478878\n",
      "iteration 1536, dc_loss: 0.12148793786764145, tv_loss: 0.03192640841007233\n",
      "iteration 1537, dc_loss: 0.12132950127124786, tv_loss: 0.032093629240989685\n",
      "iteration 1538, dc_loss: 0.1215582862496376, tv_loss: 0.03189125284552574\n",
      "iteration 1539, dc_loss: 0.12135269492864609, tv_loss: 0.03218276426196098\n",
      "iteration 1540, dc_loss: 0.12166823446750641, tv_loss: 0.03185388073325157\n",
      "iteration 1541, dc_loss: 0.12142561376094818, tv_loss: 0.03220191225409508\n",
      "iteration 1542, dc_loss: 0.12171874195337296, tv_loss: 0.031833015382289886\n",
      "iteration 1543, dc_loss: 0.12128189951181412, tv_loss: 0.03221907839179039\n",
      "iteration 1544, dc_loss: 0.12144444137811661, tv_loss: 0.031849365681409836\n",
      "iteration 1545, dc_loss: 0.12100523710250854, tv_loss: 0.032132260501384735\n",
      "iteration 1546, dc_loss: 0.12105986475944519, tv_loss: 0.03194141015410423\n",
      "iteration 1547, dc_loss: 0.12088612467050552, tv_loss: 0.03204748407006264\n",
      "iteration 1548, dc_loss: 0.12087991088628769, tv_loss: 0.03204195201396942\n",
      "iteration 1549, dc_loss: 0.1209859848022461, tv_loss: 0.031945254653692245\n",
      "iteration 1550, dc_loss: 0.12083060294389725, tv_loss: 0.03212140500545502\n",
      "iteration 1551, dc_loss: 0.12098128348588943, tv_loss: 0.0319090373814106\n",
      "iteration 1552, dc_loss: 0.12070753425359726, tv_loss: 0.03212588280439377\n",
      "iteration 1553, dc_loss: 0.12080999463796616, tv_loss: 0.0319298580288887\n",
      "iteration 1554, dc_loss: 0.12058541923761368, tv_loss: 0.03209303691983223\n",
      "iteration 1555, dc_loss: 0.12065248936414719, tv_loss: 0.03198311850428581\n",
      "iteration 1556, dc_loss: 0.12053809314966202, tv_loss: 0.03206567093729973\n",
      "iteration 1557, dc_loss: 0.12051725387573242, tv_loss: 0.03204300254583359\n",
      "iteration 1558, dc_loss: 0.12052465230226517, tv_loss: 0.03199123218655586\n",
      "iteration 1559, dc_loss: 0.12041543424129486, tv_loss: 0.032059092074632645\n",
      "iteration 1560, dc_loss: 0.12044057995080948, tv_loss: 0.03198673576116562\n",
      "iteration 1561, dc_loss: 0.12029974162578583, tv_loss: 0.03208692744374275\n",
      "iteration 1562, dc_loss: 0.12035111337900162, tv_loss: 0.031999580562114716\n",
      "iteration 1563, dc_loss: 0.12022886425256729, tv_loss: 0.0320817194879055\n",
      "iteration 1564, dc_loss: 0.12026536464691162, tv_loss: 0.031990714371204376\n",
      "iteration 1565, dc_loss: 0.12015897035598755, tv_loss: 0.032055191695690155\n",
      "iteration 1566, dc_loss: 0.12014047801494598, tv_loss: 0.032034799456596375\n",
      "iteration 1567, dc_loss: 0.12010790407657623, tv_loss: 0.032029882073402405\n",
      "iteration 1568, dc_loss: 0.12005843222141266, tv_loss: 0.03205917775630951\n",
      "iteration 1569, dc_loss: 0.12002250552177429, tv_loss: 0.032047759741544724\n",
      "iteration 1570, dc_loss: 0.11994554847478867, tv_loss: 0.032074954360723495\n",
      "iteration 1571, dc_loss: 0.1199287548661232, tv_loss: 0.032031893730163574\n",
      "iteration 1572, dc_loss: 0.11987526714801788, tv_loss: 0.03204378858208656\n",
      "iteration 1573, dc_loss: 0.1198631152510643, tv_loss: 0.03203262761235237\n",
      "iteration 1574, dc_loss: 0.11979010701179504, tv_loss: 0.03209519013762474\n",
      "iteration 1575, dc_loss: 0.11978597939014435, tv_loss: 0.03204070404171944\n",
      "iteration 1576, dc_loss: 0.11972149461507797, tv_loss: 0.03206159546971321\n",
      "iteration 1577, dc_loss: 0.11971535533666611, tv_loss: 0.032038431614637375\n",
      "iteration 1578, dc_loss: 0.11963111162185669, tv_loss: 0.0320899598300457\n",
      "iteration 1579, dc_loss: 0.11962538957595825, tv_loss: 0.03206044062972069\n",
      "iteration 1580, dc_loss: 0.11956482380628586, tv_loss: 0.03206348419189453\n",
      "iteration 1581, dc_loss: 0.11953727155923843, tv_loss: 0.032063573598861694\n",
      "iteration 1582, dc_loss: 0.11947960406541824, tv_loss: 0.03207496926188469\n",
      "iteration 1583, dc_loss: 0.11948462575674057, tv_loss: 0.03204880654811859\n",
      "iteration 1584, dc_loss: 0.1194053590297699, tv_loss: 0.032088518142700195\n",
      "iteration 1585, dc_loss: 0.1194295883178711, tv_loss: 0.032027535140514374\n",
      "iteration 1586, dc_loss: 0.11933860182762146, tv_loss: 0.03210614249110222\n",
      "iteration 1587, dc_loss: 0.11941883713006973, tv_loss: 0.03200007975101471\n",
      "iteration 1588, dc_loss: 0.11927769333124161, tv_loss: 0.03216153755784035\n",
      "iteration 1589, dc_loss: 0.11942467838525772, tv_loss: 0.03200426325201988\n",
      "iteration 1590, dc_loss: 0.11929522454738617, tv_loss: 0.0321640707552433\n",
      "iteration 1591, dc_loss: 0.11952943354845047, tv_loss: 0.03194504976272583\n",
      "iteration 1592, dc_loss: 0.11932186782360077, tv_loss: 0.032232340425252914\n",
      "iteration 1593, dc_loss: 0.11964967101812363, tv_loss: 0.0319029837846756\n",
      "iteration 1594, dc_loss: 0.11929251998662949, tv_loss: 0.03230699151754379\n",
      "iteration 1595, dc_loss: 0.11964325606822968, tv_loss: 0.03185626491904259\n",
      "iteration 1596, dc_loss: 0.11913476139307022, tv_loss: 0.03228873386979103\n",
      "iteration 1597, dc_loss: 0.11937388777732849, tv_loss: 0.03189682960510254\n",
      "iteration 1598, dc_loss: 0.1189308762550354, tv_loss: 0.032197389751672745\n",
      "iteration 1599, dc_loss: 0.11895684152841568, tv_loss: 0.03202557936310768\n",
      "iteration 1600, dc_loss: 0.11886110156774521, tv_loss: 0.03205699473619461\n",
      "iteration 1601, dc_loss: 0.11874812096357346, tv_loss: 0.03216409310698509\n",
      "iteration 1602, dc_loss: 0.11888400465250015, tv_loss: 0.03198833018541336\n",
      "iteration 1603, dc_loss: 0.11871755123138428, tv_loss: 0.03206603229045868\n",
      "iteration 1604, dc_loss: 0.11866084486246109, tv_loss: 0.032128311693668365\n",
      "iteration 1605, dc_loss: 0.11877133697271347, tv_loss: 0.031993426382541656\n",
      "iteration 1606, dc_loss: 0.11860620230436325, tv_loss: 0.032090432941913605\n",
      "iteration 1607, dc_loss: 0.11856181174516678, tv_loss: 0.03212640807032585\n",
      "iteration 1608, dc_loss: 0.11866351217031479, tv_loss: 0.03200652077794075\n",
      "iteration 1609, dc_loss: 0.11851972341537476, tv_loss: 0.03209507465362549\n",
      "iteration 1610, dc_loss: 0.11847046762704849, tv_loss: 0.03210935369133949\n",
      "iteration 1611, dc_loss: 0.11855024844408035, tv_loss: 0.03201127052307129\n",
      "iteration 1612, dc_loss: 0.1184251457452774, tv_loss: 0.03209064528346062\n",
      "iteration 1613, dc_loss: 0.11839329451322556, tv_loss: 0.03208727389574051\n",
      "iteration 1614, dc_loss: 0.1184311956167221, tv_loss: 0.03203997761011124\n",
      "iteration 1615, dc_loss: 0.11833476275205612, tv_loss: 0.03211710974574089\n",
      "iteration 1616, dc_loss: 0.11832023411989212, tv_loss: 0.03209000453352928\n",
      "iteration 1617, dc_loss: 0.11831899732351303, tv_loss: 0.03205960988998413\n",
      "iteration 1618, dc_loss: 0.11823984980583191, tv_loss: 0.03211040794849396\n",
      "iteration 1619, dc_loss: 0.11824770271778107, tv_loss: 0.0320592001080513\n",
      "iteration 1620, dc_loss: 0.11821280419826508, tv_loss: 0.032069131731987\n",
      "iteration 1621, dc_loss: 0.11815148591995239, tv_loss: 0.03211551159620285\n",
      "iteration 1622, dc_loss: 0.11815588176250458, tv_loss: 0.0320749506354332\n",
      "iteration 1623, dc_loss: 0.11811815947294235, tv_loss: 0.03208602964878082\n",
      "iteration 1624, dc_loss: 0.1180654913187027, tv_loss: 0.032102592289447784\n",
      "iteration 1625, dc_loss: 0.11807253211736679, tv_loss: 0.03206523507833481\n",
      "iteration 1626, dc_loss: 0.11802099645137787, tv_loss: 0.03208300843834877\n",
      "iteration 1627, dc_loss: 0.11798158288002014, tv_loss: 0.03209509700536728\n",
      "iteration 1628, dc_loss: 0.11798155307769775, tv_loss: 0.03206750005483627\n",
      "iteration 1629, dc_loss: 0.11792679131031036, tv_loss: 0.03209413215517998\n",
      "iteration 1630, dc_loss: 0.11789858341217041, tv_loss: 0.03209913522005081\n",
      "iteration 1631, dc_loss: 0.1178898960351944, tv_loss: 0.0320969857275486\n",
      "iteration 1632, dc_loss: 0.11783906817436218, tv_loss: 0.03211307153105736\n",
      "iteration 1633, dc_loss: 0.11782269924879074, tv_loss: 0.03209015727043152\n",
      "iteration 1634, dc_loss: 0.11779429018497467, tv_loss: 0.032082609832286835\n",
      "iteration 1635, dc_loss: 0.11774709075689316, tv_loss: 0.032110221683979034\n",
      "iteration 1636, dc_loss: 0.11773697286844254, tv_loss: 0.032101474702358246\n",
      "iteration 1637, dc_loss: 0.11769381910562515, tv_loss: 0.03211481124162674\n",
      "iteration 1638, dc_loss: 0.11766593903303146, tv_loss: 0.03211572393774986\n",
      "iteration 1639, dc_loss: 0.1176556795835495, tv_loss: 0.03208489716053009\n",
      "iteration 1640, dc_loss: 0.1176123321056366, tv_loss: 0.03210156038403511\n",
      "iteration 1641, dc_loss: 0.11758453398942947, tv_loss: 0.032117802649736404\n",
      "iteration 1642, dc_loss: 0.1175532117486, tv_loss: 0.03212601691484451\n",
      "iteration 1643, dc_loss: 0.11752082407474518, tv_loss: 0.03210584446787834\n",
      "iteration 1644, dc_loss: 0.11749663949012756, tv_loss: 0.03211134672164917\n",
      "iteration 1645, dc_loss: 0.11747299879789352, tv_loss: 0.03210512176156044\n",
      "iteration 1646, dc_loss: 0.1174393892288208, tv_loss: 0.032122354954481125\n",
      "iteration 1647, dc_loss: 0.1174086332321167, tv_loss: 0.032117970287799835\n",
      "iteration 1648, dc_loss: 0.11738467216491699, tv_loss: 0.03210627660155296\n",
      "iteration 1649, dc_loss: 0.11735189706087112, tv_loss: 0.032107796519994736\n",
      "iteration 1650, dc_loss: 0.11732016503810883, tv_loss: 0.03212186321616173\n",
      "iteration 1651, dc_loss: 0.11730079352855682, tv_loss: 0.03211264684796333\n",
      "iteration 1652, dc_loss: 0.11726997047662735, tv_loss: 0.0321185365319252\n",
      "iteration 1653, dc_loss: 0.11723851412534714, tv_loss: 0.03211616724729538\n",
      "iteration 1654, dc_loss: 0.11720772832632065, tv_loss: 0.03211034834384918\n",
      "iteration 1655, dc_loss: 0.11718158423900604, tv_loss: 0.03210952505469322\n",
      "iteration 1656, dc_loss: 0.1171555146574974, tv_loss: 0.032109566032886505\n",
      "iteration 1657, dc_loss: 0.11712843924760818, tv_loss: 0.0321073941886425\n",
      "iteration 1658, dc_loss: 0.11709585040807724, tv_loss: 0.03211045637726784\n",
      "iteration 1659, dc_loss: 0.11707023531198502, tv_loss: 0.03211188316345215\n",
      "iteration 1660, dc_loss: 0.11703016608953476, tv_loss: 0.03212646767497063\n",
      "iteration 1661, dc_loss: 0.11701418459415436, tv_loss: 0.03211187198758125\n",
      "iteration 1662, dc_loss: 0.1169978454709053, tv_loss: 0.032098811119794846\n",
      "iteration 1663, dc_loss: 0.11694569885730743, tv_loss: 0.03212004154920578\n",
      "iteration 1664, dc_loss: 0.1169285699725151, tv_loss: 0.03211518004536629\n",
      "iteration 1665, dc_loss: 0.11690202355384827, tv_loss: 0.03213643655180931\n",
      "iteration 1666, dc_loss: 0.11686694622039795, tv_loss: 0.032130103558301926\n",
      "iteration 1667, dc_loss: 0.11684462428092957, tv_loss: 0.03213559463620186\n",
      "iteration 1668, dc_loss: 0.1168099045753479, tv_loss: 0.032128773629665375\n",
      "iteration 1669, dc_loss: 0.11679065227508545, tv_loss: 0.032120730727910995\n",
      "iteration 1670, dc_loss: 0.11676183342933655, tv_loss: 0.03212493658065796\n",
      "iteration 1671, dc_loss: 0.11672040820121765, tv_loss: 0.03215761110186577\n",
      "iteration 1672, dc_loss: 0.11670452356338501, tv_loss: 0.03213353455066681\n",
      "iteration 1673, dc_loss: 0.11667854338884354, tv_loss: 0.03212619945406914\n",
      "iteration 1674, dc_loss: 0.1166391521692276, tv_loss: 0.03214981406927109\n",
      "iteration 1675, dc_loss: 0.1166260614991188, tv_loss: 0.03213632479310036\n",
      "iteration 1676, dc_loss: 0.1165933832526207, tv_loss: 0.032140620052814484\n",
      "iteration 1677, dc_loss: 0.11655232310295105, tv_loss: 0.03215663507580757\n",
      "iteration 1678, dc_loss: 0.11654037982225418, tv_loss: 0.032129138708114624\n",
      "iteration 1679, dc_loss: 0.11650697886943817, tv_loss: 0.03215067461133003\n",
      "iteration 1680, dc_loss: 0.11647852510213852, tv_loss: 0.03215295076370239\n",
      "iteration 1681, dc_loss: 0.11645881086587906, tv_loss: 0.032131992280483246\n",
      "iteration 1682, dc_loss: 0.11642779409885406, tv_loss: 0.032135169953107834\n",
      "iteration 1683, dc_loss: 0.11639009416103363, tv_loss: 0.03217165917158127\n",
      "iteration 1684, dc_loss: 0.11635906249284744, tv_loss: 0.032155703753232956\n",
      "iteration 1685, dc_loss: 0.11634275317192078, tv_loss: 0.03214867040514946\n",
      "iteration 1686, dc_loss: 0.11632364243268967, tv_loss: 0.03214488923549652\n",
      "iteration 1687, dc_loss: 0.11628935486078262, tv_loss: 0.032157108187675476\n",
      "iteration 1688, dc_loss: 0.11626458168029785, tv_loss: 0.032150477170944214\n",
      "iteration 1689, dc_loss: 0.11622457951307297, tv_loss: 0.032163217663764954\n",
      "iteration 1690, dc_loss: 0.11619396507740021, tv_loss: 0.032156772911548615\n",
      "iteration 1691, dc_loss: 0.11618129909038544, tv_loss: 0.032160282135009766\n",
      "iteration 1692, dc_loss: 0.11614689230918884, tv_loss: 0.032150670886039734\n",
      "iteration 1693, dc_loss: 0.11612319201231003, tv_loss: 0.032146748155355453\n",
      "iteration 1694, dc_loss: 0.11610274016857147, tv_loss: 0.03214925527572632\n",
      "iteration 1695, dc_loss: 0.11605602502822876, tv_loss: 0.03218337148427963\n",
      "iteration 1696, dc_loss: 0.1160431057214737, tv_loss: 0.03215567395091057\n",
      "iteration 1697, dc_loss: 0.1160164326429367, tv_loss: 0.03214893862605095\n",
      "iteration 1698, dc_loss: 0.11597613245248795, tv_loss: 0.03217534348368645\n",
      "iteration 1699, dc_loss: 0.11596919596195221, tv_loss: 0.032144512981176376\n",
      "iteration 1700, dc_loss: 0.11593575030565262, tv_loss: 0.0321689210832119\n",
      "iteration 1701, dc_loss: 0.11589238047599792, tv_loss: 0.032177045941352844\n",
      "iteration 1702, dc_loss: 0.11587739735841751, tv_loss: 0.032154761254787445\n",
      "iteration 1703, dc_loss: 0.11585284024477005, tv_loss: 0.0321807824075222\n",
      "iteration 1704, dc_loss: 0.1158197745680809, tv_loss: 0.032162513583898544\n",
      "iteration 1705, dc_loss: 0.11579355597496033, tv_loss: 0.03216484561562538\n",
      "iteration 1706, dc_loss: 0.11578094959259033, tv_loss: 0.032166045159101486\n",
      "iteration 1707, dc_loss: 0.11573434621095657, tv_loss: 0.03217972069978714\n",
      "iteration 1708, dc_loss: 0.11569540947675705, tv_loss: 0.032190993428230286\n",
      "iteration 1709, dc_loss: 0.11570292711257935, tv_loss: 0.032154783606529236\n",
      "iteration 1710, dc_loss: 0.11566699296236038, tv_loss: 0.03216227889060974\n",
      "iteration 1711, dc_loss: 0.1156233698129654, tv_loss: 0.03218870982527733\n",
      "iteration 1712, dc_loss: 0.11560999602079391, tv_loss: 0.03216633200645447\n",
      "iteration 1713, dc_loss: 0.11557973176240921, tv_loss: 0.0321638360619545\n",
      "iteration 1714, dc_loss: 0.11555537581443787, tv_loss: 0.03217050060629845\n",
      "iteration 1715, dc_loss: 0.11552967131137848, tv_loss: 0.032165151089429855\n",
      "iteration 1716, dc_loss: 0.11549016088247299, tv_loss: 0.032182477414608\n",
      "iteration 1717, dc_loss: 0.11548247188329697, tv_loss: 0.03216314688324928\n",
      "iteration 1718, dc_loss: 0.11544211208820343, tv_loss: 0.03216826915740967\n",
      "iteration 1719, dc_loss: 0.11540795862674713, tv_loss: 0.0321822427213192\n",
      "iteration 1720, dc_loss: 0.11540526151657104, tv_loss: 0.03215983510017395\n",
      "iteration 1721, dc_loss: 0.1153656542301178, tv_loss: 0.03217281401157379\n",
      "iteration 1722, dc_loss: 0.11533092707395554, tv_loss: 0.032175853848457336\n",
      "iteration 1723, dc_loss: 0.11531248688697815, tv_loss: 0.03216820955276489\n",
      "iteration 1724, dc_loss: 0.11528806388378143, tv_loss: 0.032173238694667816\n",
      "iteration 1725, dc_loss: 0.11526165902614594, tv_loss: 0.03216969594359398\n",
      "iteration 1726, dc_loss: 0.11523003876209259, tv_loss: 0.032173555344343185\n",
      "iteration 1727, dc_loss: 0.1152065321803093, tv_loss: 0.03217492997646332\n",
      "iteration 1728, dc_loss: 0.11518436670303345, tv_loss: 0.03218187391757965\n",
      "iteration 1729, dc_loss: 0.11515283584594727, tv_loss: 0.032199595123529434\n",
      "iteration 1730, dc_loss: 0.1151147410273552, tv_loss: 0.03219832479953766\n",
      "iteration 1731, dc_loss: 0.11510536074638367, tv_loss: 0.032173555344343185\n",
      "iteration 1732, dc_loss: 0.11507860571146011, tv_loss: 0.032176390290260315\n",
      "iteration 1733, dc_loss: 0.11505357176065445, tv_loss: 0.03218061476945877\n",
      "iteration 1734, dc_loss: 0.1150163933634758, tv_loss: 0.032198283821344376\n",
      "iteration 1735, dc_loss: 0.11498726904392242, tv_loss: 0.03221546486020088\n",
      "iteration 1736, dc_loss: 0.1149810403585434, tv_loss: 0.032177574932575226\n",
      "iteration 1737, dc_loss: 0.1149427518248558, tv_loss: 0.03218863531947136\n",
      "iteration 1738, dc_loss: 0.11491077393293381, tv_loss: 0.032202377915382385\n",
      "iteration 1739, dc_loss: 0.11489729583263397, tv_loss: 0.03220447897911072\n",
      "iteration 1740, dc_loss: 0.11486589163541794, tv_loss: 0.032200396060943604\n",
      "iteration 1741, dc_loss: 0.1148306354880333, tv_loss: 0.0322018526494503\n",
      "iteration 1742, dc_loss: 0.11481256037950516, tv_loss: 0.032205980271101\n",
      "iteration 1743, dc_loss: 0.11480139940977097, tv_loss: 0.03219896927475929\n",
      "iteration 1744, dc_loss: 0.11475901305675507, tv_loss: 0.03219829872250557\n",
      "iteration 1745, dc_loss: 0.11472515761852264, tv_loss: 0.03221392259001732\n",
      "iteration 1746, dc_loss: 0.11470387876033783, tv_loss: 0.03220774233341217\n",
      "iteration 1747, dc_loss: 0.11469223350286484, tv_loss: 0.03220365568995476\n",
      "iteration 1748, dc_loss: 0.11466176807880402, tv_loss: 0.032189127057790756\n",
      "iteration 1749, dc_loss: 0.11463075876235962, tv_loss: 0.0322011336684227\n",
      "iteration 1750, dc_loss: 0.11459766328334808, tv_loss: 0.032233476638793945\n",
      "iteration 1751, dc_loss: 0.11458389461040497, tv_loss: 0.032195985317230225\n",
      "iteration 1752, dc_loss: 0.11455363035202026, tv_loss: 0.032207418233156204\n",
      "iteration 1753, dc_loss: 0.11452298611402512, tv_loss: 0.03221715986728668\n",
      "iteration 1754, dc_loss: 0.1144988015294075, tv_loss: 0.03220392391085625\n",
      "iteration 1755, dc_loss: 0.11448696255683899, tv_loss: 0.032215289771556854\n",
      "iteration 1756, dc_loss: 0.11445031315088272, tv_loss: 0.03219855576753616\n",
      "iteration 1757, dc_loss: 0.11441808938980103, tv_loss: 0.03222203254699707\n",
      "iteration 1758, dc_loss: 0.11441131681203842, tv_loss: 0.032213788479566574\n",
      "iteration 1759, dc_loss: 0.11436711996793747, tv_loss: 0.03221411630511284\n",
      "iteration 1760, dc_loss: 0.11434316635131836, tv_loss: 0.03221677616238594\n",
      "iteration 1761, dc_loss: 0.11432152986526489, tv_loss: 0.03223010525107384\n",
      "iteration 1762, dc_loss: 0.11431152373552322, tv_loss: 0.032205868512392044\n",
      "iteration 1763, dc_loss: 0.1142711341381073, tv_loss: 0.0322147011756897\n",
      "iteration 1764, dc_loss: 0.11423873901367188, tv_loss: 0.032225266098976135\n",
      "iteration 1765, dc_loss: 0.11421669274568558, tv_loss: 0.03223090618848801\n",
      "iteration 1766, dc_loss: 0.1142033264040947, tv_loss: 0.032227374613285065\n",
      "iteration 1767, dc_loss: 0.11417626589536667, tv_loss: 0.03220900520682335\n",
      "iteration 1768, dc_loss: 0.1141369566321373, tv_loss: 0.032232578843832016\n",
      "iteration 1769, dc_loss: 0.11411745846271515, tv_loss: 0.03223784267902374\n",
      "iteration 1770, dc_loss: 0.11409617960453033, tv_loss: 0.032229866832494736\n",
      "iteration 1771, dc_loss: 0.11407370865345001, tv_loss: 0.03221874311566353\n",
      "iteration 1772, dc_loss: 0.11404116451740265, tv_loss: 0.03225070238113403\n",
      "iteration 1773, dc_loss: 0.11401191353797913, tv_loss: 0.032240305095911026\n",
      "iteration 1774, dc_loss: 0.11399739980697632, tv_loss: 0.032224491238594055\n",
      "iteration 1775, dc_loss: 0.11397118866443634, tv_loss: 0.03224770352244377\n",
      "iteration 1776, dc_loss: 0.11393488943576813, tv_loss: 0.03225287050008774\n",
      "iteration 1777, dc_loss: 0.11393067985773087, tv_loss: 0.032229360193014145\n",
      "iteration 1778, dc_loss: 0.11388927698135376, tv_loss: 0.03223973512649536\n",
      "iteration 1779, dc_loss: 0.11386284977197647, tv_loss: 0.03226907551288605\n",
      "iteration 1780, dc_loss: 0.11383714526891708, tv_loss: 0.03223050385713577\n",
      "iteration 1781, dc_loss: 0.11383137851953506, tv_loss: 0.032241374254226685\n",
      "iteration 1782, dc_loss: 0.11378693580627441, tv_loss: 0.03225187212228775\n",
      "iteration 1783, dc_loss: 0.11377034336328506, tv_loss: 0.032232947647571564\n",
      "iteration 1784, dc_loss: 0.11374039947986603, tv_loss: 0.032246481627225876\n",
      "iteration 1785, dc_loss: 0.1137244924902916, tv_loss: 0.03224480152130127\n",
      "iteration 1786, dc_loss: 0.11368538439273834, tv_loss: 0.032259609550237656\n",
      "iteration 1787, dc_loss: 0.11368536204099655, tv_loss: 0.03221484273672104\n",
      "iteration 1788, dc_loss: 0.11364619433879852, tv_loss: 0.03225822001695633\n",
      "iteration 1789, dc_loss: 0.11364742368459702, tv_loss: 0.0322248637676239\n",
      "iteration 1790, dc_loss: 0.11358720809221268, tv_loss: 0.032253675162792206\n",
      "iteration 1791, dc_loss: 0.11361191421747208, tv_loss: 0.032220810651779175\n",
      "iteration 1792, dc_loss: 0.11355292052030563, tv_loss: 0.03228175640106201\n",
      "iteration 1793, dc_loss: 0.11358797550201416, tv_loss: 0.03221040964126587\n",
      "iteration 1794, dc_loss: 0.1135246753692627, tv_loss: 0.03226478397846222\n",
      "iteration 1795, dc_loss: 0.11357218772172928, tv_loss: 0.03219697251915932\n",
      "iteration 1796, dc_loss: 0.1134691834449768, tv_loss: 0.03230476379394531\n",
      "iteration 1797, dc_loss: 0.11352825164794922, tv_loss: 0.032224494963884354\n",
      "iteration 1798, dc_loss: 0.11341391503810883, tv_loss: 0.03228209540247917\n",
      "iteration 1799, dc_loss: 0.11344809830188751, tv_loss: 0.03220342472195625\n",
      "iteration 1800, dc_loss: 0.11335429549217224, tv_loss: 0.032290466129779816\n",
      "iteration 1801, dc_loss: 0.11337009072303772, tv_loss: 0.03222206234931946\n",
      "iteration 1802, dc_loss: 0.113289013504982, tv_loss: 0.032269034534692764\n",
      "iteration 1803, dc_loss: 0.11327923089265823, tv_loss: 0.032257676124572754\n",
      "iteration 1804, dc_loss: 0.11326955258846283, tv_loss: 0.03226019814610481\n",
      "iteration 1805, dc_loss: 0.11322514712810516, tv_loss: 0.03226732835173607\n",
      "iteration 1806, dc_loss: 0.11323628574609756, tv_loss: 0.032219842076301575\n",
      "iteration 1807, dc_loss: 0.11317914724349976, tv_loss: 0.032290395349264145\n",
      "iteration 1808, dc_loss: 0.11321523785591125, tv_loss: 0.03223100304603577\n",
      "iteration 1809, dc_loss: 0.11311338096857071, tv_loss: 0.032299868762493134\n",
      "iteration 1810, dc_loss: 0.11317801475524902, tv_loss: 0.032207198441028595\n",
      "iteration 1811, dc_loss: 0.11309615522623062, tv_loss: 0.03228001296520233\n",
      "iteration 1812, dc_loss: 0.11311080306768417, tv_loss: 0.0322260707616806\n",
      "iteration 1813, dc_loss: 0.11303433030843735, tv_loss: 0.03228797763586044\n",
      "iteration 1814, dc_loss: 0.11305848509073257, tv_loss: 0.03221410512924194\n",
      "iteration 1815, dc_loss: 0.11297378689050674, tv_loss: 0.03226882219314575\n",
      "iteration 1816, dc_loss: 0.11298154294490814, tv_loss: 0.032246120274066925\n",
      "iteration 1817, dc_loss: 0.1129252091050148, tv_loss: 0.03228488937020302\n",
      "iteration 1818, dc_loss: 0.11293574422597885, tv_loss: 0.03223167732357979\n",
      "iteration 1819, dc_loss: 0.11286834627389908, tv_loss: 0.032275982201099396\n",
      "iteration 1820, dc_loss: 0.11286803334951401, tv_loss: 0.03224823996424675\n",
      "iteration 1821, dc_loss: 0.11283589899539948, tv_loss: 0.032269664108753204\n",
      "iteration 1822, dc_loss: 0.11282949894666672, tv_loss: 0.03225865960121155\n",
      "iteration 1823, dc_loss: 0.11278585344552994, tv_loss: 0.03226161748170853\n",
      "iteration 1824, dc_loss: 0.112764373421669, tv_loss: 0.03225920721888542\n",
      "iteration 1825, dc_loss: 0.1127488762140274, tv_loss: 0.03226200118660927\n",
      "iteration 1826, dc_loss: 0.11272010207176208, tv_loss: 0.032284028828144073\n",
      "iteration 1827, dc_loss: 0.11270154267549515, tv_loss: 0.03225747123360634\n",
      "iteration 1828, dc_loss: 0.11266479641199112, tv_loss: 0.03227463737130165\n",
      "iteration 1829, dc_loss: 0.11266464740037918, tv_loss: 0.03225437179207802\n",
      "iteration 1830, dc_loss: 0.11261686682701111, tv_loss: 0.03227591887116432\n",
      "iteration 1831, dc_loss: 0.11262504756450653, tv_loss: 0.03226253017783165\n",
      "iteration 1832, dc_loss: 0.11255466192960739, tv_loss: 0.032280098646879196\n",
      "iteration 1833, dc_loss: 0.11256826668977737, tv_loss: 0.032264359295368195\n",
      "iteration 1834, dc_loss: 0.11252236366271973, tv_loss: 0.03229647874832153\n",
      "iteration 1835, dc_loss: 0.11253809928894043, tv_loss: 0.03224628046154976\n",
      "iteration 1836, dc_loss: 0.11246329545974731, tv_loss: 0.032297201454639435\n",
      "iteration 1837, dc_loss: 0.11250291764736176, tv_loss: 0.03224778175354004\n",
      "iteration 1838, dc_loss: 0.11243383586406708, tv_loss: 0.03231373056769371\n",
      "iteration 1839, dc_loss: 0.11250894516706467, tv_loss: 0.0322362519800663\n",
      "iteration 1840, dc_loss: 0.11242369562387466, tv_loss: 0.03232136368751526\n",
      "iteration 1841, dc_loss: 0.11252503097057343, tv_loss: 0.03221486136317253\n",
      "iteration 1842, dc_loss: 0.11243552714586258, tv_loss: 0.03234701231122017\n",
      "iteration 1843, dc_loss: 0.11259011179208755, tv_loss: 0.0321962833404541\n",
      "iteration 1844, dc_loss: 0.11241511255502701, tv_loss: 0.03238164260983467\n",
      "iteration 1845, dc_loss: 0.11257100850343704, tv_loss: 0.03217200189828873\n",
      "iteration 1846, dc_loss: 0.11232704669237137, tv_loss: 0.03236886486411095\n",
      "iteration 1847, dc_loss: 0.11244408786296844, tv_loss: 0.03219359740614891\n",
      "iteration 1848, dc_loss: 0.1122196763753891, tv_loss: 0.03235685080289841\n",
      "iteration 1849, dc_loss: 0.11229877173900604, tv_loss: 0.03221779316663742\n",
      "iteration 1850, dc_loss: 0.11217176914215088, tv_loss: 0.03229229897260666\n",
      "iteration 1851, dc_loss: 0.1121368482708931, tv_loss: 0.032290536910295486\n",
      "iteration 1852, dc_loss: 0.11219809204339981, tv_loss: 0.032220326364040375\n",
      "iteration 1853, dc_loss: 0.11207249015569687, tv_loss: 0.03234996274113655\n",
      "iteration 1854, dc_loss: 0.11221348494291306, tv_loss: 0.032201528549194336\n",
      "iteration 1855, dc_loss: 0.11203783750534058, tv_loss: 0.0323636420071125\n",
      "iteration 1856, dc_loss: 0.11215001344680786, tv_loss: 0.03220449388027191\n",
      "iteration 1857, dc_loss: 0.1119808778166771, tv_loss: 0.03233572468161583\n",
      "iteration 1858, dc_loss: 0.11199957877397537, tv_loss: 0.03225408494472504\n",
      "iteration 1859, dc_loss: 0.11195708811283112, tv_loss: 0.0322674922645092\n",
      "iteration 1860, dc_loss: 0.11190379410982132, tv_loss: 0.03230312094092369\n",
      "iteration 1861, dc_loss: 0.11194095015525818, tv_loss: 0.032252196222543716\n",
      "iteration 1862, dc_loss: 0.11185135692358017, tv_loss: 0.03232589364051819\n",
      "iteration 1863, dc_loss: 0.11190865933895111, tv_loss: 0.03224297985434532\n",
      "iteration 1864, dc_loss: 0.11180820316076279, tv_loss: 0.032321102917194366\n",
      "iteration 1865, dc_loss: 0.11184658110141754, tv_loss: 0.03226012736558914\n",
      "iteration 1866, dc_loss: 0.11179094016551971, tv_loss: 0.032291512936353683\n",
      "iteration 1867, dc_loss: 0.11177613586187363, tv_loss: 0.03229188546538353\n",
      "iteration 1868, dc_loss: 0.11175607144832611, tv_loss: 0.03229830786585808\n",
      "iteration 1869, dc_loss: 0.11172318458557129, tv_loss: 0.032302893698215485\n",
      "iteration 1870, dc_loss: 0.11169607192277908, tv_loss: 0.032284606248140335\n",
      "iteration 1871, dc_loss: 0.11165457963943481, tv_loss: 0.03229539468884468\n",
      "iteration 1872, dc_loss: 0.11164107918739319, tv_loss: 0.03228285536170006\n",
      "iteration 1873, dc_loss: 0.11161680519580841, tv_loss: 0.0322839580476284\n",
      "iteration 1874, dc_loss: 0.11158858984708786, tv_loss: 0.0323021374642849\n",
      "iteration 1875, dc_loss: 0.11159292608499527, tv_loss: 0.032284341752529144\n",
      "iteration 1876, dc_loss: 0.1115579679608345, tv_loss: 0.032315388321876526\n",
      "iteration 1877, dc_loss: 0.11152645945549011, tv_loss: 0.03230694308876991\n",
      "iteration 1878, dc_loss: 0.11149675399065018, tv_loss: 0.032308485358953476\n",
      "iteration 1879, dc_loss: 0.11149744689464569, tv_loss: 0.03228341415524483\n",
      "iteration 1880, dc_loss: 0.11146795004606247, tv_loss: 0.03229875490069389\n",
      "iteration 1881, dc_loss: 0.11142229288816452, tv_loss: 0.03233477473258972\n",
      "iteration 1882, dc_loss: 0.11144693195819855, tv_loss: 0.03228900209069252\n",
      "iteration 1883, dc_loss: 0.1113738864660263, tv_loss: 0.03233276307582855\n",
      "iteration 1884, dc_loss: 0.11140002310276031, tv_loss: 0.03227613493800163\n",
      "iteration 1885, dc_loss: 0.11133739352226257, tv_loss: 0.03232672065496445\n",
      "iteration 1886, dc_loss: 0.11136820912361145, tv_loss: 0.03228411450982094\n",
      "iteration 1887, dc_loss: 0.11128610372543335, tv_loss: 0.03234229236841202\n",
      "iteration 1888, dc_loss: 0.11130575835704803, tv_loss: 0.03229065239429474\n",
      "iteration 1889, dc_loss: 0.11127211153507233, tv_loss: 0.0323074534535408\n",
      "iteration 1890, dc_loss: 0.11126182973384857, tv_loss: 0.03230494633316994\n",
      "iteration 1891, dc_loss: 0.11122111976146698, tv_loss: 0.032324593514204025\n",
      "iteration 1892, dc_loss: 0.11121857166290283, tv_loss: 0.03231744468212128\n",
      "iteration 1893, dc_loss: 0.11117599159479141, tv_loss: 0.03232286497950554\n",
      "iteration 1894, dc_loss: 0.1111929789185524, tv_loss: 0.032283421605825424\n",
      "iteration 1895, dc_loss: 0.11111295968294144, tv_loss: 0.03235101327300072\n",
      "iteration 1896, dc_loss: 0.11116687208414078, tv_loss: 0.03227446600794792\n",
      "iteration 1897, dc_loss: 0.11107762902975082, tv_loss: 0.032371584326028824\n",
      "iteration 1898, dc_loss: 0.11118146032094955, tv_loss: 0.032253704965114594\n",
      "iteration 1899, dc_loss: 0.1110660582780838, tv_loss: 0.03238428384065628\n",
      "iteration 1900, dc_loss: 0.11122589558362961, tv_loss: 0.032230328768491745\n",
      "iteration 1901, dc_loss: 0.11109257489442825, tv_loss: 0.032427169382572174\n",
      "iteration 1902, dc_loss: 0.11127755790948868, tv_loss: 0.03221357986330986\n",
      "iteration 1903, dc_loss: 0.11107882857322693, tv_loss: 0.032426659017801285\n",
      "iteration 1904, dc_loss: 0.1112336814403534, tv_loss: 0.03220357373356819\n",
      "iteration 1905, dc_loss: 0.11096595227718353, tv_loss: 0.032425280660390854\n",
      "iteration 1906, dc_loss: 0.11109531670808792, tv_loss: 0.032209500670433044\n",
      "iteration 1907, dc_loss: 0.11085525900125504, tv_loss: 0.03240257501602173\n",
      "iteration 1908, dc_loss: 0.11094512045383453, tv_loss: 0.03225809335708618\n",
      "iteration 1909, dc_loss: 0.1108323410153389, tv_loss: 0.03234304487705231\n",
      "iteration 1910, dc_loss: 0.11082792282104492, tv_loss: 0.03231319785118103\n",
      "iteration 1911, dc_loss: 0.11084023118019104, tv_loss: 0.032289471477270126\n",
      "iteration 1912, dc_loss: 0.11073937267065048, tv_loss: 0.03239147737622261\n",
      "iteration 1913, dc_loss: 0.11081614345312119, tv_loss: 0.03229294344782829\n",
      "iteration 1914, dc_loss: 0.11069026589393616, tv_loss: 0.032383665442466736\n",
      "iteration 1915, dc_loss: 0.11080127209424973, tv_loss: 0.0322602242231369\n",
      "iteration 1916, dc_loss: 0.11067108064889908, tv_loss: 0.0323726162314415\n",
      "iteration 1917, dc_loss: 0.11072279512882233, tv_loss: 0.032305702567100525\n",
      "iteration 1918, dc_loss: 0.11062412708997726, tv_loss: 0.03235534951090813\n",
      "iteration 1919, dc_loss: 0.11062729358673096, tv_loss: 0.032308027148246765\n",
      "iteration 1920, dc_loss: 0.11058071255683899, tv_loss: 0.03232612833380699\n",
      "iteration 1921, dc_loss: 0.1105373352766037, tv_loss: 0.03236822783946991\n",
      "iteration 1922, dc_loss: 0.11057521402835846, tv_loss: 0.03230395168066025\n",
      "iteration 1923, dc_loss: 0.11051324009895325, tv_loss: 0.03236207365989685\n",
      "iteration 1924, dc_loss: 0.1105244979262352, tv_loss: 0.03232238069176674\n",
      "iteration 1925, dc_loss: 0.11047067493200302, tv_loss: 0.03235412389039993\n",
      "iteration 1926, dc_loss: 0.11048558354377747, tv_loss: 0.03232599422335625\n",
      "iteration 1927, dc_loss: 0.11040280014276505, tv_loss: 0.03235769644379616\n",
      "iteration 1928, dc_loss: 0.1103985607624054, tv_loss: 0.03232914209365845\n",
      "iteration 1929, dc_loss: 0.11037524044513702, tv_loss: 0.03234310448169708\n",
      "iteration 1930, dc_loss: 0.11036093533039093, tv_loss: 0.03234720230102539\n",
      "iteration 1931, dc_loss: 0.11033755540847778, tv_loss: 0.032345157116651535\n",
      "iteration 1932, dc_loss: 0.1103309914469719, tv_loss: 0.032343752682209015\n",
      "iteration 1933, dc_loss: 0.11031053215265274, tv_loss: 0.03233272582292557\n",
      "iteration 1934, dc_loss: 0.11029179394245148, tv_loss: 0.03232257440686226\n",
      "iteration 1935, dc_loss: 0.11023085564374924, tv_loss: 0.032365404069423676\n",
      "iteration 1936, dc_loss: 0.1102309376001358, tv_loss: 0.032331887632608414\n",
      "iteration 1937, dc_loss: 0.11017133295536041, tv_loss: 0.03236560523509979\n",
      "iteration 1938, dc_loss: 0.11022858321666718, tv_loss: 0.0323067307472229\n",
      "iteration 1939, dc_loss: 0.11013081669807434, tv_loss: 0.03239165246486664\n",
      "iteration 1940, dc_loss: 0.11020555347204208, tv_loss: 0.032283347100019455\n",
      "iteration 1941, dc_loss: 0.11010995507240295, tv_loss: 0.03238116204738617\n",
      "iteration 1942, dc_loss: 0.11017286032438278, tv_loss: 0.032315488904714584\n",
      "iteration 1943, dc_loss: 0.1100793331861496, tv_loss: 0.03239438310265541\n",
      "iteration 1944, dc_loss: 0.11019162088632584, tv_loss: 0.03228138014674187\n",
      "iteration 1945, dc_loss: 0.11006247252225876, tv_loss: 0.032413896173238754\n",
      "iteration 1946, dc_loss: 0.11020366102457047, tv_loss: 0.032273080199956894\n",
      "iteration 1947, dc_loss: 0.1100638285279274, tv_loss: 0.032438550144433975\n",
      "iteration 1948, dc_loss: 0.11023330688476562, tv_loss: 0.03225648030638695\n",
      "iteration 1949, dc_loss: 0.11003187298774719, tv_loss: 0.03245773911476135\n",
      "iteration 1950, dc_loss: 0.11017338931560516, tv_loss: 0.03224679082632065\n",
      "iteration 1951, dc_loss: 0.10991868376731873, tv_loss: 0.03245864808559418\n",
      "iteration 1952, dc_loss: 0.11005236208438873, tv_loss: 0.03227502852678299\n",
      "iteration 1953, dc_loss: 0.10983607918024063, tv_loss: 0.03243373706936836\n",
      "iteration 1954, dc_loss: 0.10989350825548172, tv_loss: 0.03231704980134964\n",
      "iteration 1955, dc_loss: 0.10981635749340057, tv_loss: 0.0323559045791626\n",
      "iteration 1956, dc_loss: 0.10979011654853821, tv_loss: 0.032363779842853546\n",
      "iteration 1957, dc_loss: 0.10981317609548569, tv_loss: 0.03234631568193436\n",
      "iteration 1958, dc_loss: 0.10973453521728516, tv_loss: 0.03242523968219757\n",
      "iteration 1959, dc_loss: 0.1098354384303093, tv_loss: 0.0322885662317276\n",
      "iteration 1960, dc_loss: 0.10967753827571869, tv_loss: 0.032427262514829636\n",
      "iteration 1961, dc_loss: 0.10978379100561142, tv_loss: 0.032306969165802\n",
      "iteration 1962, dc_loss: 0.10966654866933823, tv_loss: 0.032418910413980484\n",
      "iteration 1963, dc_loss: 0.10972657799720764, tv_loss: 0.03232235461473465\n",
      "iteration 1964, dc_loss: 0.10962175577878952, tv_loss: 0.032394230365753174\n",
      "iteration 1965, dc_loss: 0.10965516418218613, tv_loss: 0.03234105557203293\n",
      "iteration 1966, dc_loss: 0.10961101204156876, tv_loss: 0.03238336369395256\n",
      "iteration 1967, dc_loss: 0.1095820963382721, tv_loss: 0.032381217926740646\n",
      "iteration 1968, dc_loss: 0.10953759402036667, tv_loss: 0.032372862100601196\n",
      "iteration 1969, dc_loss: 0.10951986908912659, tv_loss: 0.03235511854290962\n",
      "iteration 1970, dc_loss: 0.10948846489191055, tv_loss: 0.0323798805475235\n",
      "iteration 1971, dc_loss: 0.10946771502494812, tv_loss: 0.03238280490040779\n",
      "iteration 1972, dc_loss: 0.10945075750350952, tv_loss: 0.0323653519153595\n",
      "iteration 1973, dc_loss: 0.10942891240119934, tv_loss: 0.03236810117959976\n",
      "iteration 1974, dc_loss: 0.10941079258918762, tv_loss: 0.03238324075937271\n",
      "iteration 1975, dc_loss: 0.10940241068601608, tv_loss: 0.03237904608249664\n",
      "iteration 1976, dc_loss: 0.10937488824129105, tv_loss: 0.0323735848069191\n",
      "iteration 1977, dc_loss: 0.10934053361415863, tv_loss: 0.03237433359026909\n",
      "iteration 1978, dc_loss: 0.109337218105793, tv_loss: 0.032354455441236496\n",
      "iteration 1979, dc_loss: 0.10929442942142487, tv_loss: 0.03238723799586296\n",
      "iteration 1980, dc_loss: 0.10928353667259216, tv_loss: 0.032387178391218185\n",
      "iteration 1981, dc_loss: 0.10924248397350311, tv_loss: 0.032386504113674164\n",
      "iteration 1982, dc_loss: 0.10926435142755508, tv_loss: 0.032352514564991\n",
      "iteration 1983, dc_loss: 0.10920655727386475, tv_loss: 0.032393310219049454\n",
      "iteration 1984, dc_loss: 0.10923916101455688, tv_loss: 0.0323590449988842\n",
      "iteration 1985, dc_loss: 0.10916362702846527, tv_loss: 0.03241470083594322\n",
      "iteration 1986, dc_loss: 0.1092129573225975, tv_loss: 0.0323379747569561\n",
      "iteration 1987, dc_loss: 0.10912998765707016, tv_loss: 0.032406996935606\n",
      "iteration 1988, dc_loss: 0.10920033603906631, tv_loss: 0.03233567997813225\n",
      "iteration 1989, dc_loss: 0.10910007357597351, tv_loss: 0.032446712255477905\n",
      "iteration 1990, dc_loss: 0.10922185331583023, tv_loss: 0.032322049140930176\n",
      "iteration 1991, dc_loss: 0.109110988676548, tv_loss: 0.0324520468711853\n",
      "iteration 1992, dc_loss: 0.10928796976804733, tv_loss: 0.032285645604133606\n",
      "iteration 1993, dc_loss: 0.10914497077465057, tv_loss: 0.032484639436006546\n",
      "iteration 1994, dc_loss: 0.1093728095293045, tv_loss: 0.03224998340010643\n",
      "iteration 1995, dc_loss: 0.10914326459169388, tv_loss: 0.032514676451683044\n",
      "iteration 1996, dc_loss: 0.10936354845762253, tv_loss: 0.0322258286178112\n",
      "iteration 1997, dc_loss: 0.10901722311973572, tv_loss: 0.03252555802464485\n",
      "iteration 1998, dc_loss: 0.10919710993766785, tv_loss: 0.03224779665470123\n",
      "iteration 1999, dc_loss: 0.10888330638408661, tv_loss: 0.03247515857219696\n",
      "iteration 2000, dc_loss: 0.10898695141077042, tv_loss: 0.03230433911085129\n",
      "iteration 2001, dc_loss: 0.10883970558643341, tv_loss: 0.03241254389286041\n",
      "iteration 2002, dc_loss: 0.10880986601114273, tv_loss: 0.03240610659122467\n",
      "iteration 2003, dc_loss: 0.10886967927217484, tv_loss: 0.03233053907752037\n",
      "iteration 2004, dc_loss: 0.10879594087600708, tv_loss: 0.03238075599074364\n",
      "iteration 2005, dc_loss: 0.10877479612827301, tv_loss: 0.03238244354724884\n",
      "iteration 2006, dc_loss: 0.10879760980606079, tv_loss: 0.032357875257730484\n",
      "iteration 2007, dc_loss: 0.10872340202331543, tv_loss: 0.03240276500582695\n",
      "iteration 2008, dc_loss: 0.10871989279985428, tv_loss: 0.03237829729914665\n",
      "iteration 2009, dc_loss: 0.10874955356121063, tv_loss: 0.032341428101062775\n",
      "iteration 2010, dc_loss: 0.10868039727210999, tv_loss: 0.03240617737174034\n",
      "iteration 2011, dc_loss: 0.10867708176374435, tv_loss: 0.032380230724811554\n",
      "iteration 2012, dc_loss: 0.10869630426168442, tv_loss: 0.03234289959073067\n",
      "iteration 2013, dc_loss: 0.1086212545633316, tv_loss: 0.03239616006612778\n",
      "iteration 2014, dc_loss: 0.10862299799919128, tv_loss: 0.03237365931272507\n",
      "iteration 2015, dc_loss: 0.10863620787858963, tv_loss: 0.032363489270210266\n",
      "iteration 2016, dc_loss: 0.10857746750116348, tv_loss: 0.03241613507270813\n",
      "iteration 2017, dc_loss: 0.10858654230833054, tv_loss: 0.03237150236964226\n",
      "iteration 2018, dc_loss: 0.1085686907172203, tv_loss: 0.03235974162817001\n",
      "iteration 2019, dc_loss: 0.10852430760860443, tv_loss: 0.03239718824625015\n",
      "iteration 2020, dc_loss: 0.10854393243789673, tv_loss: 0.032358117401599884\n",
      "iteration 2021, dc_loss: 0.1085130050778389, tv_loss: 0.03236778825521469\n",
      "iteration 2022, dc_loss: 0.108475461602211, tv_loss: 0.032398298382759094\n",
      "iteration 2023, dc_loss: 0.10849255323410034, tv_loss: 0.032376229763031006\n",
      "iteration 2024, dc_loss: 0.10846475511789322, tv_loss: 0.0323774553835392\n",
      "iteration 2025, dc_loss: 0.10843361914157867, tv_loss: 0.032390303909778595\n",
      "iteration 2026, dc_loss: 0.10844749212265015, tv_loss: 0.03236300125718117\n",
      "iteration 2027, dc_loss: 0.1083967536687851, tv_loss: 0.03238848224282265\n",
      "iteration 2028, dc_loss: 0.10837941616773605, tv_loss: 0.03239607810974121\n",
      "iteration 2029, dc_loss: 0.1084042638540268, tv_loss: 0.03237491473555565\n",
      "iteration 2030, dc_loss: 0.10835219919681549, tv_loss: 0.03240574896335602\n",
      "iteration 2031, dc_loss: 0.10834017395973206, tv_loss: 0.03239046409726143\n",
      "iteration 2032, dc_loss: 0.1083453968167305, tv_loss: 0.032363757491111755\n",
      "iteration 2033, dc_loss: 0.10830677300691605, tv_loss: 0.032388340681791306\n",
      "iteration 2034, dc_loss: 0.10829410701990128, tv_loss: 0.03239551559090614\n",
      "iteration 2035, dc_loss: 0.10828220099210739, tv_loss: 0.03240112587809563\n",
      "iteration 2036, dc_loss: 0.10825738310813904, tv_loss: 0.03239436820149422\n",
      "iteration 2037, dc_loss: 0.10825585573911667, tv_loss: 0.03237633779644966\n",
      "iteration 2038, dc_loss: 0.1082330197095871, tv_loss: 0.03238818049430847\n",
      "iteration 2039, dc_loss: 0.10820963233709335, tv_loss: 0.03239250183105469\n",
      "iteration 2040, dc_loss: 0.1082051694393158, tv_loss: 0.03239702060818672\n",
      "iteration 2041, dc_loss: 0.10818447172641754, tv_loss: 0.03239584341645241\n",
      "iteration 2042, dc_loss: 0.10815803706645966, tv_loss: 0.032394565641880035\n",
      "iteration 2043, dc_loss: 0.10816078633069992, tv_loss: 0.03236958757042885\n",
      "iteration 2044, dc_loss: 0.10813240706920624, tv_loss: 0.03239542245864868\n",
      "iteration 2045, dc_loss: 0.10811049491167068, tv_loss: 0.032417818903923035\n",
      "iteration 2046, dc_loss: 0.10811541229486465, tv_loss: 0.03238741308450699\n",
      "iteration 2047, dc_loss: 0.1080814003944397, tv_loss: 0.03239251673221588\n",
      "iteration 2048, dc_loss: 0.10806196182966232, tv_loss: 0.03239237889647484\n",
      "iteration 2049, dc_loss: 0.10806061327457428, tv_loss: 0.032386988401412964\n",
      "iteration 2050, dc_loss: 0.10803480446338654, tv_loss: 0.03240608051419258\n",
      "iteration 2051, dc_loss: 0.1080288216471672, tv_loss: 0.03240859508514404\n",
      "iteration 2052, dc_loss: 0.10800418257713318, tv_loss: 0.03239809349179268\n",
      "iteration 2053, dc_loss: 0.10798383504152298, tv_loss: 0.032393068075180054\n",
      "iteration 2054, dc_loss: 0.1079801544547081, tv_loss: 0.03239012509584427\n",
      "iteration 2055, dc_loss: 0.10796088725328445, tv_loss: 0.03241141140460968\n",
      "iteration 2056, dc_loss: 0.10794167965650558, tv_loss: 0.0324159599840641\n",
      "iteration 2057, dc_loss: 0.10792232304811478, tv_loss: 0.03239917755126953\n",
      "iteration 2058, dc_loss: 0.10790956020355225, tv_loss: 0.032394323498010635\n",
      "iteration 2059, dc_loss: 0.10788600146770477, tv_loss: 0.032406389713287354\n",
      "iteration 2060, dc_loss: 0.10788200795650482, tv_loss: 0.03239874541759491\n",
      "iteration 2061, dc_loss: 0.10786836594343185, tv_loss: 0.03239686042070389\n",
      "iteration 2062, dc_loss: 0.10783690959215164, tv_loss: 0.032414548099040985\n",
      "iteration 2063, dc_loss: 0.10783308744430542, tv_loss: 0.03239312022924423\n",
      "iteration 2064, dc_loss: 0.1078239381313324, tv_loss: 0.0323864184319973\n",
      "iteration 2065, dc_loss: 0.10779213160276413, tv_loss: 0.032418400049209595\n",
      "iteration 2066, dc_loss: 0.10777776688337326, tv_loss: 0.03241477534174919\n",
      "iteration 2067, dc_loss: 0.10776589810848236, tv_loss: 0.03241090849041939\n",
      "iteration 2068, dc_loss: 0.10775834321975708, tv_loss: 0.03239373117685318\n",
      "iteration 2069, dc_loss: 0.10773813724517822, tv_loss: 0.032399795949459076\n",
      "iteration 2070, dc_loss: 0.10770561546087265, tv_loss: 0.032441798597574234\n",
      "iteration 2071, dc_loss: 0.10770183056592941, tv_loss: 0.03241178020834923\n",
      "iteration 2072, dc_loss: 0.10768430680036545, tv_loss: 0.032406799495220184\n",
      "iteration 2073, dc_loss: 0.10767639428377151, tv_loss: 0.032404351979494095\n",
      "iteration 2074, dc_loss: 0.10765952616930008, tv_loss: 0.03240608796477318\n",
      "iteration 2075, dc_loss: 0.10763170570135117, tv_loss: 0.032433848828077316\n",
      "iteration 2076, dc_loss: 0.10762923210859299, tv_loss: 0.03239666670560837\n",
      "iteration 2077, dc_loss: 0.10760817676782608, tv_loss: 0.03241332992911339\n",
      "iteration 2078, dc_loss: 0.10757873952388763, tv_loss: 0.032440707087516785\n",
      "iteration 2079, dc_loss: 0.10758128762245178, tv_loss: 0.032408252358436584\n",
      "iteration 2080, dc_loss: 0.10756795853376389, tv_loss: 0.03240729868412018\n",
      "iteration 2081, dc_loss: 0.10753802955150604, tv_loss: 0.032428305596113205\n",
      "iteration 2082, dc_loss: 0.10752870887517929, tv_loss: 0.032423995435237885\n",
      "iteration 2083, dc_loss: 0.10751548409461975, tv_loss: 0.03241674602031708\n",
      "iteration 2084, dc_loss: 0.10749810189008713, tv_loss: 0.03240848332643509\n",
      "iteration 2085, dc_loss: 0.10747519135475159, tv_loss: 0.03244059160351753\n",
      "iteration 2086, dc_loss: 0.10746242105960846, tv_loss: 0.032423585653305054\n",
      "iteration 2087, dc_loss: 0.10745774954557419, tv_loss: 0.03240035101771355\n",
      "iteration 2088, dc_loss: 0.10743110626935959, tv_loss: 0.032427411526441574\n",
      "iteration 2089, dc_loss: 0.10741335898637772, tv_loss: 0.03243805468082428\n",
      "iteration 2090, dc_loss: 0.1074097603559494, tv_loss: 0.03241760656237602\n",
      "iteration 2091, dc_loss: 0.1073828861117363, tv_loss: 0.03241650387644768\n",
      "iteration 2092, dc_loss: 0.10737072676420212, tv_loss: 0.032428037375211716\n",
      "iteration 2093, dc_loss: 0.10736258327960968, tv_loss: 0.032435331493616104\n",
      "iteration 2094, dc_loss: 0.10734278708696365, tv_loss: 0.032417796552181244\n",
      "iteration 2095, dc_loss: 0.10731613636016846, tv_loss: 0.032430510967969894\n",
      "iteration 2096, dc_loss: 0.10730805993080139, tv_loss: 0.032425303012132645\n",
      "iteration 2097, dc_loss: 0.10729251056909561, tv_loss: 0.03243224695324898\n",
      "iteration 2098, dc_loss: 0.1072755828499794, tv_loss: 0.03242332488298416\n",
      "iteration 2099, dc_loss: 0.10726436227560043, tv_loss: 0.03241639956831932\n",
      "iteration 2100, dc_loss: 0.10725125670433044, tv_loss: 0.03242667391896248\n",
      "iteration 2101, dc_loss: 0.10722694545984268, tv_loss: 0.03243430331349373\n",
      "iteration 2102, dc_loss: 0.10721095651388168, tv_loss: 0.03242949768900871\n",
      "iteration 2103, dc_loss: 0.10720527172088623, tv_loss: 0.032410431653261185\n",
      "iteration 2104, dc_loss: 0.10718492418527603, tv_loss: 0.0324157178401947\n",
      "iteration 2105, dc_loss: 0.10716086626052856, tv_loss: 0.0324297733604908\n",
      "iteration 2106, dc_loss: 0.10715847462415695, tv_loss: 0.032415200024843216\n",
      "iteration 2107, dc_loss: 0.10713972896337509, tv_loss: 0.0324254147708416\n",
      "iteration 2108, dc_loss: 0.1071200966835022, tv_loss: 0.0324246771633625\n",
      "iteration 2109, dc_loss: 0.10710648447275162, tv_loss: 0.032421231269836426\n",
      "iteration 2110, dc_loss: 0.1070883497595787, tv_loss: 0.032416220754384995\n",
      "iteration 2111, dc_loss: 0.10706977546215057, tv_loss: 0.032427165657281876\n",
      "iteration 2112, dc_loss: 0.10706115514039993, tv_loss: 0.032415445894002914\n",
      "iteration 2113, dc_loss: 0.10705017298460007, tv_loss: 0.032408785074949265\n",
      "iteration 2114, dc_loss: 0.10702550411224365, tv_loss: 0.032420869916677475\n",
      "iteration 2115, dc_loss: 0.10700839012861252, tv_loss: 0.03241995349526405\n",
      "iteration 2116, dc_loss: 0.10700284689664841, tv_loss: 0.03241746872663498\n",
      "iteration 2117, dc_loss: 0.10697948187589645, tv_loss: 0.03242635354399681\n",
      "iteration 2118, dc_loss: 0.10696260631084442, tv_loss: 0.03244500979781151\n",
      "iteration 2119, dc_loss: 0.10695347189903259, tv_loss: 0.032428886741399765\n",
      "iteration 2120, dc_loss: 0.10693235695362091, tv_loss: 0.032429806888103485\n",
      "iteration 2121, dc_loss: 0.10691647231578827, tv_loss: 0.03242330625653267\n",
      "iteration 2122, dc_loss: 0.10690376162528992, tv_loss: 0.032416265457868576\n",
      "iteration 2123, dc_loss: 0.1068902388215065, tv_loss: 0.032421354204416275\n",
      "iteration 2124, dc_loss: 0.10687088966369629, tv_loss: 0.03242611885070801\n",
      "iteration 2125, dc_loss: 0.10685436427593231, tv_loss: 0.032434456050395966\n",
      "iteration 2126, dc_loss: 0.10685193538665771, tv_loss: 0.03243153169751167\n",
      "iteration 2127, dc_loss: 0.1068195030093193, tv_loss: 0.03245101496577263\n",
      "iteration 2128, dc_loss: 0.10680980235338211, tv_loss: 0.03242959827184677\n",
      "iteration 2129, dc_loss: 0.10679797828197479, tv_loss: 0.032424360513687134\n",
      "iteration 2130, dc_loss: 0.10677868127822876, tv_loss: 0.0324273407459259\n",
      "iteration 2131, dc_loss: 0.10676314681768417, tv_loss: 0.03243178129196167\n",
      "iteration 2132, dc_loss: 0.10674768686294556, tv_loss: 0.032443828880786896\n",
      "iteration 2133, dc_loss: 0.10673771798610687, tv_loss: 0.03245612606406212\n",
      "iteration 2134, dc_loss: 0.10671591758728027, tv_loss: 0.032439541071653366\n",
      "iteration 2135, dc_loss: 0.10670297592878342, tv_loss: 0.03243125602602959\n",
      "iteration 2136, dc_loss: 0.10668673366308212, tv_loss: 0.03244598209857941\n",
      "iteration 2137, dc_loss: 0.10667446255683899, tv_loss: 0.032456979155540466\n",
      "iteration 2138, dc_loss: 0.10665935277938843, tv_loss: 0.0324380099773407\n",
      "iteration 2139, dc_loss: 0.10663808137178421, tv_loss: 0.032439377158880234\n",
      "iteration 2140, dc_loss: 0.10662616789340973, tv_loss: 0.03244354575872421\n",
      "iteration 2141, dc_loss: 0.1066163033246994, tv_loss: 0.03246057778596878\n",
      "iteration 2142, dc_loss: 0.10659939795732498, tv_loss: 0.03244514763355255\n",
      "iteration 2143, dc_loss: 0.10657583177089691, tv_loss: 0.03244423493742943\n",
      "iteration 2144, dc_loss: 0.10656233876943588, tv_loss: 0.03246011212468147\n",
      "iteration 2145, dc_loss: 0.10654906928539276, tv_loss: 0.03247271478176117\n",
      "iteration 2146, dc_loss: 0.10653842240571976, tv_loss: 0.03244194760918617\n",
      "iteration 2147, dc_loss: 0.10652320086956024, tv_loss: 0.03244289383292198\n",
      "iteration 2148, dc_loss: 0.10649911314249039, tv_loss: 0.032472554594278336\n",
      "iteration 2149, dc_loss: 0.10649305582046509, tv_loss: 0.032456450164318085\n",
      "iteration 2150, dc_loss: 0.10647692531347275, tv_loss: 0.032446760684251785\n",
      "iteration 2151, dc_loss: 0.10645738244056702, tv_loss: 0.03246188908815384\n",
      "iteration 2152, dc_loss: 0.10644707828760147, tv_loss: 0.03246471658349037\n",
      "iteration 2153, dc_loss: 0.1064232587814331, tv_loss: 0.0324554443359375\n",
      "iteration 2154, dc_loss: 0.10641533881425858, tv_loss: 0.03245139122009277\n",
      "iteration 2155, dc_loss: 0.106397345662117, tv_loss: 0.032480835914611816\n",
      "iteration 2156, dc_loss: 0.10638122260570526, tv_loss: 0.03245505318045616\n",
      "iteration 2157, dc_loss: 0.1063738539814949, tv_loss: 0.032456595450639725\n",
      "iteration 2158, dc_loss: 0.10635370016098022, tv_loss: 0.03247600421309471\n",
      "iteration 2159, dc_loss: 0.1063290387392044, tv_loss: 0.032468248158693314\n",
      "iteration 2160, dc_loss: 0.10632585734128952, tv_loss: 0.03245406597852707\n",
      "iteration 2161, dc_loss: 0.10630782693624496, tv_loss: 0.0324690118432045\n",
      "iteration 2162, dc_loss: 0.1062849909067154, tv_loss: 0.0324690155684948\n",
      "iteration 2163, dc_loss: 0.10627736151218414, tv_loss: 0.03245183825492859\n",
      "iteration 2164, dc_loss: 0.10626678168773651, tv_loss: 0.032461319118738174\n",
      "iteration 2165, dc_loss: 0.10624342411756516, tv_loss: 0.03247170150279999\n",
      "iteration 2166, dc_loss: 0.10623812675476074, tv_loss: 0.03245050460100174\n",
      "iteration 2167, dc_loss: 0.10621672868728638, tv_loss: 0.03244940564036369\n",
      "iteration 2168, dc_loss: 0.10619499534368515, tv_loss: 0.03247055411338806\n",
      "iteration 2169, dc_loss: 0.10618452727794647, tv_loss: 0.03246267884969711\n",
      "iteration 2170, dc_loss: 0.10617542266845703, tv_loss: 0.032448671758174896\n",
      "iteration 2171, dc_loss: 0.10615191608667374, tv_loss: 0.03245953842997551\n",
      "iteration 2172, dc_loss: 0.10614100843667984, tv_loss: 0.032454974949359894\n",
      "iteration 2173, dc_loss: 0.10613783448934555, tv_loss: 0.03244572505354881\n",
      "iteration 2174, dc_loss: 0.10611067712306976, tv_loss: 0.03246583417057991\n",
      "iteration 2175, dc_loss: 0.1060868427157402, tv_loss: 0.03246263042092323\n",
      "iteration 2176, dc_loss: 0.10607998073101044, tv_loss: 0.032453812658786774\n",
      "iteration 2177, dc_loss: 0.10607171803712845, tv_loss: 0.032447900623083115\n",
      "iteration 2178, dc_loss: 0.10604441910982132, tv_loss: 0.03246423974633217\n",
      "iteration 2179, dc_loss: 0.10604390501976013, tv_loss: 0.03245152533054352\n",
      "iteration 2180, dc_loss: 0.10602391511201859, tv_loss: 0.032460398972034454\n",
      "iteration 2181, dc_loss: 0.10600364953279495, tv_loss: 0.032457876950502396\n",
      "iteration 2182, dc_loss: 0.10598248243331909, tv_loss: 0.032460685819387436\n",
      "iteration 2183, dc_loss: 0.10597127676010132, tv_loss: 0.032454803586006165\n",
      "iteration 2184, dc_loss: 0.10596668720245361, tv_loss: 0.03245225176215172\n",
      "iteration 2185, dc_loss: 0.10594521462917328, tv_loss: 0.032454151660203934\n",
      "iteration 2186, dc_loss: 0.10593614727258682, tv_loss: 0.032448917627334595\n",
      "iteration 2187, dc_loss: 0.1059160903096199, tv_loss: 0.032458215951919556\n",
      "iteration 2188, dc_loss: 0.10589346289634705, tv_loss: 0.0324629545211792\n",
      "iteration 2189, dc_loss: 0.105891153216362, tv_loss: 0.0324544720351696\n",
      "iteration 2190, dc_loss: 0.10588391125202179, tv_loss: 0.03244280815124512\n",
      "iteration 2191, dc_loss: 0.10585562139749527, tv_loss: 0.03246337175369263\n",
      "iteration 2192, dc_loss: 0.1058458685874939, tv_loss: 0.03246686980128288\n",
      "iteration 2193, dc_loss: 0.10583645850419998, tv_loss: 0.032472964376211166\n",
      "iteration 2194, dc_loss: 0.10583251714706421, tv_loss: 0.03247273340821266\n",
      "iteration 2195, dc_loss: 0.10581687837839127, tv_loss: 0.0324699766933918\n",
      "iteration 2196, dc_loss: 0.10581274330615997, tv_loss: 0.03246254473924637\n",
      "iteration 2197, dc_loss: 0.10579430311918259, tv_loss: 0.03246289864182472\n",
      "iteration 2198, dc_loss: 0.10577623546123505, tv_loss: 0.03246146813035011\n",
      "iteration 2199, dc_loss: 0.10575872659683228, tv_loss: 0.03246394917368889\n",
      "iteration 2200, dc_loss: 0.10574787855148315, tv_loss: 0.032449737191200256\n",
      "iteration 2201, dc_loss: 0.10570953041315079, tv_loss: 0.032473817467689514\n",
      "iteration 2202, dc_loss: 0.1056973859667778, tv_loss: 0.03246396780014038\n",
      "iteration 2203, dc_loss: 0.10568578541278839, tv_loss: 0.03246511518955231\n",
      "iteration 2204, dc_loss: 0.10567469894886017, tv_loss: 0.03246801346540451\n",
      "iteration 2205, dc_loss: 0.10564233362674713, tv_loss: 0.03248722478747368\n",
      "iteration 2206, dc_loss: 0.10564033687114716, tv_loss: 0.03247514367103577\n",
      "iteration 2207, dc_loss: 0.10563072562217712, tv_loss: 0.03246939927339554\n",
      "iteration 2208, dc_loss: 0.10560876131057739, tv_loss: 0.03247544541954994\n",
      "iteration 2209, dc_loss: 0.1056109145283699, tv_loss: 0.03246287256479263\n",
      "iteration 2210, dc_loss: 0.1055862158536911, tv_loss: 0.03247404843568802\n",
      "iteration 2211, dc_loss: 0.10558272898197174, tv_loss: 0.03246032074093819\n",
      "iteration 2212, dc_loss: 0.10555870831012726, tv_loss: 0.03246983140707016\n",
      "iteration 2213, dc_loss: 0.10553467273712158, tv_loss: 0.032481636852025986\n",
      "iteration 2214, dc_loss: 0.10552965849637985, tv_loss: 0.03246862813830376\n",
      "iteration 2215, dc_loss: 0.1055116131901741, tv_loss: 0.0324677936732769\n",
      "iteration 2216, dc_loss: 0.10549385845661163, tv_loss: 0.03247480466961861\n",
      "iteration 2217, dc_loss: 0.10546834766864777, tv_loss: 0.03249244764447212\n",
      "iteration 2218, dc_loss: 0.10546446591615677, tv_loss: 0.032482583075761795\n",
      "iteration 2219, dc_loss: 0.10545208305120468, tv_loss: 0.032479699701070786\n",
      "iteration 2220, dc_loss: 0.10543016344308853, tv_loss: 0.03248099237680435\n",
      "iteration 2221, dc_loss: 0.10541392862796783, tv_loss: 0.03247513622045517\n",
      "iteration 2222, dc_loss: 0.10540222376585007, tv_loss: 0.032472457736730576\n",
      "iteration 2223, dc_loss: 0.10539040714502335, tv_loss: 0.03247293829917908\n",
      "iteration 2224, dc_loss: 0.10537358373403549, tv_loss: 0.032486479729413986\n",
      "iteration 2225, dc_loss: 0.10536440461874008, tv_loss: 0.03247899189591408\n",
      "iteration 2226, dc_loss: 0.1053570806980133, tv_loss: 0.03248036652803421\n",
      "iteration 2227, dc_loss: 0.10534802079200745, tv_loss: 0.03247141093015671\n",
      "iteration 2228, dc_loss: 0.10532259941101074, tv_loss: 0.03247968107461929\n",
      "iteration 2229, dc_loss: 0.105320505797863, tv_loss: 0.03247211501002312\n",
      "iteration 2230, dc_loss: 0.10529648512601852, tv_loss: 0.0324750691652298\n",
      "iteration 2231, dc_loss: 0.1052752211689949, tv_loss: 0.03247577324509621\n",
      "iteration 2232, dc_loss: 0.10526043176651001, tv_loss: 0.03248824179172516\n",
      "iteration 2233, dc_loss: 0.10526236146688461, tv_loss: 0.03246689215302467\n",
      "iteration 2234, dc_loss: 0.10523537546396255, tv_loss: 0.032484691590070724\n",
      "iteration 2235, dc_loss: 0.1052229031920433, tv_loss: 0.032496511936187744\n",
      "iteration 2236, dc_loss: 0.10521004348993301, tv_loss: 0.032509494572877884\n",
      "iteration 2237, dc_loss: 0.10520517826080322, tv_loss: 0.032480109483003616\n",
      "iteration 2238, dc_loss: 0.10517922788858414, tv_loss: 0.03248924762010574\n",
      "iteration 2239, dc_loss: 0.10516837984323502, tv_loss: 0.03248443454504013\n",
      "iteration 2240, dc_loss: 0.10514999181032181, tv_loss: 0.03249795362353325\n",
      "iteration 2241, dc_loss: 0.10513009876012802, tv_loss: 0.03250465542078018\n",
      "iteration 2242, dc_loss: 0.10511479526758194, tv_loss: 0.032485075294971466\n",
      "iteration 2243, dc_loss: 0.10510607063770294, tv_loss: 0.032486677169799805\n",
      "iteration 2244, dc_loss: 0.10507579892873764, tv_loss: 0.032506030052900314\n",
      "iteration 2245, dc_loss: 0.10507513582706451, tv_loss: 0.03249906003475189\n",
      "iteration 2246, dc_loss: 0.10505429655313492, tv_loss: 0.03249097615480423\n",
      "iteration 2247, dc_loss: 0.10505592823028564, tv_loss: 0.032475270330905914\n",
      "iteration 2248, dc_loss: 0.10502193123102188, tv_loss: 0.03251287341117859\n",
      "iteration 2249, dc_loss: 0.10504771023988724, tv_loss: 0.0324983224272728\n",
      "iteration 2250, dc_loss: 0.1050122007727623, tv_loss: 0.0325155146420002\n",
      "iteration 2251, dc_loss: 0.10503820329904556, tv_loss: 0.032476700842380524\n",
      "iteration 2252, dc_loss: 0.10499056428670883, tv_loss: 0.03249913454055786\n",
      "iteration 2253, dc_loss: 0.10500063747167587, tv_loss: 0.032499585300683975\n",
      "iteration 2254, dc_loss: 0.10494847595691681, tv_loss: 0.03252428397536278\n",
      "iteration 2255, dc_loss: 0.10495486110448837, tv_loss: 0.03248489648103714\n",
      "iteration 2256, dc_loss: 0.10491141676902771, tv_loss: 0.0325031504034996\n",
      "iteration 2257, dc_loss: 0.10491801053285599, tv_loss: 0.03249898925423622\n",
      "iteration 2258, dc_loss: 0.1048860102891922, tv_loss: 0.032537322491407394\n",
      "iteration 2259, dc_loss: 0.10490695387125015, tv_loss: 0.03248200938105583\n",
      "iteration 2260, dc_loss: 0.10487455129623413, tv_loss: 0.03251229599118233\n",
      "iteration 2261, dc_loss: 0.10488835722208023, tv_loss: 0.03250217065215111\n",
      "iteration 2262, dc_loss: 0.10487159341573715, tv_loss: 0.032525110989809036\n",
      "iteration 2263, dc_loss: 0.10488484054803848, tv_loss: 0.03247454762458801\n",
      "iteration 2264, dc_loss: 0.10481959581375122, tv_loss: 0.03253309801220894\n",
      "iteration 2265, dc_loss: 0.10487362742424011, tv_loss: 0.03247524052858353\n",
      "iteration 2266, dc_loss: 0.10477455705404282, tv_loss: 0.03255883976817131\n",
      "iteration 2267, dc_loss: 0.10484779626131058, tv_loss: 0.03245394304394722\n",
      "iteration 2268, dc_loss: 0.1047348901629448, tv_loss: 0.03255920112133026\n",
      "iteration 2269, dc_loss: 0.10484066605567932, tv_loss: 0.03245764225721359\n",
      "iteration 2270, dc_loss: 0.1047288030385971, tv_loss: 0.03257138282060623\n",
      "iteration 2271, dc_loss: 0.10484690964221954, tv_loss: 0.0324346199631691\n",
      "iteration 2272, dc_loss: 0.10470188409090042, tv_loss: 0.03255525976419449\n",
      "iteration 2273, dc_loss: 0.10477820038795471, tv_loss: 0.0324469655752182\n",
      "iteration 2274, dc_loss: 0.10463758558034897, tv_loss: 0.032564885914325714\n",
      "iteration 2275, dc_loss: 0.10472294688224792, tv_loss: 0.03244861960411072\n",
      "iteration 2276, dc_loss: 0.10463198274374008, tv_loss: 0.03253146633505821\n",
      "iteration 2277, dc_loss: 0.10466568171977997, tv_loss: 0.03248582035303116\n",
      "iteration 2278, dc_loss: 0.10462358593940735, tv_loss: 0.032527655363082886\n",
      "iteration 2279, dc_loss: 0.10462163388729095, tv_loss: 0.03249914199113846\n",
      "iteration 2280, dc_loss: 0.10458686947822571, tv_loss: 0.03250305727124214\n",
      "iteration 2281, dc_loss: 0.1045491099357605, tv_loss: 0.03251355513930321\n",
      "iteration 2282, dc_loss: 0.10456105321645737, tv_loss: 0.03248429298400879\n",
      "iteration 2283, dc_loss: 0.10452093183994293, tv_loss: 0.03251204639673233\n",
      "iteration 2284, dc_loss: 0.1045435443520546, tv_loss: 0.03248920664191246\n",
      "iteration 2285, dc_loss: 0.10453237593173981, tv_loss: 0.03250506892800331\n",
      "iteration 2286, dc_loss: 0.1045093759894371, tv_loss: 0.0325017124414444\n",
      "iteration 2287, dc_loss: 0.10446734726428986, tv_loss: 0.032515935599803925\n",
      "iteration 2288, dc_loss: 0.104472815990448, tv_loss: 0.032494332641363144\n",
      "iteration 2289, dc_loss: 0.10443875193595886, tv_loss: 0.03251631185412407\n",
      "iteration 2290, dc_loss: 0.10445909202098846, tv_loss: 0.03251071274280548\n",
      "iteration 2291, dc_loss: 0.10442499071359634, tv_loss: 0.03253258019685745\n",
      "iteration 2292, dc_loss: 0.10447219014167786, tv_loss: 0.03248041868209839\n",
      "iteration 2293, dc_loss: 0.10439196974039078, tv_loss: 0.03253484517335892\n",
      "iteration 2294, dc_loss: 0.10440996289253235, tv_loss: 0.032495640218257904\n",
      "iteration 2295, dc_loss: 0.10436493158340454, tv_loss: 0.03253960609436035\n",
      "iteration 2296, dc_loss: 0.10439590364694595, tv_loss: 0.03250977396965027\n",
      "iteration 2297, dc_loss: 0.10435926169157028, tv_loss: 0.032543059438467026\n",
      "iteration 2298, dc_loss: 0.10439830273389816, tv_loss: 0.03248623386025429\n",
      "iteration 2299, dc_loss: 0.10433349013328552, tv_loss: 0.032531242817640305\n",
      "iteration 2300, dc_loss: 0.10434674471616745, tv_loss: 0.032492127269506454\n",
      "iteration 2301, dc_loss: 0.10428151488304138, tv_loss: 0.032546769827604294\n",
      "iteration 2302, dc_loss: 0.1043384000658989, tv_loss: 0.03248400613665581\n",
      "iteration 2303, dc_loss: 0.10426197946071625, tv_loss: 0.032563019543886185\n",
      "iteration 2304, dc_loss: 0.10433433949947357, tv_loss: 0.03248021751642227\n",
      "iteration 2305, dc_loss: 0.10424145311117172, tv_loss: 0.032545305788517\n",
      "iteration 2306, dc_loss: 0.10428158193826675, tv_loss: 0.032468728721141815\n",
      "iteration 2307, dc_loss: 0.10418090224266052, tv_loss: 0.03255319222807884\n",
      "iteration 2308, dc_loss: 0.10424894094467163, tv_loss: 0.03247777000069618\n",
      "iteration 2309, dc_loss: 0.104173943400383, tv_loss: 0.032561566680669785\n",
      "iteration 2310, dc_loss: 0.10421794652938843, tv_loss: 0.03250687196850777\n",
      "iteration 2311, dc_loss: 0.10414616018533707, tv_loss: 0.03254560008645058\n",
      "iteration 2312, dc_loss: 0.10415537655353546, tv_loss: 0.032506171613931656\n",
      "iteration 2313, dc_loss: 0.10411179810762405, tv_loss: 0.03253013640642166\n",
      "iteration 2314, dc_loss: 0.1041298359632492, tv_loss: 0.03250330686569214\n",
      "iteration 2315, dc_loss: 0.10409867763519287, tv_loss: 0.03252079337835312\n",
      "iteration 2316, dc_loss: 0.10409443080425262, tv_loss: 0.03250753879547119\n",
      "iteration 2317, dc_loss: 0.1040496677160263, tv_loss: 0.032528381794691086\n",
      "iteration 2318, dc_loss: 0.1040448397397995, tv_loss: 0.032516639679670334\n",
      "iteration 2319, dc_loss: 0.10401929169893265, tv_loss: 0.03253304585814476\n",
      "iteration 2320, dc_loss: 0.10402791202068329, tv_loss: 0.0325261726975441\n",
      "iteration 2321, dc_loss: 0.10401637107133865, tv_loss: 0.032523058354854584\n",
      "iteration 2322, dc_loss: 0.10398337244987488, tv_loss: 0.03253808245062828\n",
      "iteration 2323, dc_loss: 0.10398958623409271, tv_loss: 0.032516155391931534\n",
      "iteration 2324, dc_loss: 0.10393771529197693, tv_loss: 0.03253668174147606\n",
      "iteration 2325, dc_loss: 0.1039763018488884, tv_loss: 0.03250104933977127\n",
      "iteration 2326, dc_loss: 0.10390898585319519, tv_loss: 0.032559093087911606\n",
      "iteration 2327, dc_loss: 0.10396919399499893, tv_loss: 0.0324878953397274\n",
      "iteration 2328, dc_loss: 0.10390367358922958, tv_loss: 0.032548364251852036\n",
      "iteration 2329, dc_loss: 0.10395010560750961, tv_loss: 0.03248061239719391\n",
      "iteration 2330, dc_loss: 0.10385879874229431, tv_loss: 0.032568663358688354\n",
      "iteration 2331, dc_loss: 0.1039450392127037, tv_loss: 0.03248005732893944\n",
      "iteration 2332, dc_loss: 0.10385921597480774, tv_loss: 0.032571032643318176\n",
      "iteration 2333, dc_loss: 0.10397516191005707, tv_loss: 0.03246839717030525\n",
      "iteration 2334, dc_loss: 0.1038699746131897, tv_loss: 0.03259112685918808\n",
      "iteration 2335, dc_loss: 0.10399926453828812, tv_loss: 0.032453831285238266\n",
      "iteration 2336, dc_loss: 0.1038724035024643, tv_loss: 0.03261217847466469\n",
      "iteration 2337, dc_loss: 0.10401266813278198, tv_loss: 0.03243524581193924\n",
      "iteration 2338, dc_loss: 0.1038370355963707, tv_loss: 0.032606590539216995\n",
      "iteration 2339, dc_loss: 0.10394416004419327, tv_loss: 0.03245209902524948\n",
      "iteration 2340, dc_loss: 0.10375086963176727, tv_loss: 0.03262098878622055\n",
      "iteration 2341, dc_loss: 0.10381581634283066, tv_loss: 0.032504886388778687\n",
      "iteration 2342, dc_loss: 0.10369092226028442, tv_loss: 0.03257297724485397\n",
      "iteration 2343, dc_loss: 0.10371662676334381, tv_loss: 0.032520830631256104\n",
      "iteration 2344, dc_loss: 0.10371417552232742, tv_loss: 0.03253358229994774\n",
      "iteration 2345, dc_loss: 0.10367932915687561, tv_loss: 0.0325719378888607\n",
      "iteration 2346, dc_loss: 0.10372737050056458, tv_loss: 0.032503899186849594\n",
      "iteration 2347, dc_loss: 0.10363861173391342, tv_loss: 0.032576773315668106\n",
      "iteration 2348, dc_loss: 0.1036963015794754, tv_loss: 0.03250405192375183\n",
      "iteration 2349, dc_loss: 0.10360490530729294, tv_loss: 0.0325617752969265\n",
      "iteration 2350, dc_loss: 0.10365089029073715, tv_loss: 0.03249642625451088\n",
      "iteration 2351, dc_loss: 0.10357337445020676, tv_loss: 0.032558634877204895\n",
      "iteration 2352, dc_loss: 0.10359735786914825, tv_loss: 0.032528284937143326\n",
      "iteration 2353, dc_loss: 0.10356738418340683, tv_loss: 0.03255399689078331\n",
      "iteration 2354, dc_loss: 0.10357016324996948, tv_loss: 0.03254503011703491\n",
      "iteration 2355, dc_loss: 0.10353085398674011, tv_loss: 0.032554950565099716\n",
      "iteration 2356, dc_loss: 0.10353146493434906, tv_loss: 0.03252740576863289\n",
      "iteration 2357, dc_loss: 0.1035110205411911, tv_loss: 0.03252946957945824\n",
      "iteration 2358, dc_loss: 0.1034904420375824, tv_loss: 0.03254283592104912\n",
      "iteration 2359, dc_loss: 0.10348428040742874, tv_loss: 0.03252996504306793\n",
      "iteration 2360, dc_loss: 0.10346930474042892, tv_loss: 0.032537370920181274\n",
      "iteration 2361, dc_loss: 0.10346201807260513, tv_loss: 0.03253140300512314\n",
      "iteration 2362, dc_loss: 0.10342579334974289, tv_loss: 0.032557807862758636\n",
      "iteration 2363, dc_loss: 0.10344244539737701, tv_loss: 0.03252753987908363\n",
      "iteration 2364, dc_loss: 0.10339467972517014, tv_loss: 0.03256317228078842\n",
      "iteration 2365, dc_loss: 0.10340265929698944, tv_loss: 0.03253015875816345\n",
      "iteration 2366, dc_loss: 0.10337436944246292, tv_loss: 0.03254610300064087\n",
      "iteration 2367, dc_loss: 0.10336703062057495, tv_loss: 0.03253445401787758\n",
      "iteration 2368, dc_loss: 0.10335636883974075, tv_loss: 0.03253354877233505\n",
      "iteration 2369, dc_loss: 0.10332895070314407, tv_loss: 0.032551493495702744\n",
      "iteration 2370, dc_loss: 0.10333282500505447, tv_loss: 0.032535530626773834\n",
      "iteration 2371, dc_loss: 0.10330330580472946, tv_loss: 0.03257274627685547\n",
      "iteration 2372, dc_loss: 0.10330982506275177, tv_loss: 0.03255036100745201\n",
      "iteration 2373, dc_loss: 0.10327500104904175, tv_loss: 0.03255924955010414\n",
      "iteration 2374, dc_loss: 0.10326743125915527, tv_loss: 0.03254872187972069\n",
      "iteration 2375, dc_loss: 0.10327330976724625, tv_loss: 0.03254220262169838\n",
      "iteration 2376, dc_loss: 0.10323657095432281, tv_loss: 0.032570116221904755\n",
      "iteration 2377, dc_loss: 0.10322995483875275, tv_loss: 0.03256644308567047\n",
      "iteration 2378, dc_loss: 0.10320659726858139, tv_loss: 0.03256302699446678\n",
      "iteration 2379, dc_loss: 0.10322980582714081, tv_loss: 0.03253389149904251\n",
      "iteration 2380, dc_loss: 0.10319029539823532, tv_loss: 0.03256767615675926\n",
      "iteration 2381, dc_loss: 0.10321005433797836, tv_loss: 0.032550010830163956\n",
      "iteration 2382, dc_loss: 0.10318512469530106, tv_loss: 0.03255852684378624\n",
      "iteration 2383, dc_loss: 0.10323654115200043, tv_loss: 0.03252419829368591\n",
      "iteration 2384, dc_loss: 0.10318955034017563, tv_loss: 0.03259028494358063\n",
      "iteration 2385, dc_loss: 0.1032726988196373, tv_loss: 0.03253040462732315\n",
      "iteration 2386, dc_loss: 0.10319487750530243, tv_loss: 0.03260849416255951\n",
      "iteration 2387, dc_loss: 0.1032935157418251, tv_loss: 0.03249560669064522\n",
      "iteration 2388, dc_loss: 0.10317514091730118, tv_loss: 0.0326143279671669\n",
      "iteration 2389, dc_loss: 0.10325907915830612, tv_loss: 0.03250129148364067\n",
      "iteration 2390, dc_loss: 0.10312388837337494, tv_loss: 0.03260648623108864\n",
      "iteration 2391, dc_loss: 0.10319186747074127, tv_loss: 0.032513584941625595\n",
      "iteration 2392, dc_loss: 0.10308326780796051, tv_loss: 0.03261192888021469\n",
      "iteration 2393, dc_loss: 0.10314883291721344, tv_loss: 0.03252203017473221\n",
      "iteration 2394, dc_loss: 0.10306355357170105, tv_loss: 0.03257525712251663\n",
      "iteration 2395, dc_loss: 0.10311097651720047, tv_loss: 0.03251221403479576\n",
      "iteration 2396, dc_loss: 0.10300688445568085, tv_loss: 0.03260822594165802\n",
      "iteration 2397, dc_loss: 0.10304564982652664, tv_loss: 0.03253195807337761\n",
      "iteration 2398, dc_loss: 0.1029597595334053, tv_loss: 0.03259677439928055\n",
      "iteration 2399, dc_loss: 0.10299384593963623, tv_loss: 0.03253856673836708\n",
      "iteration 2400, dc_loss: 0.10295207798480988, tv_loss: 0.03256512060761452\n",
      "iteration 2401, dc_loss: 0.10294409096240997, tv_loss: 0.0325627401471138\n",
      "iteration 2402, dc_loss: 0.10292167961597443, tv_loss: 0.03253544121980667\n",
      "iteration 2403, dc_loss: 0.10289674252271652, tv_loss: 0.03255406767129898\n",
      "iteration 2404, dc_loss: 0.10289810597896576, tv_loss: 0.032561518251895905\n",
      "iteration 2405, dc_loss: 0.10288919508457184, tv_loss: 0.03253105655312538\n",
      "iteration 2406, dc_loss: 0.10286533832550049, tv_loss: 0.03254895284771919\n",
      "iteration 2407, dc_loss: 0.10285436362028122, tv_loss: 0.03255413845181465\n",
      "iteration 2408, dc_loss: 0.10285317897796631, tv_loss: 0.032538238912820816\n",
      "iteration 2409, dc_loss: 0.10282965749502182, tv_loss: 0.032547082751989365\n",
      "iteration 2410, dc_loss: 0.10281746089458466, tv_loss: 0.03255745396018028\n",
      "iteration 2411, dc_loss: 0.10281270742416382, tv_loss: 0.0325467586517334\n",
      "iteration 2412, dc_loss: 0.1027931421995163, tv_loss: 0.032559093087911606\n",
      "iteration 2413, dc_loss: 0.10279051959514618, tv_loss: 0.032548416405916214\n",
      "iteration 2414, dc_loss: 0.10278294235467911, tv_loss: 0.03254012390971184\n",
      "iteration 2415, dc_loss: 0.10275623947381973, tv_loss: 0.032555099576711655\n",
      "iteration 2416, dc_loss: 0.10275214910507202, tv_loss: 0.032557401806116104\n",
      "iteration 2417, dc_loss: 0.10274495929479599, tv_loss: 0.032562755048274994\n",
      "iteration 2418, dc_loss: 0.10273566842079163, tv_loss: 0.03255945444107056\n",
      "iteration 2419, dc_loss: 0.10271964967250824, tv_loss: 0.032550495117902756\n",
      "iteration 2420, dc_loss: 0.10270337760448456, tv_loss: 0.03255374729633331\n",
      "iteration 2421, dc_loss: 0.10270087420940399, tv_loss: 0.03255651518702507\n",
      "iteration 2422, dc_loss: 0.1026896983385086, tv_loss: 0.03256285563111305\n",
      "iteration 2423, dc_loss: 0.10266852378845215, tv_loss: 0.032569754868745804\n",
      "iteration 2424, dc_loss: 0.10266593843698502, tv_loss: 0.032554399222135544\n",
      "iteration 2425, dc_loss: 0.102662593126297, tv_loss: 0.03254237025976181\n",
      "iteration 2426, dc_loss: 0.10263895988464355, tv_loss: 0.03255320340394974\n",
      "iteration 2427, dc_loss: 0.10263767838478088, tv_loss: 0.03254232555627823\n",
      "iteration 2428, dc_loss: 0.10262709110975266, tv_loss: 0.03254581242799759\n",
      "iteration 2429, dc_loss: 0.10259837657213211, tv_loss: 0.03256314620375633\n",
      "iteration 2430, dc_loss: 0.10260678827762604, tv_loss: 0.03255157917737961\n",
      "iteration 2431, dc_loss: 0.10259389877319336, tv_loss: 0.03257045894861221\n",
      "iteration 2432, dc_loss: 0.10257584601640701, tv_loss: 0.03257349506020546\n",
      "iteration 2433, dc_loss: 0.10257342457771301, tv_loss: 0.03255147114396095\n",
      "iteration 2434, dc_loss: 0.10256039351224899, tv_loss: 0.032554369419813156\n",
      "iteration 2435, dc_loss: 0.1025439202785492, tv_loss: 0.032558634877204895\n",
      "iteration 2436, dc_loss: 0.10253816097974777, tv_loss: 0.032561179250478745\n",
      "iteration 2437, dc_loss: 0.10252667218446732, tv_loss: 0.032571934163570404\n",
      "iteration 2438, dc_loss: 0.10251753777265549, tv_loss: 0.0325683169066906\n",
      "iteration 2439, dc_loss: 0.10251384228467941, tv_loss: 0.03255178779363632\n",
      "iteration 2440, dc_loss: 0.10249058157205582, tv_loss: 0.032561369240283966\n",
      "iteration 2441, dc_loss: 0.10248124599456787, tv_loss: 0.03255986422300339\n",
      "iteration 2442, dc_loss: 0.10248009860515594, tv_loss: 0.03255564346909523\n",
      "iteration 2443, dc_loss: 0.1024620309472084, tv_loss: 0.03256503492593765\n",
      "iteration 2444, dc_loss: 0.10245051234960556, tv_loss: 0.03257635235786438\n",
      "iteration 2445, dc_loss: 0.10244100540876389, tv_loss: 0.03257187828421593\n",
      "iteration 2446, dc_loss: 0.10243959724903107, tv_loss: 0.03255580738186836\n",
      "iteration 2447, dc_loss: 0.10242599993944168, tv_loss: 0.03255655989050865\n",
      "iteration 2448, dc_loss: 0.10240311175584793, tv_loss: 0.03256545960903168\n",
      "iteration 2449, dc_loss: 0.1023997887969017, tv_loss: 0.032559581100940704\n",
      "iteration 2450, dc_loss: 0.1023985892534256, tv_loss: 0.03255586326122284\n",
      "iteration 2451, dc_loss: 0.1023770421743393, tv_loss: 0.0325748547911644\n",
      "iteration 2452, dc_loss: 0.10236337035894394, tv_loss: 0.03258570283651352\n",
      "iteration 2453, dc_loss: 0.10236553847789764, tv_loss: 0.03256429731845856\n",
      "iteration 2454, dc_loss: 0.102353036403656, tv_loss: 0.03256041184067726\n",
      "iteration 2455, dc_loss: 0.10233411937952042, tv_loss: 0.03256653621792793\n",
      "iteration 2456, dc_loss: 0.10232596844434738, tv_loss: 0.032567378133535385\n",
      "iteration 2457, dc_loss: 0.10232418030500412, tv_loss: 0.032560136169195175\n",
      "iteration 2458, dc_loss: 0.10230853408575058, tv_loss: 0.03257022425532341\n",
      "iteration 2459, dc_loss: 0.10228893905878067, tv_loss: 0.032582443207502365\n",
      "iteration 2460, dc_loss: 0.10228927433490753, tv_loss: 0.03256812319159508\n",
      "iteration 2461, dc_loss: 0.10228073596954346, tv_loss: 0.03256409987807274\n",
      "iteration 2462, dc_loss: 0.10226043313741684, tv_loss: 0.03257492184638977\n",
      "iteration 2463, dc_loss: 0.10225803405046463, tv_loss: 0.03256417438387871\n",
      "iteration 2464, dc_loss: 0.10224568098783493, tv_loss: 0.03256832808256149\n",
      "iteration 2465, dc_loss: 0.10223179310560226, tv_loss: 0.0325709767639637\n",
      "iteration 2466, dc_loss: 0.10222430527210236, tv_loss: 0.03257139399647713\n",
      "iteration 2467, dc_loss: 0.10221218317747116, tv_loss: 0.03257926553487778\n",
      "iteration 2468, dc_loss: 0.10220767557621002, tv_loss: 0.03257786110043526\n",
      "iteration 2469, dc_loss: 0.10219428688287735, tv_loss: 0.03257410600781441\n",
      "iteration 2470, dc_loss: 0.10217811167240143, tv_loss: 0.03257019817829132\n",
      "iteration 2471, dc_loss: 0.10217150300741196, tv_loss: 0.03256883844733238\n",
      "iteration 2472, dc_loss: 0.10216338187456131, tv_loss: 0.03257543593645096\n",
      "iteration 2473, dc_loss: 0.10215301811695099, tv_loss: 0.03258613869547844\n",
      "iteration 2474, dc_loss: 0.10214366763830185, tv_loss: 0.03257942944765091\n",
      "iteration 2475, dc_loss: 0.10213013738393784, tv_loss: 0.03256956860423088\n",
      "iteration 2476, dc_loss: 0.10211849957704544, tv_loss: 0.03257232904434204\n",
      "iteration 2477, dc_loss: 0.10211169719696045, tv_loss: 0.032570499926805496\n",
      "iteration 2478, dc_loss: 0.10210248827934265, tv_loss: 0.03258366510272026\n",
      "iteration 2479, dc_loss: 0.10209158807992935, tv_loss: 0.03258686140179634\n",
      "iteration 2480, dc_loss: 0.10207698494195938, tv_loss: 0.03258293494582176\n",
      "iteration 2481, dc_loss: 0.10206860303878784, tv_loss: 0.03257107734680176\n",
      "iteration 2482, dc_loss: 0.10205888748168945, tv_loss: 0.03257033973932266\n",
      "iteration 2483, dc_loss: 0.10204781591892242, tv_loss: 0.0325784832239151\n",
      "iteration 2484, dc_loss: 0.10204461961984634, tv_loss: 0.03257713466882706\n",
      "iteration 2485, dc_loss: 0.10202724486589432, tv_loss: 0.032588060945272446\n",
      "iteration 2486, dc_loss: 0.10201095044612885, tv_loss: 0.03258870914578438\n",
      "iteration 2487, dc_loss: 0.10201047360897064, tv_loss: 0.032569438219070435\n",
      "iteration 2488, dc_loss: 0.10200120508670807, tv_loss: 0.032569799572229385\n",
      "iteration 2489, dc_loss: 0.10198123008012772, tv_loss: 0.03257797285914421\n",
      "iteration 2490, dc_loss: 0.1019807904958725, tv_loss: 0.03256799653172493\n",
      "iteration 2491, dc_loss: 0.1019660010933876, tv_loss: 0.03258093446493149\n",
      "iteration 2492, dc_loss: 0.10195380449295044, tv_loss: 0.03258766978979111\n",
      "iteration 2493, dc_loss: 0.10194924473762512, tv_loss: 0.03258822485804558\n",
      "iteration 2494, dc_loss: 0.10193616896867752, tv_loss: 0.03258025273680687\n",
      "iteration 2495, dc_loss: 0.10192293673753738, tv_loss: 0.03258125111460686\n",
      "iteration 2496, dc_loss: 0.10191226750612259, tv_loss: 0.032589953392744064\n",
      "iteration 2497, dc_loss: 0.10191059112548828, tv_loss: 0.032597579061985016\n",
      "iteration 2498, dc_loss: 0.10189726948738098, tv_loss: 0.03258552774786949\n",
      "iteration 2499, dc_loss: 0.10188117623329163, tv_loss: 0.032580967992544174\n",
      "iteration 2500, dc_loss: 0.10187356173992157, tv_loss: 0.03259037435054779\n",
      "iteration 2501, dc_loss: 0.10186374187469482, tv_loss: 0.032597266137599945\n",
      "iteration 2502, dc_loss: 0.10185626894235611, tv_loss: 0.03259271755814552\n",
      "iteration 2503, dc_loss: 0.1018415167927742, tv_loss: 0.032583560794591904\n",
      "iteration 2504, dc_loss: 0.1018371731042862, tv_loss: 0.03258049488067627\n",
      "iteration 2505, dc_loss: 0.1018306240439415, tv_loss: 0.032583389431238174\n",
      "iteration 2506, dc_loss: 0.10180916637182236, tv_loss: 0.03259234502911568\n",
      "iteration 2507, dc_loss: 0.10180085897445679, tv_loss: 0.03259317949414253\n",
      "iteration 2508, dc_loss: 0.10179316997528076, tv_loss: 0.032579224556684494\n",
      "iteration 2509, dc_loss: 0.10178156942129135, tv_loss: 0.03258037567138672\n",
      "iteration 2510, dc_loss: 0.1017778143286705, tv_loss: 0.03257688507437706\n",
      "iteration 2511, dc_loss: 0.10175938159227371, tv_loss: 0.03258781507611275\n",
      "iteration 2512, dc_loss: 0.1017516627907753, tv_loss: 0.032595835626125336\n",
      "iteration 2513, dc_loss: 0.10174597054719925, tv_loss: 0.03259514644742012\n",
      "iteration 2514, dc_loss: 0.1017344743013382, tv_loss: 0.03258984908461571\n",
      "iteration 2515, dc_loss: 0.1017199456691742, tv_loss: 0.032589901238679886\n",
      "iteration 2516, dc_loss: 0.10171136260032654, tv_loss: 0.03258420154452324\n",
      "iteration 2517, dc_loss: 0.1016976460814476, tv_loss: 0.032585740089416504\n",
      "iteration 2518, dc_loss: 0.10169069468975067, tv_loss: 0.032593101263046265\n",
      "iteration 2519, dc_loss: 0.10168585181236267, tv_loss: 0.03259468823671341\n",
      "iteration 2520, dc_loss: 0.10167047381401062, tv_loss: 0.032594695687294006\n",
      "iteration 2521, dc_loss: 0.1016605943441391, tv_loss: 0.032585833221673965\n",
      "iteration 2522, dc_loss: 0.10164476931095123, tv_loss: 0.032601241022348404\n",
      "iteration 2523, dc_loss: 0.10164015740156174, tv_loss: 0.0326034314930439\n",
      "iteration 2524, dc_loss: 0.10163743048906326, tv_loss: 0.032599642872810364\n",
      "iteration 2525, dc_loss: 0.10161779075860977, tv_loss: 0.03259335830807686\n",
      "iteration 2526, dc_loss: 0.10161199420690536, tv_loss: 0.03259454295039177\n",
      "iteration 2527, dc_loss: 0.10160215944051743, tv_loss: 0.03260233253240585\n",
      "iteration 2528, dc_loss: 0.10158701241016388, tv_loss: 0.03261062130331993\n",
      "iteration 2529, dc_loss: 0.10158492624759674, tv_loss: 0.03258625790476799\n",
      "iteration 2530, dc_loss: 0.10157434642314911, tv_loss: 0.03259241580963135\n",
      "iteration 2531, dc_loss: 0.10154929757118225, tv_loss: 0.032622452825307846\n",
      "iteration 2532, dc_loss: 0.10155032575130463, tv_loss: 0.03259987384080887\n",
      "iteration 2533, dc_loss: 0.10155042260885239, tv_loss: 0.032589834183454514\n",
      "iteration 2534, dc_loss: 0.10152082145214081, tv_loss: 0.03261825442314148\n",
      "iteration 2535, dc_loss: 0.10152396559715271, tv_loss: 0.032597050070762634\n",
      "iteration 2536, dc_loss: 0.10151693969964981, tv_loss: 0.03259756788611412\n",
      "iteration 2537, dc_loss: 0.10149629414081573, tv_loss: 0.03260345384478569\n",
      "iteration 2538, dc_loss: 0.10148707777261734, tv_loss: 0.03260669857263565\n",
      "iteration 2539, dc_loss: 0.10147947072982788, tv_loss: 0.03260691091418266\n",
      "iteration 2540, dc_loss: 0.10147230327129364, tv_loss: 0.032596610486507416\n",
      "iteration 2541, dc_loss: 0.10145240277051926, tv_loss: 0.03260505199432373\n",
      "iteration 2542, dc_loss: 0.10144805908203125, tv_loss: 0.032614566385746\n",
      "iteration 2543, dc_loss: 0.10144698619842529, tv_loss: 0.03260399028658867\n",
      "iteration 2544, dc_loss: 0.10142160207033157, tv_loss: 0.03260236978530884\n",
      "iteration 2545, dc_loss: 0.10142023861408234, tv_loss: 0.03261420875787735\n",
      "iteration 2546, dc_loss: 0.10141590237617493, tv_loss: 0.03260641172528267\n",
      "iteration 2547, dc_loss: 0.10139582306146622, tv_loss: 0.03260137140750885\n",
      "iteration 2548, dc_loss: 0.10138638317584991, tv_loss: 0.03261132165789604\n",
      "iteration 2549, dc_loss: 0.1013781949877739, tv_loss: 0.03261127695441246\n",
      "iteration 2550, dc_loss: 0.101361945271492, tv_loss: 0.03261563181877136\n",
      "iteration 2551, dc_loss: 0.10136374086141586, tv_loss: 0.03260056674480438\n",
      "iteration 2552, dc_loss: 0.10135315358638763, tv_loss: 0.03260120376944542\n",
      "iteration 2553, dc_loss: 0.10133275389671326, tv_loss: 0.032609980553388596\n",
      "iteration 2554, dc_loss: 0.10133369266986847, tv_loss: 0.03261365741491318\n",
      "iteration 2555, dc_loss: 0.1013147383928299, tv_loss: 0.032605718821287155\n",
      "iteration 2556, dc_loss: 0.10130993276834488, tv_loss: 0.03260879963636398\n",
      "iteration 2557, dc_loss: 0.10130034387111664, tv_loss: 0.03262094780802727\n",
      "iteration 2558, dc_loss: 0.10128167271614075, tv_loss: 0.03261028602719307\n",
      "iteration 2559, dc_loss: 0.10127879679203033, tv_loss: 0.0326174795627594\n",
      "iteration 2560, dc_loss: 0.1012732982635498, tv_loss: 0.03260987624526024\n",
      "iteration 2561, dc_loss: 0.10126352310180664, tv_loss: 0.03260514512658119\n",
      "iteration 2562, dc_loss: 0.10124295204877853, tv_loss: 0.03262775018811226\n",
      "iteration 2563, dc_loss: 0.10123869776725769, tv_loss: 0.032613303512334824\n",
      "iteration 2564, dc_loss: 0.10123053193092346, tv_loss: 0.03262181580066681\n",
      "iteration 2565, dc_loss: 0.10120794922113419, tv_loss: 0.032630570232868195\n",
      "iteration 2566, dc_loss: 0.10120544582605362, tv_loss: 0.032620035111904144\n",
      "iteration 2567, dc_loss: 0.10120921581983566, tv_loss: 0.032629724591970444\n",
      "iteration 2568, dc_loss: 0.10119125992059708, tv_loss: 0.03260203078389168\n",
      "iteration 2569, dc_loss: 0.10117807239294052, tv_loss: 0.032638974487781525\n",
      "iteration 2570, dc_loss: 0.10116574913263321, tv_loss: 0.0326274111866951\n",
      "iteration 2571, dc_loss: 0.10115482658147812, tv_loss: 0.03261410817503929\n",
      "iteration 2572, dc_loss: 0.10114945471286774, tv_loss: 0.03264148533344269\n",
      "iteration 2573, dc_loss: 0.1011311262845993, tv_loss: 0.03262590616941452\n",
      "iteration 2574, dc_loss: 0.1011335477232933, tv_loss: 0.032623887062072754\n",
      "iteration 2575, dc_loss: 0.1011214330792427, tv_loss: 0.032626982778310776\n",
      "iteration 2576, dc_loss: 0.1011071652173996, tv_loss: 0.03262295573949814\n",
      "iteration 2577, dc_loss: 0.10110379755496979, tv_loss: 0.032618239521980286\n",
      "iteration 2578, dc_loss: 0.10108833014965057, tv_loss: 0.03262430801987648\n",
      "iteration 2579, dc_loss: 0.101071298122406, tv_loss: 0.03264209255576134\n",
      "iteration 2580, dc_loss: 0.1010693684220314, tv_loss: 0.03260333463549614\n",
      "iteration 2581, dc_loss: 0.10106189548969269, tv_loss: 0.032633330672979355\n",
      "iteration 2582, dc_loss: 0.10103810578584671, tv_loss: 0.032633762806653976\n",
      "iteration 2583, dc_loss: 0.10104730725288391, tv_loss: 0.032603345811367035\n",
      "iteration 2584, dc_loss: 0.10102857649326324, tv_loss: 0.03262997046113014\n",
      "iteration 2585, dc_loss: 0.10101339221000671, tv_loss: 0.032621756196022034\n",
      "iteration 2586, dc_loss: 0.10101281106472015, tv_loss: 0.03262884169816971\n",
      "iteration 2587, dc_loss: 0.10100528597831726, tv_loss: 0.03260808065533638\n",
      "iteration 2588, dc_loss: 0.10098395496606827, tv_loss: 0.03263446316123009\n",
      "iteration 2589, dc_loss: 0.1009725034236908, tv_loss: 0.03265032172203064\n",
      "iteration 2590, dc_loss: 0.1009756326675415, tv_loss: 0.03260480612516403\n",
      "iteration 2591, dc_loss: 0.10096514970064163, tv_loss: 0.03265385702252388\n",
      "iteration 2592, dc_loss: 0.10095773637294769, tv_loss: 0.03261001780629158\n",
      "iteration 2593, dc_loss: 0.10093286633491516, tv_loss: 0.03265567868947983\n",
      "iteration 2594, dc_loss: 0.10093921422958374, tv_loss: 0.03262674808502197\n",
      "iteration 2595, dc_loss: 0.10093028098344803, tv_loss: 0.03262786939740181\n",
      "iteration 2596, dc_loss: 0.10092417150735855, tv_loss: 0.03263699635863304\n",
      "iteration 2597, dc_loss: 0.10091305524110794, tv_loss: 0.03263745456933975\n",
      "iteration 2598, dc_loss: 0.10092367976903915, tv_loss: 0.032634422183036804\n",
      "iteration 2599, dc_loss: 0.10090313851833344, tv_loss: 0.03261806070804596\n",
      "iteration 2600, dc_loss: 0.10087886452674866, tv_loss: 0.032682184129953384\n",
      "iteration 2601, dc_loss: 0.10087625682353973, tv_loss: 0.03261689469218254\n",
      "iteration 2602, dc_loss: 0.10085862874984741, tv_loss: 0.03266938403248787\n",
      "iteration 2603, dc_loss: 0.10084810107946396, tv_loss: 0.03263210132718086\n",
      "iteration 2604, dc_loss: 0.10083654522895813, tv_loss: 0.03266531229019165\n",
      "iteration 2605, dc_loss: 0.10082990676164627, tv_loss: 0.03264078497886658\n",
      "iteration 2606, dc_loss: 0.10082953423261642, tv_loss: 0.032654087990522385\n",
      "iteration 2607, dc_loss: 0.10080377012491226, tv_loss: 0.032645631581544876\n",
      "iteration 2608, dc_loss: 0.10078851878643036, tv_loss: 0.03267381712794304\n",
      "iteration 2609, dc_loss: 0.10077685862779617, tv_loss: 0.03264216333627701\n",
      "iteration 2610, dc_loss: 0.10079246759414673, tv_loss: 0.03266018256545067\n",
      "iteration 2611, dc_loss: 0.10077544301748276, tv_loss: 0.03263199329376221\n",
      "iteration 2612, dc_loss: 0.10074524581432343, tv_loss: 0.03268823400139809\n",
      "iteration 2613, dc_loss: 0.10073386132717133, tv_loss: 0.03265933692455292\n",
      "iteration 2614, dc_loss: 0.10075929760932922, tv_loss: 0.03265856206417084\n",
      "iteration 2615, dc_loss: 0.10073816776275635, tv_loss: 0.032641779631376266\n",
      "iteration 2616, dc_loss: 0.1007109135389328, tv_loss: 0.032679688185453415\n",
      "iteration 2617, dc_loss: 0.10068704187870026, tv_loss: 0.03267688676714897\n",
      "iteration 2618, dc_loss: 0.10071255266666412, tv_loss: 0.0326496846973896\n",
      "iteration 2619, dc_loss: 0.10070552676916122, tv_loss: 0.032632213085889816\n",
      "iteration 2620, dc_loss: 0.10067666321992874, tv_loss: 0.03266459330916405\n",
      "iteration 2621, dc_loss: 0.10065823793411255, tv_loss: 0.0326562263071537\n",
      "iteration 2622, dc_loss: 0.10068049281835556, tv_loss: 0.032649241387844086\n",
      "iteration 2623, dc_loss: 0.10065604001283646, tv_loss: 0.032641056925058365\n",
      "iteration 2624, dc_loss: 0.10064289718866348, tv_loss: 0.032675549387931824\n",
      "iteration 2625, dc_loss: 0.10061252117156982, tv_loss: 0.032675500959157944\n",
      "iteration 2626, dc_loss: 0.10064591467380524, tv_loss: 0.032653000205755234\n",
      "iteration 2627, dc_loss: 0.10062366724014282, tv_loss: 0.03265320509672165\n",
      "iteration 2628, dc_loss: 0.1006176620721817, tv_loss: 0.032649870961904526\n",
      "iteration 2629, dc_loss: 0.10058002173900604, tv_loss: 0.03268520534038544\n",
      "iteration 2630, dc_loss: 0.10059724003076553, tv_loss: 0.032639920711517334\n",
      "iteration 2631, dc_loss: 0.10057881474494934, tv_loss: 0.03267235681414604\n",
      "iteration 2632, dc_loss: 0.10058370232582092, tv_loss: 0.032639969140291214\n",
      "iteration 2633, dc_loss: 0.1005358099937439, tv_loss: 0.032673079520463943\n",
      "iteration 2634, dc_loss: 0.1005476638674736, tv_loss: 0.032678063958883286\n",
      "iteration 2635, dc_loss: 0.10054108500480652, tv_loss: 0.03262472152709961\n",
      "iteration 2636, dc_loss: 0.10053239017724991, tv_loss: 0.03268938884139061\n",
      "iteration 2637, dc_loss: 0.10049524903297424, tv_loss: 0.032665934413671494\n",
      "iteration 2638, dc_loss: 0.10050556808710098, tv_loss: 0.0326855406165123\n",
      "iteration 2639, dc_loss: 0.10051065683364868, tv_loss: 0.03264833614230156\n",
      "iteration 2640, dc_loss: 0.10048384219408035, tv_loss: 0.03266472369432449\n",
      "iteration 2641, dc_loss: 0.10046224296092987, tv_loss: 0.032697875052690506\n",
      "iteration 2642, dc_loss: 0.10045500099658966, tv_loss: 0.03265247866511345\n",
      "iteration 2643, dc_loss: 0.10047329217195511, tv_loss: 0.032683249562978745\n",
      "iteration 2644, dc_loss: 0.10043957084417343, tv_loss: 0.032659661024808884\n",
      "iteration 2645, dc_loss: 0.10044381022453308, tv_loss: 0.032684702426195145\n",
      "iteration 2646, dc_loss: 0.10042112320661545, tv_loss: 0.03267605975270271\n",
      "iteration 2647, dc_loss: 0.10041780024766922, tv_loss: 0.03267411142587662\n",
      "iteration 2648, dc_loss: 0.10039941221475601, tv_loss: 0.032692816108465195\n",
      "iteration 2649, dc_loss: 0.10040412843227386, tv_loss: 0.032640453428030014\n",
      "iteration 2650, dc_loss: 0.10038711875677109, tv_loss: 0.03270282223820686\n",
      "iteration 2651, dc_loss: 0.10037305951118469, tv_loss: 0.03265602886676788\n",
      "iteration 2652, dc_loss: 0.10036312788724899, tv_loss: 0.03270767629146576\n",
      "iteration 2653, dc_loss: 0.10036672651767731, tv_loss: 0.03266315534710884\n",
      "iteration 2654, dc_loss: 0.10035338997840881, tv_loss: 0.03268280252814293\n",
      "iteration 2655, dc_loss: 0.10032912343740463, tv_loss: 0.03270423039793968\n",
      "iteration 2656, dc_loss: 0.10033505409955978, tv_loss: 0.03266520798206329\n",
      "iteration 2657, dc_loss: 0.10034219920635223, tv_loss: 0.032697681337594986\n",
      "iteration 2658, dc_loss: 0.10033189505338669, tv_loss: 0.032663147896528244\n",
      "iteration 2659, dc_loss: 0.10030557960271835, tv_loss: 0.03272485360503197\n",
      "iteration 2660, dc_loss: 0.10029944032430649, tv_loss: 0.03267914056777954\n",
      "iteration 2661, dc_loss: 0.1002802699804306, tv_loss: 0.03273928910493851\n",
      "iteration 2662, dc_loss: 0.10029110312461853, tv_loss: 0.03267231211066246\n",
      "iteration 2663, dc_loss: 0.10027366876602173, tv_loss: 0.03271373733878136\n",
      "iteration 2664, dc_loss: 0.10027948021888733, tv_loss: 0.03265848755836487\n",
      "iteration 2665, dc_loss: 0.10023126006126404, tv_loss: 0.03273724764585495\n",
      "iteration 2666, dc_loss: 0.10024293512105942, tv_loss: 0.03268539533019066\n",
      "iteration 2667, dc_loss: 0.1002269759774208, tv_loss: 0.032701924443244934\n",
      "iteration 2668, dc_loss: 0.10024713724851608, tv_loss: 0.03266710788011551\n",
      "iteration 2669, dc_loss: 0.10019862651824951, tv_loss: 0.03270190209150314\n",
      "iteration 2670, dc_loss: 0.10019570589065552, tv_loss: 0.03271433338522911\n",
      "iteration 2671, dc_loss: 0.10018932819366455, tv_loss: 0.03267557546496391\n",
      "iteration 2672, dc_loss: 0.10019765794277191, tv_loss: 0.032719630748033524\n",
      "iteration 2673, dc_loss: 0.10016913712024689, tv_loss: 0.03270205855369568\n",
      "iteration 2674, dc_loss: 0.10015088319778442, tv_loss: 0.03273719549179077\n",
      "iteration 2675, dc_loss: 0.10016463696956635, tv_loss: 0.0326925665140152\n",
      "iteration 2676, dc_loss: 0.10014289617538452, tv_loss: 0.03272625058889389\n",
      "iteration 2677, dc_loss: 0.1001448929309845, tv_loss: 0.03268150985240936\n",
      "iteration 2678, dc_loss: 0.10012305527925491, tv_loss: 0.03274274617433548\n",
      "iteration 2679, dc_loss: 0.10014458745718002, tv_loss: 0.032654035836458206\n",
      "iteration 2680, dc_loss: 0.10010629147291183, tv_loss: 0.03274918720126152\n",
      "iteration 2681, dc_loss: 0.10010506957769394, tv_loss: 0.03267640992999077\n",
      "iteration 2682, dc_loss: 0.10007493942975998, tv_loss: 0.032778725028038025\n",
      "iteration 2683, dc_loss: 0.100101538002491, tv_loss: 0.032683588564395905\n",
      "iteration 2684, dc_loss: 0.10006354004144669, tv_loss: 0.03277629241347313\n",
      "iteration 2685, dc_loss: 0.10005855560302734, tv_loss: 0.0327618271112442\n",
      "iteration 2686, dc_loss: 0.10002555698156357, tv_loss: 0.03273222595453262\n",
      "iteration 2687, dc_loss: 0.1000683531165123, tv_loss: 0.03270874172449112\n",
      "iteration 2688, dc_loss: 0.10001780837774277, tv_loss: 0.032705940306186676\n",
      "iteration 2689, dc_loss: 0.10002044588327408, tv_loss: 0.032710302621126175\n",
      "iteration 2690, dc_loss: 0.0999954342842102, tv_loss: 0.03271881863474846\n",
      "iteration 2691, dc_loss: 0.10003119707107544, tv_loss: 0.032652419060468674\n",
      "iteration 2692, dc_loss: 0.09999478608369827, tv_loss: 0.03273240849375725\n",
      "iteration 2693, dc_loss: 0.09997989982366562, tv_loss: 0.032690417021512985\n",
      "iteration 2694, dc_loss: 0.09996223449707031, tv_loss: 0.032761674374341965\n",
      "iteration 2695, dc_loss: 0.09997650235891342, tv_loss: 0.03268986940383911\n",
      "iteration 2696, dc_loss: 0.09994789212942123, tv_loss: 0.03275701031088829\n",
      "iteration 2697, dc_loss: 0.09992873668670654, tv_loss: 0.03272995352745056\n",
      "iteration 2698, dc_loss: 0.0999215766787529, tv_loss: 0.032756634056568146\n",
      "iteration 2699, dc_loss: 0.09992744028568268, tv_loss: 0.03273525834083557\n",
      "iteration 2700, dc_loss: 0.09991484880447388, tv_loss: 0.0327046737074852\n",
      "iteration 2701, dc_loss: 0.09989146143198013, tv_loss: 0.03272488713264465\n",
      "iteration 2702, dc_loss: 0.09989214688539505, tv_loss: 0.032718852162361145\n",
      "iteration 2703, dc_loss: 0.09988953918218613, tv_loss: 0.0326998308300972\n",
      "iteration 2704, dc_loss: 0.09988183528184891, tv_loss: 0.032731957733631134\n",
      "iteration 2705, dc_loss: 0.09985934942960739, tv_loss: 0.03272143006324768\n",
      "iteration 2706, dc_loss: 0.09987429529428482, tv_loss: 0.03274182975292206\n",
      "iteration 2707, dc_loss: 0.09988390654325485, tv_loss: 0.03269759565591812\n",
      "iteration 2708, dc_loss: 0.09985797107219696, tv_loss: 0.03274422883987427\n",
      "iteration 2709, dc_loss: 0.09982845932245255, tv_loss: 0.03274055942893028\n",
      "iteration 2710, dc_loss: 0.09981436282396317, tv_loss: 0.03275168687105179\n",
      "iteration 2711, dc_loss: 0.09983185678720474, tv_loss: 0.03270478546619415\n",
      "iteration 2712, dc_loss: 0.0997997298836708, tv_loss: 0.032733574509620667\n",
      "iteration 2713, dc_loss: 0.09978923946619034, tv_loss: 0.032720014452934265\n",
      "iteration 2714, dc_loss: 0.09977426379919052, tv_loss: 0.03274533152580261\n",
      "iteration 2715, dc_loss: 0.09980267286300659, tv_loss: 0.032691504806280136\n",
      "iteration 2716, dc_loss: 0.09975985437631607, tv_loss: 0.032766878604888916\n",
      "iteration 2717, dc_loss: 0.09977347403764725, tv_loss: 0.032736726105213165\n",
      "iteration 2718, dc_loss: 0.09974516928195953, tv_loss: 0.032749924808740616\n",
      "iteration 2719, dc_loss: 0.09979544579982758, tv_loss: 0.032684553414583206\n",
      "iteration 2720, dc_loss: 0.09973263740539551, tv_loss: 0.0327431857585907\n",
      "iteration 2721, dc_loss: 0.09975631535053253, tv_loss: 0.03268929570913315\n",
      "iteration 2722, dc_loss: 0.09970611333847046, tv_loss: 0.03277267888188362\n",
      "iteration 2723, dc_loss: 0.09976806491613388, tv_loss: 0.032671086490154266\n",
      "iteration 2724, dc_loss: 0.09969425946474075, tv_loss: 0.03278830274939537\n",
      "iteration 2725, dc_loss: 0.09974666684865952, tv_loss: 0.0327221117913723\n",
      "iteration 2726, dc_loss: 0.09968187659978867, tv_loss: 0.032766055315732956\n",
      "iteration 2727, dc_loss: 0.09976500272750854, tv_loss: 0.03267250582575798\n",
      "iteration 2728, dc_loss: 0.09965772181749344, tv_loss: 0.03276141360402107\n",
      "iteration 2729, dc_loss: 0.09967916458845139, tv_loss: 0.032693471759557724\n",
      "iteration 2730, dc_loss: 0.09962011128664017, tv_loss: 0.03276669234037399\n",
      "iteration 2731, dc_loss: 0.0996701568365097, tv_loss: 0.03268805518746376\n",
      "iteration 2732, dc_loss: 0.09964574128389359, tv_loss: 0.03275737538933754\n",
      "iteration 2733, dc_loss: 0.09963095188140869, tv_loss: 0.032750148326158524\n",
      "iteration 2734, dc_loss: 0.09961271286010742, tv_loss: 0.03273089602589607\n",
      "iteration 2735, dc_loss: 0.09959443658590317, tv_loss: 0.03272099047899246\n",
      "iteration 2736, dc_loss: 0.09960506856441498, tv_loss: 0.032718557864427567\n",
      "iteration 2737, dc_loss: 0.09958945214748383, tv_loss: 0.03273211792111397\n",
      "iteration 2738, dc_loss: 0.09959767013788223, tv_loss: 0.03274025768041611\n",
      "iteration 2739, dc_loss: 0.0995829626917839, tv_loss: 0.03271446377038956\n",
      "iteration 2740, dc_loss: 0.09954750537872314, tv_loss: 0.03274180740118027\n",
      "iteration 2741, dc_loss: 0.09951809048652649, tv_loss: 0.0327516607940197\n",
      "iteration 2742, dc_loss: 0.09952405095100403, tv_loss: 0.03274063766002655\n",
      "iteration 2743, dc_loss: 0.09954848885536194, tv_loss: 0.032703518867492676\n",
      "iteration 2744, dc_loss: 0.09952041506767273, tv_loss: 0.03273681178689003\n",
      "iteration 2745, dc_loss: 0.09949978440999985, tv_loss: 0.03271941468119621\n",
      "iteration 2746, dc_loss: 0.09947703778743744, tv_loss: 0.032749347388744354\n",
      "iteration 2747, dc_loss: 0.09949736297130585, tv_loss: 0.032703593373298645\n",
      "iteration 2748, dc_loss: 0.09947121143341064, tv_loss: 0.03275435417890549\n",
      "iteration 2749, dc_loss: 0.09946177899837494, tv_loss: 0.03275018930435181\n",
      "iteration 2750, dc_loss: 0.0994407907128334, tv_loss: 0.032738614827394485\n",
      "iteration 2751, dc_loss: 0.09945884346961975, tv_loss: 0.03270157054066658\n",
      "iteration 2752, dc_loss: 0.09942413121461868, tv_loss: 0.032738640904426575\n",
      "iteration 2753, dc_loss: 0.09942270815372467, tv_loss: 0.032722506672143936\n",
      "iteration 2754, dc_loss: 0.09940332174301147, tv_loss: 0.0327545665204525\n",
      "iteration 2755, dc_loss: 0.09942331165075302, tv_loss: 0.03269331902265549\n",
      "iteration 2756, dc_loss: 0.09937731176614761, tv_loss: 0.03276250883936882\n",
      "iteration 2757, dc_loss: 0.09937327355146408, tv_loss: 0.03274817392230034\n",
      "iteration 2758, dc_loss: 0.09935737401247025, tv_loss: 0.032740987837314606\n",
      "iteration 2759, dc_loss: 0.09937851876020432, tv_loss: 0.0327095203101635\n",
      "iteration 2760, dc_loss: 0.09934887290000916, tv_loss: 0.03272975981235504\n",
      "iteration 2761, dc_loss: 0.09933266043663025, tv_loss: 0.032722752541303635\n",
      "iteration 2762, dc_loss: 0.09932349622249603, tv_loss: 0.03274663910269737\n",
      "iteration 2763, dc_loss: 0.09934065490961075, tv_loss: 0.03269314765930176\n",
      "iteration 2764, dc_loss: 0.09929930418729782, tv_loss: 0.032767657190561295\n",
      "iteration 2765, dc_loss: 0.09929849952459335, tv_loss: 0.03274455666542053\n",
      "iteration 2766, dc_loss: 0.09927628934383392, tv_loss: 0.03275420516729355\n",
      "iteration 2767, dc_loss: 0.09931333363056183, tv_loss: 0.03270795941352844\n",
      "iteration 2768, dc_loss: 0.09926901012659073, tv_loss: 0.032734375447034836\n",
      "iteration 2769, dc_loss: 0.09926709532737732, tv_loss: 0.03272093087434769\n",
      "iteration 2770, dc_loss: 0.09924395382404327, tv_loss: 0.03275502100586891\n",
      "iteration 2771, dc_loss: 0.09927810728549957, tv_loss: 0.03269082307815552\n",
      "iteration 2772, dc_loss: 0.09922825545072556, tv_loss: 0.032770656049251556\n",
      "iteration 2773, dc_loss: 0.0992484837770462, tv_loss: 0.03273121640086174\n",
      "iteration 2774, dc_loss: 0.0992085188627243, tv_loss: 0.03276554122567177\n",
      "iteration 2775, dc_loss: 0.09927887469530106, tv_loss: 0.032685812562704086\n",
      "iteration 2776, dc_loss: 0.09919574856758118, tv_loss: 0.032771944999694824\n",
      "iteration 2777, dc_loss: 0.09925645589828491, tv_loss: 0.032698579132556915\n",
      "iteration 2778, dc_loss: 0.09918591380119324, tv_loss: 0.03279094770550728\n",
      "iteration 2779, dc_loss: 0.09929365664720535, tv_loss: 0.03266135975718498\n",
      "iteration 2780, dc_loss: 0.0991760715842247, tv_loss: 0.03279811516404152\n",
      "iteration 2781, dc_loss: 0.09924664348363876, tv_loss: 0.032699789851903915\n",
      "iteration 2782, dc_loss: 0.09913615882396698, tv_loss: 0.03279242664575577\n",
      "iteration 2783, dc_loss: 0.09922338277101517, tv_loss: 0.03266973793506622\n",
      "iteration 2784, dc_loss: 0.09911008179187775, tv_loss: 0.0327753983438015\n",
      "iteration 2785, dc_loss: 0.09913738071918488, tv_loss: 0.03272273391485214\n",
      "iteration 2786, dc_loss: 0.0991051122546196, tv_loss: 0.03274876996874809\n",
      "iteration 2787, dc_loss: 0.09912171959877014, tv_loss: 0.032708607614040375\n",
      "iteration 2788, dc_loss: 0.09910742938518524, tv_loss: 0.03273638337850571\n",
      "iteration 2789, dc_loss: 0.09905374050140381, tv_loss: 0.032768357545137405\n",
      "iteration 2790, dc_loss: 0.0990886464715004, tv_loss: 0.03272740915417671\n",
      "iteration 2791, dc_loss: 0.09906773269176483, tv_loss: 0.032728131860494614\n",
      "iteration 2792, dc_loss: 0.09907674044370651, tv_loss: 0.03272987902164459\n",
      "iteration 2793, dc_loss: 0.09903571754693985, tv_loss: 0.03275459259748459\n",
      "iteration 2794, dc_loss: 0.09904355555772781, tv_loss: 0.03273915871977806\n",
      "iteration 2795, dc_loss: 0.09904972463846207, tv_loss: 0.0327143520116806\n",
      "iteration 2796, dc_loss: 0.0990239754319191, tv_loss: 0.032745808362960815\n",
      "iteration 2797, dc_loss: 0.09900383651256561, tv_loss: 0.03275185450911522\n",
      "iteration 2798, dc_loss: 0.0990096777677536, tv_loss: 0.03273984417319298\n",
      "iteration 2799, dc_loss: 0.09902593493461609, tv_loss: 0.0327102355659008\n",
      "iteration 2800, dc_loss: 0.09902435541152954, tv_loss: 0.032735396176576614\n",
      "iteration 2801, dc_loss: 0.09899426251649857, tv_loss: 0.032754767686128616\n",
      "iteration 2802, dc_loss: 0.09897290915250778, tv_loss: 0.032706789672374725\n",
      "iteration 2803, dc_loss: 0.09896756708621979, tv_loss: 0.032701268792152405\n",
      "iteration 2804, dc_loss: 0.09896724671125412, tv_loss: 0.032702215015888214\n",
      "iteration 2805, dc_loss: 0.0989496037364006, tv_loss: 0.03271282836794853\n",
      "iteration 2806, dc_loss: 0.09891746193170547, tv_loss: 0.03270658105611801\n",
      "iteration 2807, dc_loss: 0.09893513470888138, tv_loss: 0.032685287296772\n",
      "iteration 2808, dc_loss: 0.09895116090774536, tv_loss: 0.03268779069185257\n",
      "iteration 2809, dc_loss: 0.09889690577983856, tv_loss: 0.03269035741686821\n",
      "iteration 2810, dc_loss: 0.09889398515224457, tv_loss: 0.03272368758916855\n",
      "iteration 2811, dc_loss: 0.09891137480735779, tv_loss: 0.032674726098775864\n",
      "iteration 2812, dc_loss: 0.09889446198940277, tv_loss: 0.03271164372563362\n",
      "iteration 2813, dc_loss: 0.09887050837278366, tv_loss: 0.03269653394818306\n",
      "iteration 2814, dc_loss: 0.09887675195932388, tv_loss: 0.032709915190935135\n",
      "iteration 2815, dc_loss: 0.09886843711137772, tv_loss: 0.03268983215093613\n",
      "iteration 2816, dc_loss: 0.09885300695896149, tv_loss: 0.032708048820495605\n",
      "iteration 2817, dc_loss: 0.09884607046842575, tv_loss: 0.03269188851118088\n",
      "iteration 2818, dc_loss: 0.0988449975848198, tv_loss: 0.03269829601049423\n",
      "iteration 2819, dc_loss: 0.09884131699800491, tv_loss: 0.03268369287252426\n",
      "iteration 2820, dc_loss: 0.0988185852766037, tv_loss: 0.032699279487133026\n",
      "iteration 2821, dc_loss: 0.09880924224853516, tv_loss: 0.03271083906292915\n",
      "iteration 2822, dc_loss: 0.09881352633237839, tv_loss: 0.03268648311495781\n",
      "iteration 2823, dc_loss: 0.09880728274583817, tv_loss: 0.03270118311047554\n",
      "iteration 2824, dc_loss: 0.09879084676504135, tv_loss: 0.032691631466150284\n",
      "iteration 2825, dc_loss: 0.09878136217594147, tv_loss: 0.0327228307723999\n",
      "iteration 2826, dc_loss: 0.09878131002187729, tv_loss: 0.03268970921635628\n",
      "iteration 2827, dc_loss: 0.09877846390008926, tv_loss: 0.03271220996975899\n",
      "iteration 2828, dc_loss: 0.09876007586717606, tv_loss: 0.03270645812153816\n",
      "iteration 2829, dc_loss: 0.09875175356864929, tv_loss: 0.032718461006879807\n",
      "iteration 2830, dc_loss: 0.09875371307134628, tv_loss: 0.032698772847652435\n",
      "iteration 2831, dc_loss: 0.09874480962753296, tv_loss: 0.03270547091960907\n",
      "iteration 2832, dc_loss: 0.09873543679714203, tv_loss: 0.03269985690712929\n",
      "iteration 2833, dc_loss: 0.09872572124004364, tv_loss: 0.032706841826438904\n",
      "iteration 2834, dc_loss: 0.0987161174416542, tv_loss: 0.032699573785066605\n",
      "iteration 2835, dc_loss: 0.09871627390384674, tv_loss: 0.03270026668906212\n",
      "iteration 2836, dc_loss: 0.09870640188455582, tv_loss: 0.03269351273775101\n",
      "iteration 2837, dc_loss: 0.09869405627250671, tv_loss: 0.03271091356873512\n",
      "iteration 2838, dc_loss: 0.09868979454040527, tv_loss: 0.032692279666662216\n",
      "iteration 2839, dc_loss: 0.0986872985959053, tv_loss: 0.0327090248465538\n",
      "iteration 2840, dc_loss: 0.09867626428604126, tv_loss: 0.032691944390535355\n",
      "iteration 2841, dc_loss: 0.09866832196712494, tv_loss: 0.03271840140223503\n",
      "iteration 2842, dc_loss: 0.09865764528512955, tv_loss: 0.03270428255200386\n",
      "iteration 2843, dc_loss: 0.0986565500497818, tv_loss: 0.032714903354644775\n",
      "iteration 2844, dc_loss: 0.09864848107099533, tv_loss: 0.03270547091960907\n",
      "iteration 2845, dc_loss: 0.09863433241844177, tv_loss: 0.03271232917904854\n",
      "iteration 2846, dc_loss: 0.09863090515136719, tv_loss: 0.03270517662167549\n",
      "iteration 2847, dc_loss: 0.09862802922725677, tv_loss: 0.03269953280687332\n",
      "iteration 2848, dc_loss: 0.09861527383327484, tv_loss: 0.032704487442970276\n",
      "iteration 2849, dc_loss: 0.09860805422067642, tv_loss: 0.0327003188431263\n",
      "iteration 2850, dc_loss: 0.09860401600599289, tv_loss: 0.03269661217927933\n",
      "iteration 2851, dc_loss: 0.09859495609998703, tv_loss: 0.03270173817873001\n",
      "iteration 2852, dc_loss: 0.0985870510339737, tv_loss: 0.03269744664430618\n",
      "iteration 2853, dc_loss: 0.09857963025569916, tv_loss: 0.032706644386053085\n",
      "iteration 2854, dc_loss: 0.09857441484928131, tv_loss: 0.03269451484084129\n",
      "iteration 2855, dc_loss: 0.09856446832418442, tv_loss: 0.03270989656448364\n",
      "iteration 2856, dc_loss: 0.0985577404499054, tv_loss: 0.032699037343263626\n",
      "iteration 2857, dc_loss: 0.09855391085147858, tv_loss: 0.03270462900400162\n",
      "iteration 2858, dc_loss: 0.09854406863451004, tv_loss: 0.032700568437576294\n",
      "iteration 2859, dc_loss: 0.0985330119729042, tv_loss: 0.03270873799920082\n",
      "iteration 2860, dc_loss: 0.09853268414735794, tv_loss: 0.032699406147003174\n",
      "iteration 2861, dc_loss: 0.09852469712495804, tv_loss: 0.032697778195142746\n",
      "iteration 2862, dc_loss: 0.09851323068141937, tv_loss: 0.032699115574359894\n",
      "iteration 2863, dc_loss: 0.09850963205099106, tv_loss: 0.03270157426595688\n",
      "iteration 2864, dc_loss: 0.09850142896175385, tv_loss: 0.032691583037376404\n",
      "iteration 2865, dc_loss: 0.09848970174789429, tv_loss: 0.032710254192352295\n",
      "iteration 2866, dc_loss: 0.09848887473344803, tv_loss: 0.0326901413500309\n",
      "iteration 2867, dc_loss: 0.09848660975694656, tv_loss: 0.03270237147808075\n",
      "iteration 2868, dc_loss: 0.09847117960453033, tv_loss: 0.03269783779978752\n",
      "iteration 2869, dc_loss: 0.09845653176307678, tv_loss: 0.032720573246479034\n",
      "iteration 2870, dc_loss: 0.09845852851867676, tv_loss: 0.032698340713977814\n",
      "iteration 2871, dc_loss: 0.09846051037311554, tv_loss: 0.03270407021045685\n",
      "iteration 2872, dc_loss: 0.09844270348548889, tv_loss: 0.03270338103175163\n",
      "iteration 2873, dc_loss: 0.0984291210770607, tv_loss: 0.032718122005462646\n",
      "iteration 2874, dc_loss: 0.09842979907989502, tv_loss: 0.03270331397652626\n",
      "iteration 2875, dc_loss: 0.09842806309461594, tv_loss: 0.03269962966442108\n",
      "iteration 2876, dc_loss: 0.09841623157262802, tv_loss: 0.032702382653951645\n",
      "iteration 2877, dc_loss: 0.09840337187051773, tv_loss: 0.03270552679896355\n",
      "iteration 2878, dc_loss: 0.09839961677789688, tv_loss: 0.032698605209589005\n",
      "iteration 2879, dc_loss: 0.0984000414609909, tv_loss: 0.0326969288289547\n",
      "iteration 2880, dc_loss: 0.09838636964559555, tv_loss: 0.032694652676582336\n",
      "iteration 2881, dc_loss: 0.09837321192026138, tv_loss: 0.032712168991565704\n",
      "iteration 2882, dc_loss: 0.0983738899230957, tv_loss: 0.03269116207957268\n",
      "iteration 2883, dc_loss: 0.0983719751238823, tv_loss: 0.03270528465509415\n",
      "iteration 2884, dc_loss: 0.09835433959960938, tv_loss: 0.032699551433324814\n",
      "iteration 2885, dc_loss: 0.0983462780714035, tv_loss: 0.03271994739770889\n",
      "iteration 2886, dc_loss: 0.09834491461515427, tv_loss: 0.03269863501191139\n",
      "iteration 2887, dc_loss: 0.09834013879299164, tv_loss: 0.032709408551454544\n",
      "iteration 2888, dc_loss: 0.09832507371902466, tv_loss: 0.032710276544094086\n",
      "iteration 2889, dc_loss: 0.09831968694925308, tv_loss: 0.03270771726965904\n",
      "iteration 2890, dc_loss: 0.09831597656011581, tv_loss: 0.032701775431632996\n",
      "iteration 2891, dc_loss: 0.09830955415964127, tv_loss: 0.032701268792152405\n",
      "iteration 2892, dc_loss: 0.09829734265804291, tv_loss: 0.032703664153814316\n",
      "iteration 2893, dc_loss: 0.09829297661781311, tv_loss: 0.03270091116428375\n",
      "iteration 2894, dc_loss: 0.09828821569681168, tv_loss: 0.03269476443529129\n",
      "iteration 2895, dc_loss: 0.09828108549118042, tv_loss: 0.03270237147808075\n",
      "iteration 2896, dc_loss: 0.09826838970184326, tv_loss: 0.032697468996047974\n",
      "iteration 2897, dc_loss: 0.0982622280716896, tv_loss: 0.03271103650331497\n",
      "iteration 2898, dc_loss: 0.09825978428125381, tv_loss: 0.032691001892089844\n",
      "iteration 2899, dc_loss: 0.09825468808412552, tv_loss: 0.032703425735235214\n",
      "iteration 2900, dc_loss: 0.09823696315288544, tv_loss: 0.03270453214645386\n",
      "iteration 2901, dc_loss: 0.09823323041200638, tv_loss: 0.03270840272307396\n",
      "iteration 2902, dc_loss: 0.09823630005121231, tv_loss: 0.032689329236745834\n",
      "iteration 2903, dc_loss: 0.09822544455528259, tv_loss: 0.0327053926885128\n",
      "iteration 2904, dc_loss: 0.0982067734003067, tv_loss: 0.03270340710878372\n",
      "iteration 2905, dc_loss: 0.09820391982793808, tv_loss: 0.03271672874689102\n",
      "iteration 2906, dc_loss: 0.09820418059825897, tv_loss: 0.03269250690937042\n",
      "iteration 2907, dc_loss: 0.09819632768630981, tv_loss: 0.03270946815609932\n",
      "iteration 2908, dc_loss: 0.09818117320537567, tv_loss: 0.032707057893276215\n",
      "iteration 2909, dc_loss: 0.09817913174629211, tv_loss: 0.03270702809095383\n",
      "iteration 2910, dc_loss: 0.09817343205213547, tv_loss: 0.032702427357435226\n",
      "iteration 2911, dc_loss: 0.09816181659698486, tv_loss: 0.0327068567276001\n",
      "iteration 2912, dc_loss: 0.09815767407417297, tv_loss: 0.03270009905099869\n",
      "iteration 2913, dc_loss: 0.09815027564764023, tv_loss: 0.03270763158798218\n",
      "iteration 2914, dc_loss: 0.09813902527093887, tv_loss: 0.032701168209314346\n",
      "iteration 2915, dc_loss: 0.09813826531171799, tv_loss: 0.032711245119571686\n",
      "iteration 2916, dc_loss: 0.09812942147254944, tv_loss: 0.0326995924115181\n",
      "iteration 2917, dc_loss: 0.098116934299469, tv_loss: 0.03271770104765892\n",
      "iteration 2918, dc_loss: 0.09811276942491531, tv_loss: 0.03270671144127846\n",
      "iteration 2919, dc_loss: 0.09811054170131683, tv_loss: 0.032704055309295654\n",
      "iteration 2920, dc_loss: 0.09809806942939758, tv_loss: 0.03270949795842171\n",
      "iteration 2921, dc_loss: 0.09809041023254395, tv_loss: 0.032705746591091156\n",
      "iteration 2922, dc_loss: 0.0980844497680664, tv_loss: 0.03270381689071655\n",
      "iteration 2923, dc_loss: 0.09807771444320679, tv_loss: 0.03271234408020973\n",
      "iteration 2924, dc_loss: 0.0980682447552681, tv_loss: 0.0327012874186039\n",
      "iteration 2925, dc_loss: 0.09806519746780396, tv_loss: 0.032714392989873886\n",
      "iteration 2926, dc_loss: 0.09805882722139359, tv_loss: 0.032701220363378525\n",
      "iteration 2927, dc_loss: 0.09804855287075043, tv_loss: 0.03271407634019852\n",
      "iteration 2928, dc_loss: 0.09803922474384308, tv_loss: 0.03270994499325752\n",
      "iteration 2929, dc_loss: 0.09803588688373566, tv_loss: 0.03271033987402916\n",
      "iteration 2930, dc_loss: 0.09802663326263428, tv_loss: 0.03270795941352844\n",
      "iteration 2931, dc_loss: 0.09802249819040298, tv_loss: 0.03270724043250084\n",
      "iteration 2932, dc_loss: 0.09801267087459564, tv_loss: 0.03270906209945679\n",
      "iteration 2933, dc_loss: 0.09800460934638977, tv_loss: 0.0327116921544075\n",
      "iteration 2934, dc_loss: 0.09800136834383011, tv_loss: 0.032698359340429306\n",
      "iteration 2935, dc_loss: 0.09799586981534958, tv_loss: 0.03271503001451492\n",
      "iteration 2936, dc_loss: 0.09798195958137512, tv_loss: 0.032705117017030716\n",
      "iteration 2937, dc_loss: 0.09797637909650803, tv_loss: 0.032721586525440216\n",
      "iteration 2938, dc_loss: 0.0979718267917633, tv_loss: 0.032705627381801605\n",
      "iteration 2939, dc_loss: 0.09796639531850815, tv_loss: 0.03271062672138214\n",
      "iteration 2940, dc_loss: 0.09795436263084412, tv_loss: 0.03271351009607315\n",
      "iteration 2941, dc_loss: 0.09794659912586212, tv_loss: 0.03271448612213135\n",
      "iteration 2942, dc_loss: 0.0979425385594368, tv_loss: 0.03270391374826431\n",
      "iteration 2943, dc_loss: 0.09793640673160553, tv_loss: 0.03271514177322388\n",
      "iteration 2944, dc_loss: 0.09792701154947281, tv_loss: 0.032706767320632935\n",
      "iteration 2945, dc_loss: 0.09792280942201614, tv_loss: 0.03271283209323883\n",
      "iteration 2946, dc_loss: 0.09791342169046402, tv_loss: 0.03270694985985756\n",
      "iteration 2947, dc_loss: 0.09790582954883575, tv_loss: 0.03271894529461861\n",
      "iteration 2948, dc_loss: 0.09789782017469406, tv_loss: 0.032708290964365005\n",
      "iteration 2949, dc_loss: 0.09789233654737473, tv_loss: 0.032717496156692505\n",
      "iteration 2950, dc_loss: 0.09788434952497482, tv_loss: 0.032710615545511246\n",
      "iteration 2951, dc_loss: 0.09787668287754059, tv_loss: 0.032715726643800735\n",
      "iteration 2952, dc_loss: 0.09787153452634811, tv_loss: 0.03270948305726051\n",
      "iteration 2953, dc_loss: 0.09786415100097656, tv_loss: 0.032715462148189545\n",
      "iteration 2954, dc_loss: 0.09785482287406921, tv_loss: 0.03270971402525902\n",
      "iteration 2955, dc_loss: 0.09785176813602448, tv_loss: 0.03271523490548134\n",
      "iteration 2956, dc_loss: 0.09784011542797089, tv_loss: 0.0327138788998127\n",
      "iteration 2957, dc_loss: 0.09783262014389038, tv_loss: 0.032716188579797745\n",
      "iteration 2958, dc_loss: 0.09782875329256058, tv_loss: 0.03270794451236725\n",
      "iteration 2959, dc_loss: 0.09782174974679947, tv_loss: 0.03271551802754402\n",
      "iteration 2960, dc_loss: 0.0978105217218399, tv_loss: 0.032712142914533615\n",
      "iteration 2961, dc_loss: 0.09780722111463547, tv_loss: 0.03271457925438881\n",
      "iteration 2962, dc_loss: 0.09779998660087585, tv_loss: 0.03270938619971275\n",
      "iteration 2963, dc_loss: 0.09779021143913269, tv_loss: 0.03271884098649025\n",
      "iteration 2964, dc_loss: 0.09778527170419693, tv_loss: 0.03270725905895233\n",
      "iteration 2965, dc_loss: 0.09778033196926117, tv_loss: 0.03271685913205147\n",
      "iteration 2966, dc_loss: 0.09776881337165833, tv_loss: 0.03271201625466347\n",
      "iteration 2967, dc_loss: 0.09776397049427032, tv_loss: 0.03271967172622681\n",
      "iteration 2968, dc_loss: 0.09776011109352112, tv_loss: 0.03270910307765007\n",
      "iteration 2969, dc_loss: 0.0977453663945198, tv_loss: 0.03272325545549393\n",
      "iteration 2970, dc_loss: 0.09773855656385422, tv_loss: 0.0327199324965477\n",
      "iteration 2971, dc_loss: 0.09774115681648254, tv_loss: 0.03270990774035454\n",
      "iteration 2972, dc_loss: 0.09772790968418121, tv_loss: 0.03271526098251343\n",
      "iteration 2973, dc_loss: 0.09771662950515747, tv_loss: 0.03272281959652901\n",
      "iteration 2974, dc_loss: 0.09771590679883957, tv_loss: 0.03270946443080902\n",
      "iteration 2975, dc_loss: 0.0977102592587471, tv_loss: 0.032717764377593994\n",
      "iteration 2976, dc_loss: 0.09769661724567413, tv_loss: 0.032717883586883545\n",
      "iteration 2977, dc_loss: 0.09769345074892044, tv_loss: 0.03271796181797981\n",
      "iteration 2978, dc_loss: 0.09768813103437424, tv_loss: 0.032711271196603775\n",
      "iteration 2979, dc_loss: 0.09767492860555649, tv_loss: 0.032724376767873764\n",
      "iteration 2980, dc_loss: 0.09767307341098785, tv_loss: 0.032713383436203\n",
      "iteration 2981, dc_loss: 0.09766656905412674, tv_loss: 0.03271597996354103\n",
      "iteration 2982, dc_loss: 0.09765467047691345, tv_loss: 0.03271438553929329\n",
      "iteration 2983, dc_loss: 0.09764895588159561, tv_loss: 0.03272690623998642\n",
      "iteration 2984, dc_loss: 0.09764531254768372, tv_loss: 0.032708149403333664\n",
      "iteration 2985, dc_loss: 0.09763448685407639, tv_loss: 0.032728828489780426\n",
      "iteration 2986, dc_loss: 0.09762586653232574, tv_loss: 0.03271998465061188\n",
      "iteration 2987, dc_loss: 0.09762343019247055, tv_loss: 0.032718073576688766\n",
      "iteration 2988, dc_loss: 0.09761587530374527, tv_loss: 0.032719727605581284\n",
      "iteration 2989, dc_loss: 0.09760819375514984, tv_loss: 0.03271954879164696\n",
      "iteration 2990, dc_loss: 0.0975959524512291, tv_loss: 0.03271680697798729\n",
      "iteration 2991, dc_loss: 0.09759390354156494, tv_loss: 0.03272276744246483\n",
      "iteration 2992, dc_loss: 0.0975862443447113, tv_loss: 0.03271728754043579\n",
      "iteration 2993, dc_loss: 0.09757760912179947, tv_loss: 0.03272242099046707\n",
      "iteration 2994, dc_loss: 0.0975709930062294, tv_loss: 0.03272038325667381\n",
      "iteration 2995, dc_loss: 0.0975622683763504, tv_loss: 0.032722510397434235\n",
      "iteration 2996, dc_loss: 0.09755925834178925, tv_loss: 0.032719388604164124\n",
      "iteration 2997, dc_loss: 0.09755414724349976, tv_loss: 0.03271391615271568\n",
      "iteration 2998, dc_loss: 0.09754011780023575, tv_loss: 0.03271816298365593\n",
      "iteration 2999, dc_loss: 0.09752976894378662, tv_loss: 0.03273223713040352\n",
      "iteration 3000, dc_loss: 0.09753242135047913, tv_loss: 0.032711051404476166\n",
      "iteration 3001, dc_loss: 0.09752481430768967, tv_loss: 0.03271617740392685\n",
      "iteration 3002, dc_loss: 0.0975121557712555, tv_loss: 0.03272378444671631\n",
      "iteration 3003, dc_loss: 0.09751154482364655, tv_loss: 0.03271472826600075\n",
      "iteration 3004, dc_loss: 0.09749757498502731, tv_loss: 0.032716743648052216\n",
      "iteration 3005, dc_loss: 0.09748893976211548, tv_loss: 0.03272717818617821\n",
      "iteration 3006, dc_loss: 0.09749016165733337, tv_loss: 0.03271493688225746\n",
      "iteration 3007, dc_loss: 0.09747853875160217, tv_loss: 0.032718051224946976\n",
      "iteration 3008, dc_loss: 0.09746745228767395, tv_loss: 0.032723844051361084\n",
      "iteration 3009, dc_loss: 0.09747288376092911, tv_loss: 0.03270989656448364\n",
      "iteration 3010, dc_loss: 0.09745438396930695, tv_loss: 0.03272290155291557\n",
      "iteration 3011, dc_loss: 0.09744992852210999, tv_loss: 0.032716304063797\n",
      "iteration 3012, dc_loss: 0.09744735807180405, tv_loss: 0.03271103650331497\n",
      "iteration 3013, dc_loss: 0.0974331945180893, tv_loss: 0.03271808475255966\n",
      "iteration 3014, dc_loss: 0.09743098169565201, tv_loss: 0.03271522372961044\n",
      "iteration 3015, dc_loss: 0.09742048382759094, tv_loss: 0.032720647752285004\n",
      "iteration 3016, dc_loss: 0.09741663187742233, tv_loss: 0.03271206468343735\n",
      "iteration 3017, dc_loss: 0.09741037338972092, tv_loss: 0.03271118178963661\n",
      "iteration 3018, dc_loss: 0.0973932296037674, tv_loss: 0.032721471041440964\n",
      "iteration 3019, dc_loss: 0.09740085154771805, tv_loss: 0.032708585262298584\n",
      "iteration 3020, dc_loss: 0.09738578647375107, tv_loss: 0.03271997720003128\n",
      "iteration 3021, dc_loss: 0.09738071262836456, tv_loss: 0.032719798386096954\n",
      "iteration 3022, dc_loss: 0.09736857563257217, tv_loss: 0.03272193297743797\n",
      "iteration 3023, dc_loss: 0.09736872464418411, tv_loss: 0.03271462768316269\n",
      "iteration 3024, dc_loss: 0.09735529869794846, tv_loss: 0.03272182494401932\n",
      "iteration 3025, dc_loss: 0.09735377877950668, tv_loss: 0.03271658346056938\n",
      "iteration 3026, dc_loss: 0.09734369069337845, tv_loss: 0.032723307609558105\n",
      "iteration 3027, dc_loss: 0.09733700752258301, tv_loss: 0.03272385522723198\n",
      "iteration 3028, dc_loss: 0.097330741584301, tv_loss: 0.03272240608930588\n",
      "iteration 3029, dc_loss: 0.09732136875391006, tv_loss: 0.0327225886285305\n",
      "iteration 3030, dc_loss: 0.09731471538543701, tv_loss: 0.032721396535634995\n",
      "iteration 3031, dc_loss: 0.0973094180226326, tv_loss: 0.032720983028411865\n",
      "iteration 3032, dc_loss: 0.0973009392619133, tv_loss: 0.03272099047899246\n",
      "iteration 3033, dc_loss: 0.09729406982660294, tv_loss: 0.032717637717723846\n",
      "iteration 3034, dc_loss: 0.09729065746068954, tv_loss: 0.03271540254354477\n",
      "iteration 3035, dc_loss: 0.09728135913610458, tv_loss: 0.032722100615501404\n",
      "iteration 3036, dc_loss: 0.097267284989357, tv_loss: 0.03272922337055206\n",
      "iteration 3037, dc_loss: 0.09727242588996887, tv_loss: 0.032721493393182755\n",
      "iteration 3038, dc_loss: 0.09725556522607803, tv_loss: 0.03272915631532669\n",
      "iteration 3039, dc_loss: 0.09725382179021835, tv_loss: 0.03271821513772011\n",
      "iteration 3040, dc_loss: 0.09724453091621399, tv_loss: 0.03272130712866783\n",
      "iteration 3041, dc_loss: 0.0972433015704155, tv_loss: 0.03271540254354477\n",
      "iteration 3042, dc_loss: 0.09722690284252167, tv_loss: 0.03272674232721329\n",
      "iteration 3043, dc_loss: 0.09723066538572311, tv_loss: 0.03271748125553131\n",
      "iteration 3044, dc_loss: 0.0972113087773323, tv_loss: 0.032727912068367004\n",
      "iteration 3045, dc_loss: 0.09721764922142029, tv_loss: 0.03271281719207764\n",
      "iteration 3046, dc_loss: 0.09719602018594742, tv_loss: 0.03273003548383713\n",
      "iteration 3047, dc_loss: 0.09720063954591751, tv_loss: 0.03272151201963425\n",
      "iteration 3048, dc_loss: 0.0971883237361908, tv_loss: 0.03272976726293564\n",
      "iteration 3049, dc_loss: 0.09718608111143112, tv_loss: 0.03272167220711708\n",
      "iteration 3050, dc_loss: 0.09716866910457611, tv_loss: 0.03272661939263344\n",
      "iteration 3051, dc_loss: 0.09717067331075668, tv_loss: 0.032719481736421585\n",
      "iteration 3052, dc_loss: 0.0971573069691658, tv_loss: 0.032729409635066986\n",
      "iteration 3053, dc_loss: 0.0971597209572792, tv_loss: 0.0327213890850544\n",
      "iteration 3054, dc_loss: 0.09713996946811676, tv_loss: 0.03273611515760422\n",
      "iteration 3055, dc_loss: 0.0971497967839241, tv_loss: 0.03272106498479843\n",
      "iteration 3056, dc_loss: 0.09713374823331833, tv_loss: 0.03272801265120506\n",
      "iteration 3057, dc_loss: 0.09712930768728256, tv_loss: 0.032727163285017014\n",
      "iteration 3058, dc_loss: 0.09711918979883194, tv_loss: 0.032733868807554245\n",
      "iteration 3059, dc_loss: 0.09712005406618118, tv_loss: 0.03272973373532295\n",
      "iteration 3060, dc_loss: 0.09710633009672165, tv_loss: 0.03273409605026245\n",
      "iteration 3061, dc_loss: 0.09711021929979324, tv_loss: 0.03272058814764023\n",
      "iteration 3062, dc_loss: 0.09709171205759048, tv_loss: 0.03273126855492592\n",
      "iteration 3063, dc_loss: 0.09709586203098297, tv_loss: 0.032722704112529755\n",
      "iteration 3064, dc_loss: 0.09707370400428772, tv_loss: 0.03273911774158478\n",
      "iteration 3065, dc_loss: 0.09709128737449646, tv_loss: 0.03271346539258957\n",
      "iteration 3066, dc_loss: 0.09705259650945663, tv_loss: 0.032744526863098145\n",
      "iteration 3067, dc_loss: 0.09707824885845184, tv_loss: 0.03271164745092392\n",
      "iteration 3068, dc_loss: 0.09703641384840012, tv_loss: 0.03274475410580635\n",
      "iteration 3069, dc_loss: 0.09706549346446991, tv_loss: 0.03271103650331497\n",
      "iteration 3070, dc_loss: 0.09702075272798538, tv_loss: 0.0327477864921093\n",
      "iteration 3071, dc_loss: 0.09705426543951035, tv_loss: 0.032708924263715744\n",
      "iteration 3072, dc_loss: 0.09700819104909897, tv_loss: 0.032750338315963745\n",
      "iteration 3073, dc_loss: 0.09703443199396133, tv_loss: 0.032718732953071594\n",
      "iteration 3074, dc_loss: 0.09700259566307068, tv_loss: 0.03274788334965706\n",
      "iteration 3075, dc_loss: 0.0970122218132019, tv_loss: 0.03272750601172447\n",
      "iteration 3076, dc_loss: 0.0969899520277977, tv_loss: 0.03273657336831093\n",
      "iteration 3077, dc_loss: 0.09699124842882156, tv_loss: 0.03272690251469612\n",
      "iteration 3078, dc_loss: 0.09698096662759781, tv_loss: 0.03273386508226395\n",
      "iteration 3079, dc_loss: 0.09697166085243225, tv_loss: 0.03273814544081688\n",
      "iteration 3080, dc_loss: 0.09697240591049194, tv_loss: 0.03272933512926102\n",
      "iteration 3081, dc_loss: 0.09696514904499054, tv_loss: 0.032731376588344574\n",
      "iteration 3082, dc_loss: 0.09696824848651886, tv_loss: 0.03272365406155586\n",
      "iteration 3083, dc_loss: 0.09694653749465942, tv_loss: 0.0327407605946064\n",
      "iteration 3084, dc_loss: 0.09695693850517273, tv_loss: 0.03272788226604462\n",
      "iteration 3085, dc_loss: 0.09693636745214462, tv_loss: 0.03274136781692505\n",
      "iteration 3086, dc_loss: 0.09694662690162659, tv_loss: 0.03272390365600586\n",
      "iteration 3087, dc_loss: 0.0969230905175209, tv_loss: 0.03274067863821983\n",
      "iteration 3088, dc_loss: 0.09693073481321335, tv_loss: 0.032725825905799866\n",
      "iteration 3089, dc_loss: 0.09690552204847336, tv_loss: 0.03274175897240639\n",
      "iteration 3090, dc_loss: 0.09691638499498367, tv_loss: 0.03272177278995514\n",
      "iteration 3091, dc_loss: 0.09688840806484222, tv_loss: 0.032742079347372055\n",
      "iteration 3092, dc_loss: 0.09689163416624069, tv_loss: 0.03272877633571625\n",
      "iteration 3093, dc_loss: 0.09687398374080658, tv_loss: 0.03273952007293701\n",
      "iteration 3094, dc_loss: 0.09687354415655136, tv_loss: 0.03273415192961693\n",
      "iteration 3095, dc_loss: 0.09686446934938431, tv_loss: 0.032738734036684036\n",
      "iteration 3096, dc_loss: 0.09684905409812927, tv_loss: 0.03274238109588623\n",
      "iteration 3097, dc_loss: 0.09685121476650238, tv_loss: 0.03273213654756546\n",
      "iteration 3098, dc_loss: 0.09683962911367416, tv_loss: 0.03273594379425049\n",
      "iteration 3099, dc_loss: 0.09683222323656082, tv_loss: 0.032734621316194534\n",
      "iteration 3100, dc_loss: 0.09682758897542953, tv_loss: 0.03273263946175575\n",
      "iteration 3101, dc_loss: 0.09681730717420578, tv_loss: 0.03273647651076317\n",
      "iteration 3102, dc_loss: 0.09682295471429825, tv_loss: 0.03272583335638046\n",
      "iteration 3103, dc_loss: 0.09680332988500595, tv_loss: 0.03274264559149742\n",
      "iteration 3104, dc_loss: 0.09680168330669403, tv_loss: 0.032738469541072845\n",
      "iteration 3105, dc_loss: 0.09678752720355988, tv_loss: 0.03274386003613472\n",
      "iteration 3106, dc_loss: 0.09679676592350006, tv_loss: 0.032727133482694626\n",
      "iteration 3107, dc_loss: 0.09677740931510925, tv_loss: 0.03274301812052727\n",
      "iteration 3108, dc_loss: 0.09678898751735687, tv_loss: 0.032730016857385635\n",
      "iteration 3109, dc_loss: 0.09676291793584824, tv_loss: 0.03275245055556297\n",
      "iteration 3110, dc_loss: 0.09677907079458237, tv_loss: 0.03272722661495209\n",
      "iteration 3111, dc_loss: 0.096748486161232, tv_loss: 0.03274969011545181\n",
      "iteration 3112, dc_loss: 0.09677569568157196, tv_loss: 0.03271932899951935\n",
      "iteration 3113, dc_loss: 0.09673695266246796, tv_loss: 0.03275128826498985\n",
      "iteration 3114, dc_loss: 0.09676530212163925, tv_loss: 0.03271646052598953\n",
      "iteration 3115, dc_loss: 0.09672118723392487, tv_loss: 0.03275980055332184\n",
      "iteration 3116, dc_loss: 0.09675313532352448, tv_loss: 0.032721251249313354\n",
      "iteration 3117, dc_loss: 0.0967024713754654, tv_loss: 0.03276140242815018\n",
      "iteration 3118, dc_loss: 0.09672760218381882, tv_loss: 0.03272268548607826\n",
      "iteration 3119, dc_loss: 0.0966840609908104, tv_loss: 0.03275537118315697\n",
      "iteration 3120, dc_loss: 0.096703439950943, tv_loss: 0.03272318094968796\n",
      "iteration 3121, dc_loss: 0.09666763246059418, tv_loss: 0.03275119513273239\n",
      "iteration 3122, dc_loss: 0.09668122231960297, tv_loss: 0.03273256868124008\n",
      "iteration 3123, dc_loss: 0.09665164351463318, tv_loss: 0.03275011107325554\n",
      "iteration 3124, dc_loss: 0.09666676819324493, tv_loss: 0.032723959535360336\n",
      "iteration 3125, dc_loss: 0.09664157778024673, tv_loss: 0.032745856791734695\n",
      "iteration 3126, dc_loss: 0.09664937853813171, tv_loss: 0.03273134306073189\n",
      "iteration 3127, dc_loss: 0.09663250297307968, tv_loss: 0.03274136036634445\n",
      "iteration 3128, dc_loss: 0.09663610905408859, tv_loss: 0.0327380895614624\n",
      "iteration 3129, dc_loss: 0.09662459045648575, tv_loss: 0.032749682664871216\n",
      "iteration 3130, dc_loss: 0.09662710130214691, tv_loss: 0.032740551978349686\n",
      "iteration 3131, dc_loss: 0.09661504626274109, tv_loss: 0.032746367156505585\n",
      "iteration 3132, dc_loss: 0.0966176763176918, tv_loss: 0.032741788774728775\n",
      "iteration 3133, dc_loss: 0.0966111496090889, tv_loss: 0.03274160623550415\n",
      "iteration 3134, dc_loss: 0.09661484509706497, tv_loss: 0.0327388197183609\n",
      "iteration 3135, dc_loss: 0.09660756587982178, tv_loss: 0.032746754586696625\n",
      "iteration 3136, dc_loss: 0.0965934544801712, tv_loss: 0.03274917230010033\n",
      "iteration 3137, dc_loss: 0.09658537060022354, tv_loss: 0.03274206444621086\n",
      "iteration 3138, dc_loss: 0.09657610207796097, tv_loss: 0.032743003219366074\n",
      "iteration 3139, dc_loss: 0.09656316787004471, tv_loss: 0.03274641931056976\n",
      "iteration 3140, dc_loss: 0.09655184298753738, tv_loss: 0.03274884074926376\n",
      "iteration 3141, dc_loss: 0.09654049575328827, tv_loss: 0.03275398164987564\n",
      "iteration 3142, dc_loss: 0.09654779732227325, tv_loss: 0.032738521695137024\n",
      "iteration 3143, dc_loss: 0.09653639793395996, tv_loss: 0.03275071457028389\n",
      "iteration 3144, dc_loss: 0.09654145687818527, tv_loss: 0.03274576738476753\n",
      "iteration 3145, dc_loss: 0.09652774780988693, tv_loss: 0.0327584408223629\n",
      "iteration 3146, dc_loss: 0.09651994705200195, tv_loss: 0.03275063633918762\n",
      "iteration 3147, dc_loss: 0.09650515764951706, tv_loss: 0.03274912387132645\n",
      "iteration 3148, dc_loss: 0.09648459404706955, tv_loss: 0.032755639404058456\n",
      "iteration 3149, dc_loss: 0.09649735689163208, tv_loss: 0.03273438289761543\n",
      "iteration 3150, dc_loss: 0.09647016227245331, tv_loss: 0.03276435658335686\n",
      "iteration 3151, dc_loss: 0.09650302678346634, tv_loss: 0.032725851982831955\n",
      "iteration 3152, dc_loss: 0.09646318852901459, tv_loss: 0.03276672214269638\n",
      "iteration 3153, dc_loss: 0.09650194644927979, tv_loss: 0.0327284149825573\n",
      "iteration 3154, dc_loss: 0.09644490480422974, tv_loss: 0.03277452290058136\n",
      "iteration 3155, dc_loss: 0.09648559987545013, tv_loss: 0.032722413539886475\n",
      "iteration 3156, dc_loss: 0.09642763435840607, tv_loss: 0.03277162089943886\n",
      "iteration 3157, dc_loss: 0.09646295756101608, tv_loss: 0.03272517770528793\n",
      "iteration 3158, dc_loss: 0.0964093953371048, tv_loss: 0.03277551755309105\n",
      "iteration 3159, dc_loss: 0.09645749628543854, tv_loss: 0.03272641450166702\n",
      "iteration 3160, dc_loss: 0.09640790522098541, tv_loss: 0.03277077525854111\n",
      "iteration 3161, dc_loss: 0.09644453227519989, tv_loss: 0.032735664397478104\n",
      "iteration 3162, dc_loss: 0.0963997170329094, tv_loss: 0.032768115401268005\n",
      "iteration 3163, dc_loss: 0.09641478955745697, tv_loss: 0.03273984044790268\n",
      "iteration 3164, dc_loss: 0.09638267755508423, tv_loss: 0.03276130557060242\n",
      "iteration 3165, dc_loss: 0.09639469534158707, tv_loss: 0.03273714706301689\n",
      "iteration 3166, dc_loss: 0.09637144953012466, tv_loss: 0.03275703266263008\n",
      "iteration 3167, dc_loss: 0.09637151658535004, tv_loss: 0.03275128826498985\n",
      "iteration 3168, dc_loss: 0.09637192636728287, tv_loss: 0.03274703770875931\n",
      "iteration 3169, dc_loss: 0.09635266661643982, tv_loss: 0.03276214003562927\n",
      "iteration 3170, dc_loss: 0.09635692089796066, tv_loss: 0.03274635970592499\n",
      "iteration 3171, dc_loss: 0.09632546454668045, tv_loss: 0.0327652208507061\n",
      "iteration 3172, dc_loss: 0.09633495658636093, tv_loss: 0.03274538740515709\n",
      "iteration 3173, dc_loss: 0.09630466997623444, tv_loss: 0.03276504948735237\n",
      "iteration 3174, dc_loss: 0.09632531553506851, tv_loss: 0.03273707255721092\n",
      "iteration 3175, dc_loss: 0.0962987169623375, tv_loss: 0.032762475311756134\n",
      "iteration 3176, dc_loss: 0.09630556404590607, tv_loss: 0.032745812088251114\n",
      "iteration 3177, dc_loss: 0.09628620743751526, tv_loss: 0.03276323899626732\n",
      "iteration 3178, dc_loss: 0.09628619998693466, tv_loss: 0.032752808183431625\n",
      "iteration 3179, dc_loss: 0.0962790995836258, tv_loss: 0.032750800251960754\n",
      "iteration 3180, dc_loss: 0.0962606742978096, tv_loss: 0.0327657125890255\n",
      "iteration 3181, dc_loss: 0.09626810997724533, tv_loss: 0.032749369740486145\n",
      "iteration 3182, dc_loss: 0.09624987095594406, tv_loss: 0.03275812789797783\n",
      "iteration 3183, dc_loss: 0.09625746309757233, tv_loss: 0.03274999558925629\n",
      "iteration 3184, dc_loss: 0.09624344855546951, tv_loss: 0.03275642544031143\n",
      "iteration 3185, dc_loss: 0.09624139964580536, tv_loss: 0.03275537118315697\n",
      "iteration 3186, dc_loss: 0.09622931480407715, tv_loss: 0.03276240453124046\n",
      "iteration 3187, dc_loss: 0.0962277203798294, tv_loss: 0.032758284360170364\n",
      "iteration 3188, dc_loss: 0.09622717648744583, tv_loss: 0.032749004662036896\n",
      "iteration 3189, dc_loss: 0.09620819240808487, tv_loss: 0.03276507183909416\n",
      "iteration 3190, dc_loss: 0.09622874855995178, tv_loss: 0.032744523137807846\n",
      "iteration 3191, dc_loss: 0.0961989238858223, tv_loss: 0.032777007669210434\n",
      "iteration 3192, dc_loss: 0.0962362065911293, tv_loss: 0.03273801878094673\n",
      "iteration 3193, dc_loss: 0.09619227051734924, tv_loss: 0.0327882394194603\n",
      "iteration 3194, dc_loss: 0.09626054018735886, tv_loss: 0.03272207826375961\n",
      "iteration 3195, dc_loss: 0.09617935121059418, tv_loss: 0.03280870243906975\n",
      "iteration 3196, dc_loss: 0.09626856446266174, tv_loss: 0.03270561620593071\n",
      "iteration 3197, dc_loss: 0.0961589589715004, tv_loss: 0.032808855175971985\n",
      "iteration 3198, dc_loss: 0.09623219072818756, tv_loss: 0.03271295502781868\n",
      "iteration 3199, dc_loss: 0.09612763673067093, tv_loss: 0.032806411385536194\n",
      "iteration 3200, dc_loss: 0.09619340300559998, tv_loss: 0.03272317349910736\n",
      "iteration 3201, dc_loss: 0.09611210972070694, tv_loss: 0.03278603032231331\n",
      "iteration 3202, dc_loss: 0.09612429887056351, tv_loss: 0.03275222331285477\n",
      "iteration 3203, dc_loss: 0.09612853825092316, tv_loss: 0.03273966908454895\n",
      "iteration 3204, dc_loss: 0.09609673917293549, tv_loss: 0.03277887403964996\n",
      "iteration 3205, dc_loss: 0.09611598402261734, tv_loss: 0.03274545073509216\n",
      "iteration 3206, dc_loss: 0.09610223770141602, tv_loss: 0.03274362161755562\n",
      "iteration 3207, dc_loss: 0.09607702493667603, tv_loss: 0.03277318552136421\n",
      "iteration 3208, dc_loss: 0.09609875828027725, tv_loss: 0.03274470940232277\n",
      "iteration 3209, dc_loss: 0.09607909619808197, tv_loss: 0.03274993225932121\n",
      "iteration 3210, dc_loss: 0.0960586816072464, tv_loss: 0.032766975462436676\n",
      "iteration 3211, dc_loss: 0.09607842564582825, tv_loss: 0.032745685428380966\n",
      "iteration 3212, dc_loss: 0.09605792909860611, tv_loss: 0.03275340422987938\n",
      "iteration 3213, dc_loss: 0.09604912996292114, tv_loss: 0.03275912255048752\n",
      "iteration 3214, dc_loss: 0.09606099873781204, tv_loss: 0.03274444118142128\n",
      "iteration 3215, dc_loss: 0.09604000300168991, tv_loss: 0.03275590389966965\n",
      "iteration 3216, dc_loss: 0.09603049606084824, tv_loss: 0.03275984525680542\n",
      "iteration 3217, dc_loss: 0.09603467583656311, tv_loss: 0.03275107964873314\n",
      "iteration 3218, dc_loss: 0.0960235670208931, tv_loss: 0.03275537118315697\n",
      "iteration 3219, dc_loss: 0.09601530432701111, tv_loss: 0.03275882452726364\n",
      "iteration 3220, dc_loss: 0.09601682424545288, tv_loss: 0.032753825187683105\n",
      "iteration 3221, dc_loss: 0.09600655734539032, tv_loss: 0.03275773301720619\n",
      "iteration 3222, dc_loss: 0.09600052982568741, tv_loss: 0.03275913745164871\n",
      "iteration 3223, dc_loss: 0.0959988459944725, tv_loss: 0.032755594700574875\n",
      "iteration 3224, dc_loss: 0.09598793089389801, tv_loss: 0.032760560512542725\n",
      "iteration 3225, dc_loss: 0.09598636627197266, tv_loss: 0.03275628015398979\n",
      "iteration 3226, dc_loss: 0.09598120301961899, tv_loss: 0.0327545702457428\n",
      "iteration 3227, dc_loss: 0.09597135335206985, tv_loss: 0.03275926783680916\n",
      "iteration 3228, dc_loss: 0.09596596658229828, tv_loss: 0.03275832161307335\n",
      "iteration 3229, dc_loss: 0.09596407413482666, tv_loss: 0.03275563567876816\n",
      "iteration 3230, dc_loss: 0.09595495462417603, tv_loss: 0.032758161425590515\n",
      "iteration 3231, dc_loss: 0.0959525778889656, tv_loss: 0.03275521844625473\n",
      "iteration 3232, dc_loss: 0.09594795107841492, tv_loss: 0.03275243565440178\n",
      "iteration 3233, dc_loss: 0.09593790024518967, tv_loss: 0.0327589251101017\n",
      "iteration 3234, dc_loss: 0.09593173116445541, tv_loss: 0.032762739807367325\n",
      "iteration 3235, dc_loss: 0.0959332138299942, tv_loss: 0.03275464102625847\n",
      "iteration 3236, dc_loss: 0.09591834992170334, tv_loss: 0.03276275843381882\n",
      "iteration 3237, dc_loss: 0.09591861814260483, tv_loss: 0.032754600048065186\n",
      "iteration 3238, dc_loss: 0.0959116742014885, tv_loss: 0.03275803104043007\n",
      "iteration 3239, dc_loss: 0.0959058627486229, tv_loss: 0.032763540744781494\n",
      "iteration 3240, dc_loss: 0.0959019884467125, tv_loss: 0.032764822244644165\n",
      "iteration 3241, dc_loss: 0.09589583426713943, tv_loss: 0.032763343304395676\n",
      "iteration 3242, dc_loss: 0.09588488191366196, tv_loss: 0.03276549652218819\n",
      "iteration 3243, dc_loss: 0.09588762372732162, tv_loss: 0.03275502473115921\n",
      "iteration 3244, dc_loss: 0.09587857127189636, tv_loss: 0.032758865505456924\n",
      "iteration 3245, dc_loss: 0.09586973488330841, tv_loss: 0.032764732837677\n",
      "iteration 3246, dc_loss: 0.09586921334266663, tv_loss: 0.032762277871370316\n",
      "iteration 3247, dc_loss: 0.09586119651794434, tv_loss: 0.03276682272553444\n",
      "iteration 3248, dc_loss: 0.09585340321063995, tv_loss: 0.03276621922850609\n",
      "iteration 3249, dc_loss: 0.0958540290594101, tv_loss: 0.032758474349975586\n",
      "iteration 3250, dc_loss: 0.09584250301122665, tv_loss: 0.03276233375072479\n",
      "iteration 3251, dc_loss: 0.09583792090415955, tv_loss: 0.03276034817099571\n",
      "iteration 3252, dc_loss: 0.09583481401205063, tv_loss: 0.03276076167821884\n",
      "iteration 3253, dc_loss: 0.09583046287298203, tv_loss: 0.032763946801424026\n",
      "iteration 3254, dc_loss: 0.09582027792930603, tv_loss: 0.03276872634887695\n",
      "iteration 3255, dc_loss: 0.0958169475197792, tv_loss: 0.03276482969522476\n",
      "iteration 3256, dc_loss: 0.09581020474433899, tv_loss: 0.03276452049612999\n",
      "iteration 3257, dc_loss: 0.09580783545970917, tv_loss: 0.032759103924036026\n",
      "iteration 3258, dc_loss: 0.09579914063215256, tv_loss: 0.032765038311481476\n",
      "iteration 3259, dc_loss: 0.09579310566186905, tv_loss: 0.032766055315732956\n",
      "iteration 3260, dc_loss: 0.09578977525234222, tv_loss: 0.032762158662080765\n",
      "iteration 3261, dc_loss: 0.09578444063663483, tv_loss: 0.03275866061449051\n",
      "iteration 3262, dc_loss: 0.09577825665473938, tv_loss: 0.0327608659863472\n",
      "iteration 3263, dc_loss: 0.09576990455389023, tv_loss: 0.032766081392765045\n",
      "iteration 3264, dc_loss: 0.09576836973428726, tv_loss: 0.03276108577847481\n",
      "iteration 3265, dc_loss: 0.09575942903757095, tv_loss: 0.03276463598012924\n",
      "iteration 3266, dc_loss: 0.09575783461332321, tv_loss: 0.032761264592409134\n",
      "iteration 3267, dc_loss: 0.0957496240735054, tv_loss: 0.032763514667749405\n",
      "iteration 3268, dc_loss: 0.09574428200721741, tv_loss: 0.03276429697871208\n",
      "iteration 3269, dc_loss: 0.09573777765035629, tv_loss: 0.032765310257673264\n",
      "iteration 3270, dc_loss: 0.09573478251695633, tv_loss: 0.032761428505182266\n",
      "iteration 3271, dc_loss: 0.0957266315817833, tv_loss: 0.032765112817287445\n",
      "iteration 3272, dc_loss: 0.0957237258553505, tv_loss: 0.03276300057768822\n",
      "iteration 3273, dc_loss: 0.09571487456560135, tv_loss: 0.03276735544204712\n",
      "iteration 3274, dc_loss: 0.09571180492639542, tv_loss: 0.03276483342051506\n",
      "iteration 3275, dc_loss: 0.09570272266864777, tv_loss: 0.03276924043893814\n",
      "iteration 3276, dc_loss: 0.0957019031047821, tv_loss: 0.03276442736387253\n",
      "iteration 3277, dc_loss: 0.09569335728883743, tv_loss: 0.03276825696229935\n",
      "iteration 3278, dc_loss: 0.09568764269351959, tv_loss: 0.0327698215842247\n",
      "iteration 3279, dc_loss: 0.09568551927804947, tv_loss: 0.03277100622653961\n",
      "iteration 3280, dc_loss: 0.09567562490701675, tv_loss: 0.03277967497706413\n",
      "iteration 3281, dc_loss: 0.09567304700613022, tv_loss: 0.03276875242590904\n",
      "iteration 3282, dc_loss: 0.09566596895456314, tv_loss: 0.03276580199599266\n",
      "iteration 3283, dc_loss: 0.09565902501344681, tv_loss: 0.03276824951171875\n",
      "iteration 3284, dc_loss: 0.09565587341785431, tv_loss: 0.03276824206113815\n",
      "iteration 3285, dc_loss: 0.09564916789531708, tv_loss: 0.03276984021067619\n",
      "iteration 3286, dc_loss: 0.09564291685819626, tv_loss: 0.03276819735765457\n",
      "iteration 3287, dc_loss: 0.09564173966646194, tv_loss: 0.0327632836997509\n",
      "iteration 3288, dc_loss: 0.09563201665878296, tv_loss: 0.032770540565252304\n",
      "iteration 3289, dc_loss: 0.09562569111585617, tv_loss: 0.03277553990483284\n",
      "iteration 3290, dc_loss: 0.09562082588672638, tv_loss: 0.03277929499745369\n",
      "iteration 3291, dc_loss: 0.09561800211668015, tv_loss: 0.03277169167995453\n",
      "iteration 3292, dc_loss: 0.09561066329479218, tv_loss: 0.032769132405519485\n",
      "iteration 3293, dc_loss: 0.09560682624578476, tv_loss: 0.03276719152927399\n",
      "iteration 3294, dc_loss: 0.09559591114521027, tv_loss: 0.0327761173248291\n",
      "iteration 3295, dc_loss: 0.09559255093336105, tv_loss: 0.032777752727270126\n",
      "iteration 3296, dc_loss: 0.09558956325054169, tv_loss: 0.0327734611928463\n",
      "iteration 3297, dc_loss: 0.09558527916669846, tv_loss: 0.03276778757572174\n",
      "iteration 3298, dc_loss: 0.09557919204235077, tv_loss: 0.032766588032245636\n",
      "iteration 3299, dc_loss: 0.09556925296783447, tv_loss: 0.03277178108692169\n",
      "iteration 3300, dc_loss: 0.09556274861097336, tv_loss: 0.03277324512600899\n",
      "iteration 3301, dc_loss: 0.09556469321250916, tv_loss: 0.032768476754426956\n",
      "iteration 3302, dc_loss: 0.09555132687091827, tv_loss: 0.03277747705578804\n",
      "iteration 3303, dc_loss: 0.09555098414421082, tv_loss: 0.03277292475104332\n",
      "iteration 3304, dc_loss: 0.09554503113031387, tv_loss: 0.03277306631207466\n",
      "iteration 3305, dc_loss: 0.09553655982017517, tv_loss: 0.03277382627129555\n",
      "iteration 3306, dc_loss: 0.09553327411413193, tv_loss: 0.032770510762929916\n",
      "iteration 3307, dc_loss: 0.09552790224552155, tv_loss: 0.03276921808719635\n",
      "iteration 3308, dc_loss: 0.09551927447319031, tv_loss: 0.032775621861219406\n",
      "iteration 3309, dc_loss: 0.09551773220300674, tv_loss: 0.032772231847047806\n",
      "iteration 3310, dc_loss: 0.09550916403532028, tv_loss: 0.032773733139038086\n",
      "iteration 3311, dc_loss: 0.09550344198942184, tv_loss: 0.0327724888920784\n",
      "iteration 3312, dc_loss: 0.09550345689058304, tv_loss: 0.03276651352643967\n",
      "iteration 3313, dc_loss: 0.09549263119697571, tv_loss: 0.032776594161987305\n",
      "iteration 3314, dc_loss: 0.09548705816268921, tv_loss: 0.03277747705578804\n",
      "iteration 3315, dc_loss: 0.09548342227935791, tv_loss: 0.0327720008790493\n",
      "iteration 3316, dc_loss: 0.09547560662031174, tv_loss: 0.03277299180626869\n",
      "iteration 3317, dc_loss: 0.09547194093465805, tv_loss: 0.03277144208550453\n",
      "iteration 3318, dc_loss: 0.09546788036823273, tv_loss: 0.03276890143752098\n",
      "iteration 3319, dc_loss: 0.09545588493347168, tv_loss: 0.032776396721601486\n",
      "iteration 3320, dc_loss: 0.09545700997114182, tv_loss: 0.03277473896741867\n",
      "iteration 3321, dc_loss: 0.09544746577739716, tv_loss: 0.032777152955532074\n",
      "iteration 3322, dc_loss: 0.0954444631934166, tv_loss: 0.032773490995168686\n",
      "iteration 3323, dc_loss: 0.09544017165899277, tv_loss: 0.032771386206150055\n",
      "iteration 3324, dc_loss: 0.09542970359325409, tv_loss: 0.03277428448200226\n",
      "iteration 3325, dc_loss: 0.0954255759716034, tv_loss: 0.03277628496289253\n",
      "iteration 3326, dc_loss: 0.09542398899793625, tv_loss: 0.032778941094875336\n",
      "iteration 3327, dc_loss: 0.09541486948728561, tv_loss: 0.032783277332782745\n",
      "iteration 3328, dc_loss: 0.09541043639183044, tv_loss: 0.03278142958879471\n",
      "iteration 3329, dc_loss: 0.0954042598605156, tv_loss: 0.03277648240327835\n",
      "iteration 3330, dc_loss: 0.0953977033495903, tv_loss: 0.03277549520134926\n",
      "iteration 3331, dc_loss: 0.09539802372455597, tv_loss: 0.03276835009455681\n",
      "iteration 3332, dc_loss: 0.09538476169109344, tv_loss: 0.03277742862701416\n",
      "iteration 3333, dc_loss: 0.09538104385137558, tv_loss: 0.032780978828668594\n",
      "iteration 3334, dc_loss: 0.09537860751152039, tv_loss: 0.032775867730379105\n",
      "iteration 3335, dc_loss: 0.09536788612604141, tv_loss: 0.032780904322862625\n",
      "iteration 3336, dc_loss: 0.09536900371313095, tv_loss: 0.0327715128660202\n",
      "iteration 3337, dc_loss: 0.09536116570234299, tv_loss: 0.03277328237891197\n",
      "iteration 3338, dc_loss: 0.09535219520330429, tv_loss: 0.03277762234210968\n",
      "iteration 3339, dc_loss: 0.09534738212823868, tv_loss: 0.03277698904275894\n",
      "iteration 3340, dc_loss: 0.09534665197134018, tv_loss: 0.0327734500169754\n",
      "iteration 3341, dc_loss: 0.09533421695232391, tv_loss: 0.03278306871652603\n",
      "iteration 3342, dc_loss: 0.09533567726612091, tv_loss: 0.032774269580841064\n",
      "iteration 3343, dc_loss: 0.09532305598258972, tv_loss: 0.03278045356273651\n",
      "iteration 3344, dc_loss: 0.09532555192708969, tv_loss: 0.03277387097477913\n",
      "iteration 3345, dc_loss: 0.09531453251838684, tv_loss: 0.03277956694364548\n",
      "iteration 3346, dc_loss: 0.09530842304229736, tv_loss: 0.03278529644012451\n",
      "iteration 3347, dc_loss: 0.09530596435070038, tv_loss: 0.032782915979623795\n",
      "iteration 3348, dc_loss: 0.09529674798250198, tv_loss: 0.03278697282075882\n",
      "iteration 3349, dc_loss: 0.09529414772987366, tv_loss: 0.03278174623847008\n",
      "iteration 3350, dc_loss: 0.09528864175081253, tv_loss: 0.03277694061398506\n",
      "iteration 3351, dc_loss: 0.09528054296970367, tv_loss: 0.03277696669101715\n",
      "iteration 3352, dc_loss: 0.09528332203626633, tv_loss: 0.032770123332738876\n",
      "iteration 3353, dc_loss: 0.09526476263999939, tv_loss: 0.03278440609574318\n",
      "iteration 3354, dc_loss: 0.0952644795179367, tv_loss: 0.032781898975372314\n",
      "iteration 3355, dc_loss: 0.09526266157627106, tv_loss: 0.03278467431664467\n",
      "iteration 3356, dc_loss: 0.09525199234485626, tv_loss: 0.03279126435518265\n",
      "iteration 3357, dc_loss: 0.09525241702795029, tv_loss: 0.032778915017843246\n",
      "iteration 3358, dc_loss: 0.09524146467447281, tv_loss: 0.03278337046504021\n",
      "iteration 3359, dc_loss: 0.09523265808820724, tv_loss: 0.03278397396206856\n",
      "iteration 3360, dc_loss: 0.09523626416921616, tv_loss: 0.03277542442083359\n",
      "iteration 3361, dc_loss: 0.0952259749174118, tv_loss: 0.032784488052129745\n",
      "iteration 3362, dc_loss: 0.09522350877523422, tv_loss: 0.03278280422091484\n",
      "iteration 3363, dc_loss: 0.09521294385194778, tv_loss: 0.032785020768642426\n",
      "iteration 3364, dc_loss: 0.09521057456731796, tv_loss: 0.03278317302465439\n",
      "iteration 3365, dc_loss: 0.09520338475704193, tv_loss: 0.03278141841292381\n",
      "iteration 3366, dc_loss: 0.09519720077514648, tv_loss: 0.032781876623630524\n",
      "iteration 3367, dc_loss: 0.09519461542367935, tv_loss: 0.03278224915266037\n",
      "iteration 3368, dc_loss: 0.095186248421669, tv_loss: 0.03278438374400139\n",
      "iteration 3369, dc_loss: 0.09518269449472427, tv_loss: 0.03278400003910065\n",
      "iteration 3370, dc_loss: 0.09517888724803925, tv_loss: 0.032778408378362656\n",
      "iteration 3371, dc_loss: 0.09516770392656326, tv_loss: 0.03278360143303871\n",
      "iteration 3372, dc_loss: 0.0951661542057991, tv_loss: 0.032779715955257416\n",
      "iteration 3373, dc_loss: 0.09515716880559921, tv_loss: 0.032783739268779755\n",
      "iteration 3374, dc_loss: 0.0951547771692276, tv_loss: 0.032780971378088\n",
      "iteration 3375, dc_loss: 0.0951460525393486, tv_loss: 0.032788656651973724\n",
      "iteration 3376, dc_loss: 0.09514588117599487, tv_loss: 0.03278283402323723\n",
      "iteration 3377, dc_loss: 0.09513745456933975, tv_loss: 0.03278129920363426\n",
      "iteration 3378, dc_loss: 0.09513109177350998, tv_loss: 0.03278326615691185\n",
      "iteration 3379, dc_loss: 0.09512653946876526, tv_loss: 0.03278505429625511\n",
      "iteration 3380, dc_loss: 0.09511748701334, tv_loss: 0.032788943499326706\n",
      "iteration 3381, dc_loss: 0.09511523693799973, tv_loss: 0.03278403729200363\n",
      "iteration 3382, dc_loss: 0.09510764479637146, tv_loss: 0.03278667852282524\n",
      "iteration 3383, dc_loss: 0.0951070487499237, tv_loss: 0.03278115764260292\n",
      "iteration 3384, dc_loss: 0.09509729593992233, tv_loss: 0.03278735280036926\n",
      "iteration 3385, dc_loss: 0.09509064257144928, tv_loss: 0.032787855714559555\n",
      "iteration 3386, dc_loss: 0.09508838504552841, tv_loss: 0.032784465700387955\n",
      "iteration 3387, dc_loss: 0.09508088231086731, tv_loss: 0.03278718516230583\n",
      "iteration 3388, dc_loss: 0.09508052468299866, tv_loss: 0.0327824130654335\n",
      "iteration 3389, dc_loss: 0.09506527334451675, tv_loss: 0.03279723599553108\n",
      "iteration 3390, dc_loss: 0.0950649082660675, tv_loss: 0.03279218450188637\n",
      "iteration 3391, dc_loss: 0.09506040066480637, tv_loss: 0.03278930485248566\n",
      "iteration 3392, dc_loss: 0.09505396336317062, tv_loss: 0.03278789296746254\n",
      "iteration 3393, dc_loss: 0.095045305788517, tv_loss: 0.03278876096010208\n",
      "iteration 3394, dc_loss: 0.09504301846027374, tv_loss: 0.032784897834062576\n",
      "iteration 3395, dc_loss: 0.09503374993801117, tv_loss: 0.03279230371117592\n",
      "iteration 3396, dc_loss: 0.0950348749756813, tv_loss: 0.03278319165110588\n",
      "iteration 3397, dc_loss: 0.09502673894166946, tv_loss: 0.032788097858428955\n",
      "iteration 3398, dc_loss: 0.09502160549163818, tv_loss: 0.03279315307736397\n",
      "iteration 3399, dc_loss: 0.09501441568136215, tv_loss: 0.03279319033026695\n",
      "iteration 3400, dc_loss: 0.09500613808631897, tv_loss: 0.03279366344213486\n",
      "iteration 3401, dc_loss: 0.09500426054000854, tv_loss: 0.03278703987598419\n",
      "iteration 3402, dc_loss: 0.09499680995941162, tv_loss: 0.032786861062049866\n",
      "iteration 3403, dc_loss: 0.09499378502368927, tv_loss: 0.0327843502163887\n",
      "iteration 3404, dc_loss: 0.09498750418424606, tv_loss: 0.032786641269922256\n",
      "iteration 3405, dc_loss: 0.0949755609035492, tv_loss: 0.03279615938663483\n",
      "iteration 3406, dc_loss: 0.0949811264872551, tv_loss: 0.032787714153528214\n",
      "iteration 3407, dc_loss: 0.09497092664241791, tv_loss: 0.032790981233119965\n",
      "iteration 3408, dc_loss: 0.09496064484119415, tv_loss: 0.03279558941721916\n",
      "iteration 3409, dc_loss: 0.09496050328016281, tv_loss: 0.03279094025492668\n",
      "iteration 3410, dc_loss: 0.09495429694652557, tv_loss: 0.0327942930161953\n",
      "iteration 3411, dc_loss: 0.0949445366859436, tv_loss: 0.03279836103320122\n",
      "iteration 3412, dc_loss: 0.0949457511305809, tv_loss: 0.03278589993715286\n",
      "iteration 3413, dc_loss: 0.09493660926818848, tv_loss: 0.032788462936878204\n",
      "iteration 3414, dc_loss: 0.09492424130439758, tv_loss: 0.0327981673181057\n",
      "iteration 3415, dc_loss: 0.09493107348680496, tv_loss: 0.032783668488264084\n",
      "iteration 3416, dc_loss: 0.09491921961307526, tv_loss: 0.03279272839426994\n",
      "iteration 3417, dc_loss: 0.09491882473230362, tv_loss: 0.032789312303066254\n",
      "iteration 3418, dc_loss: 0.09490356594324112, tv_loss: 0.03279691934585571\n",
      "iteration 3419, dc_loss: 0.09490680694580078, tv_loss: 0.03278648480772972\n",
      "iteration 3420, dc_loss: 0.09489399194717407, tv_loss: 0.0327945239841938\n",
      "iteration 3421, dc_loss: 0.09489171206951141, tv_loss: 0.03279172256588936\n",
      "iteration 3422, dc_loss: 0.09489132463932037, tv_loss: 0.03278796747326851\n",
      "iteration 3423, dc_loss: 0.09487506002187729, tv_loss: 0.03279874846339226\n",
      "iteration 3424, dc_loss: 0.09487948566675186, tv_loss: 0.0327896773815155\n",
      "iteration 3425, dc_loss: 0.09486952424049377, tv_loss: 0.03279493376612663\n",
      "iteration 3426, dc_loss: 0.09486164152622223, tv_loss: 0.0327938087284565\n",
      "iteration 3427, dc_loss: 0.09486088901758194, tv_loss: 0.03279005363583565\n",
      "iteration 3428, dc_loss: 0.09484890848398209, tv_loss: 0.032798219472169876\n",
      "iteration 3429, dc_loss: 0.09485343843698502, tv_loss: 0.03278695419430733\n",
      "iteration 3430, dc_loss: 0.09484048932790756, tv_loss: 0.032797206193208694\n",
      "iteration 3431, dc_loss: 0.09483424574136734, tv_loss: 0.03280406445264816\n",
      "iteration 3432, dc_loss: 0.09483174979686737, tv_loss: 0.03280365467071533\n",
      "iteration 3433, dc_loss: 0.09482257813215256, tv_loss: 0.03279966115951538\n",
      "iteration 3434, dc_loss: 0.09482001513242722, tv_loss: 0.03279345855116844\n",
      "iteration 3435, dc_loss: 0.0948190838098526, tv_loss: 0.03278689831495285\n",
      "iteration 3436, dc_loss: 0.0948035791516304, tv_loss: 0.03279726207256317\n",
      "iteration 3437, dc_loss: 0.09480804204940796, tv_loss: 0.03279067575931549\n",
      "iteration 3438, dc_loss: 0.09479063749313354, tv_loss: 0.03280763700604439\n",
      "iteration 3439, dc_loss: 0.09479999542236328, tv_loss: 0.03279007226228714\n",
      "iteration 3440, dc_loss: 0.09478050470352173, tv_loss: 0.03280215710401535\n",
      "iteration 3441, dc_loss: 0.09478592127561569, tv_loss: 0.03279462084174156\n",
      "iteration 3442, dc_loss: 0.09477414935827255, tv_loss: 0.0328017883002758\n",
      "iteration 3443, dc_loss: 0.09476640075445175, tv_loss: 0.03280019387602806\n",
      "iteration 3444, dc_loss: 0.09476545453071594, tv_loss: 0.032795246690511703\n",
      "iteration 3445, dc_loss: 0.09475605189800262, tv_loss: 0.03280556946992874\n",
      "iteration 3446, dc_loss: 0.094753697514534, tv_loss: 0.03280443698167801\n",
      "iteration 3447, dc_loss: 0.0947466641664505, tv_loss: 0.03280274197459221\n",
      "iteration 3448, dc_loss: 0.09474431723356247, tv_loss: 0.03279637172818184\n",
      "iteration 3449, dc_loss: 0.09473098069429398, tv_loss: 0.03280157595872879\n",
      "iteration 3450, dc_loss: 0.09473418444395065, tv_loss: 0.03279516100883484\n",
      "iteration 3451, dc_loss: 0.09472212940454483, tv_loss: 0.03280170261859894\n",
      "iteration 3452, dc_loss: 0.09472028911113739, tv_loss: 0.03279852867126465\n",
      "iteration 3453, dc_loss: 0.09471271187067032, tv_loss: 0.03280027210712433\n",
      "iteration 3454, dc_loss: 0.09471001476049423, tv_loss: 0.032797910273075104\n",
      "iteration 3455, dc_loss: 0.09470630437135696, tv_loss: 0.03279818594455719\n",
      "iteration 3456, dc_loss: 0.09468823671340942, tv_loss: 0.03281014785170555\n",
      "iteration 3457, dc_loss: 0.09469819068908691, tv_loss: 0.032796140760183334\n",
      "iteration 3458, dc_loss: 0.09468544274568558, tv_loss: 0.03280264884233475\n",
      "iteration 3459, dc_loss: 0.09467875957489014, tv_loss: 0.03279995918273926\n",
      "iteration 3460, dc_loss: 0.09467557072639465, tv_loss: 0.03279552981257439\n",
      "iteration 3461, dc_loss: 0.09466764330863953, tv_loss: 0.03279650956392288\n",
      "iteration 3462, dc_loss: 0.09467007219791412, tv_loss: 0.03279092535376549\n",
      "iteration 3463, dc_loss: 0.0946495309472084, tv_loss: 0.03280854970216751\n",
      "iteration 3464, dc_loss: 0.09466081857681274, tv_loss: 0.032797202467918396\n",
      "iteration 3465, dc_loss: 0.09463698416948318, tv_loss: 0.03281525522470474\n",
      "iteration 3466, dc_loss: 0.09464751183986664, tv_loss: 0.03279529884457588\n",
      "iteration 3467, dc_loss: 0.09463799744844437, tv_loss: 0.03280064836144447\n",
      "iteration 3468, dc_loss: 0.09462660551071167, tv_loss: 0.03280598670244217\n",
      "iteration 3469, dc_loss: 0.09462478011846542, tv_loss: 0.03280413895845413\n",
      "iteration 3470, dc_loss: 0.09461760520935059, tv_loss: 0.032804638147354126\n",
      "iteration 3471, dc_loss: 0.09461934864521027, tv_loss: 0.032796379178762436\n",
      "iteration 3472, dc_loss: 0.09460342675447464, tv_loss: 0.03280721977353096\n",
      "iteration 3473, dc_loss: 0.09460059553384781, tv_loss: 0.03280819207429886\n",
      "iteration 3474, dc_loss: 0.09459832310676575, tv_loss: 0.03280946612358093\n",
      "iteration 3475, dc_loss: 0.0945885181427002, tv_loss: 0.03280918300151825\n",
      "iteration 3476, dc_loss: 0.09458857774734497, tv_loss: 0.03280077129602432\n",
      "iteration 3477, dc_loss: 0.09458056092262268, tv_loss: 0.03280309960246086\n",
      "iteration 3478, dc_loss: 0.0945780947804451, tv_loss: 0.032801706343889236\n",
      "iteration 3479, dc_loss: 0.09457051753997803, tv_loss: 0.03280740603804588\n",
      "iteration 3480, dc_loss: 0.09457020461559296, tv_loss: 0.0328030027449131\n",
      "iteration 3481, dc_loss: 0.09456034749746323, tv_loss: 0.032805219292640686\n",
      "iteration 3482, dc_loss: 0.09456415474414825, tv_loss: 0.03279857710003853\n",
      "iteration 3483, dc_loss: 0.09454870969057083, tv_loss: 0.03281009942293167\n",
      "iteration 3484, dc_loss: 0.09456625580787659, tv_loss: 0.032792460173368454\n",
      "iteration 3485, dc_loss: 0.09454309940338135, tv_loss: 0.032817013561725616\n",
      "iteration 3486, dc_loss: 0.09456213563680649, tv_loss: 0.03279507905244827\n",
      "iteration 3487, dc_loss: 0.09453655034303665, tv_loss: 0.03282106667757034\n",
      "iteration 3488, dc_loss: 0.09454847872257233, tv_loss: 0.032805491238832474\n",
      "iteration 3489, dc_loss: 0.09452879428863525, tv_loss: 0.03281198441982269\n",
      "iteration 3490, dc_loss: 0.0945250391960144, tv_loss: 0.03280005231499672\n",
      "iteration 3491, dc_loss: 0.09450819343328476, tv_loss: 0.032807834446430206\n",
      "iteration 3492, dc_loss: 0.09449844062328339, tv_loss: 0.03281141072511673\n",
      "iteration 3493, dc_loss: 0.09449950605630875, tv_loss: 0.03280575945973396\n",
      "iteration 3494, dc_loss: 0.09448070079088211, tv_loss: 0.03281892091035843\n",
      "iteration 3495, dc_loss: 0.09448607265949249, tv_loss: 0.03280680626630783\n",
      "iteration 3496, dc_loss: 0.09447167813777924, tv_loss: 0.03280922397971153\n",
      "iteration 3497, dc_loss: 0.09447243064641953, tv_loss: 0.03280461207032204\n",
      "iteration 3498, dc_loss: 0.09446866810321808, tv_loss: 0.03281162306666374\n",
      "iteration 3499, dc_loss: 0.09446297585964203, tv_loss: 0.03281733766198158\n",
      "iteration 3500, dc_loss: 0.09445319324731827, tv_loss: 0.03281497582793236\n",
      "iteration 3501, dc_loss: 0.094462089240551, tv_loss: 0.0328054204583168\n",
      "iteration 3502, dc_loss: 0.09443473070859909, tv_loss: 0.032820310443639755\n",
      "iteration 3503, dc_loss: 0.09444545954465866, tv_loss: 0.03280109167098999\n",
      "iteration 3504, dc_loss: 0.09442761540412903, tv_loss: 0.03282123804092407\n",
      "iteration 3505, dc_loss: 0.09443048387765884, tv_loss: 0.03280915319919586\n",
      "iteration 3506, dc_loss: 0.09441865980625153, tv_loss: 0.03281142935156822\n",
      "iteration 3507, dc_loss: 0.0944131538271904, tv_loss: 0.03281186893582344\n",
      "iteration 3508, dc_loss: 0.09440484642982483, tv_loss: 0.032820019870996475\n",
      "iteration 3509, dc_loss: 0.09440252184867859, tv_loss: 0.032813575118780136\n",
      "iteration 3510, dc_loss: 0.09440062195062637, tv_loss: 0.03281246870756149\n",
      "iteration 3511, dc_loss: 0.09438559412956238, tv_loss: 0.03281938284635544\n",
      "iteration 3512, dc_loss: 0.09439268708229065, tv_loss: 0.03280872851610184\n",
      "iteration 3513, dc_loss: 0.09437287598848343, tv_loss: 0.03282179683446884\n",
      "iteration 3514, dc_loss: 0.09437832236289978, tv_loss: 0.032816458493471146\n",
      "iteration 3515, dc_loss: 0.09436869621276855, tv_loss: 0.03281155601143837\n",
      "iteration 3516, dc_loss: 0.0943676009774208, tv_loss: 0.032812584191560745\n",
      "iteration 3517, dc_loss: 0.09435431659221649, tv_loss: 0.03282364085316658\n",
      "iteration 3518, dc_loss: 0.09435459226369858, tv_loss: 0.032812293618917465\n",
      "iteration 3519, dc_loss: 0.09434492141008377, tv_loss: 0.03281547501683235\n",
      "iteration 3520, dc_loss: 0.09434506297111511, tv_loss: 0.03281891345977783\n",
      "iteration 3521, dc_loss: 0.09434302151203156, tv_loss: 0.03281005471944809\n",
      "iteration 3522, dc_loss: 0.09433727711439133, tv_loss: 0.03281146287918091\n",
      "iteration 3523, dc_loss: 0.09431634843349457, tv_loss: 0.03283088281750679\n",
      "iteration 3524, dc_loss: 0.09433919191360474, tv_loss: 0.032808247953653336\n",
      "iteration 3525, dc_loss: 0.09431653469800949, tv_loss: 0.03282255306839943\n",
      "iteration 3526, dc_loss: 0.09433069825172424, tv_loss: 0.03280514106154442\n",
      "iteration 3527, dc_loss: 0.09430738538503647, tv_loss: 0.03282717242836952\n",
      "iteration 3528, dc_loss: 0.0943167582154274, tv_loss: 0.0328158475458622\n",
      "iteration 3529, dc_loss: 0.09429267048835754, tv_loss: 0.03283565118908882\n",
      "iteration 3530, dc_loss: 0.09431995451450348, tv_loss: 0.03279632702469826\n",
      "iteration 3531, dc_loss: 0.09428062289953232, tv_loss: 0.03283294290304184\n",
      "iteration 3532, dc_loss: 0.09430912882089615, tv_loss: 0.03280704841017723\n",
      "iteration 3533, dc_loss: 0.0942763015627861, tv_loss: 0.03283094987273216\n",
      "iteration 3534, dc_loss: 0.0942973867058754, tv_loss: 0.03280067443847656\n",
      "iteration 3535, dc_loss: 0.09425888955593109, tv_loss: 0.032842572778463364\n",
      "iteration 3536, dc_loss: 0.09430031478404999, tv_loss: 0.03279521316289902\n",
      "iteration 3537, dc_loss: 0.09425660222768784, tv_loss: 0.032838113605976105\n",
      "iteration 3538, dc_loss: 0.09428085386753082, tv_loss: 0.032805588096380234\n",
      "iteration 3539, dc_loss: 0.09424109756946564, tv_loss: 0.03283562511205673\n",
      "iteration 3540, dc_loss: 0.09426510334014893, tv_loss: 0.03279551491141319\n",
      "iteration 3541, dc_loss: 0.09422667324542999, tv_loss: 0.03282499313354492\n",
      "iteration 3542, dc_loss: 0.09423358738422394, tv_loss: 0.032814040780067444\n",
      "iteration 3543, dc_loss: 0.09422412514686584, tv_loss: 0.03281741216778755\n",
      "iteration 3544, dc_loss: 0.09421198815107346, tv_loss: 0.03282463550567627\n",
      "iteration 3545, dc_loss: 0.09423694759607315, tv_loss: 0.03279966861009598\n",
      "iteration 3546, dc_loss: 0.09419699758291245, tv_loss: 0.03283784165978432\n",
      "iteration 3547, dc_loss: 0.094220831990242, tv_loss: 0.03279871866106987\n",
      "iteration 3548, dc_loss: 0.09418243169784546, tv_loss: 0.032831016927957535\n",
      "iteration 3549, dc_loss: 0.09419650584459305, tv_loss: 0.03280973806977272\n",
      "iteration 3550, dc_loss: 0.09417158365249634, tv_loss: 0.03282647207379341\n",
      "iteration 3551, dc_loss: 0.0941748172044754, tv_loss: 0.03281824290752411\n",
      "iteration 3552, dc_loss: 0.09416857361793518, tv_loss: 0.03282181918621063\n",
      "iteration 3553, dc_loss: 0.09415794909000397, tv_loss: 0.03282279893755913\n",
      "iteration 3554, dc_loss: 0.09416201710700989, tv_loss: 0.0328105166554451\n",
      "iteration 3555, dc_loss: 0.09414467960596085, tv_loss: 0.03282657638192177\n",
      "iteration 3556, dc_loss: 0.0941452905535698, tv_loss: 0.0328206904232502\n",
      "iteration 3557, dc_loss: 0.09413093328475952, tv_loss: 0.0328272320330143\n",
      "iteration 3558, dc_loss: 0.0941406711935997, tv_loss: 0.032812610268592834\n",
      "iteration 3559, dc_loss: 0.09412304311990738, tv_loss: 0.03282594680786133\n",
      "iteration 3560, dc_loss: 0.09412683546543121, tv_loss: 0.03281885385513306\n",
      "iteration 3561, dc_loss: 0.09411612153053284, tv_loss: 0.032821789383888245\n",
      "iteration 3562, dc_loss: 0.09411151707172394, tv_loss: 0.03281961753964424\n",
      "iteration 3563, dc_loss: 0.09411326795816422, tv_loss: 0.0328141450881958\n",
      "iteration 3564, dc_loss: 0.09409631788730621, tv_loss: 0.03283023461699486\n",
      "iteration 3565, dc_loss: 0.09410227835178375, tv_loss: 0.032816510647535324\n",
      "iteration 3566, dc_loss: 0.09409049898386002, tv_loss: 0.03282257914543152\n",
      "iteration 3567, dc_loss: 0.09409218281507492, tv_loss: 0.03281796723604202\n",
      "iteration 3568, dc_loss: 0.0940755307674408, tv_loss: 0.03282763808965683\n",
      "iteration 3569, dc_loss: 0.09407797455787659, tv_loss: 0.032819557934999466\n",
      "iteration 3570, dc_loss: 0.09406950324773788, tv_loss: 0.032821979373693466\n",
      "iteration 3571, dc_loss: 0.09405577182769775, tv_loss: 0.03283034637570381\n",
      "iteration 3572, dc_loss: 0.09406633675098419, tv_loss: 0.032811544835567474\n",
      "iteration 3573, dc_loss: 0.09404806792736053, tv_loss: 0.03282156586647034\n",
      "iteration 3574, dc_loss: 0.09403891116380692, tv_loss: 0.032828278839588165\n",
      "iteration 3575, dc_loss: 0.09403809905052185, tv_loss: 0.032822202891111374\n",
      "iteration 3576, dc_loss: 0.0940316915512085, tv_loss: 0.03282257914543152\n",
      "iteration 3577, dc_loss: 0.09402506053447723, tv_loss: 0.03282258287072182\n",
      "iteration 3578, dc_loss: 0.09401808679103851, tv_loss: 0.0328219011425972\n",
      "iteration 3579, dc_loss: 0.09401729702949524, tv_loss: 0.032819587737321854\n",
      "iteration 3580, dc_loss: 0.09400191903114319, tv_loss: 0.03282797336578369\n",
      "iteration 3581, dc_loss: 0.09401151537895203, tv_loss: 0.03280958533287048\n",
      "iteration 3582, dc_loss: 0.09399136155843735, tv_loss: 0.032825104892253876\n",
      "iteration 3583, dc_loss: 0.09399071335792542, tv_loss: 0.03282220661640167\n",
      "iteration 3584, dc_loss: 0.09398791193962097, tv_loss: 0.03281812742352486\n",
      "iteration 3585, dc_loss: 0.09398321807384491, tv_loss: 0.03282066807150841\n",
      "iteration 3586, dc_loss: 0.09397348761558533, tv_loss: 0.03282609209418297\n",
      "iteration 3587, dc_loss: 0.09397584199905396, tv_loss: 0.03281836584210396\n",
      "iteration 3588, dc_loss: 0.09396250545978546, tv_loss: 0.03282877057790756\n",
      "iteration 3589, dc_loss: 0.09397298842668533, tv_loss: 0.032825130969285965\n",
      "iteration 3590, dc_loss: 0.09395689517259598, tv_loss: 0.03283995762467384\n",
      "iteration 3591, dc_loss: 0.0939696729183197, tv_loss: 0.03281960263848305\n",
      "iteration 3592, dc_loss: 0.09395042061805725, tv_loss: 0.032830800861120224\n",
      "iteration 3593, dc_loss: 0.09395986050367355, tv_loss: 0.032819412648677826\n",
      "iteration 3594, dc_loss: 0.09394040703773499, tv_loss: 0.032838910818099976\n",
      "iteration 3595, dc_loss: 0.09395022690296173, tv_loss: 0.03282776102423668\n",
      "iteration 3596, dc_loss: 0.09392421692609787, tv_loss: 0.032838042825460434\n",
      "iteration 3597, dc_loss: 0.09393619000911713, tv_loss: 0.03281925618648529\n",
      "iteration 3598, dc_loss: 0.09391139447689056, tv_loss: 0.03284170851111412\n",
      "iteration 3599, dc_loss: 0.0939411148428917, tv_loss: 0.03281707316637039\n",
      "iteration 3600, dc_loss: 0.09390978515148163, tv_loss: 0.03284837305545807\n",
      "iteration 3601, dc_loss: 0.09393053501844406, tv_loss: 0.03281964734196663\n",
      "iteration 3602, dc_loss: 0.0938916876912117, tv_loss: 0.03283338621258736\n",
      "iteration 3603, dc_loss: 0.09388002008199692, tv_loss: 0.03283132612705231\n",
      "iteration 3604, dc_loss: 0.09390894323587418, tv_loss: 0.03281540796160698\n",
      "iteration 3605, dc_loss: 0.09388034790754318, tv_loss: 0.03283679485321045\n",
      "iteration 3606, dc_loss: 0.09386631846427917, tv_loss: 0.03282730653882027\n",
      "iteration 3607, dc_loss: 0.09388524293899536, tv_loss: 0.03281782567501068\n",
      "iteration 3608, dc_loss: 0.09386738389730453, tv_loss: 0.03283395618200302\n",
      "iteration 3609, dc_loss: 0.09385551512241364, tv_loss: 0.03282884880900383\n",
      "iteration 3610, dc_loss: 0.09386537969112396, tv_loss: 0.03281518071889877\n",
      "iteration 3611, dc_loss: 0.093851737678051, tv_loss: 0.032832950353622437\n",
      "iteration 3612, dc_loss: 0.09384259581565857, tv_loss: 0.03282628208398819\n",
      "iteration 3613, dc_loss: 0.09384545683860779, tv_loss: 0.032819900661706924\n",
      "iteration 3614, dc_loss: 0.09383940696716309, tv_loss: 0.03282499313354492\n",
      "iteration 3615, dc_loss: 0.0938335731625557, tv_loss: 0.032825272530317307\n",
      "iteration 3616, dc_loss: 0.09382496029138565, tv_loss: 0.0328267365694046\n",
      "iteration 3617, dc_loss: 0.09381905198097229, tv_loss: 0.032830215990543365\n",
      "iteration 3618, dc_loss: 0.09382058680057526, tv_loss: 0.032819733023643494\n",
      "iteration 3619, dc_loss: 0.09381016343832016, tv_loss: 0.03282707557082176\n",
      "iteration 3620, dc_loss: 0.0938044860959053, tv_loss: 0.032834239304065704\n",
      "iteration 3621, dc_loss: 0.0938098132610321, tv_loss: 0.03282002732157707\n",
      "iteration 3622, dc_loss: 0.09379792213439941, tv_loss: 0.03282307833433151\n",
      "iteration 3623, dc_loss: 0.09378857910633087, tv_loss: 0.032831527292728424\n",
      "iteration 3624, dc_loss: 0.09379049390554428, tv_loss: 0.032827429473400116\n",
      "iteration 3625, dc_loss: 0.09378187358379364, tv_loss: 0.0328284315764904\n",
      "iteration 3626, dc_loss: 0.09377893805503845, tv_loss: 0.032825492322444916\n",
      "iteration 3627, dc_loss: 0.0937778502702713, tv_loss: 0.03282299265265465\n",
      "iteration 3628, dc_loss: 0.09376820176839828, tv_loss: 0.03282870724797249\n",
      "iteration 3629, dc_loss: 0.0937633141875267, tv_loss: 0.03282889351248741\n",
      "iteration 3630, dc_loss: 0.09376418590545654, tv_loss: 0.03281981498003006\n",
      "iteration 3631, dc_loss: 0.09375737607479095, tv_loss: 0.03282308205962181\n",
      "iteration 3632, dc_loss: 0.09374915808439255, tv_loss: 0.03283100202679634\n",
      "iteration 3633, dc_loss: 0.09374900162220001, tv_loss: 0.03282706066966057\n",
      "iteration 3634, dc_loss: 0.09374472498893738, tv_loss: 0.03282114863395691\n",
      "iteration 3635, dc_loss: 0.09373665601015091, tv_loss: 0.03282681480050087\n",
      "iteration 3636, dc_loss: 0.09373275190591812, tv_loss: 0.032826438546180725\n",
      "iteration 3637, dc_loss: 0.09372830390930176, tv_loss: 0.032826218754053116\n",
      "iteration 3638, dc_loss: 0.09372372180223465, tv_loss: 0.03282871097326279\n",
      "iteration 3639, dc_loss: 0.09372258931398392, tv_loss: 0.03282816708087921\n",
      "iteration 3640, dc_loss: 0.09371808171272278, tv_loss: 0.03282586857676506\n",
      "iteration 3641, dc_loss: 0.09371171146631241, tv_loss: 0.03282438963651657\n",
      "iteration 3642, dc_loss: 0.09370500594377518, tv_loss: 0.03282908722758293\n",
      "iteration 3643, dc_loss: 0.09370189905166626, tv_loss: 0.03282634913921356\n",
      "iteration 3644, dc_loss: 0.0936991348862648, tv_loss: 0.03282353654503822\n",
      "iteration 3645, dc_loss: 0.09369318187236786, tv_loss: 0.03282920643687248\n",
      "iteration 3646, dc_loss: 0.09368614852428436, tv_loss: 0.03283027186989784\n",
      "iteration 3647, dc_loss: 0.09368652105331421, tv_loss: 0.03282763436436653\n",
      "iteration 3648, dc_loss: 0.09367969632148743, tv_loss: 0.03282710164785385\n",
      "iteration 3649, dc_loss: 0.09367549419403076, tv_loss: 0.03282710537314415\n",
      "iteration 3650, dc_loss: 0.09367215633392334, tv_loss: 0.032824527472257614\n",
      "iteration 3651, dc_loss: 0.09366527199745178, tv_loss: 0.03282622620463371\n",
      "iteration 3652, dc_loss: 0.09366127103567123, tv_loss: 0.03282470256090164\n",
      "iteration 3653, dc_loss: 0.09365903586149216, tv_loss: 0.03282395005226135\n",
      "iteration 3654, dc_loss: 0.09365247935056686, tv_loss: 0.03282453119754791\n",
      "iteration 3655, dc_loss: 0.09364602714776993, tv_loss: 0.032828494906425476\n",
      "iteration 3656, dc_loss: 0.09364794939756393, tv_loss: 0.032824765890836716\n",
      "iteration 3657, dc_loss: 0.09363815933465958, tv_loss: 0.03283015266060829\n",
      "iteration 3658, dc_loss: 0.09363514930009842, tv_loss: 0.03283097594976425\n",
      "iteration 3659, dc_loss: 0.09363144636154175, tv_loss: 0.032826997339725494\n",
      "iteration 3660, dc_loss: 0.09362458437681198, tv_loss: 0.032829649746418\n",
      "iteration 3661, dc_loss: 0.09362127631902695, tv_loss: 0.03282764554023743\n",
      "iteration 3662, dc_loss: 0.09361673891544342, tv_loss: 0.03282618895173073\n",
      "iteration 3663, dc_loss: 0.09361464530229568, tv_loss: 0.03282454237341881\n",
      "iteration 3664, dc_loss: 0.09360748529434204, tv_loss: 0.03282932564616203\n",
      "iteration 3665, dc_loss: 0.09360328316688538, tv_loss: 0.03283223882317543\n",
      "iteration 3666, dc_loss: 0.09360068291425705, tv_loss: 0.0328323133289814\n",
      "iteration 3667, dc_loss: 0.09359346330165863, tv_loss: 0.03283634036779404\n",
      "iteration 3668, dc_loss: 0.09359052032232285, tv_loss: 0.03283720836043358\n",
      "iteration 3669, dc_loss: 0.09358575195074081, tv_loss: 0.0328337624669075\n",
      "iteration 3670, dc_loss: 0.09357975423336029, tv_loss: 0.0328301265835762\n",
      "iteration 3671, dc_loss: 0.0935765877366066, tv_loss: 0.03282782435417175\n",
      "iteration 3672, dc_loss: 0.093574158847332, tv_loss: 0.03282547742128372\n",
      "iteration 3673, dc_loss: 0.09356766939163208, tv_loss: 0.03283190727233887\n",
      "iteration 3674, dc_loss: 0.09356480836868286, tv_loss: 0.03283308073878288\n",
      "iteration 3675, dc_loss: 0.09355784952640533, tv_loss: 0.032833147794008255\n",
      "iteration 3676, dc_loss: 0.09355345368385315, tv_loss: 0.032828908413648605\n",
      "iteration 3677, dc_loss: 0.09355109930038452, tv_loss: 0.03282756358385086\n",
      "iteration 3678, dc_loss: 0.09354404360055923, tv_loss: 0.03283040225505829\n",
      "iteration 3679, dc_loss: 0.09354373067617416, tv_loss: 0.03282826766371727\n",
      "iteration 3680, dc_loss: 0.093535415828228, tv_loss: 0.03283562883734703\n",
      "iteration 3681, dc_loss: 0.09353035688400269, tv_loss: 0.03283882886171341\n",
      "iteration 3682, dc_loss: 0.09353048354387283, tv_loss: 0.03283802047371864\n",
      "iteration 3683, dc_loss: 0.09352181106805801, tv_loss: 0.03283926099538803\n",
      "iteration 3684, dc_loss: 0.09351889044046402, tv_loss: 0.032832179218530655\n",
      "iteration 3685, dc_loss: 0.09351450949907303, tv_loss: 0.03283194452524185\n",
      "iteration 3686, dc_loss: 0.09350831061601639, tv_loss: 0.03283512219786644\n",
      "iteration 3687, dc_loss: 0.09350739419460297, tv_loss: 0.03283112868666649\n",
      "iteration 3688, dc_loss: 0.09350178390741348, tv_loss: 0.03283052146434784\n",
      "iteration 3689, dc_loss: 0.0934925526380539, tv_loss: 0.032836705446243286\n",
      "iteration 3690, dc_loss: 0.0934939831495285, tv_loss: 0.03283482789993286\n",
      "iteration 3691, dc_loss: 0.0934867113828659, tv_loss: 0.032841380685567856\n",
      "iteration 3692, dc_loss: 0.09348281472921371, tv_loss: 0.03283892199397087\n",
      "iteration 3693, dc_loss: 0.09348027408123016, tv_loss: 0.0328313447535038\n",
      "iteration 3694, dc_loss: 0.0934721976518631, tv_loss: 0.03283467888832092\n",
      "iteration 3695, dc_loss: 0.09346753358840942, tv_loss: 0.03283442556858063\n",
      "iteration 3696, dc_loss: 0.09346648305654526, tv_loss: 0.03283347934484482\n",
      "iteration 3697, dc_loss: 0.09346172958612442, tv_loss: 0.032833293080329895\n",
      "iteration 3698, dc_loss: 0.09345830231904984, tv_loss: 0.032831914722919464\n",
      "iteration 3699, dc_loss: 0.09344935417175293, tv_loss: 0.03283672034740448\n",
      "iteration 3700, dc_loss: 0.09344267100095749, tv_loss: 0.03283658251166344\n",
      "iteration 3701, dc_loss: 0.09344743937253952, tv_loss: 0.03282906115055084\n",
      "iteration 3702, dc_loss: 0.09343423694372177, tv_loss: 0.03283900395035744\n",
      "iteration 3703, dc_loss: 0.09343700110912323, tv_loss: 0.032831985503435135\n",
      "iteration 3704, dc_loss: 0.09342982620000839, tv_loss: 0.03283340483903885\n",
      "iteration 3705, dc_loss: 0.09342442452907562, tv_loss: 0.032835960388183594\n",
      "iteration 3706, dc_loss: 0.09342001378536224, tv_loss: 0.03283761814236641\n",
      "iteration 3707, dc_loss: 0.09341681748628616, tv_loss: 0.032842006534338\n",
      "iteration 3708, dc_loss: 0.09340820461511612, tv_loss: 0.0328463576734066\n",
      "iteration 3709, dc_loss: 0.09340832382440567, tv_loss: 0.032837361097335815\n",
      "iteration 3710, dc_loss: 0.09339988231658936, tv_loss: 0.032837387174367905\n",
      "iteration 3711, dc_loss: 0.0933995470404625, tv_loss: 0.03283097594976425\n",
      "iteration 3712, dc_loss: 0.0933966264128685, tv_loss: 0.03283047676086426\n",
      "iteration 3713, dc_loss: 0.09338680654764175, tv_loss: 0.032836031168699265\n",
      "iteration 3714, dc_loss: 0.09338366240262985, tv_loss: 0.03283529356122017\n",
      "iteration 3715, dc_loss: 0.09337937831878662, tv_loss: 0.032837312668561935\n",
      "iteration 3716, dc_loss: 0.09337354451417923, tv_loss: 0.032839953899383545\n",
      "iteration 3717, dc_loss: 0.09337612241506577, tv_loss: 0.03283332660794258\n",
      "iteration 3718, dc_loss: 0.09336400777101517, tv_loss: 0.032841701060533524\n",
      "iteration 3719, dc_loss: 0.09335929900407791, tv_loss: 0.032847121357917786\n",
      "iteration 3720, dc_loss: 0.09336048364639282, tv_loss: 0.03284238278865814\n",
      "iteration 3721, dc_loss: 0.09335240721702576, tv_loss: 0.032841019332408905\n",
      "iteration 3722, dc_loss: 0.09334725886583328, tv_loss: 0.03283611685037613\n",
      "iteration 3723, dc_loss: 0.09334555268287659, tv_loss: 0.03283426910638809\n",
      "iteration 3724, dc_loss: 0.09333883970975876, tv_loss: 0.032834768295288086\n",
      "iteration 3725, dc_loss: 0.0933370441198349, tv_loss: 0.03283373638987541\n",
      "iteration 3726, dc_loss: 0.09332942217588425, tv_loss: 0.03284080699086189\n",
      "iteration 3727, dc_loss: 0.09332167357206345, tv_loss: 0.03284537047147751\n",
      "iteration 3728, dc_loss: 0.09332817792892456, tv_loss: 0.03283218294382095\n",
      "iteration 3729, dc_loss: 0.09331630915403366, tv_loss: 0.03283734247088432\n",
      "iteration 3730, dc_loss: 0.09330815076828003, tv_loss: 0.032841358333826065\n",
      "iteration 3731, dc_loss: 0.09331143647432327, tv_loss: 0.03283577039837837\n",
      "iteration 3732, dc_loss: 0.09330195188522339, tv_loss: 0.03284536674618721\n",
      "iteration 3733, dc_loss: 0.09330103546380997, tv_loss: 0.03284327685832977\n",
      "iteration 3734, dc_loss: 0.09329403936862946, tv_loss: 0.03284839913249016\n",
      "iteration 3735, dc_loss: 0.09328901022672653, tv_loss: 0.03284522518515587\n",
      "iteration 3736, dc_loss: 0.09328611940145493, tv_loss: 0.03283897414803505\n",
      "iteration 3737, dc_loss: 0.09327922016382217, tv_loss: 0.03284067660570145\n",
      "iteration 3738, dc_loss: 0.09327472001314163, tv_loss: 0.032843440771102905\n",
      "iteration 3739, dc_loss: 0.09327653050422668, tv_loss: 0.0328381210565567\n",
      "iteration 3740, dc_loss: 0.09326742589473724, tv_loss: 0.03284173086285591\n",
      "iteration 3741, dc_loss: 0.09326191246509552, tv_loss: 0.032843999564647675\n",
      "iteration 3742, dc_loss: 0.09325946867465973, tv_loss: 0.03283989056944847\n",
      "iteration 3743, dc_loss: 0.09325245022773743, tv_loss: 0.03284456580877304\n",
      "iteration 3744, dc_loss: 0.09325277805328369, tv_loss: 0.03283855691552162\n",
      "iteration 3745, dc_loss: 0.09324391931295395, tv_loss: 0.03284299373626709\n",
      "iteration 3746, dc_loss: 0.09323599189519882, tv_loss: 0.032844968140125275\n",
      "iteration 3747, dc_loss: 0.0932401642203331, tv_loss: 0.03283529356122017\n",
      "iteration 3748, dc_loss: 0.09323254227638245, tv_loss: 0.03283990919589996\n",
      "iteration 3749, dc_loss: 0.09322566539049149, tv_loss: 0.03284250199794769\n",
      "iteration 3750, dc_loss: 0.09322483092546463, tv_loss: 0.032841842621564865\n",
      "iteration 3751, dc_loss: 0.09321662783622742, tv_loss: 0.03284963220357895\n",
      "iteration 3752, dc_loss: 0.0932122990489006, tv_loss: 0.03284657001495361\n",
      "iteration 3753, dc_loss: 0.09321072697639465, tv_loss: 0.032841358333826065\n",
      "iteration 3754, dc_loss: 0.09320329874753952, tv_loss: 0.03284425288438797\n",
      "iteration 3755, dc_loss: 0.0932001918554306, tv_loss: 0.03284338489174843\n",
      "iteration 3756, dc_loss: 0.09319712221622467, tv_loss: 0.03284271061420441\n",
      "iteration 3757, dc_loss: 0.09319142997264862, tv_loss: 0.032843682914972305\n",
      "iteration 3758, dc_loss: 0.09318606555461884, tv_loss: 0.0328444167971611\n",
      "iteration 3759, dc_loss: 0.09317976236343384, tv_loss: 0.03284739330410957\n",
      "iteration 3760, dc_loss: 0.09317629039287567, tv_loss: 0.032844312489032745\n",
      "iteration 3761, dc_loss: 0.09317784756422043, tv_loss: 0.03283872827887535\n",
      "iteration 3762, dc_loss: 0.09316697716712952, tv_loss: 0.032844413071870804\n",
      "iteration 3763, dc_loss: 0.09316357970237732, tv_loss: 0.03284170478582382\n",
      "iteration 3764, dc_loss: 0.09315884858369827, tv_loss: 0.03284243121743202\n",
      "iteration 3765, dc_loss: 0.09315592050552368, tv_loss: 0.03284194692969322\n",
      "iteration 3766, dc_loss: 0.09314893186092377, tv_loss: 0.0328475721180439\n",
      "iteration 3767, dc_loss: 0.09314745664596558, tv_loss: 0.032843463122844696\n",
      "iteration 3768, dc_loss: 0.09314093738794327, tv_loss: 0.032844074070453644\n",
      "iteration 3769, dc_loss: 0.09313877671957016, tv_loss: 0.03284468501806259\n",
      "iteration 3770, dc_loss: 0.09313207119703293, tv_loss: 0.03284686803817749\n",
      "iteration 3771, dc_loss: 0.09312644600868225, tv_loss: 0.032848868519067764\n",
      "iteration 3772, dc_loss: 0.0931255891919136, tv_loss: 0.03284304216504097\n",
      "iteration 3773, dc_loss: 0.09311671555042267, tv_loss: 0.03284602239727974\n",
      "iteration 3774, dc_loss: 0.09311293065547943, tv_loss: 0.03284575045108795\n",
      "iteration 3775, dc_loss: 0.09311222285032272, tv_loss: 0.0328417643904686\n",
      "iteration 3776, dc_loss: 0.0931040421128273, tv_loss: 0.03284410387277603\n",
      "iteration 3777, dc_loss: 0.09310217201709747, tv_loss: 0.032843612134456635\n",
      "iteration 3778, dc_loss: 0.09309615939855576, tv_loss: 0.032847810536623\n",
      "iteration 3779, dc_loss: 0.09309135377407074, tv_loss: 0.03285155072808266\n",
      "iteration 3780, dc_loss: 0.09308741241693497, tv_loss: 0.03285200521349907\n",
      "iteration 3781, dc_loss: 0.09308363497257233, tv_loss: 0.03285161778330803\n",
      "iteration 3782, dc_loss: 0.09307532012462616, tv_loss: 0.032850876450538635\n",
      "iteration 3783, dc_loss: 0.09307362139225006, tv_loss: 0.03284737095236778\n",
      "iteration 3784, dc_loss: 0.09307079762220383, tv_loss: 0.03284570947289467\n",
      "iteration 3785, dc_loss: 0.09306582063436508, tv_loss: 0.032847434282302856\n",
      "iteration 3786, dc_loss: 0.09305840730667114, tv_loss: 0.032852593809366226\n",
      "iteration 3787, dc_loss: 0.09305757284164429, tv_loss: 0.032849352806806564\n",
      "iteration 3788, dc_loss: 0.0930509939789772, tv_loss: 0.03285154327750206\n",
      "iteration 3789, dc_loss: 0.09304628521203995, tv_loss: 0.0328514389693737\n",
      "iteration 3790, dc_loss: 0.09304079413414001, tv_loss: 0.032851219177246094\n",
      "iteration 3791, dc_loss: 0.09303755313158035, tv_loss: 0.03284955024719238\n",
      "iteration 3792, dc_loss: 0.09303267300128937, tv_loss: 0.032848842442035675\n",
      "iteration 3793, dc_loss: 0.09302982687950134, tv_loss: 0.0328463613986969\n",
      "iteration 3794, dc_loss: 0.09302675724029541, tv_loss: 0.03284540772438049\n",
      "iteration 3795, dc_loss: 0.0930180773139, tv_loss: 0.03284992650151253\n",
      "iteration 3796, dc_loss: 0.09301283210515976, tv_loss: 0.032851431518793106\n",
      "iteration 3797, dc_loss: 0.09301058948040009, tv_loss: 0.03284996375441551\n",
      "iteration 3798, dc_loss: 0.09300308674573898, tv_loss: 0.032852653414011\n",
      "iteration 3799, dc_loss: 0.09300438314676285, tv_loss: 0.03284730017185211\n",
      "iteration 3800, dc_loss: 0.09300016611814499, tv_loss: 0.03284595534205437\n",
      "iteration 3801, dc_loss: 0.092988982796669, tv_loss: 0.03285268321633339\n",
      "iteration 3802, dc_loss: 0.09298889338970184, tv_loss: 0.03285086899995804\n",
      "iteration 3803, dc_loss: 0.09298272430896759, tv_loss: 0.032857682555913925\n",
      "iteration 3804, dc_loss: 0.09298024326562881, tv_loss: 0.03285565227270126\n",
      "iteration 3805, dc_loss: 0.09297267347574234, tv_loss: 0.03285416588187218\n",
      "iteration 3806, dc_loss: 0.0929734855890274, tv_loss: 0.0328487902879715\n",
      "iteration 3807, dc_loss: 0.0929613932967186, tv_loss: 0.03285312280058861\n",
      "iteration 3808, dc_loss: 0.09296402335166931, tv_loss: 0.032846350222826004\n",
      "iteration 3809, dc_loss: 0.09295317530632019, tv_loss: 0.0328526645898819\n",
      "iteration 3810, dc_loss: 0.0929546132683754, tv_loss: 0.032845623791217804\n",
      "iteration 3811, dc_loss: 0.0929451510310173, tv_loss: 0.03285088762640953\n",
      "iteration 3812, dc_loss: 0.09294712543487549, tv_loss: 0.03284521400928497\n",
      "iteration 3813, dc_loss: 0.09293930977582932, tv_loss: 0.032850902527570724\n",
      "iteration 3814, dc_loss: 0.09293337166309357, tv_loss: 0.032853733748197556\n",
      "iteration 3815, dc_loss: 0.09292604774236679, tv_loss: 0.03285926952958107\n",
      "iteration 3816, dc_loss: 0.09292861819267273, tv_loss: 0.03285534679889679\n",
      "iteration 3817, dc_loss: 0.09291885048151016, tv_loss: 0.03285720571875572\n",
      "iteration 3818, dc_loss: 0.09292024374008179, tv_loss: 0.032848432660102844\n",
      "iteration 3819, dc_loss: 0.09290581941604614, tv_loss: 0.03286084160208702\n",
      "iteration 3820, dc_loss: 0.09290783107280731, tv_loss: 0.0328526571393013\n",
      "iteration 3821, dc_loss: 0.09290450811386108, tv_loss: 0.03285070136189461\n",
      "iteration 3822, dc_loss: 0.09289855509996414, tv_loss: 0.03285704180598259\n",
      "iteration 3823, dc_loss: 0.09288866072893143, tv_loss: 0.03285898268222809\n",
      "iteration 3824, dc_loss: 0.09288857132196426, tv_loss: 0.03285369649529457\n",
      "iteration 3825, dc_loss: 0.09288639575242996, tv_loss: 0.032859161496162415\n",
      "iteration 3826, dc_loss: 0.09288039803504944, tv_loss: 0.032853420823812485\n",
      "iteration 3827, dc_loss: 0.09287436306476593, tv_loss: 0.03285844624042511\n",
      "iteration 3828, dc_loss: 0.0928683802485466, tv_loss: 0.03285841643810272\n",
      "iteration 3829, dc_loss: 0.09286563843488693, tv_loss: 0.032860007137060165\n",
      "iteration 3830, dc_loss: 0.09286279231309891, tv_loss: 0.03285858780145645\n",
      "iteration 3831, dc_loss: 0.09285879135131836, tv_loss: 0.03285817429423332\n",
      "iteration 3832, dc_loss: 0.09284967929124832, tv_loss: 0.03285886347293854\n",
      "iteration 3833, dc_loss: 0.09284649789333344, tv_loss: 0.03285762295126915\n",
      "iteration 3834, dc_loss: 0.0928436741232872, tv_loss: 0.032864946871995926\n",
      "iteration 3835, dc_loss: 0.09284184873104095, tv_loss: 0.03285355865955353\n",
      "iteration 3836, dc_loss: 0.09283393621444702, tv_loss: 0.03285811096429825\n",
      "iteration 3837, dc_loss: 0.0928279235959053, tv_loss: 0.03285916894674301\n",
      "iteration 3838, dc_loss: 0.09282305836677551, tv_loss: 0.03286133334040642\n",
      "iteration 3839, dc_loss: 0.09282076358795166, tv_loss: 0.032861385494470596\n",
      "iteration 3840, dc_loss: 0.09281715005636215, tv_loss: 0.032857950776815414\n",
      "iteration 3841, dc_loss: 0.09281305968761444, tv_loss: 0.032857611775398254\n",
      "iteration 3842, dc_loss: 0.09280408918857574, tv_loss: 0.03286215662956238\n",
      "iteration 3843, dc_loss: 0.09280899167060852, tv_loss: 0.0328558050096035\n",
      "iteration 3844, dc_loss: 0.09279654175043106, tv_loss: 0.03286247327923775\n",
      "iteration 3845, dc_loss: 0.09279448539018631, tv_loss: 0.032858703285455704\n",
      "iteration 3846, dc_loss: 0.09278763830661774, tv_loss: 0.03286311402916908\n",
      "iteration 3847, dc_loss: 0.09279342740774155, tv_loss: 0.03285672515630722\n",
      "iteration 3848, dc_loss: 0.09277649968862534, tv_loss: 0.03286614641547203\n",
      "iteration 3849, dc_loss: 0.09278766065835953, tv_loss: 0.03285739943385124\n",
      "iteration 3850, dc_loss: 0.0927719846367836, tv_loss: 0.03286602720618248\n",
      "iteration 3851, dc_loss: 0.0927918329834938, tv_loss: 0.0328463651239872\n",
      "iteration 3852, dc_loss: 0.09276054799556732, tv_loss: 0.032885197550058365\n",
      "iteration 3853, dc_loss: 0.09279059618711472, tv_loss: 0.03284015506505966\n",
      "iteration 3854, dc_loss: 0.09275445342063904, tv_loss: 0.03287702798843384\n",
      "iteration 3855, dc_loss: 0.09276889264583588, tv_loss: 0.0328509546816349\n",
      "iteration 3856, dc_loss: 0.09274007380008698, tv_loss: 0.03286932781338692\n",
      "iteration 3857, dc_loss: 0.09274323284626007, tv_loss: 0.03285845369100571\n",
      "iteration 3858, dc_loss: 0.0927363857626915, tv_loss: 0.03286309912800789\n",
      "iteration 3859, dc_loss: 0.09272920340299606, tv_loss: 0.03286884352564812\n",
      "iteration 3860, dc_loss: 0.09274342656135559, tv_loss: 0.03285227715969086\n",
      "iteration 3861, dc_loss: 0.09271439909934998, tv_loss: 0.03287224844098091\n",
      "iteration 3862, dc_loss: 0.09272300451993942, tv_loss: 0.03285751864314079\n",
      "iteration 3863, dc_loss: 0.09271209686994553, tv_loss: 0.03286319971084595\n",
      "iteration 3864, dc_loss: 0.09270770847797394, tv_loss: 0.032865941524505615\n",
      "iteration 3865, dc_loss: 0.09271444380283356, tv_loss: 0.03285333141684532\n",
      "iteration 3866, dc_loss: 0.09269392490386963, tv_loss: 0.032871063798666\n",
      "iteration 3867, dc_loss: 0.09269557148218155, tv_loss: 0.032865822315216064\n",
      "iteration 3868, dc_loss: 0.09268931299448013, tv_loss: 0.03286329284310341\n",
      "iteration 3869, dc_loss: 0.09268303215503693, tv_loss: 0.03287046402692795\n",
      "iteration 3870, dc_loss: 0.09268607199192047, tv_loss: 0.032861486077308655\n",
      "iteration 3871, dc_loss: 0.09267354756593704, tv_loss: 0.03286894038319588\n",
      "iteration 3872, dc_loss: 0.09268002212047577, tv_loss: 0.03285691514611244\n",
      "iteration 3873, dc_loss: 0.09266359359025955, tv_loss: 0.03286753222346306\n",
      "iteration 3874, dc_loss: 0.09265805035829544, tv_loss: 0.032870132476091385\n",
      "iteration 3875, dc_loss: 0.09266199916601181, tv_loss: 0.03285884112119675\n",
      "iteration 3876, dc_loss: 0.09265130758285522, tv_loss: 0.03286832198500633\n",
      "iteration 3877, dc_loss: 0.09265550225973129, tv_loss: 0.032858747988939285\n",
      "iteration 3878, dc_loss: 0.09264528751373291, tv_loss: 0.03286496177315712\n",
      "iteration 3879, dc_loss: 0.09263607859611511, tv_loss: 0.03286967799067497\n",
      "iteration 3880, dc_loss: 0.0926336720585823, tv_loss: 0.032865267246961594\n",
      "iteration 3881, dc_loss: 0.09263116866350174, tv_loss: 0.0328625924885273\n",
      "iteration 3882, dc_loss: 0.09262704104185104, tv_loss: 0.03286543861031532\n",
      "iteration 3883, dc_loss: 0.09262106567621231, tv_loss: 0.03286338597536087\n",
      "iteration 3884, dc_loss: 0.09261874109506607, tv_loss: 0.03286393731832504\n",
      "iteration 3885, dc_loss: 0.092608742415905, tv_loss: 0.03287094086408615\n",
      "iteration 3886, dc_loss: 0.09260709583759308, tv_loss: 0.03286390379071236\n",
      "iteration 3887, dc_loss: 0.09260547906160355, tv_loss: 0.0328637957572937\n",
      "iteration 3888, dc_loss: 0.09259743988513947, tv_loss: 0.03286620229482651\n",
      "iteration 3889, dc_loss: 0.09259787946939468, tv_loss: 0.0328587181866169\n",
      "iteration 3890, dc_loss: 0.09258563816547394, tv_loss: 0.0328698493540287\n",
      "iteration 3891, dc_loss: 0.09258005023002625, tv_loss: 0.03287070244550705\n",
      "iteration 3892, dc_loss: 0.0925874263048172, tv_loss: 0.03285796195268631\n",
      "iteration 3893, dc_loss: 0.09257226437330246, tv_loss: 0.03286602720618248\n",
      "iteration 3894, dc_loss: 0.09256747364997864, tv_loss: 0.03286820650100708\n",
      "iteration 3895, dc_loss: 0.09257280081510544, tv_loss: 0.03286074474453926\n",
      "iteration 3896, dc_loss: 0.09256190061569214, tv_loss: 0.03286490961909294\n",
      "iteration 3897, dc_loss: 0.0925522893667221, tv_loss: 0.03287138044834137\n",
      "iteration 3898, dc_loss: 0.09255613386631012, tv_loss: 0.03286219388246536\n",
      "iteration 3899, dc_loss: 0.09254739433526993, tv_loss: 0.0328671894967556\n",
      "iteration 3900, dc_loss: 0.09254256635904312, tv_loss: 0.032868608832359314\n",
      "iteration 3901, dc_loss: 0.09253870695829391, tv_loss: 0.032868269830942154\n",
      "iteration 3902, dc_loss: 0.09253550320863724, tv_loss: 0.03286626562476158\n",
      "iteration 3903, dc_loss: 0.09253174066543579, tv_loss: 0.032864611595869064\n",
      "iteration 3904, dc_loss: 0.09252403676509857, tv_loss: 0.03286835923790932\n",
      "iteration 3905, dc_loss: 0.09251751005649567, tv_loss: 0.03286736086010933\n",
      "iteration 3906, dc_loss: 0.09251655638217926, tv_loss: 0.032863520085811615\n",
      "iteration 3907, dc_loss: 0.09251273423433304, tv_loss: 0.032865460962057114\n",
      "iteration 3908, dc_loss: 0.09251187741756439, tv_loss: 0.03286284953355789\n",
      "iteration 3909, dc_loss: 0.09249913692474365, tv_loss: 0.032868657261133194\n",
      "iteration 3910, dc_loss: 0.09250272065401077, tv_loss: 0.03286353498697281\n",
      "iteration 3911, dc_loss: 0.09249404817819595, tv_loss: 0.032867249101400375\n",
      "iteration 3912, dc_loss: 0.09249455481767654, tv_loss: 0.0328606441617012\n",
      "iteration 3913, dc_loss: 0.09248118847608566, tv_loss: 0.03287162631750107\n",
      "iteration 3914, dc_loss: 0.09249258041381836, tv_loss: 0.03285660222172737\n",
      "iteration 3915, dc_loss: 0.0924767553806305, tv_loss: 0.03287053108215332\n",
      "iteration 3916, dc_loss: 0.09248732030391693, tv_loss: 0.032861750572919846\n",
      "iteration 3917, dc_loss: 0.09247255325317383, tv_loss: 0.032872673124074936\n",
      "iteration 3918, dc_loss: 0.0924898236989975, tv_loss: 0.03285925090312958\n",
      "iteration 3919, dc_loss: 0.09246109426021576, tv_loss: 0.032886408269405365\n",
      "iteration 3920, dc_loss: 0.09248161315917969, tv_loss: 0.03285997733473778\n",
      "iteration 3921, dc_loss: 0.09246042370796204, tv_loss: 0.03287847340106964\n",
      "iteration 3922, dc_loss: 0.09246321767568588, tv_loss: 0.03286241739988327\n",
      "iteration 3923, dc_loss: 0.09243850409984589, tv_loss: 0.032877419143915176\n",
      "iteration 3924, dc_loss: 0.09244825690984726, tv_loss: 0.0328572653234005\n",
      "iteration 3925, dc_loss: 0.09242431819438934, tv_loss: 0.032874349504709244\n",
      "iteration 3926, dc_loss: 0.09243177622556686, tv_loss: 0.032862842082977295\n",
      "iteration 3927, dc_loss: 0.09241824597120285, tv_loss: 0.032870061695575714\n",
      "iteration 3928, dc_loss: 0.09241316467523575, tv_loss: 0.03287532553076744\n",
      "iteration 3929, dc_loss: 0.09241730719804764, tv_loss: 0.032869622111320496\n",
      "iteration 3930, dc_loss: 0.09240897744894028, tv_loss: 0.032872334122657776\n",
      "iteration 3931, dc_loss: 0.09240716695785522, tv_loss: 0.03287225216627121\n",
      "iteration 3932, dc_loss: 0.09239954501390457, tv_loss: 0.03287038579583168\n",
      "iteration 3933, dc_loss: 0.092405304312706, tv_loss: 0.03285994753241539\n",
      "iteration 3934, dc_loss: 0.0923868790268898, tv_loss: 0.03287731856107712\n",
      "iteration 3935, dc_loss: 0.09239060431718826, tv_loss: 0.032865606248378754\n",
      "iteration 3936, dc_loss: 0.0923743024468422, tv_loss: 0.03287503123283386\n",
      "iteration 3937, dc_loss: 0.09238512068986893, tv_loss: 0.032858967781066895\n",
      "iteration 3938, dc_loss: 0.09236812591552734, tv_loss: 0.03287206590175629\n",
      "iteration 3939, dc_loss: 0.09237516671419144, tv_loss: 0.03286101296544075\n",
      "iteration 3940, dc_loss: 0.09235702455043793, tv_loss: 0.032880257815122604\n",
      "iteration 3941, dc_loss: 0.09236035495996475, tv_loss: 0.03287016600370407\n",
      "iteration 3942, dc_loss: 0.09235017001628876, tv_loss: 0.03287466987967491\n",
      "iteration 3943, dc_loss: 0.09236212819814682, tv_loss: 0.032858956605196\n",
      "iteration 3944, dc_loss: 0.09233509749174118, tv_loss: 0.03287937492132187\n",
      "iteration 3945, dc_loss: 0.09235106408596039, tv_loss: 0.03285971283912659\n",
      "iteration 3946, dc_loss: 0.09233125299215317, tv_loss: 0.03287797048687935\n",
      "iteration 3947, dc_loss: 0.09234011173248291, tv_loss: 0.032863445580005646\n",
      "iteration 3948, dc_loss: 0.09231968224048615, tv_loss: 0.032879702746868134\n",
      "iteration 3949, dc_loss: 0.09232985973358154, tv_loss: 0.032865677028894424\n",
      "iteration 3950, dc_loss: 0.09231308847665787, tv_loss: 0.03287750482559204\n",
      "iteration 3951, dc_loss: 0.09231606125831604, tv_loss: 0.032868847250938416\n",
      "iteration 3952, dc_loss: 0.09230411797761917, tv_loss: 0.032875459641218185\n",
      "iteration 3953, dc_loss: 0.0923059806227684, tv_loss: 0.0328686349093914\n",
      "iteration 3954, dc_loss: 0.09229334443807602, tv_loss: 0.0328773632645607\n",
      "iteration 3955, dc_loss: 0.09229514747858047, tv_loss: 0.03287384286522865\n",
      "iteration 3956, dc_loss: 0.09228461235761642, tv_loss: 0.032886188477277756\n",
      "iteration 3957, dc_loss: 0.0922832265496254, tv_loss: 0.03288198262453079\n",
      "iteration 3958, dc_loss: 0.09227985888719559, tv_loss: 0.032872505486011505\n",
      "iteration 3959, dc_loss: 0.0922713503241539, tv_loss: 0.03287651389837265\n",
      "iteration 3960, dc_loss: 0.0922766849398613, tv_loss: 0.032868679612874985\n",
      "iteration 3961, dc_loss: 0.0922587662935257, tv_loss: 0.03287995234131813\n",
      "iteration 3962, dc_loss: 0.09226629137992859, tv_loss: 0.03286908194422722\n",
      "iteration 3963, dc_loss: 0.09225034713745117, tv_loss: 0.03287836164236069\n",
      "iteration 3964, dc_loss: 0.09226192533969879, tv_loss: 0.0328625924885273\n",
      "iteration 3965, dc_loss: 0.09224122762680054, tv_loss: 0.03288078308105469\n",
      "iteration 3966, dc_loss: 0.09226041287183762, tv_loss: 0.03286052122712135\n",
      "iteration 3967, dc_loss: 0.09223289787769318, tv_loss: 0.032891228795051575\n",
      "iteration 3968, dc_loss: 0.09226012229919434, tv_loss: 0.032869137823581696\n",
      "iteration 3969, dc_loss: 0.09222753345966339, tv_loss: 0.03289507329463959\n",
      "iteration 3970, dc_loss: 0.09225456416606903, tv_loss: 0.03286181017756462\n",
      "iteration 3971, dc_loss: 0.09222742170095444, tv_loss: 0.03288768231868744\n",
      "iteration 3972, dc_loss: 0.09225475788116455, tv_loss: 0.03286410868167877\n",
      "iteration 3973, dc_loss: 0.09222070127725601, tv_loss: 0.03290044888854027\n",
      "iteration 3974, dc_loss: 0.09225234389305115, tv_loss: 0.032859917730093\n",
      "iteration 3975, dc_loss: 0.09221336245536804, tv_loss: 0.03289355710148811\n",
      "iteration 3976, dc_loss: 0.09222929179668427, tv_loss: 0.03286634758114815\n",
      "iteration 3977, dc_loss: 0.09219193458557129, tv_loss: 0.03289254754781723\n",
      "iteration 3978, dc_loss: 0.09220482409000397, tv_loss: 0.0328693725168705\n",
      "iteration 3979, dc_loss: 0.0921829491853714, tv_loss: 0.03288586810231209\n",
      "iteration 3980, dc_loss: 0.09217958897352219, tv_loss: 0.03288065642118454\n",
      "iteration 3981, dc_loss: 0.09218444675207138, tv_loss: 0.03287555277347565\n",
      "iteration 3982, dc_loss: 0.09216655045747757, tv_loss: 0.03288979455828667\n",
      "iteration 3983, dc_loss: 0.0921865776181221, tv_loss: 0.03287028893828392\n",
      "iteration 3984, dc_loss: 0.09215950965881348, tv_loss: 0.032891545444726944\n",
      "iteration 3985, dc_loss: 0.092173732817173, tv_loss: 0.03287174925208092\n",
      "iteration 3986, dc_loss: 0.09214936941862106, tv_loss: 0.03289322927594185\n",
      "iteration 3987, dc_loss: 0.09215997904539108, tv_loss: 0.03287277743220329\n",
      "iteration 3988, dc_loss: 0.09213948249816895, tv_loss: 0.032892581075429916\n",
      "iteration 3989, dc_loss: 0.0921449288725853, tv_loss: 0.032873306423425674\n",
      "iteration 3990, dc_loss: 0.0921352207660675, tv_loss: 0.032881662249565125\n",
      "iteration 3991, dc_loss: 0.09212527424097061, tv_loss: 0.032889608293771744\n",
      "iteration 3992, dc_loss: 0.09213639795780182, tv_loss: 0.03287411481142044\n",
      "iteration 3993, dc_loss: 0.0921206995844841, tv_loss: 0.0328821986913681\n",
      "iteration 3994, dc_loss: 0.09212096035480499, tv_loss: 0.03287729620933533\n",
      "iteration 3995, dc_loss: 0.09211049973964691, tv_loss: 0.0328894779086113\n",
      "iteration 3996, dc_loss: 0.09211266785860062, tv_loss: 0.032877955585718155\n",
      "iteration 3997, dc_loss: 0.0921010673046112, tv_loss: 0.0328846201300621\n",
      "iteration 3998, dc_loss: 0.09210192412137985, tv_loss: 0.032877519726753235\n",
      "iteration 3999, dc_loss: 0.09208502620458603, tv_loss: 0.03288630023598671\n",
      "iteration 4000, dc_loss: 0.09209437668323517, tv_loss: 0.032879047095775604\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.953230619430542, tv_loss: 0.0003438346029724926\n",
      "iteration 2, dc_loss: 2.824640989303589, tv_loss: 0.003585376078262925\n",
      "iteration 3, dc_loss: 2.749671697616577, tv_loss: 0.006073945667594671\n",
      "iteration 4, dc_loss: 2.7008719444274902, tv_loss: 0.00792218092828989\n",
      "iteration 5, dc_loss: 2.671436071395874, tv_loss: 0.009071550332009792\n",
      "iteration 6, dc_loss: 2.6513469219207764, tv_loss: 0.009749146178364754\n",
      "iteration 7, dc_loss: 2.63533616065979, tv_loss: 0.010172576643526554\n",
      "iteration 8, dc_loss: 2.6196398735046387, tv_loss: 0.010546237230300903\n",
      "iteration 9, dc_loss: 2.6034185886383057, tv_loss: 0.010917861945927143\n",
      "iteration 10, dc_loss: 2.5874154567718506, tv_loss: 0.011237578466534615\n",
      "iteration 11, dc_loss: 2.571352243423462, tv_loss: 0.011504360474646091\n",
      "iteration 12, dc_loss: 2.5555624961853027, tv_loss: 0.011751309037208557\n",
      "iteration 13, dc_loss: 2.540461301803589, tv_loss: 0.012014058418571949\n",
      "iteration 14, dc_loss: 2.5259487628936768, tv_loss: 0.012307981960475445\n",
      "iteration 15, dc_loss: 2.5120108127593994, tv_loss: 0.012598127126693726\n",
      "iteration 16, dc_loss: 2.498236656188965, tv_loss: 0.012823550030589104\n",
      "iteration 17, dc_loss: 2.4842989444732666, tv_loss: 0.012957888655364513\n",
      "iteration 18, dc_loss: 2.470371723175049, tv_loss: 0.01303491648286581\n",
      "iteration 19, dc_loss: 2.456613063812256, tv_loss: 0.013105095364153385\n",
      "iteration 20, dc_loss: 2.443256139755249, tv_loss: 0.013146750628948212\n",
      "iteration 21, dc_loss: 2.430084705352783, tv_loss: 0.013096107169985771\n",
      "iteration 22, dc_loss: 2.416680097579956, tv_loss: 0.01294398307800293\n",
      "iteration 23, dc_loss: 2.4029743671417236, tv_loss: 0.012716146185994148\n",
      "iteration 24, dc_loss: 2.3892459869384766, tv_loss: 0.012405584566295147\n",
      "iteration 25, dc_loss: 2.375737190246582, tv_loss: 0.012006387114524841\n",
      "iteration 26, dc_loss: 2.3624041080474854, tv_loss: 0.011509895324707031\n",
      "iteration 27, dc_loss: 2.349217414855957, tv_loss: 0.010896659456193447\n",
      "iteration 28, dc_loss: 2.335984706878662, tv_loss: 0.010243390686810017\n",
      "iteration 29, dc_loss: 2.322805643081665, tv_loss: 0.00960985291749239\n",
      "iteration 30, dc_loss: 2.310023307800293, tv_loss: 0.008957654237747192\n",
      "iteration 31, dc_loss: 2.297494888305664, tv_loss: 0.008412018418312073\n",
      "iteration 32, dc_loss: 2.2850284576416016, tv_loss: 0.008093656040728092\n",
      "iteration 33, dc_loss: 2.2726898193359375, tv_loss: 0.00791199505329132\n",
      "iteration 34, dc_loss: 2.260397434234619, tv_loss: 0.007859411649405956\n",
      "iteration 35, dc_loss: 2.2481181621551514, tv_loss: 0.007922252640128136\n",
      "iteration 36, dc_loss: 2.2359657287597656, tv_loss: 0.008051950484514236\n",
      "iteration 37, dc_loss: 2.224062204360962, tv_loss: 0.008190590888261795\n",
      "iteration 38, dc_loss: 2.2122702598571777, tv_loss: 0.008287792094051838\n",
      "iteration 39, dc_loss: 2.2004740238189697, tv_loss: 0.00834382139146328\n",
      "iteration 40, dc_loss: 2.1888108253479004, tv_loss: 0.008338943123817444\n",
      "iteration 41, dc_loss: 2.1773428916931152, tv_loss: 0.008296100422739983\n",
      "iteration 42, dc_loss: 2.1660444736480713, tv_loss: 0.00821660552173853\n",
      "iteration 43, dc_loss: 2.154773473739624, tv_loss: 0.00823334138840437\n",
      "iteration 44, dc_loss: 2.1438395977020264, tv_loss: 0.008269736543297768\n",
      "iteration 45, dc_loss: 2.1333577632904053, tv_loss: 0.008472862653434277\n",
      "iteration 46, dc_loss: 2.123004913330078, tv_loss: 0.00848525483161211\n",
      "iteration 47, dc_loss: 2.1118829250335693, tv_loss: 0.008732680231332779\n",
      "iteration 48, dc_loss: 2.0996532440185547, tv_loss: 0.008785668760538101\n",
      "iteration 49, dc_loss: 2.089649200439453, tv_loss: 0.008844796568155289\n",
      "iteration 50, dc_loss: 2.0791471004486084, tv_loss: 0.009050429798662663\n",
      "iteration 51, dc_loss: 2.067382335662842, tv_loss: 0.009116879664361477\n",
      "iteration 52, dc_loss: 2.0577640533447266, tv_loss: 0.009165884926915169\n",
      "iteration 53, dc_loss: 2.046903610229492, tv_loss: 0.00937496405094862\n",
      "iteration 54, dc_loss: 2.035900831222534, tv_loss: 0.009462734684348106\n",
      "iteration 55, dc_loss: 2.0263922214508057, tv_loss: 0.009488099254667759\n",
      "iteration 56, dc_loss: 2.0153050422668457, tv_loss: 0.009684493765234947\n",
      "iteration 57, dc_loss: 2.005227565765381, tv_loss: 0.009802415035665035\n",
      "iteration 58, dc_loss: 1.9953011274337769, tv_loss: 0.009825493209064007\n",
      "iteration 59, dc_loss: 1.9847372770309448, tv_loss: 0.009973762556910515\n",
      "iteration 60, dc_loss: 1.975013017654419, tv_loss: 0.010130436159670353\n",
      "iteration 61, dc_loss: 1.96485435962677, tv_loss: 0.01018416229635477\n",
      "iteration 62, dc_loss: 1.9550451040267944, tv_loss: 0.010281053371727467\n",
      "iteration 63, dc_loss: 1.945249319076538, tv_loss: 0.01045971643179655\n",
      "iteration 64, dc_loss: 1.9352914094924927, tv_loss: 0.010546042583882809\n",
      "iteration 65, dc_loss: 1.9258517026901245, tv_loss: 0.010617777705192566\n",
      "iteration 66, dc_loss: 1.9160584211349487, tv_loss: 0.010795414447784424\n",
      "iteration 67, dc_loss: 1.9064304828643799, tv_loss: 0.010896412655711174\n",
      "iteration 68, dc_loss: 1.8971489667892456, tv_loss: 0.010954891331493855\n",
      "iteration 69, dc_loss: 1.8875272274017334, tv_loss: 0.0111141512170434\n",
      "iteration 70, dc_loss: 1.8781468868255615, tv_loss: 0.01122425775974989\n",
      "iteration 71, dc_loss: 1.8689862489700317, tv_loss: 0.011291143484413624\n",
      "iteration 72, dc_loss: 1.8595956563949585, tv_loss: 0.01144054252654314\n",
      "iteration 73, dc_loss: 1.8504080772399902, tv_loss: 0.011551748029887676\n",
      "iteration 74, dc_loss: 1.841422438621521, tv_loss: 0.011614875867962837\n",
      "iteration 75, dc_loss: 1.8322522640228271, tv_loss: 0.011759860441088676\n",
      "iteration 76, dc_loss: 1.8232160806655884, tv_loss: 0.011862709186971188\n",
      "iteration 77, dc_loss: 1.8143993616104126, tv_loss: 0.011928243562579155\n",
      "iteration 78, dc_loss: 1.8054618835449219, tv_loss: 0.012071052566170692\n",
      "iteration 79, dc_loss: 1.7965902090072632, tv_loss: 0.012150505557656288\n",
      "iteration 80, dc_loss: 1.7878895998001099, tv_loss: 0.01223580352962017\n",
      "iteration 81, dc_loss: 1.7791907787322998, tv_loss: 0.01237936969846487\n",
      "iteration 82, dc_loss: 1.7705156803131104, tv_loss: 0.012450763955712318\n",
      "iteration 83, dc_loss: 1.761912226676941, tv_loss: 0.012548480182886124\n",
      "iteration 84, dc_loss: 1.753427267074585, tv_loss: 0.01268069725483656\n",
      "iteration 85, dc_loss: 1.7451261281967163, tv_loss: 0.012722926214337349\n",
      "iteration 86, dc_loss: 1.7368515729904175, tv_loss: 0.012901117093861103\n",
      "iteration 87, dc_loss: 1.7290171384811401, tv_loss: 0.012989172711968422\n",
      "iteration 88, dc_loss: 1.7211631536483765, tv_loss: 0.013131284154951572\n",
      "iteration 89, dc_loss: 1.7129143476486206, tv_loss: 0.013196885585784912\n",
      "iteration 90, dc_loss: 1.7037264108657837, tv_loss: 0.013228976167738438\n",
      "iteration 91, dc_loss: 1.6953296661376953, tv_loss: 0.013324705883860588\n",
      "iteration 92, dc_loss: 1.6878312826156616, tv_loss: 0.013447046279907227\n",
      "iteration 93, dc_loss: 1.6796656847000122, tv_loss: 0.013571166433393955\n",
      "iteration 94, dc_loss: 1.6711732149124146, tv_loss: 0.013629264198243618\n",
      "iteration 95, dc_loss: 1.663268804550171, tv_loss: 0.013665741309523582\n",
      "iteration 96, dc_loss: 1.6556081771850586, tv_loss: 0.013785448856651783\n",
      "iteration 97, dc_loss: 1.6476471424102783, tv_loss: 0.013949080370366573\n",
      "iteration 98, dc_loss: 1.6395946741104126, tv_loss: 0.013975749723613262\n",
      "iteration 99, dc_loss: 1.6319093704223633, tv_loss: 0.014026706106960773\n",
      "iteration 100, dc_loss: 1.6243021488189697, tv_loss: 0.01409098133444786\n",
      "iteration 101, dc_loss: 1.6164470911026, tv_loss: 0.014209823682904243\n",
      "iteration 102, dc_loss: 1.6087734699249268, tv_loss: 0.014372486621141434\n",
      "iteration 103, dc_loss: 1.601331114768982, tv_loss: 0.014376560226082802\n",
      "iteration 104, dc_loss: 1.5937185287475586, tv_loss: 0.01446474064141512\n",
      "iteration 105, dc_loss: 1.5861021280288696, tv_loss: 0.014516392722725868\n",
      "iteration 106, dc_loss: 1.5786634683609009, tv_loss: 0.014606071636080742\n",
      "iteration 107, dc_loss: 1.5713368654251099, tv_loss: 0.014726251363754272\n",
      "iteration 108, dc_loss: 1.5640164613723755, tv_loss: 0.014834253117442131\n",
      "iteration 109, dc_loss: 1.556636929512024, tv_loss: 0.01490944717079401\n",
      "iteration 110, dc_loss: 1.5493502616882324, tv_loss: 0.01496909186244011\n",
      "iteration 111, dc_loss: 1.5420668125152588, tv_loss: 0.015050527639687061\n",
      "iteration 112, dc_loss: 1.5348386764526367, tv_loss: 0.015135681256651878\n",
      "iteration 113, dc_loss: 1.5277137756347656, tv_loss: 0.01517118327319622\n",
      "iteration 114, dc_loss: 1.5205552577972412, tv_loss: 0.015306341461837292\n",
      "iteration 115, dc_loss: 1.513593077659607, tv_loss: 0.015317876823246479\n",
      "iteration 116, dc_loss: 1.506637454032898, tv_loss: 0.015470207668840885\n",
      "iteration 117, dc_loss: 1.4998717308044434, tv_loss: 0.015478021465241909\n",
      "iteration 118, dc_loss: 1.493077278137207, tv_loss: 0.01563202776014805\n",
      "iteration 119, dc_loss: 1.4861946105957031, tv_loss: 0.01557956263422966\n",
      "iteration 120, dc_loss: 1.4790012836456299, tv_loss: 0.0157486405223608\n",
      "iteration 121, dc_loss: 1.4719288349151611, tv_loss: 0.01571410521864891\n",
      "iteration 122, dc_loss: 1.4649803638458252, tv_loss: 0.01580684818327427\n",
      "iteration 123, dc_loss: 1.4583052396774292, tv_loss: 0.01593363657593727\n",
      "iteration 124, dc_loss: 1.4518953561782837, tv_loss: 0.015937531366944313\n",
      "iteration 125, dc_loss: 1.4452269077301025, tv_loss: 0.016189824789762497\n",
      "iteration 126, dc_loss: 1.4384815692901611, tv_loss: 0.016079211607575417\n",
      "iteration 127, dc_loss: 1.4315686225891113, tv_loss: 0.016246160492300987\n",
      "iteration 128, dc_loss: 1.4250494241714478, tv_loss: 0.016323832795023918\n",
      "iteration 129, dc_loss: 1.418763279914856, tv_loss: 0.016313087195158005\n",
      "iteration 130, dc_loss: 1.412195086479187, tv_loss: 0.016482815146446228\n",
      "iteration 131, dc_loss: 1.405560851097107, tv_loss: 0.016427645459771156\n",
      "iteration 132, dc_loss: 1.3988749980926514, tv_loss: 0.01656241901218891\n",
      "iteration 133, dc_loss: 1.3924970626831055, tv_loss: 0.016649076715111732\n",
      "iteration 134, dc_loss: 1.386269450187683, tv_loss: 0.016636621206998825\n",
      "iteration 135, dc_loss: 1.3798669576644897, tv_loss: 0.016776910051703453\n",
      "iteration 136, dc_loss: 1.3735077381134033, tv_loss: 0.016749704256653786\n",
      "iteration 137, dc_loss: 1.3670798540115356, tv_loss: 0.016846243292093277\n",
      "iteration 138, dc_loss: 1.3607909679412842, tv_loss: 0.01694718562066555\n",
      "iteration 139, dc_loss: 1.3547632694244385, tv_loss: 0.0169923547655344\n",
      "iteration 140, dc_loss: 1.3487187623977661, tv_loss: 0.0171489417552948\n",
      "iteration 141, dc_loss: 1.3426896333694458, tv_loss: 0.017102817073464394\n",
      "iteration 142, dc_loss: 1.3362892866134644, tv_loss: 0.01724427007138729\n",
      "iteration 143, dc_loss: 1.329986810684204, tv_loss: 0.017226267606019974\n",
      "iteration 144, dc_loss: 1.3238184452056885, tv_loss: 0.01729312725365162\n",
      "iteration 145, dc_loss: 1.3178398609161377, tv_loss: 0.01737828738987446\n",
      "iteration 146, dc_loss: 1.3119834661483765, tv_loss: 0.01736351288855076\n",
      "iteration 147, dc_loss: 1.305907964706421, tv_loss: 0.017527902498841286\n",
      "iteration 148, dc_loss: 1.300013542175293, tv_loss: 0.017480256035923958\n",
      "iteration 149, dc_loss: 1.293969988822937, tv_loss: 0.017663253471255302\n",
      "iteration 150, dc_loss: 1.2881416082382202, tv_loss: 0.017678838223218918\n",
      "iteration 151, dc_loss: 1.2823742628097534, tv_loss: 0.017734145745635033\n",
      "iteration 152, dc_loss: 1.2766071557998657, tv_loss: 0.01788170449435711\n",
      "iteration 153, dc_loss: 1.271069884300232, tv_loss: 0.017746469005942345\n",
      "iteration 154, dc_loss: 1.2650636434555054, tv_loss: 0.01804102398455143\n",
      "iteration 155, dc_loss: 1.2593470811843872, tv_loss: 0.01783096045255661\n",
      "iteration 156, dc_loss: 1.2533389329910278, tv_loss: 0.018075836822390556\n",
      "iteration 157, dc_loss: 1.2479653358459473, tv_loss: 0.017996396869421005\n",
      "iteration 158, dc_loss: 1.2426369190216064, tv_loss: 0.018159111961722374\n",
      "iteration 159, dc_loss: 1.2372952699661255, tv_loss: 0.018129397183656693\n",
      "iteration 160, dc_loss: 1.2313799858093262, tv_loss: 0.0182588342577219\n",
      "iteration 161, dc_loss: 1.2254289388656616, tv_loss: 0.018174566328525543\n",
      "iteration 162, dc_loss: 1.2196028232574463, tv_loss: 0.01835872419178486\n",
      "iteration 163, dc_loss: 1.2144092321395874, tv_loss: 0.018362611532211304\n",
      "iteration 164, dc_loss: 1.2089413404464722, tv_loss: 0.018533987924456596\n",
      "iteration 165, dc_loss: 1.2031970024108887, tv_loss: 0.01855074055492878\n",
      "iteration 166, dc_loss: 1.197645664215088, tv_loss: 0.018528642132878304\n",
      "iteration 167, dc_loss: 1.1921976804733276, tv_loss: 0.01864553615450859\n",
      "iteration 168, dc_loss: 1.1869066953659058, tv_loss: 0.01859232224524021\n",
      "iteration 169, dc_loss: 1.181283950805664, tv_loss: 0.01870628260076046\n",
      "iteration 170, dc_loss: 1.1758558750152588, tv_loss: 0.018800940364599228\n",
      "iteration 171, dc_loss: 1.1707608699798584, tv_loss: 0.018825028091669083\n",
      "iteration 172, dc_loss: 1.1654473543167114, tv_loss: 0.019001491367816925\n",
      "iteration 173, dc_loss: 1.1601643562316895, tv_loss: 0.01887143775820732\n",
      "iteration 174, dc_loss: 1.1547083854675293, tv_loss: 0.01911376416683197\n",
      "iteration 175, dc_loss: 1.1499207019805908, tv_loss: 0.01889253966510296\n",
      "iteration 176, dc_loss: 1.1446435451507568, tv_loss: 0.019239110872149467\n",
      "iteration 177, dc_loss: 1.1398001909255981, tv_loss: 0.018932567909359932\n",
      "iteration 178, dc_loss: 1.1342813968658447, tv_loss: 0.019418707117438316\n",
      "iteration 179, dc_loss: 1.129454255104065, tv_loss: 0.01903412491083145\n",
      "iteration 180, dc_loss: 1.1238335371017456, tv_loss: 0.019450930878520012\n",
      "iteration 181, dc_loss: 1.1186896562576294, tv_loss: 0.019311299547553062\n",
      "iteration 182, dc_loss: 1.113705039024353, tv_loss: 0.019360896199941635\n",
      "iteration 183, dc_loss: 1.1086939573287964, tv_loss: 0.019597094506025314\n",
      "iteration 184, dc_loss: 1.104113221168518, tv_loss: 0.01926274597644806\n",
      "iteration 185, dc_loss: 1.098718285560608, tv_loss: 0.019837135449051857\n",
      "iteration 186, dc_loss: 1.0937986373901367, tv_loss: 0.019443631172180176\n",
      "iteration 187, dc_loss: 1.0889545679092407, tv_loss: 0.019626230001449585\n",
      "iteration 188, dc_loss: 1.0842393636703491, tv_loss: 0.019904527813196182\n",
      "iteration 189, dc_loss: 1.0800516605377197, tv_loss: 0.019700713455677032\n",
      "iteration 190, dc_loss: 1.0748624801635742, tv_loss: 0.019872086122632027\n",
      "iteration 191, dc_loss: 1.0696039199829102, tv_loss: 0.019833773374557495\n",
      "iteration 192, dc_loss: 1.0645533800125122, tv_loss: 0.019789399579167366\n",
      "iteration 193, dc_loss: 1.0595760345458984, tv_loss: 0.02007514424622059\n",
      "iteration 194, dc_loss: 1.0553810596466064, tv_loss: 0.01982271485030651\n",
      "iteration 195, dc_loss: 1.0503482818603516, tv_loss: 0.020212575793266296\n",
      "iteration 196, dc_loss: 1.0456211566925049, tv_loss: 0.020006608217954636\n",
      "iteration 197, dc_loss: 1.040575385093689, tv_loss: 0.020230207592248917\n",
      "iteration 198, dc_loss: 1.0359596014022827, tv_loss: 0.020259983837604523\n",
      "iteration 199, dc_loss: 1.0314241647720337, tv_loss: 0.020178675651550293\n",
      "iteration 200, dc_loss: 1.0264737606048584, tv_loss: 0.020352108404040337\n",
      "iteration 201, dc_loss: 1.0219793319702148, tv_loss: 0.02024342492222786\n",
      "iteration 202, dc_loss: 1.0172005891799927, tv_loss: 0.020461678504943848\n",
      "iteration 203, dc_loss: 1.0126901865005493, tv_loss: 0.020475678145885468\n",
      "iteration 204, dc_loss: 1.0080795288085938, tv_loss: 0.020462164655327797\n",
      "iteration 205, dc_loss: 1.003409743309021, tv_loss: 0.020621873438358307\n",
      "iteration 206, dc_loss: 0.9990411400794983, tv_loss: 0.020524829626083374\n",
      "iteration 207, dc_loss: 0.9945785999298096, tv_loss: 0.020524971187114716\n",
      "iteration 208, dc_loss: 0.9899995923042297, tv_loss: 0.020838944241404533\n",
      "iteration 209, dc_loss: 0.9863177537918091, tv_loss: 0.02046825923025608\n",
      "iteration 210, dc_loss: 0.9821609258651733, tv_loss: 0.02117243781685829\n",
      "iteration 211, dc_loss: 0.9783092141151428, tv_loss: 0.020487105473876\n",
      "iteration 212, dc_loss: 0.9726085066795349, tv_loss: 0.020940642803907394\n",
      "iteration 213, dc_loss: 0.9677607417106628, tv_loss: 0.020939989015460014\n",
      "iteration 214, dc_loss: 0.9643115401268005, tv_loss: 0.020659644156694412\n",
      "iteration 215, dc_loss: 0.9597682952880859, tv_loss: 0.021276457235217094\n",
      "iteration 216, dc_loss: 0.955553412437439, tv_loss: 0.020937565714120865\n",
      "iteration 217, dc_loss: 0.9510682225227356, tv_loss: 0.021168598905205727\n",
      "iteration 218, dc_loss: 0.9466021656990051, tv_loss: 0.021216195076704025\n",
      "iteration 219, dc_loss: 0.942500650882721, tv_loss: 0.020964624360203743\n",
      "iteration 220, dc_loss: 0.9379382133483887, tv_loss: 0.021327156573534012\n",
      "iteration 221, dc_loss: 0.9339138269424438, tv_loss: 0.02123751863837242\n",
      "iteration 222, dc_loss: 0.9294331073760986, tv_loss: 0.021382581442594528\n",
      "iteration 223, dc_loss: 0.9250596761703491, tv_loss: 0.02142328768968582\n",
      "iteration 224, dc_loss: 0.9209933876991272, tv_loss: 0.02131270430982113\n",
      "iteration 225, dc_loss: 0.9165456295013428, tv_loss: 0.0214979350566864\n",
      "iteration 226, dc_loss: 0.9125190377235413, tv_loss: 0.021405044943094254\n",
      "iteration 227, dc_loss: 0.9081652760505676, tv_loss: 0.021560225635766983\n",
      "iteration 228, dc_loss: 0.9040466547012329, tv_loss: 0.021610485389828682\n",
      "iteration 229, dc_loss: 0.8999132513999939, tv_loss: 0.02160518802702427\n",
      "iteration 230, dc_loss: 0.8957958817481995, tv_loss: 0.0217154361307621\n",
      "iteration 231, dc_loss: 0.8917657136917114, tv_loss: 0.021707359701395035\n",
      "iteration 232, dc_loss: 0.8877012133598328, tv_loss: 0.021724721416831017\n",
      "iteration 233, dc_loss: 0.8836019039154053, tv_loss: 0.021802889183163643\n",
      "iteration 234, dc_loss: 0.8796404600143433, tv_loss: 0.021829454228281975\n",
      "iteration 235, dc_loss: 0.8760252594947815, tv_loss: 0.0218926090747118\n",
      "iteration 236, dc_loss: 0.8725520968437195, tv_loss: 0.02186562307178974\n",
      "iteration 237, dc_loss: 0.8695732951164246, tv_loss: 0.022059351205825806\n",
      "iteration 238, dc_loss: 0.8657442927360535, tv_loss: 0.02173496037721634\n",
      "iteration 239, dc_loss: 0.8611003756523132, tv_loss: 0.022161392495036125\n",
      "iteration 240, dc_loss: 0.8561803698539734, tv_loss: 0.022017421200871468\n",
      "iteration 241, dc_loss: 0.8524070382118225, tv_loss: 0.0221486184746027\n",
      "iteration 242, dc_loss: 0.8490937352180481, tv_loss: 0.0222761407494545\n",
      "iteration 243, dc_loss: 0.8452295064926147, tv_loss: 0.021954411640763283\n",
      "iteration 244, dc_loss: 0.8405786156654358, tv_loss: 0.022376924753189087\n",
      "iteration 245, dc_loss: 0.8366492390632629, tv_loss: 0.022499145939946175\n",
      "iteration 246, dc_loss: 0.83318030834198, tv_loss: 0.022145751863718033\n",
      "iteration 247, dc_loss: 0.8292444944381714, tv_loss: 0.022473523393273354\n",
      "iteration 248, dc_loss: 0.825433075428009, tv_loss: 0.022617287933826447\n",
      "iteration 249, dc_loss: 0.82146817445755, tv_loss: 0.022384177893400192\n",
      "iteration 250, dc_loss: 0.8176572918891907, tv_loss: 0.022391846403479576\n",
      "iteration 251, dc_loss: 0.81401127576828, tv_loss: 0.022582629695534706\n",
      "iteration 252, dc_loss: 0.8104547262191772, tv_loss: 0.0226883627474308\n",
      "iteration 253, dc_loss: 0.8066760897636414, tv_loss: 0.02254670485854149\n",
      "iteration 254, dc_loss: 0.8026877045631409, tv_loss: 0.022572720423340797\n",
      "iteration 255, dc_loss: 0.7990140318870544, tv_loss: 0.022552968934178352\n",
      "iteration 256, dc_loss: 0.7954142093658447, tv_loss: 0.022724127396941185\n",
      "iteration 257, dc_loss: 0.7920157313346863, tv_loss: 0.022795962169766426\n",
      "iteration 258, dc_loss: 0.7884217500686646, tv_loss: 0.0229549091309309\n",
      "iteration 259, dc_loss: 0.785142719745636, tv_loss: 0.022748567163944244\n",
      "iteration 260, dc_loss: 0.7813911437988281, tv_loss: 0.023045865818858147\n",
      "iteration 261, dc_loss: 0.7783252596855164, tv_loss: 0.022624721750617027\n",
      "iteration 262, dc_loss: 0.7745985984802246, tv_loss: 0.023405082523822784\n",
      "iteration 263, dc_loss: 0.7717475295066833, tv_loss: 0.0226618479937315\n",
      "iteration 264, dc_loss: 0.7675761580467224, tv_loss: 0.023371653631329536\n",
      "iteration 265, dc_loss: 0.763891339302063, tv_loss: 0.022946538403630257\n",
      "iteration 266, dc_loss: 0.7603880167007446, tv_loss: 0.023061243817210197\n",
      "iteration 267, dc_loss: 0.7568688988685608, tv_loss: 0.02339118719100952\n",
      "iteration 268, dc_loss: 0.7543268203735352, tv_loss: 0.022877097129821777\n",
      "iteration 269, dc_loss: 0.7500696182250977, tv_loss: 0.023561954498291016\n",
      "iteration 270, dc_loss: 0.7465542554855347, tv_loss: 0.023129824548959732\n",
      "iteration 271, dc_loss: 0.7432419657707214, tv_loss: 0.023226022720336914\n",
      "iteration 272, dc_loss: 0.7400918006896973, tv_loss: 0.02362397313117981\n",
      "iteration 273, dc_loss: 0.7370187044143677, tv_loss: 0.02328386716544628\n",
      "iteration 274, dc_loss: 0.732950747013092, tv_loss: 0.023493941873311996\n",
      "iteration 275, dc_loss: 0.7293630838394165, tv_loss: 0.023481333628296852\n",
      "iteration 276, dc_loss: 0.7265354990959167, tv_loss: 0.023283587768673897\n",
      "iteration 277, dc_loss: 0.7229023575782776, tv_loss: 0.023887034505605698\n",
      "iteration 278, dc_loss: 0.7197321057319641, tv_loss: 0.02340824156999588\n",
      "iteration 279, dc_loss: 0.7161083221435547, tv_loss: 0.023512903600931168\n",
      "iteration 280, dc_loss: 0.7128522396087646, tv_loss: 0.023769840598106384\n",
      "iteration 281, dc_loss: 0.7101062536239624, tv_loss: 0.023635417222976685\n",
      "iteration 282, dc_loss: 0.7066723108291626, tv_loss: 0.023900866508483887\n",
      "iteration 283, dc_loss: 0.7035946249961853, tv_loss: 0.023616604506969452\n",
      "iteration 284, dc_loss: 0.7003887295722961, tv_loss: 0.02386322245001793\n",
      "iteration 285, dc_loss: 0.6971526741981506, tv_loss: 0.023907482624053955\n",
      "iteration 286, dc_loss: 0.6941274404525757, tv_loss: 0.023767322301864624\n",
      "iteration 287, dc_loss: 0.6906416416168213, tv_loss: 0.02385735884308815\n",
      "iteration 288, dc_loss: 0.6872723698616028, tv_loss: 0.023925146088004112\n",
      "iteration 289, dc_loss: 0.6841309070587158, tv_loss: 0.024036629125475883\n",
      "iteration 290, dc_loss: 0.6810309290885925, tv_loss: 0.02413419634103775\n",
      "iteration 291, dc_loss: 0.6784105896949768, tv_loss: 0.023936143144965172\n",
      "iteration 292, dc_loss: 0.6749100685119629, tv_loss: 0.024214990437030792\n",
      "iteration 293, dc_loss: 0.6718581318855286, tv_loss: 0.024139536544680595\n",
      "iteration 294, dc_loss: 0.6687437891960144, tv_loss: 0.024053702130913734\n",
      "iteration 295, dc_loss: 0.6656522750854492, tv_loss: 0.024343807250261307\n",
      "iteration 296, dc_loss: 0.6635684370994568, tv_loss: 0.023878008127212524\n",
      "iteration 297, dc_loss: 0.6609351634979248, tv_loss: 0.024753907695412636\n",
      "iteration 298, dc_loss: 0.6578125357627869, tv_loss: 0.023903632536530495\n",
      "iteration 299, dc_loss: 0.6534397602081299, tv_loss: 0.024307506158947945\n",
      "iteration 300, dc_loss: 0.650357186794281, tv_loss: 0.024360839277505875\n",
      "iteration 301, dc_loss: 0.6479610204696655, tv_loss: 0.024122487753629684\n",
      "iteration 302, dc_loss: 0.6444768905639648, tv_loss: 0.02476530522108078\n",
      "iteration 303, dc_loss: 0.6415322422981262, tv_loss: 0.024358583614230156\n",
      "iteration 304, dc_loss: 0.6385924220085144, tv_loss: 0.02434227615594864\n",
      "iteration 305, dc_loss: 0.6352585554122925, tv_loss: 0.02452888898551464\n",
      "iteration 306, dc_loss: 0.6323694586753845, tv_loss: 0.0244168508797884\n",
      "iteration 307, dc_loss: 0.6292970776557922, tv_loss: 0.024696385487914085\n",
      "iteration 308, dc_loss: 0.6262969374656677, tv_loss: 0.02460067719221115\n",
      "iteration 309, dc_loss: 0.6232789754867554, tv_loss: 0.024575700983405113\n",
      "iteration 310, dc_loss: 0.6204749345779419, tv_loss: 0.024678286164999008\n",
      "iteration 311, dc_loss: 0.617782473564148, tv_loss: 0.02470284327864647\n",
      "iteration 312, dc_loss: 0.6146829724311829, tv_loss: 0.024867579340934753\n",
      "iteration 313, dc_loss: 0.6119000315666199, tv_loss: 0.024690553545951843\n",
      "iteration 314, dc_loss: 0.6089415550231934, tv_loss: 0.024805130437016487\n",
      "iteration 315, dc_loss: 0.6059867739677429, tv_loss: 0.024940824136137962\n",
      "iteration 316, dc_loss: 0.6033902764320374, tv_loss: 0.02472785860300064\n",
      "iteration 317, dc_loss: 0.6004058122634888, tv_loss: 0.02504967339336872\n",
      "iteration 318, dc_loss: 0.5980126261711121, tv_loss: 0.024776889011263847\n",
      "iteration 319, dc_loss: 0.5950317978858948, tv_loss: 0.025165608152747154\n",
      "iteration 320, dc_loss: 0.5926727056503296, tv_loss: 0.02483178861439228\n",
      "iteration 321, dc_loss: 0.590155839920044, tv_loss: 0.025094086304306984\n",
      "iteration 322, dc_loss: 0.5878551006317139, tv_loss: 0.02493639849126339\n",
      "iteration 323, dc_loss: 0.5853167772293091, tv_loss: 0.025151461362838745\n",
      "iteration 324, dc_loss: 0.5820910334587097, tv_loss: 0.02498895861208439\n",
      "iteration 325, dc_loss: 0.5787456035614014, tv_loss: 0.025099314749240875\n",
      "iteration 326, dc_loss: 0.575839638710022, tv_loss: 0.025086456909775734\n",
      "iteration 327, dc_loss: 0.5734903216362, tv_loss: 0.025299204513430595\n",
      "iteration 328, dc_loss: 0.5714715123176575, tv_loss: 0.025458132848143578\n",
      "iteration 329, dc_loss: 0.5690398812294006, tv_loss: 0.025157136842608452\n",
      "iteration 330, dc_loss: 0.5660445690155029, tv_loss: 0.025529077276587486\n",
      "iteration 331, dc_loss: 0.5632784366607666, tv_loss: 0.02518903650343418\n",
      "iteration 332, dc_loss: 0.5603606700897217, tv_loss: 0.025618575513362885\n",
      "iteration 333, dc_loss: 0.5583820939064026, tv_loss: 0.02530854195356369\n",
      "iteration 334, dc_loss: 0.555670440196991, tv_loss: 0.025604719296097755\n",
      "iteration 335, dc_loss: 0.5531193614006042, tv_loss: 0.025419961661100388\n",
      "iteration 336, dc_loss: 0.5500421524047852, tv_loss: 0.02550705336034298\n",
      "iteration 337, dc_loss: 0.5473206043243408, tv_loss: 0.025678478181362152\n",
      "iteration 338, dc_loss: 0.5451880693435669, tv_loss: 0.025466393679380417\n",
      "iteration 339, dc_loss: 0.5427624583244324, tv_loss: 0.025756875053048134\n",
      "iteration 340, dc_loss: 0.5405128002166748, tv_loss: 0.025582091882824898\n",
      "iteration 341, dc_loss: 0.5378016233444214, tv_loss: 0.025646356865763664\n",
      "iteration 342, dc_loss: 0.5351638793945312, tv_loss: 0.025923388078808784\n",
      "iteration 343, dc_loss: 0.5333424210548401, tv_loss: 0.02550438977777958\n",
      "iteration 344, dc_loss: 0.5310317873954773, tv_loss: 0.02618703991174698\n",
      "iteration 345, dc_loss: 0.5291391015052795, tv_loss: 0.025432202965021133\n",
      "iteration 346, dc_loss: 0.5257944464683533, tv_loss: 0.026176802814006805\n",
      "iteration 347, dc_loss: 0.5230550765991211, tv_loss: 0.025748493149876595\n",
      "iteration 348, dc_loss: 0.5205837488174438, tv_loss: 0.025836734101176262\n",
      "iteration 349, dc_loss: 0.518242597579956, tv_loss: 0.02613675594329834\n",
      "iteration 350, dc_loss: 0.5164346098899841, tv_loss: 0.02564512938261032\n",
      "iteration 351, dc_loss: 0.513381838798523, tv_loss: 0.026199933141469955\n",
      "iteration 352, dc_loss: 0.5111005306243896, tv_loss: 0.02588728815317154\n",
      "iteration 353, dc_loss: 0.508933424949646, tv_loss: 0.02607845887541771\n",
      "iteration 354, dc_loss: 0.5068420171737671, tv_loss: 0.02617744915187359\n",
      "iteration 355, dc_loss: 0.5049360990524292, tv_loss: 0.026020251214504242\n",
      "iteration 356, dc_loss: 0.5022169947624207, tv_loss: 0.026095755398273468\n",
      "iteration 357, dc_loss: 0.49968940019607544, tv_loss: 0.02624444104731083\n",
      "iteration 358, dc_loss: 0.497843474149704, tv_loss: 0.02590211294591427\n",
      "iteration 359, dc_loss: 0.49554550647735596, tv_loss: 0.02659350261092186\n",
      "iteration 360, dc_loss: 0.49354737997055054, tv_loss: 0.025906462222337723\n",
      "iteration 361, dc_loss: 0.49037298560142517, tv_loss: 0.026420313864946365\n",
      "iteration 362, dc_loss: 0.48800310492515564, tv_loss: 0.026213211938738823\n",
      "iteration 363, dc_loss: 0.4860284924507141, tv_loss: 0.026229824870824814\n",
      "iteration 364, dc_loss: 0.4838186800479889, tv_loss: 0.026654433459043503\n",
      "iteration 365, dc_loss: 0.4820181727409363, tv_loss: 0.026187142357230186\n",
      "iteration 366, dc_loss: 0.47939789295196533, tv_loss: 0.02660670131444931\n",
      "iteration 367, dc_loss: 0.47751739621162415, tv_loss: 0.026326294988393784\n",
      "iteration 368, dc_loss: 0.47584471106529236, tv_loss: 0.026557309553027153\n",
      "iteration 369, dc_loss: 0.47373583912849426, tv_loss: 0.026476403698325157\n",
      "iteration 370, dc_loss: 0.47143757343292236, tv_loss: 0.026537282392382622\n",
      "iteration 371, dc_loss: 0.468514621257782, tv_loss: 0.026446327567100525\n",
      "iteration 372, dc_loss: 0.46598467230796814, tv_loss: 0.026627976447343826\n",
      "iteration 373, dc_loss: 0.4643213450908661, tv_loss: 0.026571830734610558\n",
      "iteration 374, dc_loss: 0.46233633160591125, tv_loss: 0.02677781693637371\n",
      "iteration 375, dc_loss: 0.4602959156036377, tv_loss: 0.026612011715769768\n",
      "iteration 376, dc_loss: 0.457795649766922, tv_loss: 0.02660667896270752\n",
      "iteration 377, dc_loss: 0.4554423987865448, tv_loss: 0.02683515101671219\n",
      "iteration 378, dc_loss: 0.4538843333721161, tv_loss: 0.026591649278998375\n",
      "iteration 379, dc_loss: 0.45169201493263245, tv_loss: 0.0270380899310112\n",
      "iteration 380, dc_loss: 0.44982701539993286, tv_loss: 0.026603180915117264\n",
      "iteration 381, dc_loss: 0.4471570551395416, tv_loss: 0.026930565014481544\n",
      "iteration 382, dc_loss: 0.44519472122192383, tv_loss: 0.02678041346371174\n",
      "iteration 383, dc_loss: 0.4432680308818817, tv_loss: 0.02692410908639431\n",
      "iteration 384, dc_loss: 0.4413905143737793, tv_loss: 0.02696795016527176\n",
      "iteration 385, dc_loss: 0.43950873613357544, tv_loss: 0.02691272832453251\n",
      "iteration 386, dc_loss: 0.4372421205043793, tv_loss: 0.026939351111650467\n",
      "iteration 387, dc_loss: 0.4350537359714508, tv_loss: 0.02705416828393936\n",
      "iteration 388, dc_loss: 0.4333685636520386, tv_loss: 0.026903385296463966\n",
      "iteration 389, dc_loss: 0.43144339323043823, tv_loss: 0.027309060096740723\n",
      "iteration 390, dc_loss: 0.4303644597530365, tv_loss: 0.02672724425792694\n",
      "iteration 391, dc_loss: 0.4284264147281647, tv_loss: 0.027478300034999847\n",
      "iteration 392, dc_loss: 0.42719921469688416, tv_loss: 0.02667699009180069\n",
      "iteration 393, dc_loss: 0.4256041347980499, tv_loss: 0.027490846812725067\n",
      "iteration 394, dc_loss: 0.4230402112007141, tv_loss: 0.02716892585158348\n",
      "iteration 395, dc_loss: 0.420369416475296, tv_loss: 0.027192311361432076\n",
      "iteration 396, dc_loss: 0.4177823066711426, tv_loss: 0.027400817722082138\n",
      "iteration 397, dc_loss: 0.4166609048843384, tv_loss: 0.026910893619060516\n",
      "iteration 398, dc_loss: 0.4148487448692322, tv_loss: 0.02748972550034523\n",
      "iteration 399, dc_loss: 0.4128175675868988, tv_loss: 0.02724536508321762\n",
      "iteration 400, dc_loss: 0.41046836972236633, tv_loss: 0.02736085280776024\n",
      "iteration 401, dc_loss: 0.40815725922584534, tv_loss: 0.0275247972458601\n",
      "iteration 402, dc_loss: 0.40696585178375244, tv_loss: 0.027216214686632156\n",
      "iteration 403, dc_loss: 0.40534311532974243, tv_loss: 0.02729775756597519\n",
      "iteration 404, dc_loss: 0.4036804139614105, tv_loss: 0.02758995071053505\n",
      "iteration 405, dc_loss: 0.40239429473876953, tv_loss: 0.027292169630527496\n",
      "iteration 406, dc_loss: 0.4009387493133545, tv_loss: 0.02733500674366951\n",
      "iteration 407, dc_loss: 0.3992673456668854, tv_loss: 0.027633972465991974\n",
      "iteration 408, dc_loss: 0.3978952169418335, tv_loss: 0.027449967339634895\n",
      "iteration 409, dc_loss: 0.3966016173362732, tv_loss: 0.02729862369596958\n",
      "iteration 410, dc_loss: 0.39491215348243713, tv_loss: 0.02754494547843933\n",
      "iteration 411, dc_loss: 0.39355596899986267, tv_loss: 0.027640849351882935\n",
      "iteration 412, dc_loss: 0.39224883913993835, tv_loss: 0.02746874839067459\n",
      "iteration 413, dc_loss: 0.3905852735042572, tv_loss: 0.02756105177104473\n",
      "iteration 414, dc_loss: 0.3892524242401123, tv_loss: 0.027547648176550865\n",
      "iteration 415, dc_loss: 0.38794028759002686, tv_loss: 0.027466826140880585\n",
      "iteration 416, dc_loss: 0.3863501250743866, tv_loss: 0.027583038434386253\n",
      "iteration 417, dc_loss: 0.3849990665912628, tv_loss: 0.02761066146194935\n",
      "iteration 418, dc_loss: 0.3837181329727173, tv_loss: 0.027617715299129486\n",
      "iteration 419, dc_loss: 0.38223376870155334, tv_loss: 0.027814390137791634\n",
      "iteration 420, dc_loss: 0.3807982802391052, tv_loss: 0.02770186774432659\n",
      "iteration 421, dc_loss: 0.3794916868209839, tv_loss: 0.027624381706118584\n",
      "iteration 422, dc_loss: 0.3780770003795624, tv_loss: 0.027679376304149628\n",
      "iteration 423, dc_loss: 0.3766475319862366, tv_loss: 0.02782837301492691\n",
      "iteration 424, dc_loss: 0.3753853142261505, tv_loss: 0.027863606810569763\n",
      "iteration 425, dc_loss: 0.3739962577819824, tv_loss: 0.027765564620494843\n",
      "iteration 426, dc_loss: 0.37257450819015503, tv_loss: 0.02781844325363636\n",
      "iteration 427, dc_loss: 0.3713354766368866, tv_loss: 0.027711639180779457\n",
      "iteration 428, dc_loss: 0.3698936998844147, tv_loss: 0.02781200036406517\n",
      "iteration 429, dc_loss: 0.36856478452682495, tv_loss: 0.027856046333909035\n",
      "iteration 430, dc_loss: 0.3673214912414551, tv_loss: 0.02798089198768139\n",
      "iteration 431, dc_loss: 0.3659154772758484, tv_loss: 0.028019798919558525\n",
      "iteration 432, dc_loss: 0.36467015743255615, tv_loss: 0.027906175702810287\n",
      "iteration 433, dc_loss: 0.3632740080356598, tv_loss: 0.02797224000096321\n",
      "iteration 434, dc_loss: 0.3619650900363922, tv_loss: 0.02805306203663349\n",
      "iteration 435, dc_loss: 0.3606804609298706, tv_loss: 0.028053728863596916\n",
      "iteration 436, dc_loss: 0.3593606948852539, tv_loss: 0.028047936037182808\n",
      "iteration 437, dc_loss: 0.35806387662887573, tv_loss: 0.028058834373950958\n",
      "iteration 438, dc_loss: 0.3567827641963959, tv_loss: 0.02808854728937149\n",
      "iteration 439, dc_loss: 0.3554905951023102, tv_loss: 0.028147317469120026\n",
      "iteration 440, dc_loss: 0.35419753193855286, tv_loss: 0.028124086558818817\n",
      "iteration 441, dc_loss: 0.3529091477394104, tv_loss: 0.028156988322734833\n",
      "iteration 442, dc_loss: 0.35168033838272095, tv_loss: 0.028166618198156357\n",
      "iteration 443, dc_loss: 0.3503774404525757, tv_loss: 0.028234517201781273\n",
      "iteration 444, dc_loss: 0.34918251633644104, tv_loss: 0.028179561719298363\n",
      "iteration 445, dc_loss: 0.3478499948978424, tv_loss: 0.02826840430498123\n",
      "iteration 446, dc_loss: 0.34672608971595764, tv_loss: 0.028216244652867317\n",
      "iteration 447, dc_loss: 0.3453744649887085, tv_loss: 0.028366953134536743\n",
      "iteration 448, dc_loss: 0.3442869484424591, tv_loss: 0.028195010498166084\n",
      "iteration 449, dc_loss: 0.3429199755191803, tv_loss: 0.028385795652866364\n",
      "iteration 450, dc_loss: 0.34188559651374817, tv_loss: 0.028282837942242622\n",
      "iteration 451, dc_loss: 0.34050071239471436, tv_loss: 0.028424793854355812\n",
      "iteration 452, dc_loss: 0.3394726514816284, tv_loss: 0.028259877115488052\n",
      "iteration 453, dc_loss: 0.3380748927593231, tv_loss: 0.028533417731523514\n",
      "iteration 454, dc_loss: 0.33708661794662476, tv_loss: 0.028294580057263374\n",
      "iteration 455, dc_loss: 0.3356361985206604, tv_loss: 0.028541825711727142\n",
      "iteration 456, dc_loss: 0.3346027731895447, tv_loss: 0.028350191190838814\n",
      "iteration 457, dc_loss: 0.33316776156425476, tv_loss: 0.02853855863213539\n",
      "iteration 458, dc_loss: 0.332061231136322, tv_loss: 0.028408732265233994\n",
      "iteration 459, dc_loss: 0.33072707056999207, tv_loss: 0.028553778305649757\n",
      "iteration 460, dc_loss: 0.32957372069358826, tv_loss: 0.028504924848675728\n",
      "iteration 461, dc_loss: 0.3283608555793762, tv_loss: 0.02851482294499874\n",
      "iteration 462, dc_loss: 0.32717567682266235, tv_loss: 0.028568711131811142\n",
      "iteration 463, dc_loss: 0.3260604739189148, tv_loss: 0.02855125069618225\n",
      "iteration 464, dc_loss: 0.3248405158519745, tv_loss: 0.02864426001906395\n",
      "iteration 465, dc_loss: 0.3238762617111206, tv_loss: 0.028537403792142868\n",
      "iteration 466, dc_loss: 0.32264286279678345, tv_loss: 0.028738100081682205\n",
      "iteration 467, dc_loss: 0.3219214677810669, tv_loss: 0.02847183309495449\n",
      "iteration 468, dc_loss: 0.3206959664821625, tv_loss: 0.028883354738354683\n",
      "iteration 469, dc_loss: 0.3200777471065521, tv_loss: 0.028438646346330643\n",
      "iteration 470, dc_loss: 0.3185959756374359, tv_loss: 0.028921548277139664\n",
      "iteration 471, dc_loss: 0.31749603152275085, tv_loss: 0.028502903878688812\n",
      "iteration 472, dc_loss: 0.3157729506492615, tv_loss: 0.02879461459815502\n",
      "iteration 473, dc_loss: 0.314549058675766, tv_loss: 0.028738021850585938\n",
      "iteration 474, dc_loss: 0.31353047490119934, tv_loss: 0.0287492536008358\n",
      "iteration 475, dc_loss: 0.312406986951828, tv_loss: 0.028924282640218735\n",
      "iteration 476, dc_loss: 0.3117309808731079, tv_loss: 0.028592590242624283\n",
      "iteration 477, dc_loss: 0.3103742003440857, tv_loss: 0.028969304636120796\n",
      "iteration 478, dc_loss: 0.3092978298664093, tv_loss: 0.02873683162033558\n",
      "iteration 479, dc_loss: 0.30783286690711975, tv_loss: 0.028936710208654404\n",
      "iteration 480, dc_loss: 0.3066767752170563, tv_loss: 0.028857892379164696\n",
      "iteration 481, dc_loss: 0.3056620657444, tv_loss: 0.028805430978536606\n",
      "iteration 482, dc_loss: 0.3045017719268799, tv_loss: 0.028987698256969452\n",
      "iteration 483, dc_loss: 0.30369046330451965, tv_loss: 0.028837015852332115\n",
      "iteration 484, dc_loss: 0.3026142716407776, tv_loss: 0.02903429977595806\n",
      "iteration 485, dc_loss: 0.3015112578868866, tv_loss: 0.028876863420009613\n",
      "iteration 486, dc_loss: 0.30024945735931396, tv_loss: 0.028976358473300934\n",
      "iteration 487, dc_loss: 0.2991344928741455, tv_loss: 0.02901763655245304\n",
      "iteration 488, dc_loss: 0.2981388568878174, tv_loss: 0.028958195820450783\n",
      "iteration 489, dc_loss: 0.2969886064529419, tv_loss: 0.02911541238427162\n",
      "iteration 490, dc_loss: 0.29623278975486755, tv_loss: 0.028909754008054733\n",
      "iteration 491, dc_loss: 0.2950444519519806, tv_loss: 0.029183894395828247\n",
      "iteration 492, dc_loss: 0.29424774646759033, tv_loss: 0.02892153151333332\n",
      "iteration 493, dc_loss: 0.2932296395301819, tv_loss: 0.029258066788315773\n",
      "iteration 494, dc_loss: 0.2921532690525055, tv_loss: 0.02902296744287014\n",
      "iteration 495, dc_loss: 0.2908811867237091, tv_loss: 0.02920779399573803\n",
      "iteration 496, dc_loss: 0.2898807227611542, tv_loss: 0.029106900095939636\n",
      "iteration 497, dc_loss: 0.2888035476207733, tv_loss: 0.029153453186154366\n",
      "iteration 498, dc_loss: 0.28777703642845154, tv_loss: 0.02922547422349453\n",
      "iteration 499, dc_loss: 0.2867280840873718, tv_loss: 0.02915489859879017\n",
      "iteration 500, dc_loss: 0.2857033610343933, tv_loss: 0.029236191883683205\n",
      "iteration 501, dc_loss: 0.28472477197647095, tv_loss: 0.029157962650060654\n",
      "iteration 502, dc_loss: 0.28351330757141113, tv_loss: 0.02930615097284317\n",
      "iteration 503, dc_loss: 0.2827470302581787, tv_loss: 0.0291510671377182\n",
      "iteration 504, dc_loss: 0.2814774215221405, tv_loss: 0.029389502480626106\n",
      "iteration 505, dc_loss: 0.28064361214637756, tv_loss: 0.02918729931116104\n",
      "iteration 506, dc_loss: 0.2794845402240753, tv_loss: 0.029360756278038025\n",
      "iteration 507, dc_loss: 0.27859994769096375, tv_loss: 0.02920910343527794\n",
      "iteration 508, dc_loss: 0.2774980962276459, tv_loss: 0.029429350048303604\n",
      "iteration 509, dc_loss: 0.27660927176475525, tv_loss: 0.029283177107572556\n",
      "iteration 510, dc_loss: 0.27564725279808044, tv_loss: 0.0293751023709774\n",
      "iteration 511, dc_loss: 0.2748073637485504, tv_loss: 0.02943240851163864\n",
      "iteration 512, dc_loss: 0.27395427227020264, tv_loss: 0.02930598147213459\n",
      "iteration 513, dc_loss: 0.2729732096195221, tv_loss: 0.0295329038053751\n",
      "iteration 514, dc_loss: 0.27213335037231445, tv_loss: 0.029338650405406952\n",
      "iteration 515, dc_loss: 0.2710568904876709, tv_loss: 0.02959972247481346\n",
      "iteration 516, dc_loss: 0.2706594169139862, tv_loss: 0.0292042288929224\n",
      "iteration 517, dc_loss: 0.2697015702724457, tv_loss: 0.029719164595007896\n",
      "iteration 518, dc_loss: 0.26925283670425415, tv_loss: 0.029286926612257957\n",
      "iteration 519, dc_loss: 0.267579585313797, tv_loss: 0.029750769957900047\n",
      "iteration 520, dc_loss: 0.2666594684123993, tv_loss: 0.029446253553032875\n",
      "iteration 521, dc_loss: 0.2658534646034241, tv_loss: 0.02951671928167343\n",
      "iteration 522, dc_loss: 0.2647072672843933, tv_loss: 0.029731061309576035\n",
      "iteration 523, dc_loss: 0.2640921175479889, tv_loss: 0.029412755742669106\n",
      "iteration 524, dc_loss: 0.26280924677848816, tv_loss: 0.02970597892999649\n",
      "iteration 525, dc_loss: 0.26192182302474976, tv_loss: 0.029491176828742027\n",
      "iteration 526, dc_loss: 0.26075270771980286, tv_loss: 0.02959422580897808\n",
      "iteration 527, dc_loss: 0.2599300444126129, tv_loss: 0.029825206845998764\n",
      "iteration 528, dc_loss: 0.25926530361175537, tv_loss: 0.02948177047073841\n",
      "iteration 529, dc_loss: 0.25785067677497864, tv_loss: 0.029784074053168297\n",
      "iteration 530, dc_loss: 0.25707611441612244, tv_loss: 0.029573528096079826\n",
      "iteration 531, dc_loss: 0.25621500611305237, tv_loss: 0.02956380322575569\n",
      "iteration 532, dc_loss: 0.25511032342910767, tv_loss: 0.029954900965094566\n",
      "iteration 533, dc_loss: 0.2544185519218445, tv_loss: 0.029573125764727592\n",
      "iteration 534, dc_loss: 0.25331804156303406, tv_loss: 0.0296761654317379\n",
      "iteration 535, dc_loss: 0.25241684913635254, tv_loss: 0.029732555150985718\n",
      "iteration 536, dc_loss: 0.25159916281700134, tv_loss: 0.029717164114117622\n",
      "iteration 537, dc_loss: 0.25057148933410645, tv_loss: 0.029897453263401985\n",
      "iteration 538, dc_loss: 0.24981410801410675, tv_loss: 0.02975461632013321\n",
      "iteration 539, dc_loss: 0.24892733991146088, tv_loss: 0.02974509820342064\n",
      "iteration 540, dc_loss: 0.2479487508535385, tv_loss: 0.029777556657791138\n",
      "iteration 541, dc_loss: 0.24711431562900543, tv_loss: 0.029783954843878746\n",
      "iteration 542, dc_loss: 0.2463001012802124, tv_loss: 0.029901983216404915\n",
      "iteration 543, dc_loss: 0.24538744986057281, tv_loss: 0.02992507629096508\n",
      "iteration 544, dc_loss: 0.24440324306488037, tv_loss: 0.0299264807254076\n",
      "iteration 545, dc_loss: 0.24373485147953033, tv_loss: 0.02970101311802864\n",
      "iteration 546, dc_loss: 0.24260316789150238, tv_loss: 0.029923902824521065\n",
      "iteration 547, dc_loss: 0.24184632301330566, tv_loss: 0.029805338010191917\n",
      "iteration 548, dc_loss: 0.2409752458333969, tv_loss: 0.029833143576979637\n",
      "iteration 549, dc_loss: 0.2400740385055542, tv_loss: 0.02988447993993759\n",
      "iteration 550, dc_loss: 0.23926006257534027, tv_loss: 0.029851509258151054\n",
      "iteration 551, dc_loss: 0.23837032914161682, tv_loss: 0.029912373051047325\n",
      "iteration 552, dc_loss: 0.23761418461799622, tv_loss: 0.029917558655142784\n",
      "iteration 553, dc_loss: 0.23673896491527557, tv_loss: 0.030203815549612045\n",
      "iteration 554, dc_loss: 0.23598404228687286, tv_loss: 0.029926398769021034\n",
      "iteration 555, dc_loss: 0.23505614697933197, tv_loss: 0.03003092296421528\n",
      "iteration 556, dc_loss: 0.234467014670372, tv_loss: 0.029917532578110695\n",
      "iteration 557, dc_loss: 0.23345385491847992, tv_loss: 0.03034001775085926\n",
      "iteration 558, dc_loss: 0.23282526433467865, tv_loss: 0.029943494126200676\n",
      "iteration 559, dc_loss: 0.23187808692455292, tv_loss: 0.030130280181765556\n",
      "iteration 560, dc_loss: 0.2311476320028305, tv_loss: 0.030338911339640617\n",
      "iteration 561, dc_loss: 0.23024922609329224, tv_loss: 0.030123017728328705\n",
      "iteration 562, dc_loss: 0.2296195924282074, tv_loss: 0.030390862375497818\n",
      "iteration 563, dc_loss: 0.2287183403968811, tv_loss: 0.03005753457546234\n",
      "iteration 564, dc_loss: 0.22791124880313873, tv_loss: 0.030564915388822556\n",
      "iteration 565, dc_loss: 0.22727608680725098, tv_loss: 0.030031578615307808\n",
      "iteration 566, dc_loss: 0.22646473348140717, tv_loss: 0.03066425770521164\n",
      "iteration 567, dc_loss: 0.22603504359722137, tv_loss: 0.030001191422343254\n",
      "iteration 568, dc_loss: 0.22523733973503113, tv_loss: 0.030949551612138748\n",
      "iteration 569, dc_loss: 0.22483029961585999, tv_loss: 0.030148526653647423\n",
      "iteration 570, dc_loss: 0.22376205027103424, tv_loss: 0.030864020809531212\n",
      "iteration 571, dc_loss: 0.2230314165353775, tv_loss: 0.030227700248360634\n",
      "iteration 572, dc_loss: 0.22179895639419556, tv_loss: 0.03072933666408062\n",
      "iteration 573, dc_loss: 0.2208181619644165, tv_loss: 0.03051934391260147\n",
      "iteration 574, dc_loss: 0.22050058841705322, tv_loss: 0.030429884791374207\n",
      "iteration 575, dc_loss: 0.2194470465183258, tv_loss: 0.030671751126646996\n",
      "iteration 576, dc_loss: 0.21916340291500092, tv_loss: 0.030457058921456337\n",
      "iteration 577, dc_loss: 0.21785202622413635, tv_loss: 0.030697716400027275\n",
      "iteration 578, dc_loss: 0.21728532016277313, tv_loss: 0.03050358220934868\n",
      "iteration 579, dc_loss: 0.2163512259721756, tv_loss: 0.03042749874293804\n",
      "iteration 580, dc_loss: 0.2156400978565216, tv_loss: 0.03080417774617672\n",
      "iteration 581, dc_loss: 0.21511414647102356, tv_loss: 0.030373556539416313\n",
      "iteration 582, dc_loss: 0.2142067849636078, tv_loss: 0.030817514285445213\n",
      "iteration 583, dc_loss: 0.2134649008512497, tv_loss: 0.03048388846218586\n",
      "iteration 584, dc_loss: 0.21265412867069244, tv_loss: 0.03067123331129551\n",
      "iteration 585, dc_loss: 0.21176351606845856, tv_loss: 0.030612323433160782\n",
      "iteration 586, dc_loss: 0.2113994061946869, tv_loss: 0.030541762709617615\n",
      "iteration 587, dc_loss: 0.21032589673995972, tv_loss: 0.0306800976395607\n",
      "iteration 588, dc_loss: 0.2098648101091385, tv_loss: 0.03069283440709114\n",
      "iteration 589, dc_loss: 0.20884273946285248, tv_loss: 0.030739232897758484\n",
      "iteration 590, dc_loss: 0.20825417339801788, tv_loss: 0.0306550320237875\n",
      "iteration 591, dc_loss: 0.2074616700410843, tv_loss: 0.03058149293065071\n",
      "iteration 592, dc_loss: 0.20673024654388428, tv_loss: 0.030811866745352745\n",
      "iteration 593, dc_loss: 0.2061329036951065, tv_loss: 0.03052249550819397\n",
      "iteration 594, dc_loss: 0.20533223450183868, tv_loss: 0.030862931162118912\n",
      "iteration 595, dc_loss: 0.20466506481170654, tv_loss: 0.030612029135227203\n",
      "iteration 596, dc_loss: 0.20387326180934906, tv_loss: 0.030845781788229942\n",
      "iteration 597, dc_loss: 0.20314548909664154, tv_loss: 0.030704163014888763\n",
      "iteration 598, dc_loss: 0.20255862176418304, tv_loss: 0.030704660341143608\n",
      "iteration 599, dc_loss: 0.20170170068740845, tv_loss: 0.030707385390996933\n",
      "iteration 600, dc_loss: 0.2011772245168686, tv_loss: 0.030804762616753578\n",
      "iteration 601, dc_loss: 0.2003353089094162, tv_loss: 0.030785979703068733\n",
      "iteration 602, dc_loss: 0.19984330236911774, tv_loss: 0.030767804011702538\n",
      "iteration 603, dc_loss: 0.19893863797187805, tv_loss: 0.030823761597275734\n",
      "iteration 604, dc_loss: 0.1983816921710968, tv_loss: 0.030823780223727226\n",
      "iteration 605, dc_loss: 0.19762517511844635, tv_loss: 0.030753737315535545\n",
      "iteration 606, dc_loss: 0.19701793789863586, tv_loss: 0.030862675979733467\n",
      "iteration 607, dc_loss: 0.19635432958602905, tv_loss: 0.03075379692018032\n",
      "iteration 608, dc_loss: 0.1957143396139145, tv_loss: 0.030981063842773438\n",
      "iteration 609, dc_loss: 0.19532427191734314, tv_loss: 0.03071797825396061\n",
      "iteration 610, dc_loss: 0.19480538368225098, tv_loss: 0.030988862738013268\n",
      "iteration 611, dc_loss: 0.194641575217247, tv_loss: 0.030643686652183533\n",
      "iteration 612, dc_loss: 0.19426703453063965, tv_loss: 0.03118249773979187\n",
      "iteration 613, dc_loss: 0.19394069910049438, tv_loss: 0.030657269060611725\n",
      "iteration 614, dc_loss: 0.19280414283275604, tv_loss: 0.03112691268324852\n",
      "iteration 615, dc_loss: 0.19158892333507538, tv_loss: 0.030816763639450073\n",
      "iteration 616, dc_loss: 0.19036538898944855, tv_loss: 0.030925104394555092\n",
      "iteration 617, dc_loss: 0.18980824947357178, tv_loss: 0.03096308372914791\n",
      "iteration 618, dc_loss: 0.18977628648281097, tv_loss: 0.030730005353689194\n",
      "iteration 619, dc_loss: 0.1888239085674286, tv_loss: 0.031042689457535744\n",
      "iteration 620, dc_loss: 0.187985360622406, tv_loss: 0.030931230634450912\n",
      "iteration 621, dc_loss: 0.18715019524097443, tv_loss: 0.030843617394566536\n",
      "iteration 622, dc_loss: 0.18655861914157867, tv_loss: 0.0310957133769989\n",
      "iteration 623, dc_loss: 0.18626241385936737, tv_loss: 0.03081677295267582\n",
      "iteration 624, dc_loss: 0.18530122935771942, tv_loss: 0.031156882643699646\n",
      "iteration 625, dc_loss: 0.18464308977127075, tv_loss: 0.030886033549904823\n",
      "iteration 626, dc_loss: 0.18402104079723358, tv_loss: 0.03096959926187992\n",
      "iteration 627, dc_loss: 0.18336035311222076, tv_loss: 0.03102271817624569\n",
      "iteration 628, dc_loss: 0.18289010226726532, tv_loss: 0.03102693520486355\n",
      "iteration 629, dc_loss: 0.1821056753396988, tv_loss: 0.030984792858362198\n",
      "iteration 630, dc_loss: 0.1815069019794464, tv_loss: 0.031049642711877823\n",
      "iteration 631, dc_loss: 0.18086273968219757, tv_loss: 0.030954821035265923\n",
      "iteration 632, dc_loss: 0.1802545040845871, tv_loss: 0.031208720058202744\n",
      "iteration 633, dc_loss: 0.17971543967723846, tv_loss: 0.031003667041659355\n",
      "iteration 634, dc_loss: 0.17905429005622864, tv_loss: 0.0311074610799551\n",
      "iteration 635, dc_loss: 0.17835448682308197, tv_loss: 0.031060539186000824\n",
      "iteration 636, dc_loss: 0.17781881988048553, tv_loss: 0.031141657382249832\n",
      "iteration 637, dc_loss: 0.17719334363937378, tv_loss: 0.031093759462237358\n",
      "iteration 638, dc_loss: 0.17671889066696167, tv_loss: 0.031115137040615082\n",
      "iteration 639, dc_loss: 0.17601288855075836, tv_loss: 0.031054364517331123\n",
      "iteration 640, dc_loss: 0.17537276446819305, tv_loss: 0.031253714114427567\n",
      "iteration 641, dc_loss: 0.17481504380702972, tv_loss: 0.03109615668654442\n",
      "iteration 642, dc_loss: 0.17423833906650543, tv_loss: 0.031235357746481895\n",
      "iteration 643, dc_loss: 0.17369391024112701, tv_loss: 0.031112520024180412\n",
      "iteration 644, dc_loss: 0.17304761707782745, tv_loss: 0.031265705823898315\n",
      "iteration 645, dc_loss: 0.1724756807088852, tv_loss: 0.031133078038692474\n",
      "iteration 646, dc_loss: 0.1719054877758026, tv_loss: 0.031229496002197266\n",
      "iteration 647, dc_loss: 0.17129942774772644, tv_loss: 0.031154612079262733\n",
      "iteration 648, dc_loss: 0.17074504494667053, tv_loss: 0.03129000961780548\n",
      "iteration 649, dc_loss: 0.17018063366413116, tv_loss: 0.0311955064535141\n",
      "iteration 650, dc_loss: 0.1696224808692932, tv_loss: 0.031287260353565216\n",
      "iteration 651, dc_loss: 0.16905874013900757, tv_loss: 0.031180480495095253\n",
      "iteration 652, dc_loss: 0.16843032836914062, tv_loss: 0.03135664388537407\n",
      "iteration 653, dc_loss: 0.16793310642242432, tv_loss: 0.031196309253573418\n",
      "iteration 654, dc_loss: 0.1673203855752945, tv_loss: 0.03136077895760536\n",
      "iteration 655, dc_loss: 0.16689836978912354, tv_loss: 0.031161831691861153\n",
      "iteration 656, dc_loss: 0.16620895266532898, tv_loss: 0.0314493328332901\n",
      "iteration 657, dc_loss: 0.1659245491027832, tv_loss: 0.031132344156503677\n",
      "iteration 658, dc_loss: 0.16521038115024567, tv_loss: 0.03154226392507553\n",
      "iteration 659, dc_loss: 0.1652013212442398, tv_loss: 0.031070338562130928\n",
      "iteration 660, dc_loss: 0.16445180773735046, tv_loss: 0.031650617718696594\n",
      "iteration 661, dc_loss: 0.16456599533557892, tv_loss: 0.03101179376244545\n",
      "iteration 662, dc_loss: 0.16356129944324493, tv_loss: 0.03174492344260216\n",
      "iteration 663, dc_loss: 0.16347920894622803, tv_loss: 0.031026706099510193\n",
      "iteration 664, dc_loss: 0.16211768984794617, tv_loss: 0.03156040981411934\n",
      "iteration 665, dc_loss: 0.16146236658096313, tv_loss: 0.03128635883331299\n",
      "iteration 666, dc_loss: 0.1611061841249466, tv_loss: 0.031246816739439964\n",
      "iteration 667, dc_loss: 0.16036716103553772, tv_loss: 0.03153739124536514\n",
      "iteration 668, dc_loss: 0.16033318638801575, tv_loss: 0.03128096088767052\n",
      "iteration 669, dc_loss: 0.159319669008255, tv_loss: 0.031480614095926285\n",
      "iteration 670, dc_loss: 0.15885525941848755, tv_loss: 0.031375952064991\n",
      "iteration 671, dc_loss: 0.1582634299993515, tv_loss: 0.03130090609192848\n",
      "iteration 672, dc_loss: 0.15774011611938477, tv_loss: 0.03164083510637283\n",
      "iteration 673, dc_loss: 0.1575067937374115, tv_loss: 0.031216617673635483\n",
      "iteration 674, dc_loss: 0.15664570033550262, tv_loss: 0.03157796338200569\n",
      "iteration 675, dc_loss: 0.15608760714530945, tv_loss: 0.03139530494809151\n",
      "iteration 676, dc_loss: 0.15566708147525787, tv_loss: 0.03142858296632767\n",
      "iteration 677, dc_loss: 0.1550573855638504, tv_loss: 0.03151778504252434\n",
      "iteration 678, dc_loss: 0.1549062877893448, tv_loss: 0.03127281367778778\n",
      "iteration 679, dc_loss: 0.15398652851581573, tv_loss: 0.03152885660529137\n",
      "iteration 680, dc_loss: 0.15357743203639984, tv_loss: 0.03146540746092796\n",
      "iteration 681, dc_loss: 0.1531270444393158, tv_loss: 0.03134968876838684\n",
      "iteration 682, dc_loss: 0.1525280475616455, tv_loss: 0.03153524547815323\n",
      "iteration 683, dc_loss: 0.15215228497982025, tv_loss: 0.03140638768672943\n",
      "iteration 684, dc_loss: 0.15149597823619843, tv_loss: 0.03154091536998749\n",
      "iteration 685, dc_loss: 0.1511109173297882, tv_loss: 0.03140721097588539\n",
      "iteration 686, dc_loss: 0.15058690309524536, tv_loss: 0.03139915689826012\n",
      "iteration 687, dc_loss: 0.14997288584709167, tv_loss: 0.0315689779818058\n",
      "iteration 688, dc_loss: 0.14973275363445282, tv_loss: 0.031385283917188644\n",
      "iteration 689, dc_loss: 0.14908112585544586, tv_loss: 0.03149263188242912\n",
      "iteration 690, dc_loss: 0.14863255620002747, tv_loss: 0.03148648515343666\n",
      "iteration 691, dc_loss: 0.14811314642429352, tv_loss: 0.0314985029399395\n",
      "iteration 692, dc_loss: 0.14761346578598022, tv_loss: 0.031498268246650696\n",
      "iteration 693, dc_loss: 0.14721299707889557, tv_loss: 0.03143693879246712\n",
      "iteration 694, dc_loss: 0.14664709568023682, tv_loss: 0.031599316745996475\n",
      "iteration 695, dc_loss: 0.14634744822978973, tv_loss: 0.03144926577806473\n",
      "iteration 696, dc_loss: 0.1457223892211914, tv_loss: 0.031601037830114365\n",
      "iteration 697, dc_loss: 0.14545667171478271, tv_loss: 0.03140323609113693\n",
      "iteration 698, dc_loss: 0.1447952687740326, tv_loss: 0.03163895383477211\n",
      "iteration 699, dc_loss: 0.1445244550704956, tv_loss: 0.03146279603242874\n",
      "iteration 700, dc_loss: 0.14399470388889313, tv_loss: 0.03161003813147545\n",
      "iteration 701, dc_loss: 0.14374195039272308, tv_loss: 0.03148294612765312\n",
      "iteration 702, dc_loss: 0.1433044970035553, tv_loss: 0.031622469425201416\n",
      "iteration 703, dc_loss: 0.14299547672271729, tv_loss: 0.03156621381640434\n",
      "iteration 704, dc_loss: 0.1426982581615448, tv_loss: 0.03160080313682556\n",
      "iteration 705, dc_loss: 0.14213892817497253, tv_loss: 0.03161254897713661\n",
      "iteration 706, dc_loss: 0.14156265556812286, tv_loss: 0.0316193625330925\n",
      "iteration 707, dc_loss: 0.14083677530288696, tv_loss: 0.03162417933344841\n",
      "iteration 708, dc_loss: 0.14022484421730042, tv_loss: 0.03159809857606888\n",
      "iteration 709, dc_loss: 0.13968122005462646, tv_loss: 0.03166351467370987\n",
      "iteration 710, dc_loss: 0.13940642774105072, tv_loss: 0.031624626368284225\n",
      "iteration 711, dc_loss: 0.13897517323493958, tv_loss: 0.031725455075502396\n",
      "iteration 712, dc_loss: 0.13870584964752197, tv_loss: 0.03155243396759033\n",
      "iteration 713, dc_loss: 0.13804568350315094, tv_loss: 0.03174474090337753\n",
      "iteration 714, dc_loss: 0.13769733905792236, tv_loss: 0.03156934306025505\n",
      "iteration 715, dc_loss: 0.13702236115932465, tv_loss: 0.03176003694534302\n",
      "iteration 716, dc_loss: 0.13679738342761993, tv_loss: 0.03156702592968941\n",
      "iteration 717, dc_loss: 0.13617615401744843, tv_loss: 0.0318249873816967\n",
      "iteration 718, dc_loss: 0.13597846031188965, tv_loss: 0.03165978938341141\n",
      "iteration 719, dc_loss: 0.13542166352272034, tv_loss: 0.03179190307855606\n",
      "iteration 720, dc_loss: 0.13506855070590973, tv_loss: 0.031669970601797104\n",
      "iteration 721, dc_loss: 0.13447779417037964, tv_loss: 0.03178193420171738\n",
      "iteration 722, dc_loss: 0.134105384349823, tv_loss: 0.031688570976257324\n",
      "iteration 723, dc_loss: 0.1335960030555725, tv_loss: 0.0317949540913105\n",
      "iteration 724, dc_loss: 0.1332656294107437, tv_loss: 0.031760185956954956\n",
      "iteration 725, dc_loss: 0.13276226818561554, tv_loss: 0.031838759779930115\n",
      "iteration 726, dc_loss: 0.13250921666622162, tv_loss: 0.031687505543231964\n",
      "iteration 727, dc_loss: 0.13196560740470886, tv_loss: 0.031868062913417816\n",
      "iteration 728, dc_loss: 0.1318165361881256, tv_loss: 0.03164409473538399\n",
      "iteration 729, dc_loss: 0.13114893436431885, tv_loss: 0.03197912871837616\n",
      "iteration 730, dc_loss: 0.13117265701293945, tv_loss: 0.031617630273103714\n",
      "iteration 731, dc_loss: 0.13041922450065613, tv_loss: 0.032071635127067566\n",
      "iteration 732, dc_loss: 0.13042649626731873, tv_loss: 0.03166117146611214\n",
      "iteration 733, dc_loss: 0.12959611415863037, tv_loss: 0.0320405587553978\n",
      "iteration 734, dc_loss: 0.12946471571922302, tv_loss: 0.03167399391531944\n",
      "iteration 735, dc_loss: 0.1286977380514145, tv_loss: 0.031961336731910706\n",
      "iteration 736, dc_loss: 0.12844093143939972, tv_loss: 0.03175564855337143\n",
      "iteration 737, dc_loss: 0.12789508700370789, tv_loss: 0.031873367726802826\n",
      "iteration 738, dc_loss: 0.12751266360282898, tv_loss: 0.031874220818281174\n",
      "iteration 739, dc_loss: 0.12719495594501495, tv_loss: 0.03187526762485504\n",
      "iteration 740, dc_loss: 0.1266968846321106, tv_loss: 0.032000526785850525\n",
      "iteration 741, dc_loss: 0.12647731602191925, tv_loss: 0.031795598566532135\n",
      "iteration 742, dc_loss: 0.12591299414634705, tv_loss: 0.03197312355041504\n",
      "iteration 743, dc_loss: 0.12566497921943665, tv_loss: 0.03181558847427368\n",
      "iteration 744, dc_loss: 0.12512421607971191, tv_loss: 0.03195486590266228\n",
      "iteration 745, dc_loss: 0.12482580542564392, tv_loss: 0.03186902031302452\n",
      "iteration 746, dc_loss: 0.12434649467468262, tv_loss: 0.03202720731496811\n",
      "iteration 747, dc_loss: 0.12403519451618195, tv_loss: 0.031909260898828506\n",
      "iteration 748, dc_loss: 0.12360609322786331, tv_loss: 0.03196563571691513\n",
      "iteration 749, dc_loss: 0.12322825193405151, tv_loss: 0.031948626041412354\n",
      "iteration 750, dc_loss: 0.12289272248744965, tv_loss: 0.03192827105522156\n",
      "iteration 751, dc_loss: 0.12247174978256226, tv_loss: 0.032055795192718506\n",
      "iteration 752, dc_loss: 0.12219057232141495, tv_loss: 0.03194236382842064\n",
      "iteration 753, dc_loss: 0.12168911099433899, tv_loss: 0.03209073096513748\n",
      "iteration 754, dc_loss: 0.1215544268488884, tv_loss: 0.031852882355451584\n",
      "iteration 755, dc_loss: 0.12095396220684052, tv_loss: 0.03214375302195549\n",
      "iteration 756, dc_loss: 0.12092304229736328, tv_loss: 0.03188570216298103\n",
      "iteration 757, dc_loss: 0.12026488780975342, tv_loss: 0.03220584616065025\n",
      "iteration 758, dc_loss: 0.12025374174118042, tv_loss: 0.031833093613386154\n",
      "iteration 759, dc_loss: 0.1195283979177475, tv_loss: 0.03219497203826904\n",
      "iteration 760, dc_loss: 0.11946303397417068, tv_loss: 0.03189795836806297\n",
      "iteration 761, dc_loss: 0.11879534274339676, tv_loss: 0.03218178078532219\n",
      "iteration 762, dc_loss: 0.11870456486940384, tv_loss: 0.0318852923810482\n",
      "iteration 763, dc_loss: 0.11805690079927444, tv_loss: 0.032176606357097626\n",
      "iteration 764, dc_loss: 0.11796341836452484, tv_loss: 0.031951770186424255\n",
      "iteration 765, dc_loss: 0.11742506921291351, tv_loss: 0.03223804011940956\n",
      "iteration 766, dc_loss: 0.11742161214351654, tv_loss: 0.03199825808405876\n",
      "iteration 767, dc_loss: 0.1170491948723793, tv_loss: 0.03225001320242882\n",
      "iteration 768, dc_loss: 0.11728635430335999, tv_loss: 0.031922560185194016\n",
      "iteration 769, dc_loss: 0.11726615577936172, tv_loss: 0.03233049437403679\n",
      "iteration 770, dc_loss: 0.11756720393896103, tv_loss: 0.03193102777004242\n",
      "iteration 771, dc_loss: 0.11724686622619629, tv_loss: 0.03232662007212639\n",
      "iteration 772, dc_loss: 0.11626545339822769, tv_loss: 0.03207595273852348\n",
      "iteration 773, dc_loss: 0.1150861456990242, tv_loss: 0.032070714980363846\n",
      "iteration 774, dc_loss: 0.1143592819571495, tv_loss: 0.03228708729147911\n",
      "iteration 775, dc_loss: 0.1148613914847374, tv_loss: 0.03191104158759117\n",
      "iteration 776, dc_loss: 0.11429563164710999, tv_loss: 0.03240622580051422\n",
      "iteration 777, dc_loss: 0.11368896067142487, tv_loss: 0.03204192966222763\n",
      "iteration 778, dc_loss: 0.1130252406001091, tv_loss: 0.03209381178021431\n",
      "iteration 779, dc_loss: 0.11281556636095047, tv_loss: 0.032377514988183975\n",
      "iteration 780, dc_loss: 0.11304248869419098, tv_loss: 0.031963374465703964\n",
      "iteration 781, dc_loss: 0.11200925707817078, tv_loss: 0.03234739601612091\n",
      "iteration 782, dc_loss: 0.11162172257900238, tv_loss: 0.03214899078011513\n",
      "iteration 783, dc_loss: 0.11159619688987732, tv_loss: 0.03207791596651077\n",
      "iteration 784, dc_loss: 0.11115449666976929, tv_loss: 0.0323624461889267\n",
      "iteration 785, dc_loss: 0.11086692661046982, tv_loss: 0.03208508342504501\n",
      "iteration 786, dc_loss: 0.11025567352771759, tv_loss: 0.03227722644805908\n",
      "iteration 787, dc_loss: 0.11007221788167953, tv_loss: 0.032300565391778946\n",
      "iteration 788, dc_loss: 0.10993033647537231, tv_loss: 0.032140981405973434\n",
      "iteration 789, dc_loss: 0.10931465774774551, tv_loss: 0.032282717525959015\n",
      "iteration 790, dc_loss: 0.10896904766559601, tv_loss: 0.03221261128783226\n",
      "iteration 791, dc_loss: 0.10877019166946411, tv_loss: 0.032232291996479034\n",
      "iteration 792, dc_loss: 0.10849135369062424, tv_loss: 0.03234323859214783\n",
      "iteration 793, dc_loss: 0.10807295888662338, tv_loss: 0.03229577839374542\n",
      "iteration 794, dc_loss: 0.10773493349552155, tv_loss: 0.03220652416348457\n",
      "iteration 795, dc_loss: 0.10734579712152481, tv_loss: 0.03235553205013275\n",
      "iteration 796, dc_loss: 0.10722513496875763, tv_loss: 0.03227373585104942\n",
      "iteration 797, dc_loss: 0.10675375908613205, tv_loss: 0.03235900402069092\n",
      "iteration 798, dc_loss: 0.10639770328998566, tv_loss: 0.032322175800800323\n",
      "iteration 799, dc_loss: 0.10623316466808319, tv_loss: 0.03223031386733055\n",
      "iteration 800, dc_loss: 0.10577910393476486, tv_loss: 0.03248253092169762\n",
      "iteration 801, dc_loss: 0.10566335171461105, tv_loss: 0.03225289657711983\n",
      "iteration 802, dc_loss: 0.1052112728357315, tv_loss: 0.0323338583111763\n",
      "iteration 803, dc_loss: 0.10496498644351959, tv_loss: 0.03240358456969261\n",
      "iteration 804, dc_loss: 0.10485167056322098, tv_loss: 0.03229454904794693\n",
      "iteration 805, dc_loss: 0.1044970378279686, tv_loss: 0.032343920320272446\n",
      "iteration 806, dc_loss: 0.10424482077360153, tv_loss: 0.03238799795508385\n",
      "iteration 807, dc_loss: 0.10407724231481552, tv_loss: 0.03232526406645775\n",
      "iteration 808, dc_loss: 0.10377759486436844, tv_loss: 0.03237233683466911\n",
      "iteration 809, dc_loss: 0.10351577401161194, tv_loss: 0.032385971397161484\n",
      "iteration 810, dc_loss: 0.10333991050720215, tv_loss: 0.0323319248855114\n",
      "iteration 811, dc_loss: 0.10307080298662186, tv_loss: 0.03239389508962631\n",
      "iteration 812, dc_loss: 0.10280804336071014, tv_loss: 0.03239891305565834\n",
      "iteration 813, dc_loss: 0.1026187315583229, tv_loss: 0.03233795240521431\n",
      "iteration 814, dc_loss: 0.10234974324703217, tv_loss: 0.03239496797323227\n",
      "iteration 815, dc_loss: 0.10211547464132309, tv_loss: 0.032406195998191833\n",
      "iteration 816, dc_loss: 0.10190795361995697, tv_loss: 0.03236839920282364\n",
      "iteration 817, dc_loss: 0.10163220018148422, tv_loss: 0.03239972144365311\n",
      "iteration 818, dc_loss: 0.10142239928245544, tv_loss: 0.032394930720329285\n",
      "iteration 819, dc_loss: 0.10119375586509705, tv_loss: 0.03240806609392166\n",
      "iteration 820, dc_loss: 0.10093700140714645, tv_loss: 0.0324411578476429\n",
      "iteration 821, dc_loss: 0.10074454545974731, tv_loss: 0.03239702805876732\n",
      "iteration 822, dc_loss: 0.10048423707485199, tv_loss: 0.032404109835624695\n",
      "iteration 823, dc_loss: 0.10025597363710403, tv_loss: 0.03241947665810585\n",
      "iteration 824, dc_loss: 0.10005931556224823, tv_loss: 0.03240922465920448\n",
      "iteration 825, dc_loss: 0.09978486597537994, tv_loss: 0.03245134651660919\n",
      "iteration 826, dc_loss: 0.09957822412252426, tv_loss: 0.032442864030599594\n",
      "iteration 827, dc_loss: 0.09936871379613876, tv_loss: 0.032420564442873\n",
      "iteration 828, dc_loss: 0.09911827743053436, tv_loss: 0.03243386000394821\n",
      "iteration 829, dc_loss: 0.09890660643577576, tv_loss: 0.032435983419418335\n",
      "iteration 830, dc_loss: 0.09867307543754578, tv_loss: 0.032450880855321884\n",
      "iteration 831, dc_loss: 0.09845922142267227, tv_loss: 0.032428398728370667\n",
      "iteration 832, dc_loss: 0.09823454916477203, tv_loss: 0.03243651241064072\n",
      "iteration 833, dc_loss: 0.09799773246049881, tv_loss: 0.03246741741895676\n",
      "iteration 834, dc_loss: 0.09780121594667435, tv_loss: 0.032449815422296524\n",
      "iteration 835, dc_loss: 0.09755436331033707, tv_loss: 0.03250455483794212\n",
      "iteration 836, dc_loss: 0.09735103696584702, tv_loss: 0.032532792538404465\n",
      "iteration 837, dc_loss: 0.09712839871644974, tv_loss: 0.03249619901180267\n",
      "iteration 838, dc_loss: 0.09692154824733734, tv_loss: 0.032482340931892395\n",
      "iteration 839, dc_loss: 0.09667149186134338, tv_loss: 0.03256535902619362\n",
      "iteration 840, dc_loss: 0.09647826850414276, tv_loss: 0.03254970163106918\n",
      "iteration 841, dc_loss: 0.09628903865814209, tv_loss: 0.03247062489390373\n",
      "iteration 842, dc_loss: 0.09602788835763931, tv_loss: 0.03255268186330795\n",
      "iteration 843, dc_loss: 0.09583040326833725, tv_loss: 0.03258836269378662\n",
      "iteration 844, dc_loss: 0.09563320130109787, tv_loss: 0.032510675489902496\n",
      "iteration 845, dc_loss: 0.09537798911333084, tv_loss: 0.03257293626666069\n",
      "iteration 846, dc_loss: 0.09520828723907471, tv_loss: 0.032591041177511215\n",
      "iteration 847, dc_loss: 0.09498607367277145, tv_loss: 0.03253290802240372\n",
      "iteration 848, dc_loss: 0.09473542869091034, tv_loss: 0.032582711428403854\n",
      "iteration 849, dc_loss: 0.09458980709314346, tv_loss: 0.03261274844408035\n",
      "iteration 850, dc_loss: 0.09434615075588226, tv_loss: 0.032551463693380356\n",
      "iteration 851, dc_loss: 0.09410224109888077, tv_loss: 0.0326533205807209\n",
      "iteration 852, dc_loss: 0.0939611867070198, tv_loss: 0.03259490802884102\n",
      "iteration 853, dc_loss: 0.09372320771217346, tv_loss: 0.03256365656852722\n",
      "iteration 854, dc_loss: 0.0934886783361435, tv_loss: 0.032662782818078995\n",
      "iteration 855, dc_loss: 0.09332167357206345, tv_loss: 0.03258823603391647\n",
      "iteration 856, dc_loss: 0.0930936262011528, tv_loss: 0.03257828950881958\n",
      "iteration 857, dc_loss: 0.09290754795074463, tv_loss: 0.03258414939045906\n",
      "iteration 858, dc_loss: 0.09268881380558014, tv_loss: 0.03261343017220497\n",
      "iteration 859, dc_loss: 0.09246381372213364, tv_loss: 0.03262943774461746\n",
      "iteration 860, dc_loss: 0.09227899461984634, tv_loss: 0.0325930193066597\n",
      "iteration 861, dc_loss: 0.09208851307630539, tv_loss: 0.03258064016699791\n",
      "iteration 862, dc_loss: 0.09186351299285889, tv_loss: 0.032640933990478516\n",
      "iteration 863, dc_loss: 0.09166873246431351, tv_loss: 0.032604217529296875\n",
      "iteration 864, dc_loss: 0.09145861119031906, tv_loss: 0.032622575759887695\n",
      "iteration 865, dc_loss: 0.09126818925142288, tv_loss: 0.032614272087812424\n",
      "iteration 866, dc_loss: 0.09108081459999084, tv_loss: 0.032637473195791245\n",
      "iteration 867, dc_loss: 0.09085218608379364, tv_loss: 0.03267529234290123\n",
      "iteration 868, dc_loss: 0.09066137671470642, tv_loss: 0.03263961896300316\n",
      "iteration 869, dc_loss: 0.09048601984977722, tv_loss: 0.03262150660157204\n",
      "iteration 870, dc_loss: 0.09025727212429047, tv_loss: 0.03265318274497986\n",
      "iteration 871, dc_loss: 0.0900752916932106, tv_loss: 0.032674502581357956\n",
      "iteration 872, dc_loss: 0.08988849818706512, tv_loss: 0.03266172483563423\n",
      "iteration 873, dc_loss: 0.08966828137636185, tv_loss: 0.03265795484185219\n",
      "iteration 874, dc_loss: 0.08950820565223694, tv_loss: 0.03264337778091431\n",
      "iteration 875, dc_loss: 0.08926990628242493, tv_loss: 0.03269471228122711\n",
      "iteration 876, dc_loss: 0.08911296725273132, tv_loss: 0.03269026428461075\n",
      "iteration 877, dc_loss: 0.08893061429262161, tv_loss: 0.0326719656586647\n",
      "iteration 878, dc_loss: 0.08867884427309036, tv_loss: 0.03271014243364334\n",
      "iteration 879, dc_loss: 0.08855801075696945, tv_loss: 0.03264361247420311\n",
      "iteration 880, dc_loss: 0.08831214159727097, tv_loss: 0.03269471973180771\n",
      "iteration 881, dc_loss: 0.08814632147550583, tv_loss: 0.03266739845275879\n",
      "iteration 882, dc_loss: 0.08795042335987091, tv_loss: 0.03268544748425484\n",
      "iteration 883, dc_loss: 0.08776140213012695, tv_loss: 0.03270556405186653\n",
      "iteration 884, dc_loss: 0.08756571263074875, tv_loss: 0.032718587666749954\n",
      "iteration 885, dc_loss: 0.0873849168419838, tv_loss: 0.03273078799247742\n",
      "iteration 886, dc_loss: 0.08718350529670715, tv_loss: 0.032724712044000626\n",
      "iteration 887, dc_loss: 0.08703207969665527, tv_loss: 0.03270248696208\n",
      "iteration 888, dc_loss: 0.08678539097309113, tv_loss: 0.03275173902511597\n",
      "iteration 889, dc_loss: 0.08667811006307602, tv_loss: 0.032662082463502884\n",
      "iteration 890, dc_loss: 0.08642403781414032, tv_loss: 0.03273922577500343\n",
      "iteration 891, dc_loss: 0.08627267926931381, tv_loss: 0.03273250535130501\n",
      "iteration 892, dc_loss: 0.0860801562666893, tv_loss: 0.032748766243457794\n",
      "iteration 893, dc_loss: 0.08587145060300827, tv_loss: 0.032805897295475006\n",
      "iteration 894, dc_loss: 0.08576249331235886, tv_loss: 0.032713476568460464\n",
      "iteration 895, dc_loss: 0.08549106121063232, tv_loss: 0.0328054204583168\n",
      "iteration 896, dc_loss: 0.08539138734340668, tv_loss: 0.032706327736377716\n",
      "iteration 897, dc_loss: 0.08514145761728287, tv_loss: 0.03277621790766716\n",
      "iteration 898, dc_loss: 0.08500402420759201, tv_loss: 0.03278035670518875\n",
      "iteration 899, dc_loss: 0.08481062948703766, tv_loss: 0.0327824167907238\n",
      "iteration 900, dc_loss: 0.08461973816156387, tv_loss: 0.03280176222324371\n",
      "iteration 901, dc_loss: 0.08449200540781021, tv_loss: 0.03273637965321541\n",
      "iteration 902, dc_loss: 0.0842542052268982, tv_loss: 0.03280409798026085\n",
      "iteration 903, dc_loss: 0.08411537855863571, tv_loss: 0.03275586664676666\n",
      "iteration 904, dc_loss: 0.08391211926937103, tv_loss: 0.03279196098446846\n",
      "iteration 905, dc_loss: 0.08372367173433304, tv_loss: 0.03281712904572487\n",
      "iteration 906, dc_loss: 0.08358345925807953, tv_loss: 0.03279316425323486\n",
      "iteration 907, dc_loss: 0.08336378633975983, tv_loss: 0.03285575285553932\n",
      "iteration 908, dc_loss: 0.08329413086175919, tv_loss: 0.03274688497185707\n",
      "iteration 909, dc_loss: 0.08299309015274048, tv_loss: 0.03288751840591431\n",
      "iteration 910, dc_loss: 0.08293891698122025, tv_loss: 0.032756540924310684\n",
      "iteration 911, dc_loss: 0.08266457915306091, tv_loss: 0.032867688685655594\n",
      "iteration 912, dc_loss: 0.08260799944400787, tv_loss: 0.032759394496679306\n",
      "iteration 913, dc_loss: 0.08233214169740677, tv_loss: 0.032880138605833054\n",
      "iteration 914, dc_loss: 0.08233478665351868, tv_loss: 0.032733745872974396\n",
      "iteration 915, dc_loss: 0.08205065131187439, tv_loss: 0.03288625180721283\n",
      "iteration 916, dc_loss: 0.08204390108585358, tv_loss: 0.03274187073111534\n",
      "iteration 917, dc_loss: 0.08171699196100235, tv_loss: 0.03294149041175842\n",
      "iteration 918, dc_loss: 0.08175051957368851, tv_loss: 0.03282031789422035\n",
      "iteration 919, dc_loss: 0.08133892714977264, tv_loss: 0.03299398720264435\n",
      "iteration 920, dc_loss: 0.08138851821422577, tv_loss: 0.03271762654185295\n",
      "iteration 921, dc_loss: 0.08093643188476562, tv_loss: 0.03302081674337387\n",
      "iteration 922, dc_loss: 0.08094184845685959, tv_loss: 0.03281345218420029\n",
      "iteration 923, dc_loss: 0.08064431697130203, tv_loss: 0.032899241894483566\n",
      "iteration 924, dc_loss: 0.08045397698879242, tv_loss: 0.03290696442127228\n",
      "iteration 925, dc_loss: 0.08046835660934448, tv_loss: 0.032766032963991165\n",
      "iteration 926, dc_loss: 0.08013517409563065, tv_loss: 0.0329829566180706\n",
      "iteration 927, dc_loss: 0.08022391051054001, tv_loss: 0.03276892751455307\n",
      "iteration 928, dc_loss: 0.07980988174676895, tv_loss: 0.03305770084261894\n",
      "iteration 929, dc_loss: 0.07982555776834488, tv_loss: 0.03283414617180824\n",
      "iteration 930, dc_loss: 0.07948676496744156, tv_loss: 0.03294703736901283\n",
      "iteration 931, dc_loss: 0.07935876399278641, tv_loss: 0.03289002180099487\n",
      "iteration 932, dc_loss: 0.07921216636896133, tv_loss: 0.0328839085996151\n",
      "iteration 933, dc_loss: 0.07900815457105637, tv_loss: 0.032977622002363205\n",
      "iteration 934, dc_loss: 0.07894223928451538, tv_loss: 0.03287156671285629\n",
      "iteration 935, dc_loss: 0.07866240292787552, tv_loss: 0.03298858925700188\n",
      "iteration 936, dc_loss: 0.0786573737859726, tv_loss: 0.03284443914890289\n",
      "iteration 937, dc_loss: 0.07836737483739853, tv_loss: 0.03298810496926308\n",
      "iteration 938, dc_loss: 0.0783209279179573, tv_loss: 0.03289218991994858\n",
      "iteration 939, dc_loss: 0.07804669439792633, tv_loss: 0.03299528360366821\n",
      "iteration 940, dc_loss: 0.07792314887046814, tv_loss: 0.032935820519924164\n",
      "iteration 941, dc_loss: 0.07776869088411331, tv_loss: 0.032907817512750626\n",
      "iteration 942, dc_loss: 0.07756223529577255, tv_loss: 0.032969601452350616\n",
      "iteration 943, dc_loss: 0.07751046866178513, tv_loss: 0.032889094203710556\n",
      "iteration 944, dc_loss: 0.07724743336439133, tv_loss: 0.03302927315235138\n",
      "iteration 945, dc_loss: 0.07721401751041412, tv_loss: 0.032924674451351166\n",
      "iteration 946, dc_loss: 0.07694380730390549, tv_loss: 0.03302816301584244\n",
      "iteration 947, dc_loss: 0.07687584310770035, tv_loss: 0.03292388468980789\n",
      "iteration 948, dc_loss: 0.07667645812034607, tv_loss: 0.03297474607825279\n",
      "iteration 949, dc_loss: 0.07655193656682968, tv_loss: 0.0329473614692688\n",
      "iteration 950, dc_loss: 0.07635794579982758, tv_loss: 0.03298594430088997\n",
      "iteration 951, dc_loss: 0.07620532065629959, tv_loss: 0.03298959136009216\n",
      "iteration 952, dc_loss: 0.07608704268932343, tv_loss: 0.032970018684864044\n",
      "iteration 953, dc_loss: 0.07588475942611694, tv_loss: 0.033035751432180405\n",
      "iteration 954, dc_loss: 0.0757964625954628, tv_loss: 0.03295666351914406\n",
      "iteration 955, dc_loss: 0.07555589079856873, tv_loss: 0.033046092838048935\n",
      "iteration 956, dc_loss: 0.07554112374782562, tv_loss: 0.03292056545615196\n",
      "iteration 957, dc_loss: 0.07526929676532745, tv_loss: 0.03306948393583298\n",
      "iteration 958, dc_loss: 0.07530791312456131, tv_loss: 0.032916393131017685\n",
      "iteration 959, dc_loss: 0.07501982897520065, tv_loss: 0.0331101156771183\n",
      "iteration 960, dc_loss: 0.07507658004760742, tv_loss: 0.03294191136956215\n",
      "iteration 961, dc_loss: 0.07474473118782043, tv_loss: 0.033122316002845764\n",
      "iteration 962, dc_loss: 0.07481551170349121, tv_loss: 0.032908614724874496\n",
      "iteration 963, dc_loss: 0.0744892805814743, tv_loss: 0.033151112496852875\n",
      "iteration 964, dc_loss: 0.07457003742456436, tv_loss: 0.03296985477209091\n",
      "iteration 965, dc_loss: 0.07417434453964233, tv_loss: 0.03319617360830307\n",
      "iteration 966, dc_loss: 0.074273481965065, tv_loss: 0.03289001062512398\n",
      "iteration 967, dc_loss: 0.07385196536779404, tv_loss: 0.03317181393504143\n",
      "iteration 968, dc_loss: 0.07390068471431732, tv_loss: 0.03298679366707802\n",
      "iteration 969, dc_loss: 0.07350616902112961, tv_loss: 0.03314009681344032\n",
      "iteration 970, dc_loss: 0.07347535341978073, tv_loss: 0.032999586313962936\n",
      "iteration 971, dc_loss: 0.07327253371477127, tv_loss: 0.033083416521549225\n",
      "iteration 972, dc_loss: 0.07310275733470917, tv_loss: 0.0331411212682724\n",
      "iteration 973, dc_loss: 0.07302885502576828, tv_loss: 0.03303316608071327\n",
      "iteration 974, dc_loss: 0.07284604758024216, tv_loss: 0.03310501575469971\n",
      "iteration 975, dc_loss: 0.07281529158353806, tv_loss: 0.03304773569107056\n",
      "iteration 976, dc_loss: 0.07254403084516525, tv_loss: 0.03317178040742874\n",
      "iteration 977, dc_loss: 0.07252538204193115, tv_loss: 0.0330171175301075\n",
      "iteration 978, dc_loss: 0.07227231562137604, tv_loss: 0.03313607722520828\n",
      "iteration 979, dc_loss: 0.0722341537475586, tv_loss: 0.03302042931318283\n",
      "iteration 980, dc_loss: 0.0719597265124321, tv_loss: 0.03317069634795189\n",
      "iteration 981, dc_loss: 0.07191593199968338, tv_loss: 0.033047016710042953\n",
      "iteration 982, dc_loss: 0.07173261046409607, tv_loss: 0.033092133700847626\n",
      "iteration 983, dc_loss: 0.0715702474117279, tv_loss: 0.033111024647951126\n",
      "iteration 984, dc_loss: 0.07146836072206497, tv_loss: 0.0330953449010849\n",
      "iteration 985, dc_loss: 0.07129395753145218, tv_loss: 0.03313983604311943\n",
      "iteration 986, dc_loss: 0.07123707979917526, tv_loss: 0.033049724996089935\n",
      "iteration 987, dc_loss: 0.07100684940814972, tv_loss: 0.033161766827106476\n",
      "iteration 988, dc_loss: 0.07096412032842636, tv_loss: 0.03308110311627388\n",
      "iteration 989, dc_loss: 0.07074511051177979, tv_loss: 0.033185943961143494\n",
      "iteration 990, dc_loss: 0.07074156403541565, tv_loss: 0.03306801989674568\n",
      "iteration 991, dc_loss: 0.07046181708574295, tv_loss: 0.033208075910806656\n",
      "iteration 992, dc_loss: 0.07052944600582123, tv_loss: 0.033025458455085754\n",
      "iteration 993, dc_loss: 0.07021375000476837, tv_loss: 0.03325396403670311\n",
      "iteration 994, dc_loss: 0.07035783678293228, tv_loss: 0.03298038989305496\n",
      "iteration 995, dc_loss: 0.06996937841176987, tv_loss: 0.03331499546766281\n",
      "iteration 996, dc_loss: 0.07025781273841858, tv_loss: 0.032937001436948776\n",
      "iteration 997, dc_loss: 0.06980728358030319, tv_loss: 0.03338022530078888\n",
      "iteration 998, dc_loss: 0.07014276087284088, tv_loss: 0.03295844793319702\n",
      "iteration 999, dc_loss: 0.06957687437534332, tv_loss: 0.03340598940849304\n",
      "iteration 1000, dc_loss: 0.06988276541233063, tv_loss: 0.03290821984410286\n",
      "iteration 1001, dc_loss: 0.06926200538873672, tv_loss: 0.03333835303783417\n",
      "iteration 1002, dc_loss: 0.069316565990448, tv_loss: 0.033038266003131866\n",
      "iteration 1003, dc_loss: 0.06892246752977371, tv_loss: 0.03325646370649338\n",
      "iteration 1004, dc_loss: 0.06883516162633896, tv_loss: 0.033192820847034454\n",
      "iteration 1005, dc_loss: 0.06878593564033508, tv_loss: 0.03309907391667366\n",
      "iteration 1006, dc_loss: 0.06851570308208466, tv_loss: 0.0332784578204155\n",
      "iteration 1007, dc_loss: 0.06866451352834702, tv_loss: 0.03307709842920303\n",
      "iteration 1008, dc_loss: 0.06829764693975449, tv_loss: 0.03336932882666588\n",
      "iteration 1009, dc_loss: 0.06836112588644028, tv_loss: 0.03308726102113724\n",
      "iteration 1010, dc_loss: 0.06801137328147888, tv_loss: 0.03327235206961632\n",
      "iteration 1011, dc_loss: 0.06796961277723312, tv_loss: 0.03319503739476204\n",
      "iteration 1012, dc_loss: 0.06786098331212997, tv_loss: 0.03317669406533241\n",
      "iteration 1013, dc_loss: 0.06765083223581314, tv_loss: 0.03327373042702675\n",
      "iteration 1014, dc_loss: 0.06774252653121948, tv_loss: 0.03309648111462593\n",
      "iteration 1015, dc_loss: 0.06745246052742004, tv_loss: 0.033338937908411026\n",
      "iteration 1016, dc_loss: 0.06751247495412827, tv_loss: 0.03310738503932953\n",
      "iteration 1017, dc_loss: 0.06724713742733002, tv_loss: 0.03326430544257164\n",
      "iteration 1018, dc_loss: 0.06721017509698868, tv_loss: 0.033204685896635056\n",
      "iteration 1019, dc_loss: 0.06718294322490692, tv_loss: 0.03321961313486099\n",
      "iteration 1020, dc_loss: 0.06704112142324448, tv_loss: 0.033269915729761124\n",
      "iteration 1021, dc_loss: 0.06708357483148575, tv_loss: 0.033150721341371536\n",
      "iteration 1022, dc_loss: 0.06681410223245621, tv_loss: 0.03330092877149582\n",
      "iteration 1023, dc_loss: 0.06681902706623077, tv_loss: 0.03319333493709564\n",
      "iteration 1024, dc_loss: 0.06649305671453476, tv_loss: 0.03329193964600563\n",
      "iteration 1025, dc_loss: 0.06636489182710648, tv_loss: 0.03319219872355461\n",
      "iteration 1026, dc_loss: 0.0661003515124321, tv_loss: 0.03324119374155998\n",
      "iteration 1027, dc_loss: 0.06595614552497864, tv_loss: 0.03324994817376137\n",
      "iteration 1028, dc_loss: 0.06594591587781906, tv_loss: 0.033174920827150345\n",
      "iteration 1029, dc_loss: 0.06573132425546646, tv_loss: 0.03331122547388077\n",
      "iteration 1030, dc_loss: 0.06579989194869995, tv_loss: 0.03316662460565567\n",
      "iteration 1031, dc_loss: 0.06554120033979416, tv_loss: 0.033315807580947876\n",
      "iteration 1032, dc_loss: 0.06549783796072006, tv_loss: 0.03321319445967674\n",
      "iteration 1033, dc_loss: 0.06526897102594376, tv_loss: 0.033283937722444534\n",
      "iteration 1034, dc_loss: 0.06514899432659149, tv_loss: 0.03325456753373146\n",
      "iteration 1035, dc_loss: 0.0650721862912178, tv_loss: 0.033211078494787216\n",
      "iteration 1036, dc_loss: 0.06487647444009781, tv_loss: 0.03329597786068916\n",
      "iteration 1037, dc_loss: 0.06487834453582764, tv_loss: 0.03320113196969032\n",
      "iteration 1038, dc_loss: 0.06465350091457367, tv_loss: 0.03332176432013512\n",
      "iteration 1039, dc_loss: 0.06469275802373886, tv_loss: 0.03316393867135048\n",
      "iteration 1040, dc_loss: 0.06441199779510498, tv_loss: 0.033331841230392456\n",
      "iteration 1041, dc_loss: 0.06441888958215714, tv_loss: 0.03319397196173668\n",
      "iteration 1042, dc_loss: 0.06418991088867188, tv_loss: 0.03331094607710838\n",
      "iteration 1043, dc_loss: 0.06415832042694092, tv_loss: 0.03325521945953369\n",
      "iteration 1044, dc_loss: 0.06396064907312393, tv_loss: 0.03338246792554855\n",
      "iteration 1045, dc_loss: 0.06392843276262283, tv_loss: 0.033268868923187256\n",
      "iteration 1046, dc_loss: 0.0637790635228157, tv_loss: 0.03330378606915474\n",
      "iteration 1047, dc_loss: 0.0637851133942604, tv_loss: 0.033267658203840256\n",
      "iteration 1048, dc_loss: 0.0635690689086914, tv_loss: 0.033394455909729004\n",
      "iteration 1049, dc_loss: 0.06363440304994583, tv_loss: 0.03324102982878685\n",
      "iteration 1050, dc_loss: 0.06343571841716766, tv_loss: 0.03338943421840668\n",
      "iteration 1051, dc_loss: 0.06352570652961731, tv_loss: 0.03326451778411865\n",
      "iteration 1052, dc_loss: 0.06326263397932053, tv_loss: 0.03347083181142807\n",
      "iteration 1053, dc_loss: 0.06347450613975525, tv_loss: 0.03312830254435539\n",
      "iteration 1054, dc_loss: 0.06306424736976624, tv_loss: 0.03349786624312401\n",
      "iteration 1055, dc_loss: 0.06326719373464584, tv_loss: 0.03318812698125839\n",
      "iteration 1056, dc_loss: 0.06275909394025803, tv_loss: 0.03351432457566261\n",
      "iteration 1057, dc_loss: 0.06282716244459152, tv_loss: 0.03316855430603027\n",
      "iteration 1058, dc_loss: 0.062419500201940536, tv_loss: 0.03346306458115578\n",
      "iteration 1059, dc_loss: 0.06238383427262306, tv_loss: 0.03335597366094589\n",
      "iteration 1060, dc_loss: 0.06230512633919716, tv_loss: 0.03330293297767639\n",
      "iteration 1061, dc_loss: 0.06217380240559578, tv_loss: 0.03338757902383804\n",
      "iteration 1062, dc_loss: 0.06224317103624344, tv_loss: 0.033301740884780884\n",
      "iteration 1063, dc_loss: 0.06194009631872177, tv_loss: 0.03347957879304886\n",
      "iteration 1064, dc_loss: 0.061993908137083054, tv_loss: 0.03323184326291084\n",
      "iteration 1065, dc_loss: 0.061688363552093506, tv_loss: 0.033424459397792816\n",
      "iteration 1066, dc_loss: 0.06164802610874176, tv_loss: 0.033332355320453644\n",
      "iteration 1067, dc_loss: 0.06151486933231354, tv_loss: 0.03333240747451782\n",
      "iteration 1068, dc_loss: 0.06138508394360542, tv_loss: 0.033377908170223236\n",
      "iteration 1069, dc_loss: 0.06145111098885536, tv_loss: 0.03324839845299721\n",
      "iteration 1070, dc_loss: 0.061170659959316254, tv_loss: 0.03347664326429367\n",
      "iteration 1071, dc_loss: 0.06124624237418175, tv_loss: 0.033286698162555695\n",
      "iteration 1072, dc_loss: 0.06096258386969566, tv_loss: 0.03342374786734581\n",
      "iteration 1073, dc_loss: 0.06091351807117462, tv_loss: 0.03330594301223755\n",
      "iteration 1074, dc_loss: 0.06077572703361511, tv_loss: 0.03332940861582756\n",
      "iteration 1075, dc_loss: 0.0606267936527729, tv_loss: 0.033396873623132706\n",
      "iteration 1076, dc_loss: 0.060669757425785065, tv_loss: 0.03326625004410744\n",
      "iteration 1077, dc_loss: 0.060418371111154556, tv_loss: 0.03343559056520462\n",
      "iteration 1078, dc_loss: 0.060506757348775864, tv_loss: 0.0332692414522171\n",
      "iteration 1079, dc_loss: 0.06022019311785698, tv_loss: 0.03351004421710968\n",
      "iteration 1080, dc_loss: 0.060265470296144485, tv_loss: 0.033315613865852356\n",
      "iteration 1081, dc_loss: 0.06003226712346077, tv_loss: 0.03340029716491699\n",
      "iteration 1082, dc_loss: 0.059944331645965576, tv_loss: 0.033391404896974564\n",
      "iteration 1083, dc_loss: 0.059900518506765366, tv_loss: 0.033437781035900116\n",
      "iteration 1084, dc_loss: 0.05974050983786583, tv_loss: 0.03342890739440918\n",
      "iteration 1085, dc_loss: 0.059710677713155746, tv_loss: 0.03336448222398758\n",
      "iteration 1086, dc_loss: 0.0595255009829998, tv_loss: 0.0335540845990181\n",
      "iteration 1087, dc_loss: 0.0595780685544014, tv_loss: 0.03332864120602608\n",
      "iteration 1088, dc_loss: 0.059323593974113464, tv_loss: 0.03350396454334259\n",
      "iteration 1089, dc_loss: 0.05939072370529175, tv_loss: 0.03338118642568588\n",
      "iteration 1090, dc_loss: 0.059121135622262955, tv_loss: 0.033493686467409134\n",
      "iteration 1091, dc_loss: 0.059181809425354004, tv_loss: 0.03331032767891884\n",
      "iteration 1092, dc_loss: 0.05893642082810402, tv_loss: 0.03348749503493309\n",
      "iteration 1093, dc_loss: 0.05899205058813095, tv_loss: 0.03337499871850014\n",
      "iteration 1094, dc_loss: 0.05874602496623993, tv_loss: 0.03349510207772255\n",
      "iteration 1095, dc_loss: 0.05882776901125908, tv_loss: 0.03332170471549034\n",
      "iteration 1096, dc_loss: 0.05857878178358078, tv_loss: 0.03352921083569527\n",
      "iteration 1097, dc_loss: 0.058700453490018845, tv_loss: 0.03335007652640343\n",
      "iteration 1098, dc_loss: 0.058460939675569534, tv_loss: 0.033514175564050674\n",
      "iteration 1099, dc_loss: 0.05859607458114624, tv_loss: 0.03331555798649788\n",
      "iteration 1100, dc_loss: 0.05833230912685394, tv_loss: 0.03350362926721573\n",
      "iteration 1101, dc_loss: 0.05847655236721039, tv_loss: 0.0333365760743618\n",
      "iteration 1102, dc_loss: 0.05821878835558891, tv_loss: 0.03352680429816246\n",
      "iteration 1103, dc_loss: 0.05828189477324486, tv_loss: 0.033369213342666626\n",
      "iteration 1104, dc_loss: 0.057980529963970184, tv_loss: 0.033497121185064316\n",
      "iteration 1105, dc_loss: 0.05793217942118645, tv_loss: 0.03338919207453728\n",
      "iteration 1106, dc_loss: 0.05774208903312683, tv_loss: 0.03342194855213165\n",
      "iteration 1107, dc_loss: 0.057617079466581345, tv_loss: 0.03343674913048744\n",
      "iteration 1108, dc_loss: 0.05761498957872391, tv_loss: 0.033356793224811554\n",
      "iteration 1109, dc_loss: 0.05739181488752365, tv_loss: 0.033531807363033295\n",
      "iteration 1110, dc_loss: 0.05760088935494423, tv_loss: 0.033272482454776764\n",
      "iteration 1111, dc_loss: 0.05720604583621025, tv_loss: 0.03365854173898697\n",
      "iteration 1112, dc_loss: 0.057508233934640884, tv_loss: 0.03326946869492531\n",
      "iteration 1113, dc_loss: 0.05702199786901474, tv_loss: 0.03368687257170677\n",
      "iteration 1114, dc_loss: 0.057354409247636795, tv_loss: 0.03323793411254883\n",
      "iteration 1115, dc_loss: 0.05689186230301857, tv_loss: 0.0336671806871891\n",
      "iteration 1116, dc_loss: 0.057155873626470566, tv_loss: 0.03329949826002121\n",
      "iteration 1117, dc_loss: 0.05677534639835358, tv_loss: 0.033598627895116806\n",
      "iteration 1118, dc_loss: 0.056851547211408615, tv_loss: 0.0333419032394886\n",
      "iteration 1119, dc_loss: 0.05653651803731918, tv_loss: 0.03349759429693222\n",
      "iteration 1120, dc_loss: 0.0564652644097805, tv_loss: 0.03342169150710106\n",
      "iteration 1121, dc_loss: 0.0563596747815609, tv_loss: 0.03342406079173088\n",
      "iteration 1122, dc_loss: 0.05622168257832527, tv_loss: 0.03349790349602699\n",
      "iteration 1123, dc_loss: 0.05623892322182655, tv_loss: 0.03344142064452171\n",
      "iteration 1124, dc_loss: 0.056014932692050934, tv_loss: 0.03357608616352081\n",
      "iteration 1125, dc_loss: 0.05603192374110222, tv_loss: 0.03340645879507065\n",
      "iteration 1126, dc_loss: 0.05580364540219307, tv_loss: 0.03354353830218315\n",
      "iteration 1127, dc_loss: 0.055889587849378586, tv_loss: 0.03337594121694565\n",
      "iteration 1128, dc_loss: 0.05570947378873825, tv_loss: 0.033509355038404465\n",
      "iteration 1129, dc_loss: 0.055666252970695496, tv_loss: 0.03346360847353935\n",
      "iteration 1130, dc_loss: 0.05552426725625992, tv_loss: 0.03352336958050728\n",
      "iteration 1131, dc_loss: 0.05539508908987045, tv_loss: 0.033537209033966064\n",
      "iteration 1132, dc_loss: 0.05530693382024765, tv_loss: 0.03347167745232582\n",
      "iteration 1133, dc_loss: 0.05522121116518974, tv_loss: 0.03350203111767769\n",
      "iteration 1134, dc_loss: 0.055174898356199265, tv_loss: 0.033523574471473694\n",
      "iteration 1135, dc_loss: 0.055053867399692535, tv_loss: 0.0335485078394413\n",
      "iteration 1136, dc_loss: 0.055012255907058716, tv_loss: 0.033467601984739304\n",
      "iteration 1137, dc_loss: 0.054876524955034256, tv_loss: 0.03352345898747444\n",
      "iteration 1138, dc_loss: 0.05485404655337334, tv_loss: 0.03352309763431549\n",
      "iteration 1139, dc_loss: 0.05472217872738838, tv_loss: 0.0335674062371254\n",
      "iteration 1140, dc_loss: 0.054678864777088165, tv_loss: 0.03347219526767731\n",
      "iteration 1141, dc_loss: 0.05454600974917412, tv_loss: 0.03353209048509598\n",
      "iteration 1142, dc_loss: 0.05449021980166435, tv_loss: 0.033554013818502426\n",
      "iteration 1143, dc_loss: 0.0543583482503891, tv_loss: 0.03357117995619774\n",
      "iteration 1144, dc_loss: 0.054337792098522186, tv_loss: 0.03347529098391533\n",
      "iteration 1145, dc_loss: 0.054193202406167984, tv_loss: 0.03361751511693001\n",
      "iteration 1146, dc_loss: 0.05418094992637634, tv_loss: 0.033510979264974594\n",
      "iteration 1147, dc_loss: 0.054015278816223145, tv_loss: 0.03355809673666954\n",
      "iteration 1148, dc_loss: 0.0540178120136261, tv_loss: 0.03353114798665047\n",
      "iteration 1149, dc_loss: 0.053843408823013306, tv_loss: 0.033632535487413406\n",
      "iteration 1150, dc_loss: 0.05386738106608391, tv_loss: 0.033471930772066116\n",
      "iteration 1151, dc_loss: 0.053675971925258636, tv_loss: 0.033639855682849884\n",
      "iteration 1152, dc_loss: 0.053755003958940506, tv_loss: 0.03349125385284424\n",
      "iteration 1153, dc_loss: 0.05352040007710457, tv_loss: 0.03361213952302933\n",
      "iteration 1154, dc_loss: 0.05362469330430031, tv_loss: 0.03344922140240669\n",
      "iteration 1155, dc_loss: 0.0533689484000206, tv_loss: 0.033689066767692566\n",
      "iteration 1156, dc_loss: 0.053576577454805374, tv_loss: 0.03341660276055336\n",
      "iteration 1157, dc_loss: 0.053245916962623596, tv_loss: 0.03370768576860428\n",
      "iteration 1158, dc_loss: 0.05357108637690544, tv_loss: 0.033348169177770615\n",
      "iteration 1159, dc_loss: 0.05319218337535858, tv_loss: 0.03378833830356598\n",
      "iteration 1160, dc_loss: 0.053524743765592575, tv_loss: 0.03335261344909668\n",
      "iteration 1161, dc_loss: 0.05304788053035736, tv_loss: 0.03374733030796051\n",
      "iteration 1162, dc_loss: 0.053273044526576996, tv_loss: 0.033382561057806015\n",
      "iteration 1163, dc_loss: 0.05283142626285553, tv_loss: 0.03371455892920494\n",
      "iteration 1164, dc_loss: 0.05289427191019058, tv_loss: 0.033456236124038696\n",
      "iteration 1165, dc_loss: 0.05260356143116951, tv_loss: 0.033586159348487854\n",
      "iteration 1166, dc_loss: 0.05250699073076248, tv_loss: 0.033603306859731674\n",
      "iteration 1167, dc_loss: 0.05259453132748604, tv_loss: 0.03345442935824394\n",
      "iteration 1168, dc_loss: 0.05235229432582855, tv_loss: 0.03367142379283905\n",
      "iteration 1169, dc_loss: 0.05258068069815636, tv_loss: 0.03342992067337036\n",
      "iteration 1170, dc_loss: 0.0522037073969841, tv_loss: 0.033764924854040146\n",
      "iteration 1171, dc_loss: 0.05238231644034386, tv_loss: 0.03344660997390747\n",
      "iteration 1172, dc_loss: 0.05201635882258415, tv_loss: 0.03368552774190903\n",
      "iteration 1173, dc_loss: 0.052067723125219345, tv_loss: 0.03353247418999672\n",
      "iteration 1174, dc_loss: 0.05192260816693306, tv_loss: 0.03358420357108116\n",
      "iteration 1175, dc_loss: 0.05183003470301628, tv_loss: 0.033603742718696594\n",
      "iteration 1176, dc_loss: 0.05188542231917381, tv_loss: 0.033490341156721115\n",
      "iteration 1177, dc_loss: 0.05164574459195137, tv_loss: 0.033694230020046234\n",
      "iteration 1178, dc_loss: 0.05185087025165558, tv_loss: 0.03342580795288086\n",
      "iteration 1179, dc_loss: 0.05150514096021652, tv_loss: 0.033717866986989975\n",
      "iteration 1180, dc_loss: 0.05158499628305435, tv_loss: 0.03353288769721985\n",
      "iteration 1181, dc_loss: 0.05135811120271683, tv_loss: 0.03363416716456413\n",
      "iteration 1182, dc_loss: 0.051339469850063324, tv_loss: 0.033558908849954605\n",
      "iteration 1183, dc_loss: 0.05129392072558403, tv_loss: 0.033534225076436996\n",
      "iteration 1184, dc_loss: 0.051118701696395874, tv_loss: 0.03368039429187775\n",
      "iteration 1185, dc_loss: 0.05125407129526138, tv_loss: 0.03352537378668785\n",
      "iteration 1186, dc_loss: 0.050964392721652985, tv_loss: 0.03373510390520096\n",
      "iteration 1187, dc_loss: 0.05108194798231125, tv_loss: 0.033510126173496246\n",
      "iteration 1188, dc_loss: 0.05084402114152908, tv_loss: 0.033681273460388184\n",
      "iteration 1189, dc_loss: 0.05093337967991829, tv_loss: 0.03357132896780968\n",
      "iteration 1190, dc_loss: 0.05069679021835327, tv_loss: 0.03369800001382828\n",
      "iteration 1191, dc_loss: 0.050778765231370926, tv_loss: 0.033535588532686234\n",
      "iteration 1192, dc_loss: 0.050576843321323395, tv_loss: 0.03370925411581993\n",
      "iteration 1193, dc_loss: 0.05063464865088463, tv_loss: 0.03360670059919357\n",
      "iteration 1194, dc_loss: 0.0504952110350132, tv_loss: 0.03363654762506485\n",
      "iteration 1195, dc_loss: 0.050509754568338394, tv_loss: 0.03360830619931221\n",
      "iteration 1196, dc_loss: 0.0504993200302124, tv_loss: 0.033648379147052765\n",
      "iteration 1197, dc_loss: 0.05054987594485283, tv_loss: 0.03366855904459953\n",
      "iteration 1198, dc_loss: 0.05044778436422348, tv_loss: 0.03365430608391762\n",
      "iteration 1199, dc_loss: 0.050597090274095535, tv_loss: 0.03356771171092987\n",
      "iteration 1200, dc_loss: 0.05041326582431793, tv_loss: 0.033741727471351624\n",
      "iteration 1201, dc_loss: 0.05044388398528099, tv_loss: 0.03356555476784706\n",
      "iteration 1202, dc_loss: 0.04997159168124199, tv_loss: 0.03364759311079979\n",
      "iteration 1203, dc_loss: 0.04998848959803581, tv_loss: 0.033648427575826645\n",
      "iteration 1204, dc_loss: 0.05010438710451126, tv_loss: 0.0336153544485569\n",
      "iteration 1205, dc_loss: 0.049827974289655685, tv_loss: 0.03366415947675705\n",
      "iteration 1206, dc_loss: 0.049785420298576355, tv_loss: 0.033623747527599335\n",
      "iteration 1207, dc_loss: 0.049855202436447144, tv_loss: 0.03360716626048088\n",
      "iteration 1208, dc_loss: 0.04961330443620682, tv_loss: 0.033671002835035324\n",
      "iteration 1209, dc_loss: 0.04961177706718445, tv_loss: 0.03362840414047241\n",
      "iteration 1210, dc_loss: 0.0495893619954586, tv_loss: 0.03363470360636711\n",
      "iteration 1211, dc_loss: 0.04947387054562569, tv_loss: 0.033639486879110336\n",
      "iteration 1212, dc_loss: 0.049423184245824814, tv_loss: 0.03364994004368782\n",
      "iteration 1213, dc_loss: 0.04939187690615654, tv_loss: 0.03364055976271629\n",
      "iteration 1214, dc_loss: 0.04927739128470421, tv_loss: 0.03365088626742363\n",
      "iteration 1215, dc_loss: 0.04925668612122536, tv_loss: 0.03364889323711395\n",
      "iteration 1216, dc_loss: 0.04917334020137787, tv_loss: 0.03365902975201607\n",
      "iteration 1217, dc_loss: 0.04914219677448273, tv_loss: 0.03361854329705238\n",
      "iteration 1218, dc_loss: 0.04905690625309944, tv_loss: 0.0336507223546505\n",
      "iteration 1219, dc_loss: 0.048997167497873306, tv_loss: 0.03367603197693825\n",
      "iteration 1220, dc_loss: 0.0489548034965992, tv_loss: 0.0336180254817009\n",
      "iteration 1221, dc_loss: 0.04889557510614395, tv_loss: 0.03365299478173256\n",
      "iteration 1222, dc_loss: 0.04882710427045822, tv_loss: 0.033674515783786774\n",
      "iteration 1223, dc_loss: 0.048807453364133835, tv_loss: 0.033614035695791245\n",
      "iteration 1224, dc_loss: 0.04869258776307106, tv_loss: 0.033666402101516724\n",
      "iteration 1225, dc_loss: 0.04866831749677658, tv_loss: 0.033664822578430176\n",
      "iteration 1226, dc_loss: 0.04863978177309036, tv_loss: 0.03363160789012909\n",
      "iteration 1227, dc_loss: 0.04853260517120361, tv_loss: 0.03366604819893837\n",
      "iteration 1228, dc_loss: 0.04852043464779854, tv_loss: 0.03361815959215164\n",
      "iteration 1229, dc_loss: 0.04847756773233414, tv_loss: 0.033615730702877045\n",
      "iteration 1230, dc_loss: 0.048370588570833206, tv_loss: 0.0336616113781929\n",
      "iteration 1231, dc_loss: 0.048367343842983246, tv_loss: 0.033613719046115875\n",
      "iteration 1232, dc_loss: 0.048283204436302185, tv_loss: 0.0336395762860775\n",
      "iteration 1233, dc_loss: 0.04822227358818054, tv_loss: 0.033678412437438965\n",
      "iteration 1234, dc_loss: 0.048219889402389526, tv_loss: 0.033635590225458145\n",
      "iteration 1235, dc_loss: 0.04812485724687576, tv_loss: 0.03365197032690048\n",
      "iteration 1236, dc_loss: 0.04806869477033615, tv_loss: 0.03366340696811676\n",
      "iteration 1237, dc_loss: 0.048060450702905655, tv_loss: 0.03363211452960968\n",
      "iteration 1238, dc_loss: 0.04794997349381447, tv_loss: 0.03368665650486946\n",
      "iteration 1239, dc_loss: 0.04792390391230583, tv_loss: 0.033672209829092026\n",
      "iteration 1240, dc_loss: 0.04791034013032913, tv_loss: 0.0336248055100441\n",
      "iteration 1241, dc_loss: 0.04779955372214317, tv_loss: 0.033693328499794006\n",
      "iteration 1242, dc_loss: 0.047782767564058304, tv_loss: 0.03365205600857735\n",
      "iteration 1243, dc_loss: 0.04773614928126335, tv_loss: 0.03363708779215813\n",
      "iteration 1244, dc_loss: 0.04765209183096886, tv_loss: 0.03369538113474846\n",
      "iteration 1245, dc_loss: 0.047644276171922684, tv_loss: 0.033656202256679535\n",
      "iteration 1246, dc_loss: 0.04757238179445267, tv_loss: 0.03365913778543472\n",
      "iteration 1247, dc_loss: 0.0475173182785511, tv_loss: 0.03367418795824051\n",
      "iteration 1248, dc_loss: 0.04747197404503822, tv_loss: 0.033652786165475845\n",
      "iteration 1249, dc_loss: 0.04741370677947998, tv_loss: 0.03365885093808174\n",
      "iteration 1250, dc_loss: 0.04738122224807739, tv_loss: 0.03366102650761604\n",
      "iteration 1251, dc_loss: 0.04731203243136406, tv_loss: 0.03369053453207016\n",
      "iteration 1252, dc_loss: 0.04726957529783249, tv_loss: 0.03369596600532532\n",
      "iteration 1253, dc_loss: 0.04722026363015175, tv_loss: 0.03367609530687332\n",
      "iteration 1254, dc_loss: 0.047171708196401596, tv_loss: 0.03366293013095856\n",
      "iteration 1255, dc_loss: 0.04711911454796791, tv_loss: 0.03367219492793083\n",
      "iteration 1256, dc_loss: 0.04706380516290665, tv_loss: 0.033689070492982864\n",
      "iteration 1257, dc_loss: 0.04702339321374893, tv_loss: 0.033694930374622345\n",
      "iteration 1258, dc_loss: 0.04696991667151451, tv_loss: 0.03368915989995003\n",
      "iteration 1259, dc_loss: 0.046928443014621735, tv_loss: 0.033669814467430115\n",
      "iteration 1260, dc_loss: 0.04686484485864639, tv_loss: 0.0336759090423584\n",
      "iteration 1261, dc_loss: 0.04683494195342064, tv_loss: 0.033667419105768204\n",
      "iteration 1262, dc_loss: 0.04678136110305786, tv_loss: 0.03368755802512169\n",
      "iteration 1263, dc_loss: 0.046707022935152054, tv_loss: 0.03373486548662186\n",
      "iteration 1264, dc_loss: 0.04669485241174698, tv_loss: 0.033685632050037384\n",
      "iteration 1265, dc_loss: 0.046632520854473114, tv_loss: 0.03367232903838158\n",
      "iteration 1266, dc_loss: 0.04657409340143204, tv_loss: 0.033682990819215775\n",
      "iteration 1267, dc_loss: 0.04653018340468407, tv_loss: 0.03368242830038071\n",
      "iteration 1268, dc_loss: 0.046489909291267395, tv_loss: 0.03369145840406418\n",
      "iteration 1269, dc_loss: 0.046443693339824677, tv_loss: 0.033720288425683975\n",
      "iteration 1270, dc_loss: 0.04638121277093887, tv_loss: 0.033714037388563156\n",
      "iteration 1271, dc_loss: 0.046353794634342194, tv_loss: 0.03367602825164795\n",
      "iteration 1272, dc_loss: 0.0462832897901535, tv_loss: 0.033693429082632065\n",
      "iteration 1273, dc_loss: 0.046245500445365906, tv_loss: 0.033699240535497665\n",
      "iteration 1274, dc_loss: 0.04621522128582001, tv_loss: 0.03370283171534538\n",
      "iteration 1275, dc_loss: 0.046134598553180695, tv_loss: 0.03374049440026283\n",
      "iteration 1276, dc_loss: 0.04612020403146744, tv_loss: 0.033692725002765656\n",
      "iteration 1277, dc_loss: 0.046049565076828, tv_loss: 0.03369138762354851\n",
      "iteration 1278, dc_loss: 0.04600849002599716, tv_loss: 0.03369307145476341\n",
      "iteration 1279, dc_loss: 0.04598356783390045, tv_loss: 0.03367239236831665\n",
      "iteration 1280, dc_loss: 0.045897673815488815, tv_loss: 0.033730052411556244\n",
      "iteration 1281, dc_loss: 0.045873433351516724, tv_loss: 0.03373658284544945\n",
      "iteration 1282, dc_loss: 0.04581693932414055, tv_loss: 0.03373177722096443\n",
      "iteration 1283, dc_loss: 0.0457855649292469, tv_loss: 0.03369172662496567\n",
      "iteration 1284, dc_loss: 0.04572587087750435, tv_loss: 0.03370824083685875\n",
      "iteration 1285, dc_loss: 0.045676011592149734, tv_loss: 0.03373321518301964\n",
      "iteration 1286, dc_loss: 0.04564765468239784, tv_loss: 0.033734869211912155\n",
      "iteration 1287, dc_loss: 0.045570697635412216, tv_loss: 0.03374788165092468\n",
      "iteration 1288, dc_loss: 0.04556296765804291, tv_loss: 0.033684514462947845\n",
      "iteration 1289, dc_loss: 0.045501090586185455, tv_loss: 0.03370662406086922\n",
      "iteration 1290, dc_loss: 0.04544215276837349, tv_loss: 0.03374234959483147\n",
      "iteration 1291, dc_loss: 0.04542132467031479, tv_loss: 0.03373541682958603\n",
      "iteration 1292, dc_loss: 0.04534175992012024, tv_loss: 0.033747199922800064\n",
      "iteration 1293, dc_loss: 0.045331504195928574, tv_loss: 0.03369603678584099\n",
      "iteration 1294, dc_loss: 0.04525228589773178, tv_loss: 0.033734578639268875\n",
      "iteration 1295, dc_loss: 0.045224275439977646, tv_loss: 0.03371834754943848\n",
      "iteration 1296, dc_loss: 0.04519921541213989, tv_loss: 0.0337224081158638\n",
      "iteration 1297, dc_loss: 0.04511788859963417, tv_loss: 0.03377065062522888\n",
      "iteration 1298, dc_loss: 0.04510451853275299, tv_loss: 0.03371860086917877\n",
      "iteration 1299, dc_loss: 0.045021697878837585, tv_loss: 0.03373519703745842\n",
      "iteration 1300, dc_loss: 0.04500490427017212, tv_loss: 0.03371580317616463\n",
      "iteration 1301, dc_loss: 0.044959280639886856, tv_loss: 0.033711593598127365\n",
      "iteration 1302, dc_loss: 0.044900793582201004, tv_loss: 0.03372805938124657\n",
      "iteration 1303, dc_loss: 0.04486791044473648, tv_loss: 0.033717162907123566\n",
      "iteration 1304, dc_loss: 0.044819075614213943, tv_loss: 0.03373384848237038\n",
      "iteration 1305, dc_loss: 0.044758688658475876, tv_loss: 0.03377402573823929\n",
      "iteration 1306, dc_loss: 0.04474615305662155, tv_loss: 0.03373412787914276\n",
      "iteration 1307, dc_loss: 0.04466888681054115, tv_loss: 0.033742792904376984\n",
      "iteration 1308, dc_loss: 0.044661492109298706, tv_loss: 0.0337132029235363\n",
      "iteration 1309, dc_loss: 0.04458164796233177, tv_loss: 0.03374718502163887\n",
      "iteration 1310, dc_loss: 0.044553905725479126, tv_loss: 0.03373951464891434\n",
      "iteration 1311, dc_loss: 0.04452751576900482, tv_loss: 0.033751342445611954\n",
      "iteration 1312, dc_loss: 0.04444284364581108, tv_loss: 0.033790260553359985\n",
      "iteration 1313, dc_loss: 0.044439367949962616, tv_loss: 0.033714745193719864\n",
      "iteration 1314, dc_loss: 0.044370513409376144, tv_loss: 0.03373928368091583\n",
      "iteration 1315, dc_loss: 0.044334422796964645, tv_loss: 0.033749014139175415\n",
      "iteration 1316, dc_loss: 0.04429367929697037, tv_loss: 0.033761195838451385\n",
      "iteration 1317, dc_loss: 0.04423018544912338, tv_loss: 0.03377913311123848\n",
      "iteration 1318, dc_loss: 0.04423856362700462, tv_loss: 0.033710677176713943\n",
      "iteration 1319, dc_loss: 0.044133421033620834, tv_loss: 0.03376644104719162\n",
      "iteration 1320, dc_loss: 0.044130194932222366, tv_loss: 0.03372197598218918\n",
      "iteration 1321, dc_loss: 0.04407532513141632, tv_loss: 0.03374861553311348\n",
      "iteration 1322, dc_loss: 0.04401378333568573, tv_loss: 0.03377186134457588\n",
      "iteration 1323, dc_loss: 0.043998897075653076, tv_loss: 0.03375597298145294\n",
      "iteration 1324, dc_loss: 0.04393208771944046, tv_loss: 0.033759620040655136\n",
      "iteration 1325, dc_loss: 0.04390330985188484, tv_loss: 0.03373977914452553\n",
      "iteration 1326, dc_loss: 0.0438615083694458, tv_loss: 0.03374296426773071\n",
      "iteration 1327, dc_loss: 0.043809086084365845, tv_loss: 0.03375430032610893\n",
      "iteration 1328, dc_loss: 0.04377589002251625, tv_loss: 0.03375886753201485\n",
      "iteration 1329, dc_loss: 0.04373488947749138, tv_loss: 0.03378579765558243\n",
      "iteration 1330, dc_loss: 0.04367204010486603, tv_loss: 0.03378082439303398\n",
      "iteration 1331, dc_loss: 0.04366501048207283, tv_loss: 0.033730264753103256\n",
      "iteration 1332, dc_loss: 0.04358606040477753, tv_loss: 0.03379395231604576\n",
      "iteration 1333, dc_loss: 0.043576423078775406, tv_loss: 0.03377757593989372\n",
      "iteration 1334, dc_loss: 0.0435221791267395, tv_loss: 0.03377222642302513\n",
      "iteration 1335, dc_loss: 0.04345456138253212, tv_loss: 0.03378352150321007\n",
      "iteration 1336, dc_loss: 0.04346606507897377, tv_loss: 0.033728692680597305\n",
      "iteration 1337, dc_loss: 0.043362341821193695, tv_loss: 0.03378988057374954\n",
      "iteration 1338, dc_loss: 0.04337938129901886, tv_loss: 0.033734045922756195\n",
      "iteration 1339, dc_loss: 0.043312136083841324, tv_loss: 0.03377356007695198\n",
      "iteration 1340, dc_loss: 0.043258700519800186, tv_loss: 0.033793505281209946\n",
      "iteration 1341, dc_loss: 0.043250687420368195, tv_loss: 0.03375987708568573\n",
      "iteration 1342, dc_loss: 0.04315552860498428, tv_loss: 0.033804234117269516\n",
      "iteration 1343, dc_loss: 0.04317786544561386, tv_loss: 0.033725325018167496\n",
      "iteration 1344, dc_loss: 0.04309173673391342, tv_loss: 0.03377287834882736\n",
      "iteration 1345, dc_loss: 0.043061934411525726, tv_loss: 0.03377839922904968\n",
      "iteration 1346, dc_loss: 0.043031465262174606, tv_loss: 0.033775489777326584\n",
      "iteration 1347, dc_loss: 0.042967576533555984, tv_loss: 0.033799219876527786\n",
      "iteration 1348, dc_loss: 0.042984168976545334, tv_loss: 0.03374123200774193\n",
      "iteration 1349, dc_loss: 0.04286037012934685, tv_loss: 0.033808521926403046\n",
      "iteration 1350, dc_loss: 0.04289180040359497, tv_loss: 0.03374365717172623\n",
      "iteration 1351, dc_loss: 0.04281112179160118, tv_loss: 0.03377983346581459\n",
      "iteration 1352, dc_loss: 0.042758263647556305, tv_loss: 0.033792149275541306\n",
      "iteration 1353, dc_loss: 0.04275602847337723, tv_loss: 0.033752135932445526\n",
      "iteration 1354, dc_loss: 0.042685624212026596, tv_loss: 0.03380211815237999\n",
      "iteration 1355, dc_loss: 0.042678795754909515, tv_loss: 0.03377453237771988\n",
      "iteration 1356, dc_loss: 0.042596329003572464, tv_loss: 0.03380192816257477\n",
      "iteration 1357, dc_loss: 0.04257553443312645, tv_loss: 0.033783819526433945\n",
      "iteration 1358, dc_loss: 0.042546406388282776, tv_loss: 0.033752817660570145\n",
      "iteration 1359, dc_loss: 0.042472001165151596, tv_loss: 0.03380462899804115\n",
      "iteration 1360, dc_loss: 0.04247565567493439, tv_loss: 0.03377695381641388\n",
      "iteration 1361, dc_loss: 0.042413413524627686, tv_loss: 0.0338163860142231\n",
      "iteration 1362, dc_loss: 0.04237142577767372, tv_loss: 0.033803537487983704\n",
      "iteration 1363, dc_loss: 0.042344123125076294, tv_loss: 0.033775415271520615\n",
      "iteration 1364, dc_loss: 0.042283132672309875, tv_loss: 0.03379559516906738\n",
      "iteration 1365, dc_loss: 0.042289771139621735, tv_loss: 0.03374931961297989\n",
      "iteration 1366, dc_loss: 0.04218463599681854, tv_loss: 0.03381132706999779\n",
      "iteration 1367, dc_loss: 0.04220924153923988, tv_loss: 0.033750299364328384\n",
      "iteration 1368, dc_loss: 0.04212921857833862, tv_loss: 0.0338003896176815\n",
      "iteration 1369, dc_loss: 0.042112983763217926, tv_loss: 0.03381636366248131\n",
      "iteration 1370, dc_loss: 0.042055144906044006, tv_loss: 0.033829815685749054\n",
      "iteration 1371, dc_loss: 0.04201526567339897, tv_loss: 0.033791329711675644\n",
      "iteration 1372, dc_loss: 0.04199102520942688, tv_loss: 0.03378310799598694\n",
      "iteration 1373, dc_loss: 0.04192528501152992, tv_loss: 0.033830758184194565\n",
      "iteration 1374, dc_loss: 0.04191288352012634, tv_loss: 0.03381037712097168\n",
      "iteration 1375, dc_loss: 0.04185732454061508, tv_loss: 0.033824119716882706\n",
      "iteration 1376, dc_loss: 0.04185270518064499, tv_loss: 0.03376572206616402\n",
      "iteration 1377, dc_loss: 0.041773173958063126, tv_loss: 0.03380690515041351\n",
      "iteration 1378, dc_loss: 0.04176333174109459, tv_loss: 0.0337817445397377\n",
      "iteration 1379, dc_loss: 0.04171673581004143, tv_loss: 0.03380129486322403\n",
      "iteration 1380, dc_loss: 0.04165640473365784, tv_loss: 0.03384057804942131\n",
      "iteration 1381, dc_loss: 0.04166579619050026, tv_loss: 0.03380177170038223\n",
      "iteration 1382, dc_loss: 0.041566696017980576, tv_loss: 0.03385018929839134\n",
      "iteration 1383, dc_loss: 0.041629865765571594, tv_loss: 0.03373711183667183\n",
      "iteration 1384, dc_loss: 0.04147075116634369, tv_loss: 0.03386669233441353\n",
      "iteration 1385, dc_loss: 0.04157283529639244, tv_loss: 0.03372545167803764\n",
      "iteration 1386, dc_loss: 0.04139818251132965, tv_loss: 0.03385152667760849\n",
      "iteration 1387, dc_loss: 0.041494350880384445, tv_loss: 0.033729568123817444\n",
      "iteration 1388, dc_loss: 0.04133455455303192, tv_loss: 0.0338544026017189\n",
      "iteration 1389, dc_loss: 0.04142781347036362, tv_loss: 0.033731311559677124\n",
      "iteration 1390, dc_loss: 0.041282474994659424, tv_loss: 0.0338791124522686\n",
      "iteration 1391, dc_loss: 0.041377101093530655, tv_loss: 0.033782847225666046\n",
      "iteration 1392, dc_loss: 0.04122287780046463, tv_loss: 0.03390763700008392\n",
      "iteration 1393, dc_loss: 0.04133657366037369, tv_loss: 0.033726807683706284\n",
      "iteration 1394, dc_loss: 0.041122592985630035, tv_loss: 0.033906083554029465\n",
      "iteration 1395, dc_loss: 0.04126719757914543, tv_loss: 0.03370482474565506\n",
      "iteration 1396, dc_loss: 0.04105363413691521, tv_loss: 0.03389497846364975\n",
      "iteration 1397, dc_loss: 0.04117511212825775, tv_loss: 0.033741191029548645\n",
      "iteration 1398, dc_loss: 0.04096490144729614, tv_loss: 0.03392355144023895\n",
      "iteration 1399, dc_loss: 0.041047077625989914, tv_loss: 0.03376682475209236\n",
      "iteration 1400, dc_loss: 0.04090261831879616, tv_loss: 0.033846415579319\n",
      "iteration 1401, dc_loss: 0.04090161621570587, tv_loss: 0.03379356116056442\n",
      "iteration 1402, dc_loss: 0.040850888937711716, tv_loss: 0.033815737813711166\n",
      "iteration 1403, dc_loss: 0.04080682247877121, tv_loss: 0.03382941707968712\n",
      "iteration 1404, dc_loss: 0.04080040007829666, tv_loss: 0.03380177542567253\n",
      "iteration 1405, dc_loss: 0.04071449115872383, tv_loss: 0.03386359661817551\n",
      "iteration 1406, dc_loss: 0.04076600447297096, tv_loss: 0.0337856225669384\n",
      "iteration 1407, dc_loss: 0.04062535986304283, tv_loss: 0.03388933464884758\n",
      "iteration 1408, dc_loss: 0.04069560393691063, tv_loss: 0.03378069028258324\n",
      "iteration 1409, dc_loss: 0.040553852915763855, tv_loss: 0.033879250288009644\n",
      "iteration 1410, dc_loss: 0.04062960669398308, tv_loss: 0.03376433625817299\n",
      "iteration 1411, dc_loss: 0.040485307574272156, tv_loss: 0.03387832269072533\n",
      "iteration 1412, dc_loss: 0.04056818410754204, tv_loss: 0.03375016525387764\n",
      "iteration 1413, dc_loss: 0.04041587933897972, tv_loss: 0.03387148305773735\n",
      "iteration 1414, dc_loss: 0.04048903286457062, tv_loss: 0.03377290815114975\n",
      "iteration 1415, dc_loss: 0.04034990072250366, tv_loss: 0.03387612849473953\n",
      "iteration 1416, dc_loss: 0.04040931537747383, tv_loss: 0.033804845064878464\n",
      "iteration 1417, dc_loss: 0.04028687998652458, tv_loss: 0.03390593081712723\n",
      "iteration 1418, dc_loss: 0.040362436324357986, tv_loss: 0.033783599734306335\n",
      "iteration 1419, dc_loss: 0.04019981250166893, tv_loss: 0.03389803692698479\n",
      "iteration 1420, dc_loss: 0.040284059941768646, tv_loss: 0.0337749719619751\n",
      "iteration 1421, dc_loss: 0.040147535502910614, tv_loss: 0.03387885168194771\n",
      "iteration 1422, dc_loss: 0.04020580276846886, tv_loss: 0.03377337008714676\n",
      "iteration 1423, dc_loss: 0.04006825014948845, tv_loss: 0.03387720510363579\n",
      "iteration 1424, dc_loss: 0.04014185070991516, tv_loss: 0.03378986194729805\n",
      "iteration 1425, dc_loss: 0.03998256474733353, tv_loss: 0.03392072021961212\n",
      "iteration 1426, dc_loss: 0.04007937014102936, tv_loss: 0.03377874568104744\n",
      "iteration 1427, dc_loss: 0.039931073784828186, tv_loss: 0.033899519592523575\n",
      "iteration 1428, dc_loss: 0.040011536329984665, tv_loss: 0.03376854956150055\n",
      "iteration 1429, dc_loss: 0.039851993322372437, tv_loss: 0.033899035304784775\n",
      "iteration 1430, dc_loss: 0.039960600435733795, tv_loss: 0.033766862004995346\n",
      "iteration 1431, dc_loss: 0.039782531559467316, tv_loss: 0.03391512483358383\n",
      "iteration 1432, dc_loss: 0.03991621360182762, tv_loss: 0.033747412264347076\n",
      "iteration 1433, dc_loss: 0.03972664102911949, tv_loss: 0.033919110894203186\n",
      "iteration 1434, dc_loss: 0.03987830877304077, tv_loss: 0.03374943509697914\n",
      "iteration 1435, dc_loss: 0.03966004028916359, tv_loss: 0.0339771993458271\n",
      "iteration 1436, dc_loss: 0.03981827199459076, tv_loss: 0.033767230808734894\n",
      "iteration 1437, dc_loss: 0.039586614817380905, tv_loss: 0.03394828736782074\n",
      "iteration 1438, dc_loss: 0.03975106030702591, tv_loss: 0.033725183457136154\n",
      "iteration 1439, dc_loss: 0.03951110318303108, tv_loss: 0.03392814099788666\n",
      "iteration 1440, dc_loss: 0.03963826596736908, tv_loss: 0.03374793380498886\n",
      "iteration 1441, dc_loss: 0.03945353627204895, tv_loss: 0.03389831632375717\n",
      "iteration 1442, dc_loss: 0.03954347223043442, tv_loss: 0.033790118992328644\n",
      "iteration 1443, dc_loss: 0.039371270686388016, tv_loss: 0.03393927589058876\n",
      "iteration 1444, dc_loss: 0.03942300006747246, tv_loss: 0.03383565694093704\n",
      "iteration 1445, dc_loss: 0.039346903562545776, tv_loss: 0.03385640308260918\n",
      "iteration 1446, dc_loss: 0.03929678350687027, tv_loss: 0.03386535495519638\n",
      "iteration 1447, dc_loss: 0.03932610899209976, tv_loss: 0.03381022438406944\n",
      "iteration 1448, dc_loss: 0.0392390675842762, tv_loss: 0.0338633731007576\n",
      "iteration 1449, dc_loss: 0.03926708921790123, tv_loss: 0.03380601853132248\n",
      "iteration 1450, dc_loss: 0.03915303200483322, tv_loss: 0.03390192613005638\n",
      "iteration 1451, dc_loss: 0.03924008458852768, tv_loss: 0.033826254308223724\n",
      "iteration 1452, dc_loss: 0.039083417505025864, tv_loss: 0.03395487740635872\n",
      "iteration 1453, dc_loss: 0.03919551894068718, tv_loss: 0.033793047070503235\n",
      "iteration 1454, dc_loss: 0.039020437747240067, tv_loss: 0.03392511233687401\n",
      "iteration 1455, dc_loss: 0.039136290550231934, tv_loss: 0.033795785158872604\n",
      "iteration 1456, dc_loss: 0.03896300122141838, tv_loss: 0.03395076096057892\n",
      "iteration 1457, dc_loss: 0.03908366709947586, tv_loss: 0.03379199281334877\n",
      "iteration 1458, dc_loss: 0.038900766521692276, tv_loss: 0.03390897065401077\n",
      "iteration 1459, dc_loss: 0.03895711153745651, tv_loss: 0.033817026764154434\n",
      "iteration 1460, dc_loss: 0.03885035216808319, tv_loss: 0.0338703878223896\n",
      "iteration 1461, dc_loss: 0.03884925693273544, tv_loss: 0.033849623054265976\n",
      "iteration 1462, dc_loss: 0.038811229169368744, tv_loss: 0.03387652337551117\n",
      "iteration 1463, dc_loss: 0.038766779005527496, tv_loss: 0.033897314220666885\n",
      "iteration 1464, dc_loss: 0.03877608850598335, tv_loss: 0.03385656699538231\n",
      "iteration 1465, dc_loss: 0.038683827966451645, tv_loss: 0.03389732539653778\n",
      "iteration 1466, dc_loss: 0.03874872624874115, tv_loss: 0.03381224721670151\n",
      "iteration 1467, dc_loss: 0.03861381858587265, tv_loss: 0.03393298014998436\n",
      "iteration 1468, dc_loss: 0.038758955895900726, tv_loss: 0.03378654271364212\n",
      "iteration 1469, dc_loss: 0.038556020706892014, tv_loss: 0.0339965894818306\n",
      "iteration 1470, dc_loss: 0.038787174969911575, tv_loss: 0.033759478479623795\n",
      "iteration 1471, dc_loss: 0.03851131722331047, tv_loss: 0.03402402624487877\n",
      "iteration 1472, dc_loss: 0.03880290314555168, tv_loss: 0.0337115079164505\n",
      "iteration 1473, dc_loss: 0.038449324667453766, tv_loss: 0.034065477550029755\n",
      "iteration 1474, dc_loss: 0.03876776993274689, tv_loss: 0.03370114788413048\n",
      "iteration 1475, dc_loss: 0.03836079686880112, tv_loss: 0.03406171128153801\n",
      "iteration 1476, dc_loss: 0.03858138248324394, tv_loss: 0.033742114901542664\n",
      "iteration 1477, dc_loss: 0.038261644542217255, tv_loss: 0.033962808549404144\n",
      "iteration 1478, dc_loss: 0.03834294155240059, tv_loss: 0.033802103251218796\n",
      "iteration 1479, dc_loss: 0.0382327102124691, tv_loss: 0.0338732935488224\n",
      "iteration 1480, dc_loss: 0.03819321468472481, tv_loss: 0.03388978913426399\n",
      "iteration 1481, dc_loss: 0.038257941603660583, tv_loss: 0.03380260244011879\n",
      "iteration 1482, dc_loss: 0.03810754045844078, tv_loss: 0.033936306834220886\n",
      "iteration 1483, dc_loss: 0.03826215863227844, tv_loss: 0.033770449459552765\n",
      "iteration 1484, dc_loss: 0.03804726153612137, tv_loss: 0.03400256484746933\n",
      "iteration 1485, dc_loss: 0.03816315531730652, tv_loss: 0.033817898482084274\n",
      "iteration 1486, dc_loss: 0.03801017999649048, tv_loss: 0.03391800448298454\n",
      "iteration 1487, dc_loss: 0.03800755366683006, tv_loss: 0.0338832288980484\n",
      "iteration 1488, dc_loss: 0.03803609311580658, tv_loss: 0.033834103494882584\n",
      "iteration 1489, dc_loss: 0.03789925575256348, tv_loss: 0.033997200429439545\n",
      "iteration 1490, dc_loss: 0.0380110926926136, tv_loss: 0.03381911292672157\n",
      "iteration 1491, dc_loss: 0.03782396763563156, tv_loss: 0.03395804017782211\n",
      "iteration 1492, dc_loss: 0.037953075021505356, tv_loss: 0.03382232040166855\n",
      "iteration 1493, dc_loss: 0.03775655850768089, tv_loss: 0.03398645296692848\n",
      "iteration 1494, dc_loss: 0.03786598891019821, tv_loss: 0.033849071711301804\n",
      "iteration 1495, dc_loss: 0.03772738575935364, tv_loss: 0.03390752524137497\n",
      "iteration 1496, dc_loss: 0.03775233030319214, tv_loss: 0.033870287239551544\n",
      "iteration 1497, dc_loss: 0.03768272325396538, tv_loss: 0.033917661756277084\n",
      "iteration 1498, dc_loss: 0.037646979093551636, tv_loss: 0.03391861170530319\n",
      "iteration 1499, dc_loss: 0.037662237882614136, tv_loss: 0.033858705312013626\n",
      "iteration 1500, dc_loss: 0.03755418583750725, tv_loss: 0.0339297391474247\n",
      "iteration 1501, dc_loss: 0.03767695277929306, tv_loss: 0.03382068872451782\n",
      "iteration 1502, dc_loss: 0.03748767450451851, tv_loss: 0.03399171680212021\n",
      "iteration 1503, dc_loss: 0.037610143423080444, tv_loss: 0.033816538751125336\n",
      "iteration 1504, dc_loss: 0.03742663189768791, tv_loss: 0.033963482826948166\n",
      "iteration 1505, dc_loss: 0.0375196635723114, tv_loss: 0.033832404762506485\n",
      "iteration 1506, dc_loss: 0.03737899288535118, tv_loss: 0.033944737166166306\n",
      "iteration 1507, dc_loss: 0.037467118352651596, tv_loss: 0.03382977098226547\n",
      "iteration 1508, dc_loss: 0.03733896464109421, tv_loss: 0.03393974155187607\n",
      "iteration 1509, dc_loss: 0.03738997504115105, tv_loss: 0.033880677074193954\n",
      "iteration 1510, dc_loss: 0.03731519356369972, tv_loss: 0.03393985331058502\n",
      "iteration 1511, dc_loss: 0.03738198056817055, tv_loss: 0.03384438902139664\n",
      "iteration 1512, dc_loss: 0.037272389978170395, tv_loss: 0.03394676372408867\n",
      "iteration 1513, dc_loss: 0.037366826087236404, tv_loss: 0.033851396292448044\n",
      "iteration 1514, dc_loss: 0.03731765225529671, tv_loss: 0.033930376172065735\n",
      "iteration 1515, dc_loss: 0.03739986568689346, tv_loss: 0.0338437519967556\n",
      "iteration 1516, dc_loss: 0.037402473390102386, tv_loss: 0.03392685204744339\n",
      "iteration 1517, dc_loss: 0.03743593767285347, tv_loss: 0.03389477729797363\n",
      "iteration 1518, dc_loss: 0.037416841834783554, tv_loss: 0.0339360237121582\n",
      "iteration 1519, dc_loss: 0.03729711472988129, tv_loss: 0.03392704948782921\n",
      "iteration 1520, dc_loss: 0.03725460171699524, tv_loss: 0.0338381789624691\n",
      "iteration 1521, dc_loss: 0.03699909523129463, tv_loss: 0.03395844250917435\n",
      "iteration 1522, dc_loss: 0.0370502844452858, tv_loss: 0.033826690167188644\n",
      "iteration 1523, dc_loss: 0.03690338507294655, tv_loss: 0.03396529704332352\n",
      "iteration 1524, dc_loss: 0.037066828459501266, tv_loss: 0.033852189779281616\n",
      "iteration 1525, dc_loss: 0.03695622459053993, tv_loss: 0.033974841237068176\n",
      "iteration 1526, dc_loss: 0.037032611668109894, tv_loss: 0.03384649381041527\n",
      "iteration 1527, dc_loss: 0.03687093034386635, tv_loss: 0.0339425690472126\n",
      "iteration 1528, dc_loss: 0.03680599853396416, tv_loss: 0.03391370549798012\n",
      "iteration 1529, dc_loss: 0.03678521141409874, tv_loss: 0.033923838287591934\n",
      "iteration 1530, dc_loss: 0.036723602563142776, tv_loss: 0.03394332155585289\n",
      "iteration 1531, dc_loss: 0.03678569570183754, tv_loss: 0.03388496860861778\n",
      "iteration 1532, dc_loss: 0.03673464059829712, tv_loss: 0.033937618136405945\n",
      "iteration 1533, dc_loss: 0.03676847368478775, tv_loss: 0.03384890779852867\n",
      "iteration 1534, dc_loss: 0.0365951769053936, tv_loss: 0.0339861735701561\n",
      "iteration 1535, dc_loss: 0.036662351340055466, tv_loss: 0.033849187195301056\n",
      "iteration 1536, dc_loss: 0.03653787821531296, tv_loss: 0.03394578769803047\n",
      "iteration 1537, dc_loss: 0.036621227860450745, tv_loss: 0.03386237472295761\n",
      "iteration 1538, dc_loss: 0.03649262711405754, tv_loss: 0.0339813306927681\n",
      "iteration 1539, dc_loss: 0.03659908473491669, tv_loss: 0.03385613113641739\n",
      "iteration 1540, dc_loss: 0.03644867613911629, tv_loss: 0.03394876793026924\n",
      "iteration 1541, dc_loss: 0.036495186388492584, tv_loss: 0.033851757645606995\n",
      "iteration 1542, dc_loss: 0.03635840490460396, tv_loss: 0.03395265340805054\n",
      "iteration 1543, dc_loss: 0.036422234028577805, tv_loss: 0.03386398404836655\n",
      "iteration 1544, dc_loss: 0.03630455583333969, tv_loss: 0.03393929824233055\n",
      "iteration 1545, dc_loss: 0.036393243819475174, tv_loss: 0.03383402153849602\n",
      "iteration 1546, dc_loss: 0.03626958653330803, tv_loss: 0.03395288065075874\n",
      "iteration 1547, dc_loss: 0.03633567690849304, tv_loss: 0.03386200964450836\n",
      "iteration 1548, dc_loss: 0.036220844835042953, tv_loss: 0.0339394211769104\n",
      "iteration 1549, dc_loss: 0.03624469041824341, tv_loss: 0.033868446946144104\n",
      "iteration 1550, dc_loss: 0.036140356212854385, tv_loss: 0.03395643085241318\n",
      "iteration 1551, dc_loss: 0.036183152347803116, tv_loss: 0.03391927108168602\n",
      "iteration 1552, dc_loss: 0.03609200939536095, tv_loss: 0.033988628536462784\n",
      "iteration 1553, dc_loss: 0.036140430718660355, tv_loss: 0.03387707471847534\n",
      "iteration 1554, dc_loss: 0.0360458567738533, tv_loss: 0.033962927758693695\n",
      "iteration 1555, dc_loss: 0.036093030124902725, tv_loss: 0.03391672298312187\n",
      "iteration 1556, dc_loss: 0.035990893840789795, tv_loss: 0.03396853059530258\n",
      "iteration 1557, dc_loss: 0.03601427376270294, tv_loss: 0.033899128437042236\n",
      "iteration 1558, dc_loss: 0.03596033900976181, tv_loss: 0.03392336517572403\n",
      "iteration 1559, dc_loss: 0.03592999652028084, tv_loss: 0.033948998898267746\n",
      "iteration 1560, dc_loss: 0.03588784486055374, tv_loss: 0.033953066915273666\n",
      "iteration 1561, dc_loss: 0.035897232592105865, tv_loss: 0.0339043103158474\n",
      "iteration 1562, dc_loss: 0.035839200019836426, tv_loss: 0.033935945481061935\n",
      "iteration 1563, dc_loss: 0.035831499844789505, tv_loss: 0.033921707421541214\n",
      "iteration 1564, dc_loss: 0.03578701242804527, tv_loss: 0.0339658297598362\n",
      "iteration 1565, dc_loss: 0.035782843828201294, tv_loss: 0.033930618315935135\n",
      "iteration 1566, dc_loss: 0.03574822098016739, tv_loss: 0.03393053635954857\n",
      "iteration 1567, dc_loss: 0.035712432116270065, tv_loss: 0.03393978625535965\n",
      "iteration 1568, dc_loss: 0.035732876509428024, tv_loss: 0.03388246148824692\n",
      "iteration 1569, dc_loss: 0.0356447696685791, tv_loss: 0.03395682945847511\n",
      "iteration 1570, dc_loss: 0.03566093370318413, tv_loss: 0.033910997211933136\n",
      "iteration 1571, dc_loss: 0.035606492310762405, tv_loss: 0.03393293917179108\n",
      "iteration 1572, dc_loss: 0.03562868759036064, tv_loss: 0.03388930857181549\n",
      "iteration 1573, dc_loss: 0.03553120046854019, tv_loss: 0.03396078199148178\n",
      "iteration 1574, dc_loss: 0.03559943288564682, tv_loss: 0.0338718481361866\n",
      "iteration 1575, dc_loss: 0.035456765443086624, tv_loss: 0.03399602696299553\n",
      "iteration 1576, dc_loss: 0.03562343493103981, tv_loss: 0.0338253453373909\n",
      "iteration 1577, dc_loss: 0.03539272025227547, tv_loss: 0.03405488654971123\n",
      "iteration 1578, dc_loss: 0.035665590316057205, tv_loss: 0.03380561247467995\n",
      "iteration 1579, dc_loss: 0.03539034351706505, tv_loss: 0.034120600670576096\n",
      "iteration 1580, dc_loss: 0.03590890020132065, tv_loss: 0.03368310257792473\n",
      "iteration 1581, dc_loss: 0.03552393242716789, tv_loss: 0.03426896408200264\n",
      "iteration 1582, dc_loss: 0.03629976511001587, tv_loss: 0.03361235558986664\n",
      "iteration 1583, dc_loss: 0.035696182399988174, tv_loss: 0.034349240362644196\n",
      "iteration 1584, dc_loss: 0.03611329570412636, tv_loss: 0.033638738095760345\n",
      "iteration 1585, dc_loss: 0.03532439470291138, tv_loss: 0.0341494083404541\n",
      "iteration 1586, dc_loss: 0.03536786139011383, tv_loss: 0.033868297934532166\n",
      "iteration 1587, dc_loss: 0.035371001809835434, tv_loss: 0.033861663192510605\n",
      "iteration 1588, dc_loss: 0.035196702927351, tv_loss: 0.03418031707406044\n",
      "iteration 1589, dc_loss: 0.03571111708879471, tv_loss: 0.03366776928305626\n",
      "iteration 1590, dc_loss: 0.03510918468236923, tv_loss: 0.03414022549986839\n",
      "iteration 1591, dc_loss: 0.035215701907873154, tv_loss: 0.03384793922305107\n",
      "iteration 1592, dc_loss: 0.03520618751645088, tv_loss: 0.03383587673306465\n",
      "iteration 1593, dc_loss: 0.03502015024423599, tv_loss: 0.03409450873732567\n",
      "iteration 1594, dc_loss: 0.03535942733287811, tv_loss: 0.033769313246011734\n",
      "iteration 1595, dc_loss: 0.03498636558651924, tv_loss: 0.03406615927815437\n",
      "iteration 1596, dc_loss: 0.03501860797405243, tv_loss: 0.03393698111176491\n",
      "iteration 1597, dc_loss: 0.03507646545767784, tv_loss: 0.033865537494421005\n",
      "iteration 1598, dc_loss: 0.03491395339369774, tv_loss: 0.03404923900961876\n",
      "iteration 1599, dc_loss: 0.035124458372592926, tv_loss: 0.03381909802556038\n",
      "iteration 1600, dc_loss: 0.03486030176281929, tv_loss: 0.034017834812402725\n",
      "iteration 1601, dc_loss: 0.03487824276089668, tv_loss: 0.03393780067563057\n",
      "iteration 1602, dc_loss: 0.03492221236228943, tv_loss: 0.033857639878988266\n",
      "iteration 1603, dc_loss: 0.03479325398802757, tv_loss: 0.033958550542593\n",
      "iteration 1604, dc_loss: 0.03479135036468506, tv_loss: 0.033934012055397034\n",
      "iteration 1605, dc_loss: 0.03485892713069916, tv_loss: 0.03386141359806061\n",
      "iteration 1606, dc_loss: 0.03474578261375427, tv_loss: 0.03395720198750496\n",
      "iteration 1607, dc_loss: 0.03472120314836502, tv_loss: 0.03395861014723778\n",
      "iteration 1608, dc_loss: 0.034776490181684494, tv_loss: 0.033879354596138\n",
      "iteration 1609, dc_loss: 0.03468727320432663, tv_loss: 0.03392772376537323\n",
      "iteration 1610, dc_loss: 0.03467424586415291, tv_loss: 0.033936794847249985\n",
      "iteration 1611, dc_loss: 0.03470822423696518, tv_loss: 0.03390219435095787\n",
      "iteration 1612, dc_loss: 0.0346301905810833, tv_loss: 0.033950258046388626\n",
      "iteration 1613, dc_loss: 0.03460939601063728, tv_loss: 0.03393461927771568\n",
      "iteration 1614, dc_loss: 0.03464072570204735, tv_loss: 0.03388708829879761\n",
      "iteration 1615, dc_loss: 0.03456674888730049, tv_loss: 0.03393464908003807\n",
      "iteration 1616, dc_loss: 0.034546028822660446, tv_loss: 0.033955641090869904\n",
      "iteration 1617, dc_loss: 0.03457861766219139, tv_loss: 0.0338924303650856\n",
      "iteration 1618, dc_loss: 0.034513458609580994, tv_loss: 0.033928170800209045\n",
      "iteration 1619, dc_loss: 0.034487854689359665, tv_loss: 0.033932801336050034\n",
      "iteration 1620, dc_loss: 0.03451862931251526, tv_loss: 0.03390539810061455\n",
      "iteration 1621, dc_loss: 0.034458525478839874, tv_loss: 0.033975474536418915\n",
      "iteration 1622, dc_loss: 0.0344189815223217, tv_loss: 0.0339578241109848\n",
      "iteration 1623, dc_loss: 0.03446156904101372, tv_loss: 0.03388771414756775\n",
      "iteration 1624, dc_loss: 0.0344092883169651, tv_loss: 0.03394872695207596\n",
      "iteration 1625, dc_loss: 0.03436225652694702, tv_loss: 0.0339934267103672\n",
      "iteration 1626, dc_loss: 0.0343899168074131, tv_loss: 0.03390879929065704\n",
      "iteration 1627, dc_loss: 0.03434975817799568, tv_loss: 0.03392389416694641\n",
      "iteration 1628, dc_loss: 0.03431443125009537, tv_loss: 0.03398258984088898\n",
      "iteration 1629, dc_loss: 0.03433595970273018, tv_loss: 0.03391989693045616\n",
      "iteration 1630, dc_loss: 0.034289490431547165, tv_loss: 0.03393681347370148\n",
      "iteration 1631, dc_loss: 0.03425023704767227, tv_loss: 0.03397742286324501\n",
      "iteration 1632, dc_loss: 0.03428063169121742, tv_loss: 0.033955059945583344\n",
      "iteration 1633, dc_loss: 0.034239139407873154, tv_loss: 0.03392687812447548\n",
      "iteration 1634, dc_loss: 0.034201741218566895, tv_loss: 0.0339721255004406\n",
      "iteration 1635, dc_loss: 0.03420776128768921, tv_loss: 0.03396178409457207\n",
      "iteration 1636, dc_loss: 0.0341753326356411, tv_loss: 0.03394254297018051\n",
      "iteration 1637, dc_loss: 0.0341629683971405, tv_loss: 0.03392808884382248\n",
      "iteration 1638, dc_loss: 0.03415227308869362, tv_loss: 0.033952925354242325\n",
      "iteration 1639, dc_loss: 0.03411800041794777, tv_loss: 0.03396494686603546\n",
      "iteration 1640, dc_loss: 0.03410862758755684, tv_loss: 0.0339285209774971\n",
      "iteration 1641, dc_loss: 0.03408823534846306, tv_loss: 0.03392889350652695\n",
      "iteration 1642, dc_loss: 0.034055136144161224, tv_loss: 0.033949512988328934\n",
      "iteration 1643, dc_loss: 0.03406216949224472, tv_loss: 0.033935464918613434\n",
      "iteration 1644, dc_loss: 0.034031543880701065, tv_loss: 0.03393622115254402\n",
      "iteration 1645, dc_loss: 0.03399654105305672, tv_loss: 0.03394756093621254\n",
      "iteration 1646, dc_loss: 0.0340031236410141, tv_loss: 0.03391105309128761\n",
      "iteration 1647, dc_loss: 0.03398117423057556, tv_loss: 0.03391682729125023\n",
      "iteration 1648, dc_loss: 0.03395138308405876, tv_loss: 0.0339244045317173\n",
      "iteration 1649, dc_loss: 0.03394197300076485, tv_loss: 0.033915240317583084\n",
      "iteration 1650, dc_loss: 0.03390752524137497, tv_loss: 0.03393854573369026\n",
      "iteration 1651, dc_loss: 0.033903270959854126, tv_loss: 0.03393518179655075\n",
      "iteration 1652, dc_loss: 0.03390133008360863, tv_loss: 0.03394964709877968\n",
      "iteration 1653, dc_loss: 0.03386242687702179, tv_loss: 0.03395485505461693\n",
      "iteration 1654, dc_loss: 0.03383844718337059, tv_loss: 0.03393179178237915\n",
      "iteration 1655, dc_loss: 0.03383160009980202, tv_loss: 0.03392485901713371\n",
      "iteration 1656, dc_loss: 0.03380249813199043, tv_loss: 0.03396880254149437\n",
      "iteration 1657, dc_loss: 0.03379477933049202, tv_loss: 0.03396070748567581\n",
      "iteration 1658, dc_loss: 0.03378012403845787, tv_loss: 0.03392583131790161\n",
      "iteration 1659, dc_loss: 0.03374306857585907, tv_loss: 0.03395211324095726\n",
      "iteration 1660, dc_loss: 0.03374619781970978, tv_loss: 0.03397824615240097\n",
      "iteration 1661, dc_loss: 0.03372740373015404, tv_loss: 0.033943407237529755\n",
      "iteration 1662, dc_loss: 0.03369010612368584, tv_loss: 0.03394994139671326\n",
      "iteration 1663, dc_loss: 0.03368952125310898, tv_loss: 0.033966317772865295\n",
      "iteration 1664, dc_loss: 0.033676330000162125, tv_loss: 0.03396865352988243\n",
      "iteration 1665, dc_loss: 0.03363218158483505, tv_loss: 0.03395968675613403\n",
      "iteration 1666, dc_loss: 0.03362785652279854, tv_loss: 0.0339568629860878\n",
      "iteration 1667, dc_loss: 0.03361748903989792, tv_loss: 0.033963318914175034\n",
      "iteration 1668, dc_loss: 0.033589884638786316, tv_loss: 0.03395909070968628\n",
      "iteration 1669, dc_loss: 0.033590737730264664, tv_loss: 0.03392132371664047\n",
      "iteration 1670, dc_loss: 0.033553700894117355, tv_loss: 0.033945757895708084\n",
      "iteration 1671, dc_loss: 0.0335342176258564, tv_loss: 0.03395852819085121\n",
      "iteration 1672, dc_loss: 0.033532701432704926, tv_loss: 0.033944979310035706\n",
      "iteration 1673, dc_loss: 0.03349608927965164, tv_loss: 0.03395308554172516\n",
      "iteration 1674, dc_loss: 0.03349423408508301, tv_loss: 0.03392721340060234\n",
      "iteration 1675, dc_loss: 0.0334751233458519, tv_loss: 0.03392687067389488\n",
      "iteration 1676, dc_loss: 0.03343893960118294, tv_loss: 0.03394433110952377\n",
      "iteration 1677, dc_loss: 0.033440880477428436, tv_loss: 0.03392387554049492\n",
      "iteration 1678, dc_loss: 0.03341468423604965, tv_loss: 0.0339428186416626\n",
      "iteration 1679, dc_loss: 0.03339263051748276, tv_loss: 0.033954937011003494\n",
      "iteration 1680, dc_loss: 0.03339054808020592, tv_loss: 0.03395356237888336\n",
      "iteration 1681, dc_loss: 0.03336102142930031, tv_loss: 0.033955879509449005\n",
      "iteration 1682, dc_loss: 0.03333891183137894, tv_loss: 0.03394068405032158\n",
      "iteration 1683, dc_loss: 0.033344827592372894, tv_loss: 0.0339142270386219\n",
      "iteration 1684, dc_loss: 0.033304885029792786, tv_loss: 0.03393600508570671\n",
      "iteration 1685, dc_loss: 0.03328520432114601, tv_loss: 0.03394157066941261\n",
      "iteration 1686, dc_loss: 0.03328433260321617, tv_loss: 0.03393490985035896\n",
      "iteration 1687, dc_loss: 0.033252399414777756, tv_loss: 0.033955495804548264\n",
      "iteration 1688, dc_loss: 0.03323816508054733, tv_loss: 0.03396286815404892\n",
      "iteration 1689, dc_loss: 0.03322487324476242, tv_loss: 0.03394921496510506\n",
      "iteration 1690, dc_loss: 0.0332135409116745, tv_loss: 0.033931516110897064\n",
      "iteration 1691, dc_loss: 0.03319500759243965, tv_loss: 0.03392768278717995\n",
      "iteration 1692, dc_loss: 0.0331580713391304, tv_loss: 0.03394278138875961\n",
      "iteration 1693, dc_loss: 0.03315562754869461, tv_loss: 0.03392946720123291\n",
      "iteration 1694, dc_loss: 0.0331440344452858, tv_loss: 0.03394080325961113\n",
      "iteration 1695, dc_loss: 0.033113349229097366, tv_loss: 0.033973999321460724\n",
      "iteration 1696, dc_loss: 0.033106569200754166, tv_loss: 0.03396372124552727\n",
      "iteration 1697, dc_loss: 0.0330955870449543, tv_loss: 0.033940330147743225\n",
      "iteration 1698, dc_loss: 0.03306182473897934, tv_loss: 0.03394860774278641\n",
      "iteration 1699, dc_loss: 0.033048126846551895, tv_loss: 0.03394189104437828\n",
      "iteration 1700, dc_loss: 0.03303544968366623, tv_loss: 0.033933475613594055\n",
      "iteration 1701, dc_loss: 0.033023372292518616, tv_loss: 0.03392921760678291\n",
      "iteration 1702, dc_loss: 0.032995834946632385, tv_loss: 0.033941060304641724\n",
      "iteration 1703, dc_loss: 0.032986175268888474, tv_loss: 0.03393806144595146\n",
      "iteration 1704, dc_loss: 0.03298025205731392, tv_loss: 0.03393229842185974\n",
      "iteration 1705, dc_loss: 0.03294404596090317, tv_loss: 0.03396190330386162\n",
      "iteration 1706, dc_loss: 0.032929837703704834, tv_loss: 0.03397856652736664\n",
      "iteration 1707, dc_loss: 0.032919738441705704, tv_loss: 0.03395187854766846\n",
      "iteration 1708, dc_loss: 0.032888997346162796, tv_loss: 0.0339488722383976\n",
      "iteration 1709, dc_loss: 0.03288600221276283, tv_loss: 0.03394211083650589\n",
      "iteration 1710, dc_loss: 0.03287901356816292, tv_loss: 0.03392690792679787\n",
      "iteration 1711, dc_loss: 0.03283601626753807, tv_loss: 0.03395511955022812\n",
      "iteration 1712, dc_loss: 0.03283454105257988, tv_loss: 0.033938754349946976\n",
      "iteration 1713, dc_loss: 0.03282095119357109, tv_loss: 0.033937059342861176\n",
      "iteration 1714, dc_loss: 0.032784100621938705, tv_loss: 0.0339634045958519\n",
      "iteration 1715, dc_loss: 0.03279271349310875, tv_loss: 0.033929403871297836\n",
      "iteration 1716, dc_loss: 0.03277824446558952, tv_loss: 0.03393172100186348\n",
      "iteration 1717, dc_loss: 0.03273747116327286, tv_loss: 0.033969391137361526\n",
      "iteration 1718, dc_loss: 0.03273659199476242, tv_loss: 0.033940356224775314\n",
      "iteration 1719, dc_loss: 0.03271600231528282, tv_loss: 0.03394309803843498\n",
      "iteration 1720, dc_loss: 0.03269001096487045, tv_loss: 0.03397122025489807\n",
      "iteration 1721, dc_loss: 0.03269514441490173, tv_loss: 0.03392944112420082\n",
      "iteration 1722, dc_loss: 0.03266299515962601, tv_loss: 0.03394995629787445\n",
      "iteration 1723, dc_loss: 0.03264384716749191, tv_loss: 0.0339571051299572\n",
      "iteration 1724, dc_loss: 0.03263982757925987, tv_loss: 0.033939674496650696\n",
      "iteration 1725, dc_loss: 0.032606784254312515, tv_loss: 0.033949945122003555\n",
      "iteration 1726, dc_loss: 0.032614875584840775, tv_loss: 0.03392137959599495\n",
      "iteration 1727, dc_loss: 0.03258325904607773, tv_loss: 0.0339527353644371\n",
      "iteration 1728, dc_loss: 0.0325598381459713, tv_loss: 0.03396066278219223\n",
      "iteration 1729, dc_loss: 0.03256441280245781, tv_loss: 0.03394996002316475\n",
      "iteration 1730, dc_loss: 0.032529450953006744, tv_loss: 0.0339837372303009\n",
      "iteration 1731, dc_loss: 0.032514527440071106, tv_loss: 0.033980436623096466\n",
      "iteration 1732, dc_loss: 0.03250691294670105, tv_loss: 0.03396126627922058\n",
      "iteration 1733, dc_loss: 0.03249124437570572, tv_loss: 0.03394021838903427\n",
      "iteration 1734, dc_loss: 0.03246438875794411, tv_loss: 0.03395327553153038\n",
      "iteration 1735, dc_loss: 0.03245692327618599, tv_loss: 0.033944759517908096\n",
      "iteration 1736, dc_loss: 0.032444629818201065, tv_loss: 0.03394052013754845\n",
      "iteration 1737, dc_loss: 0.03241300955414772, tv_loss: 0.033949270844459534\n",
      "iteration 1738, dc_loss: 0.0324113667011261, tv_loss: 0.03394260257482529\n",
      "iteration 1739, dc_loss: 0.0323970690369606, tv_loss: 0.033958159387111664\n",
      "iteration 1740, dc_loss: 0.032364435493946075, tv_loss: 0.03400047868490219\n",
      "iteration 1741, dc_loss: 0.03235618397593498, tv_loss: 0.03396698459982872\n",
      "iteration 1742, dc_loss: 0.03235192596912384, tv_loss: 0.033938758075237274\n",
      "iteration 1743, dc_loss: 0.03231431916356087, tv_loss: 0.03396592661738396\n",
      "iteration 1744, dc_loss: 0.03230650722980499, tv_loss: 0.03398291394114494\n",
      "iteration 1745, dc_loss: 0.0322989821434021, tv_loss: 0.0339730829000473\n",
      "iteration 1746, dc_loss: 0.03226893022656441, tv_loss: 0.03396596759557724\n",
      "iteration 1747, dc_loss: 0.03227660432457924, tv_loss: 0.033944666385650635\n",
      "iteration 1748, dc_loss: 0.03223626688122749, tv_loss: 0.033974528312683105\n",
      "iteration 1749, dc_loss: 0.032222338020801544, tv_loss: 0.03397506847977638\n",
      "iteration 1750, dc_loss: 0.03224094584584236, tv_loss: 0.03393257409334183\n",
      "iteration 1751, dc_loss: 0.032175417989492416, tv_loss: 0.033975593745708466\n",
      "iteration 1752, dc_loss: 0.03219256177544594, tv_loss: 0.03393912315368652\n",
      "iteration 1753, dc_loss: 0.03217554837465286, tv_loss: 0.033951010555028915\n",
      "iteration 1754, dc_loss: 0.03213982284069061, tv_loss: 0.03397899121046066\n",
      "iteration 1755, dc_loss: 0.032139889895915985, tv_loss: 0.03398605436086655\n",
      "iteration 1756, dc_loss: 0.03213341161608696, tv_loss: 0.033963222056627274\n",
      "iteration 1757, dc_loss: 0.03211260214447975, tv_loss: 0.03395070880651474\n",
      "iteration 1758, dc_loss: 0.03209161013364792, tv_loss: 0.03397038206458092\n",
      "iteration 1759, dc_loss: 0.03207777813076973, tv_loss: 0.03398760035634041\n",
      "iteration 1760, dc_loss: 0.03208597004413605, tv_loss: 0.033970702439546585\n",
      "iteration 1761, dc_loss: 0.03202883526682854, tv_loss: 0.03398764505982399\n",
      "iteration 1762, dc_loss: 0.032096017152071, tv_loss: 0.03390571475028992\n",
      "iteration 1763, dc_loss: 0.03200098127126694, tv_loss: 0.03402900695800781\n",
      "iteration 1764, dc_loss: 0.03204333782196045, tv_loss: 0.03396959602832794\n",
      "iteration 1765, dc_loss: 0.03201446682214737, tv_loss: 0.03397030755877495\n",
      "iteration 1766, dc_loss: 0.032009582966566086, tv_loss: 0.033953532576560974\n",
      "iteration 1767, dc_loss: 0.03197775036096573, tv_loss: 0.0339956097304821\n",
      "iteration 1768, dc_loss: 0.03199046850204468, tv_loss: 0.033963773399591446\n",
      "iteration 1769, dc_loss: 0.031910866498947144, tv_loss: 0.0339965894818306\n",
      "iteration 1770, dc_loss: 0.03194088116288185, tv_loss: 0.033940352499485016\n",
      "iteration 1771, dc_loss: 0.03187350183725357, tv_loss: 0.033980924636125565\n",
      "iteration 1772, dc_loss: 0.031897127628326416, tv_loss: 0.033966582268476486\n",
      "iteration 1773, dc_loss: 0.03185253217816353, tv_loss: 0.03398866206407547\n",
      "iteration 1774, dc_loss: 0.03183089941740036, tv_loss: 0.03397784009575844\n",
      "iteration 1775, dc_loss: 0.031847305595874786, tv_loss: 0.033940043300390244\n",
      "iteration 1776, dc_loss: 0.03179401531815529, tv_loss: 0.03400317206978798\n",
      "iteration 1777, dc_loss: 0.031827885657548904, tv_loss: 0.03396881744265556\n",
      "iteration 1778, dc_loss: 0.031788356602191925, tv_loss: 0.03398073464632034\n",
      "iteration 1779, dc_loss: 0.03177359327673912, tv_loss: 0.03396258130669594\n",
      "iteration 1780, dc_loss: 0.03176901862025261, tv_loss: 0.03397122770547867\n",
      "iteration 1781, dc_loss: 0.031752314418554306, tv_loss: 0.033967290073633194\n",
      "iteration 1782, dc_loss: 0.03171134740114212, tv_loss: 0.03398832306265831\n",
      "iteration 1783, dc_loss: 0.03173491731286049, tv_loss: 0.03394745662808418\n",
      "iteration 1784, dc_loss: 0.03168246150016785, tv_loss: 0.03397868573665619\n",
      "iteration 1785, dc_loss: 0.03170568868517876, tv_loss: 0.033932607620954514\n",
      "iteration 1786, dc_loss: 0.031652338802814484, tv_loss: 0.03397664055228233\n",
      "iteration 1787, dc_loss: 0.03166407719254494, tv_loss: 0.0339474081993103\n",
      "iteration 1788, dc_loss: 0.03162188082933426, tv_loss: 0.033978529274463654\n",
      "iteration 1789, dc_loss: 0.0316183976829052, tv_loss: 0.03396531939506531\n",
      "iteration 1790, dc_loss: 0.031615376472473145, tv_loss: 0.03395925834774971\n",
      "iteration 1791, dc_loss: 0.03158134967088699, tv_loss: 0.03397662565112114\n",
      "iteration 1792, dc_loss: 0.0315847210586071, tv_loss: 0.03394992649555206\n",
      "iteration 1793, dc_loss: 0.03155973181128502, tv_loss: 0.03395768627524376\n",
      "iteration 1794, dc_loss: 0.031525880098342896, tv_loss: 0.03397432342171669\n",
      "iteration 1795, dc_loss: 0.031553950160741806, tv_loss: 0.03393637016415596\n",
      "iteration 1796, dc_loss: 0.031503815203905106, tv_loss: 0.03396809101104736\n",
      "iteration 1797, dc_loss: 0.03153035044670105, tv_loss: 0.033953871577978134\n",
      "iteration 1798, dc_loss: 0.03148375824093819, tv_loss: 0.03401372209191322\n",
      "iteration 1799, dc_loss: 0.03150423616170883, tv_loss: 0.033970318734645844\n",
      "iteration 1800, dc_loss: 0.031484510749578476, tv_loss: 0.03396448865532875\n",
      "iteration 1801, dc_loss: 0.03151027858257294, tv_loss: 0.03394301235675812\n",
      "iteration 1802, dc_loss: 0.03145111724734306, tv_loss: 0.0340215228497982\n",
      "iteration 1803, dc_loss: 0.03150191530585289, tv_loss: 0.03395269438624382\n",
      "iteration 1804, dc_loss: 0.031420715153217316, tv_loss: 0.034005846828222275\n",
      "iteration 1805, dc_loss: 0.03147997707128525, tv_loss: 0.033923614770174026\n",
      "iteration 1806, dc_loss: 0.03137362003326416, tv_loss: 0.03402116149663925\n",
      "iteration 1807, dc_loss: 0.03142821788787842, tv_loss: 0.033948689699172974\n",
      "iteration 1808, dc_loss: 0.03133231773972511, tv_loss: 0.03400794789195061\n",
      "iteration 1809, dc_loss: 0.03133784979581833, tv_loss: 0.03396696597337723\n",
      "iteration 1810, dc_loss: 0.03133297711610794, tv_loss: 0.03395170718431473\n",
      "iteration 1811, dc_loss: 0.0312935896217823, tv_loss: 0.03398292884230614\n",
      "iteration 1812, dc_loss: 0.0312795527279377, tv_loss: 0.03398105874657631\n",
      "iteration 1813, dc_loss: 0.03125464543700218, tv_loss: 0.03397362679243088\n",
      "iteration 1814, dc_loss: 0.03127538412809372, tv_loss: 0.033943772315979004\n",
      "iteration 1815, dc_loss: 0.031238991767168045, tv_loss: 0.033967722207307816\n",
      "iteration 1816, dc_loss: 0.031239645555615425, tv_loss: 0.033967018127441406\n",
      "iteration 1817, dc_loss: 0.03123326040804386, tv_loss: 0.03396780788898468\n",
      "iteration 1818, dc_loss: 0.031232045963406563, tv_loss: 0.033971358090639114\n",
      "iteration 1819, dc_loss: 0.031173111870884895, tv_loss: 0.03400377556681633\n",
      "iteration 1820, dc_loss: 0.03121156059205532, tv_loss: 0.033939287066459656\n",
      "iteration 1821, dc_loss: 0.031130382791161537, tv_loss: 0.03400634601712227\n",
      "iteration 1822, dc_loss: 0.031195372343063354, tv_loss: 0.033943839371204376\n",
      "iteration 1823, dc_loss: 0.031110422685742378, tv_loss: 0.03403281792998314\n",
      "iteration 1824, dc_loss: 0.03112146258354187, tv_loss: 0.033979080617427826\n",
      "iteration 1825, dc_loss: 0.031109577044844627, tv_loss: 0.03396092727780342\n",
      "iteration 1826, dc_loss: 0.031082576140761375, tv_loss: 0.03399118408560753\n",
      "iteration 1827, dc_loss: 0.031074484810233116, tv_loss: 0.03400762379169464\n",
      "iteration 1828, dc_loss: 0.031052537262439728, tv_loss: 0.033973559737205505\n",
      "iteration 1829, dc_loss: 0.031032970175147057, tv_loss: 0.03398653864860535\n",
      "iteration 1830, dc_loss: 0.031040513888001442, tv_loss: 0.03396116942167282\n",
      "iteration 1831, dc_loss: 0.030999621376395226, tv_loss: 0.03399856016039848\n",
      "iteration 1832, dc_loss: 0.031010808423161507, tv_loss: 0.033963121473789215\n",
      "iteration 1833, dc_loss: 0.03097894974052906, tv_loss: 0.033971626311540604\n",
      "iteration 1834, dc_loss: 0.030969412997364998, tv_loss: 0.03396761417388916\n",
      "iteration 1835, dc_loss: 0.030939210206270218, tv_loss: 0.03398657590150833\n",
      "iteration 1836, dc_loss: 0.03097034990787506, tv_loss: 0.0339558869600296\n",
      "iteration 1837, dc_loss: 0.03091607801616192, tv_loss: 0.033989522606134415\n",
      "iteration 1838, dc_loss: 0.030927104875445366, tv_loss: 0.03395460173487663\n",
      "iteration 1839, dc_loss: 0.03088628500699997, tv_loss: 0.033992841839790344\n",
      "iteration 1840, dc_loss: 0.030926022678613663, tv_loss: 0.03394119068980217\n",
      "iteration 1841, dc_loss: 0.03087771125137806, tv_loss: 0.03398524597287178\n",
      "iteration 1842, dc_loss: 0.030920643359422684, tv_loss: 0.03394109010696411\n",
      "iteration 1843, dc_loss: 0.03084968402981758, tv_loss: 0.0340166911482811\n",
      "iteration 1844, dc_loss: 0.030956046655774117, tv_loss: 0.03391089290380478\n",
      "iteration 1845, dc_loss: 0.030857563018798828, tv_loss: 0.03402016684412956\n",
      "iteration 1846, dc_loss: 0.030947351828217506, tv_loss: 0.033919237554073334\n",
      "iteration 1847, dc_loss: 0.030891356989741325, tv_loss: 0.03399622440338135\n",
      "iteration 1848, dc_loss: 0.03092288039624691, tv_loss: 0.033931028097867966\n",
      "iteration 1849, dc_loss: 0.030818814411759377, tv_loss: 0.034004077315330505\n",
      "iteration 1850, dc_loss: 0.030825994908809662, tv_loss: 0.03395477309823036\n",
      "iteration 1851, dc_loss: 0.03075307235121727, tv_loss: 0.034009963274002075\n",
      "iteration 1852, dc_loss: 0.03073149174451828, tv_loss: 0.03398435190320015\n",
      "iteration 1853, dc_loss: 0.030710598453879356, tv_loss: 0.03396118804812431\n",
      "iteration 1854, dc_loss: 0.030717900022864342, tv_loss: 0.03396817669272423\n",
      "iteration 1855, dc_loss: 0.03070475161075592, tv_loss: 0.0339849554002285\n",
      "iteration 1856, dc_loss: 0.03069455362856388, tv_loss: 0.03397957235574722\n",
      "iteration 1857, dc_loss: 0.03070329688489437, tv_loss: 0.03396343067288399\n",
      "iteration 1858, dc_loss: 0.03065807744860649, tv_loss: 0.03398854285478592\n",
      "iteration 1859, dc_loss: 0.030693767592310905, tv_loss: 0.03392888978123665\n",
      "iteration 1860, dc_loss: 0.030606551095843315, tv_loss: 0.03401435166597366\n",
      "iteration 1861, dc_loss: 0.030641252174973488, tv_loss: 0.03395844250917435\n",
      "iteration 1862, dc_loss: 0.030575724318623543, tv_loss: 0.033990100026130676\n",
      "iteration 1863, dc_loss: 0.030569536611437798, tv_loss: 0.033970918506383896\n",
      "iteration 1864, dc_loss: 0.030567007139325142, tv_loss: 0.03395908698439598\n",
      "iteration 1865, dc_loss: 0.030534453690052032, tv_loss: 0.03397922217845917\n",
      "iteration 1866, dc_loss: 0.030559301376342773, tv_loss: 0.033942557871341705\n",
      "iteration 1867, dc_loss: 0.030517293140292168, tv_loss: 0.033978916704654694\n",
      "iteration 1868, dc_loss: 0.030531292781233788, tv_loss: 0.03396766632795334\n",
      "iteration 1869, dc_loss: 0.030496930703520775, tv_loss: 0.03400479257106781\n",
      "iteration 1870, dc_loss: 0.030517810955643654, tv_loss: 0.03395767882466316\n",
      "iteration 1871, dc_loss: 0.03048323094844818, tv_loss: 0.03397858887910843\n",
      "iteration 1872, dc_loss: 0.030486060306429863, tv_loss: 0.03397034853696823\n",
      "iteration 1873, dc_loss: 0.03044632077217102, tv_loss: 0.03399147093296051\n",
      "iteration 1874, dc_loss: 0.030458718538284302, tv_loss: 0.033966802060604095\n",
      "iteration 1875, dc_loss: 0.030422218143939972, tv_loss: 0.03397781401872635\n",
      "iteration 1876, dc_loss: 0.03042243793606758, tv_loss: 0.03396080806851387\n",
      "iteration 1877, dc_loss: 0.030393825843930244, tv_loss: 0.03398360311985016\n",
      "iteration 1878, dc_loss: 0.030390523374080658, tv_loss: 0.03396524488925934\n",
      "iteration 1879, dc_loss: 0.0303538478910923, tv_loss: 0.03398769721388817\n",
      "iteration 1880, dc_loss: 0.03037770465016365, tv_loss: 0.03394898399710655\n",
      "iteration 1881, dc_loss: 0.03033054620027542, tv_loss: 0.03399210050702095\n",
      "iteration 1882, dc_loss: 0.030323699116706848, tv_loss: 0.03399106487631798\n",
      "iteration 1883, dc_loss: 0.03031122125685215, tv_loss: 0.03397997468709946\n",
      "iteration 1884, dc_loss: 0.03031306341290474, tv_loss: 0.03396701440215111\n",
      "iteration 1885, dc_loss: 0.03027038648724556, tv_loss: 0.03398709371685982\n",
      "iteration 1886, dc_loss: 0.030304070562124252, tv_loss: 0.033939529210329056\n",
      "iteration 1887, dc_loss: 0.03025001659989357, tv_loss: 0.03399350866675377\n",
      "iteration 1888, dc_loss: 0.03026755340397358, tv_loss: 0.03396230563521385\n",
      "iteration 1889, dc_loss: 0.030240628868341446, tv_loss: 0.03398929536342621\n",
      "iteration 1890, dc_loss: 0.03025582805275917, tv_loss: 0.03397226706147194\n",
      "iteration 1891, dc_loss: 0.03021821193397045, tv_loss: 0.033992454409599304\n",
      "iteration 1892, dc_loss: 0.030262304469943047, tv_loss: 0.03394138440489769\n",
      "iteration 1893, dc_loss: 0.030205657705664635, tv_loss: 0.03400398790836334\n",
      "iteration 1894, dc_loss: 0.030263952910900116, tv_loss: 0.033937569707632065\n",
      "iteration 1895, dc_loss: 0.03019639477133751, tv_loss: 0.03402518853545189\n",
      "iteration 1896, dc_loss: 0.030267680063843727, tv_loss: 0.03395523875951767\n",
      "iteration 1897, dc_loss: 0.030155684798955917, tv_loss: 0.03404249995946884\n",
      "iteration 1898, dc_loss: 0.0302262045443058, tv_loss: 0.0339302234351635\n",
      "iteration 1899, dc_loss: 0.030120152980089188, tv_loss: 0.03401314094662666\n",
      "iteration 1900, dc_loss: 0.03015749529004097, tv_loss: 0.0339445099234581\n",
      "iteration 1901, dc_loss: 0.030067402869462967, tv_loss: 0.0340123325586319\n",
      "iteration 1902, dc_loss: 0.03013160452246666, tv_loss: 0.03391963616013527\n",
      "iteration 1903, dc_loss: 0.030027665197849274, tv_loss: 0.0340108685195446\n",
      "iteration 1904, dc_loss: 0.03005867637693882, tv_loss: 0.03396586701273918\n",
      "iteration 1905, dc_loss: 0.03002588078379631, tv_loss: 0.03399554640054703\n",
      "iteration 1906, dc_loss: 0.030008045956492424, tv_loss: 0.03399590030312538\n",
      "iteration 1907, dc_loss: 0.030036959797143936, tv_loss: 0.03395836427807808\n",
      "iteration 1908, dc_loss: 0.02997090294957161, tv_loss: 0.034013569355010986\n",
      "iteration 1909, dc_loss: 0.03003404475748539, tv_loss: 0.03394469991326332\n",
      "iteration 1910, dc_loss: 0.029933152720332146, tv_loss: 0.034039389342069626\n",
      "iteration 1911, dc_loss: 0.03002830222249031, tv_loss: 0.03392739221453667\n",
      "iteration 1912, dc_loss: 0.02992059849202633, tv_loss: 0.03401658311486244\n",
      "iteration 1913, dc_loss: 0.030000686645507812, tv_loss: 0.03392759710550308\n",
      "iteration 1914, dc_loss: 0.02988513745367527, tv_loss: 0.034019093960523605\n",
      "iteration 1915, dc_loss: 0.029956938698887825, tv_loss: 0.03394583240151405\n",
      "iteration 1916, dc_loss: 0.02987946756184101, tv_loss: 0.03401578217744827\n",
      "iteration 1917, dc_loss: 0.02991853840649128, tv_loss: 0.03397674486041069\n",
      "iteration 1918, dc_loss: 0.029866496101021767, tv_loss: 0.03400455042719841\n",
      "iteration 1919, dc_loss: 0.029889734461903572, tv_loss: 0.03396115079522133\n",
      "iteration 1920, dc_loss: 0.02986406721174717, tv_loss: 0.03397931903600693\n",
      "iteration 1921, dc_loss: 0.02985367551445961, tv_loss: 0.033990249037742615\n",
      "iteration 1922, dc_loss: 0.029856260865926743, tv_loss: 0.033974286168813705\n",
      "iteration 1923, dc_loss: 0.029856234788894653, tv_loss: 0.0339968204498291\n",
      "iteration 1924, dc_loss: 0.029878728091716766, tv_loss: 0.03396908566355705\n",
      "iteration 1925, dc_loss: 0.02984343096613884, tv_loss: 0.033998988568782806\n",
      "iteration 1926, dc_loss: 0.029875794425606728, tv_loss: 0.033970899879932404\n",
      "iteration 1927, dc_loss: 0.029842287302017212, tv_loss: 0.03398188576102257\n",
      "iteration 1928, dc_loss: 0.029837794601917267, tv_loss: 0.03396908566355705\n",
      "iteration 1929, dc_loss: 0.02979537472128868, tv_loss: 0.03397512063384056\n",
      "iteration 1930, dc_loss: 0.029757006093859673, tv_loss: 0.033980678766965866\n",
      "iteration 1931, dc_loss: 0.029749689623713493, tv_loss: 0.03395618498325348\n",
      "iteration 1932, dc_loss: 0.029704168438911438, tv_loss: 0.03398217260837555\n",
      "iteration 1933, dc_loss: 0.0297397393733263, tv_loss: 0.033923566341400146\n",
      "iteration 1934, dc_loss: 0.02962610125541687, tv_loss: 0.03402789309620857\n",
      "iteration 1935, dc_loss: 0.029744809493422508, tv_loss: 0.03391974791884422\n",
      "iteration 1936, dc_loss: 0.02961607463657856, tv_loss: 0.03407201170921326\n",
      "iteration 1937, dc_loss: 0.02969065122306347, tv_loss: 0.03394902125000954\n",
      "iteration 1938, dc_loss: 0.02961563691496849, tv_loss: 0.033994078636169434\n",
      "iteration 1939, dc_loss: 0.029628852382302284, tv_loss: 0.03397052362561226\n",
      "iteration 1940, dc_loss: 0.02960112877190113, tv_loss: 0.034048523753881454\n",
      "iteration 1941, dc_loss: 0.029590995982289314, tv_loss: 0.03399974852800369\n",
      "iteration 1942, dc_loss: 0.029580315575003624, tv_loss: 0.033982258290052414\n",
      "iteration 1943, dc_loss: 0.0295573677867651, tv_loss: 0.034047115594148636\n",
      "iteration 1944, dc_loss: 0.029589546844363213, tv_loss: 0.033978790044784546\n",
      "iteration 1945, dc_loss: 0.02951148711144924, tv_loss: 0.03402816876769066\n",
      "iteration 1946, dc_loss: 0.029584486037492752, tv_loss: 0.033961985260248184\n",
      "iteration 1947, dc_loss: 0.029489638283848763, tv_loss: 0.03408004343509674\n",
      "iteration 1948, dc_loss: 0.029578926041722298, tv_loss: 0.03394782915711403\n",
      "iteration 1949, dc_loss: 0.029474297538399696, tv_loss: 0.034045543521642685\n",
      "iteration 1950, dc_loss: 0.029555793851614, tv_loss: 0.0339789018034935\n",
      "iteration 1951, dc_loss: 0.029475810006260872, tv_loss: 0.03405241668224335\n",
      "iteration 1952, dc_loss: 0.029565569013357162, tv_loss: 0.033934030681848526\n",
      "iteration 1953, dc_loss: 0.029437586665153503, tv_loss: 0.03407182916998863\n",
      "iteration 1954, dc_loss: 0.02954867109656334, tv_loss: 0.03394721448421478\n",
      "iteration 1955, dc_loss: 0.02945314534008503, tv_loss: 0.034033648669719696\n",
      "iteration 1956, dc_loss: 0.029510969296097755, tv_loss: 0.03396216779947281\n",
      "iteration 1957, dc_loss: 0.029465623199939728, tv_loss: 0.03400414064526558\n",
      "iteration 1958, dc_loss: 0.02950136736035347, tv_loss: 0.03396125137805939\n",
      "iteration 1959, dc_loss: 0.02944597415626049, tv_loss: 0.0340210422873497\n",
      "iteration 1960, dc_loss: 0.029455700889229774, tv_loss: 0.0339621901512146\n",
      "iteration 1961, dc_loss: 0.02938821353018284, tv_loss: 0.03399532288312912\n",
      "iteration 1962, dc_loss: 0.029385706409811974, tv_loss: 0.03398274630308151\n",
      "iteration 1963, dc_loss: 0.02933747135102749, tv_loss: 0.03401865065097809\n",
      "iteration 1964, dc_loss: 0.029348917305469513, tv_loss: 0.03396528214216232\n",
      "iteration 1965, dc_loss: 0.029313353821635246, tv_loss: 0.03397959843277931\n",
      "iteration 1966, dc_loss: 0.029279178008437157, tv_loss: 0.03400129824876785\n",
      "iteration 1967, dc_loss: 0.02931193634867668, tv_loss: 0.033964671194553375\n",
      "iteration 1968, dc_loss: 0.029268445447087288, tv_loss: 0.03401673585176468\n",
      "iteration 1969, dc_loss: 0.029288865625858307, tv_loss: 0.03398275747895241\n",
      "iteration 1970, dc_loss: 0.029262401163578033, tv_loss: 0.033989161252975464\n",
      "iteration 1971, dc_loss: 0.029292715713381767, tv_loss: 0.03395572677254677\n",
      "iteration 1972, dc_loss: 0.029235484078526497, tv_loss: 0.03399905934929848\n",
      "iteration 1973, dc_loss: 0.02926737815141678, tv_loss: 0.03395162150263786\n",
      "iteration 1974, dc_loss: 0.029220862314105034, tv_loss: 0.034003496170043945\n",
      "iteration 1975, dc_loss: 0.029233720153570175, tv_loss: 0.033983029425144196\n",
      "iteration 1976, dc_loss: 0.029207026585936546, tv_loss: 0.0340004563331604\n",
      "iteration 1977, dc_loss: 0.029214443638920784, tv_loss: 0.03395414352416992\n",
      "iteration 1978, dc_loss: 0.029145920649170876, tv_loss: 0.03399857133626938\n",
      "iteration 1979, dc_loss: 0.02918112277984619, tv_loss: 0.03395110368728638\n",
      "iteration 1980, dc_loss: 0.029116932302713394, tv_loss: 0.03401397168636322\n",
      "iteration 1981, dc_loss: 0.02915787324309349, tv_loss: 0.03395398333668709\n",
      "iteration 1982, dc_loss: 0.02909880504012108, tv_loss: 0.03401517495512962\n",
      "iteration 1983, dc_loss: 0.029128799214959145, tv_loss: 0.03397076576948166\n",
      "iteration 1984, dc_loss: 0.029067721217870712, tv_loss: 0.03400677442550659\n",
      "iteration 1985, dc_loss: 0.029115203768014908, tv_loss: 0.03394369035959244\n",
      "iteration 1986, dc_loss: 0.02905014529824257, tv_loss: 0.03400015830993652\n",
      "iteration 1987, dc_loss: 0.02905840054154396, tv_loss: 0.0339929535984993\n",
      "iteration 1988, dc_loss: 0.02904982678592205, tv_loss: 0.03400515019893646\n",
      "iteration 1989, dc_loss: 0.029026757925748825, tv_loss: 0.03401903808116913\n",
      "iteration 1990, dc_loss: 0.029044996947050095, tv_loss: 0.033966485410928726\n",
      "iteration 1991, dc_loss: 0.028983842581510544, tv_loss: 0.03401016816496849\n",
      "iteration 1992, dc_loss: 0.029061153531074524, tv_loss: 0.03392602875828743\n",
      "iteration 1993, dc_loss: 0.028954967856407166, tv_loss: 0.034032877534627914\n",
      "iteration 1994, dc_loss: 0.029065517708659172, tv_loss: 0.03392734378576279\n",
      "iteration 1995, dc_loss: 0.028942670673131943, tv_loss: 0.034053076058626175\n",
      "iteration 1996, dc_loss: 0.029061755165457726, tv_loss: 0.03395332396030426\n",
      "iteration 1997, dc_loss: 0.028933104127645493, tv_loss: 0.034059736877679825\n",
      "iteration 1998, dc_loss: 0.02911008708178997, tv_loss: 0.033882901072502136\n",
      "iteration 1999, dc_loss: 0.02892417274415493, tv_loss: 0.03411991149187088\n",
      "iteration 2000, dc_loss: 0.029156483709812164, tv_loss: 0.0338950976729393\n",
      "iteration 2001, dc_loss: 0.028958184644579887, tv_loss: 0.034115131944417953\n",
      "iteration 2002, dc_loss: 0.029016269370913506, tv_loss: 0.0339062474668026\n",
      "iteration 2003, dc_loss: 0.02894335612654686, tv_loss: 0.033939287066459656\n",
      "iteration 2004, dc_loss: 0.028894955292344093, tv_loss: 0.03409770503640175\n",
      "iteration 2005, dc_loss: 0.029015546664595604, tv_loss: 0.03390839323401451\n",
      "iteration 2006, dc_loss: 0.028878282755613327, tv_loss: 0.03396115079522133\n",
      "iteration 2007, dc_loss: 0.02882939763367176, tv_loss: 0.0340600349009037\n",
      "iteration 2008, dc_loss: 0.02898912876844406, tv_loss: 0.03389845788478851\n",
      "iteration 2009, dc_loss: 0.02884196676313877, tv_loss: 0.033980272710323334\n",
      "iteration 2010, dc_loss: 0.028792453929781914, tv_loss: 0.03403197601437569\n",
      "iteration 2011, dc_loss: 0.028930749744176865, tv_loss: 0.03389876335859299\n",
      "iteration 2012, dc_loss: 0.028817068785429, tv_loss: 0.033973898738622665\n",
      "iteration 2013, dc_loss: 0.028769802302122116, tv_loss: 0.03401455655694008\n",
      "iteration 2014, dc_loss: 0.028877882286906242, tv_loss: 0.033919818699359894\n",
      "iteration 2015, dc_loss: 0.02877514623105526, tv_loss: 0.03400128707289696\n",
      "iteration 2016, dc_loss: 0.028759002685546875, tv_loss: 0.034000370651483536\n",
      "iteration 2017, dc_loss: 0.02884099818766117, tv_loss: 0.033906273543834686\n",
      "iteration 2018, dc_loss: 0.028737200424075127, tv_loss: 0.033989857882261276\n",
      "iteration 2019, dc_loss: 0.028747521340847015, tv_loss: 0.03397098183631897\n",
      "iteration 2020, dc_loss: 0.028794821351766586, tv_loss: 0.033914972096681595\n",
      "iteration 2021, dc_loss: 0.02870243601500988, tv_loss: 0.03398941084742546\n",
      "iteration 2022, dc_loss: 0.028728093951940536, tv_loss: 0.03395196422934532\n",
      "iteration 2023, dc_loss: 0.028749966993927956, tv_loss: 0.03393019735813141\n",
      "iteration 2024, dc_loss: 0.02867419645190239, tv_loss: 0.03398876264691353\n",
      "iteration 2025, dc_loss: 0.028704781085252762, tv_loss: 0.03394694626331329\n",
      "iteration 2026, dc_loss: 0.028717923909425735, tv_loss: 0.03395144268870354\n",
      "iteration 2027, dc_loss: 0.028648875653743744, tv_loss: 0.034031253308057785\n",
      "iteration 2028, dc_loss: 0.028667235746979713, tv_loss: 0.03397712484002113\n",
      "iteration 2029, dc_loss: 0.028693614527583122, tv_loss: 0.0339268296957016\n",
      "iteration 2030, dc_loss: 0.028629308566451073, tv_loss: 0.034000709652900696\n",
      "iteration 2031, dc_loss: 0.028648123145103455, tv_loss: 0.03399619460105896\n",
      "iteration 2032, dc_loss: 0.028653258457779884, tv_loss: 0.03396686911582947\n",
      "iteration 2033, dc_loss: 0.028604429215192795, tv_loss: 0.033987127244472504\n",
      "iteration 2034, dc_loss: 0.028630560263991356, tv_loss: 0.03396567702293396\n",
      "iteration 2035, dc_loss: 0.028615424409508705, tv_loss: 0.03399648144841194\n",
      "iteration 2036, dc_loss: 0.028575856238603592, tv_loss: 0.034006260335445404\n",
      "iteration 2037, dc_loss: 0.028615877032279968, tv_loss: 0.03394657000899315\n",
      "iteration 2038, dc_loss: 0.028580626472830772, tv_loss: 0.03399209305644035\n",
      "iteration 2039, dc_loss: 0.02855643257498741, tv_loss: 0.03401121497154236\n",
      "iteration 2040, dc_loss: 0.02859754115343094, tv_loss: 0.03394646570086479\n",
      "iteration 2041, dc_loss: 0.02854120172560215, tv_loss: 0.03399796411395073\n",
      "iteration 2042, dc_loss: 0.028539160266518593, tv_loss: 0.033997174352407455\n",
      "iteration 2043, dc_loss: 0.028574081137776375, tv_loss: 0.03394962102174759\n",
      "iteration 2044, dc_loss: 0.028504876419901848, tv_loss: 0.034002166241407394\n",
      "iteration 2045, dc_loss: 0.028521284461021423, tv_loss: 0.03396618738770485\n",
      "iteration 2046, dc_loss: 0.028544658794999123, tv_loss: 0.03394383564591408\n",
      "iteration 2047, dc_loss: 0.028479935601353645, tv_loss: 0.03400729224085808\n",
      "iteration 2048, dc_loss: 0.028506388887763023, tv_loss: 0.03395894914865494\n",
      "iteration 2049, dc_loss: 0.028514966368675232, tv_loss: 0.033943574875593185\n",
      "iteration 2050, dc_loss: 0.028466109186410904, tv_loss: 0.03397683426737785\n",
      "iteration 2051, dc_loss: 0.028477724641561508, tv_loss: 0.03396281972527504\n",
      "iteration 2052, dc_loss: 0.028467971831560135, tv_loss: 0.03398686274886131\n",
      "iteration 2053, dc_loss: 0.028442412614822388, tv_loss: 0.03400116041302681\n",
      "iteration 2054, dc_loss: 0.02845284342765808, tv_loss: 0.03396236523985863\n",
      "iteration 2055, dc_loss: 0.028442762792110443, tv_loss: 0.03395361453294754\n",
      "iteration 2056, dc_loss: 0.02842661365866661, tv_loss: 0.03399238735437393\n",
      "iteration 2057, dc_loss: 0.028432762250304222, tv_loss: 0.033986080437898636\n",
      "iteration 2058, dc_loss: 0.028419241309165955, tv_loss: 0.03397462144494057\n",
      "iteration 2059, dc_loss: 0.02838922291994095, tv_loss: 0.03397553786635399\n",
      "iteration 2060, dc_loss: 0.028409205377101898, tv_loss: 0.0339675098657608\n",
      "iteration 2061, dc_loss: 0.028389502316713333, tv_loss: 0.0340108685195446\n",
      "iteration 2062, dc_loss: 0.028364524245262146, tv_loss: 0.0339968167245388\n",
      "iteration 2063, dc_loss: 0.028387436643242836, tv_loss: 0.033952903002500534\n",
      "iteration 2064, dc_loss: 0.02836846187710762, tv_loss: 0.0339946374297142\n",
      "iteration 2065, dc_loss: 0.028350478038191795, tv_loss: 0.0340048223733902\n",
      "iteration 2066, dc_loss: 0.02836218848824501, tv_loss: 0.033964142203330994\n",
      "iteration 2067, dc_loss: 0.028326578438282013, tv_loss: 0.03399418666958809\n",
      "iteration 2068, dc_loss: 0.028332848101854324, tv_loss: 0.03400062024593353\n",
      "iteration 2069, dc_loss: 0.028347330167889595, tv_loss: 0.03396068885922432\n",
      "iteration 2070, dc_loss: 0.02829144336283207, tv_loss: 0.03400209918618202\n",
      "iteration 2071, dc_loss: 0.028304506093263626, tv_loss: 0.0339914932847023\n",
      "iteration 2072, dc_loss: 0.02832144871354103, tv_loss: 0.03395833075046539\n",
      "iteration 2073, dc_loss: 0.028278527781367302, tv_loss: 0.03398389741778374\n",
      "iteration 2074, dc_loss: 0.028292082250118256, tv_loss: 0.033962611109018326\n",
      "iteration 2075, dc_loss: 0.028277752920985222, tv_loss: 0.03396247327327728\n",
      "iteration 2076, dc_loss: 0.028261259198188782, tv_loss: 0.03397798538208008\n",
      "iteration 2077, dc_loss: 0.028275415301322937, tv_loss: 0.03395552933216095\n",
      "iteration 2078, dc_loss: 0.028241584077477455, tv_loss: 0.03398154675960541\n",
      "iteration 2079, dc_loss: 0.0282451044768095, tv_loss: 0.03396233916282654\n",
      "iteration 2080, dc_loss: 0.02824598364531994, tv_loss: 0.03396573290228844\n",
      "iteration 2081, dc_loss: 0.028218885883688927, tv_loss: 0.03398418053984642\n",
      "iteration 2082, dc_loss: 0.028221795335412025, tv_loss: 0.03397199511528015\n",
      "iteration 2083, dc_loss: 0.02821013703942299, tv_loss: 0.033966463059186935\n",
      "iteration 2084, dc_loss: 0.02819177880883217, tv_loss: 0.03397887945175171\n",
      "iteration 2085, dc_loss: 0.02820499613881111, tv_loss: 0.03395941108465195\n",
      "iteration 2086, dc_loss: 0.028187289834022522, tv_loss: 0.03396591544151306\n",
      "iteration 2087, dc_loss: 0.02818305790424347, tv_loss: 0.03397772088646889\n",
      "iteration 2088, dc_loss: 0.028171472251415253, tv_loss: 0.033983100205659866\n",
      "iteration 2089, dc_loss: 0.028157370164990425, tv_loss: 0.03398403897881508\n",
      "iteration 2090, dc_loss: 0.028152137994766235, tv_loss: 0.03396683186292648\n",
      "iteration 2091, dc_loss: 0.028153318911790848, tv_loss: 0.033964868634939194\n",
      "iteration 2092, dc_loss: 0.028131792321801186, tv_loss: 0.03398899361491203\n",
      "iteration 2093, dc_loss: 0.028134852647781372, tv_loss: 0.033985961228609085\n",
      "iteration 2094, dc_loss: 0.0281341802328825, tv_loss: 0.03396812453866005\n",
      "iteration 2095, dc_loss: 0.02809562161564827, tv_loss: 0.03398475795984268\n",
      "iteration 2096, dc_loss: 0.028106635436415672, tv_loss: 0.033966027200222015\n",
      "iteration 2097, dc_loss: 0.028115520253777504, tv_loss: 0.033959634602069855\n",
      "iteration 2098, dc_loss: 0.028080876916646957, tv_loss: 0.03399306535720825\n",
      "iteration 2099, dc_loss: 0.028087681159377098, tv_loss: 0.03398045524954796\n",
      "iteration 2100, dc_loss: 0.028072822839021683, tv_loss: 0.03397255018353462\n",
      "iteration 2101, dc_loss: 0.028077073395252228, tv_loss: 0.03396083414554596\n",
      "iteration 2102, dc_loss: 0.028066622093319893, tv_loss: 0.033958643674850464\n",
      "iteration 2103, dc_loss: 0.02803729474544525, tv_loss: 0.033988241106271744\n",
      "iteration 2104, dc_loss: 0.028044171631336212, tv_loss: 0.03397800773382187\n",
      "iteration 2105, dc_loss: 0.02803611010313034, tv_loss: 0.0339835099875927\n",
      "iteration 2106, dc_loss: 0.028036968782544136, tv_loss: 0.03397035226225853\n",
      "iteration 2107, dc_loss: 0.02801741473376751, tv_loss: 0.033973369747400284\n",
      "iteration 2108, dc_loss: 0.028008757159113884, tv_loss: 0.03397529572248459\n",
      "iteration 2109, dc_loss: 0.028020380064845085, tv_loss: 0.033944983035326004\n",
      "iteration 2110, dc_loss: 0.027985457330942154, tv_loss: 0.03398248925805092\n",
      "iteration 2111, dc_loss: 0.027982326224446297, tv_loss: 0.03398587554693222\n",
      "iteration 2112, dc_loss: 0.027998510748147964, tv_loss: 0.03395695239305496\n",
      "iteration 2113, dc_loss: 0.027975374832749367, tv_loss: 0.03397275134921074\n",
      "iteration 2114, dc_loss: 0.02795279026031494, tv_loss: 0.03397635743021965\n",
      "iteration 2115, dc_loss: 0.02797098271548748, tv_loss: 0.033955659717321396\n",
      "iteration 2116, dc_loss: 0.027950948104262352, tv_loss: 0.03396633267402649\n",
      "iteration 2117, dc_loss: 0.027933601289987564, tv_loss: 0.03397844359278679\n",
      "iteration 2118, dc_loss: 0.02794540673494339, tv_loss: 0.0339595191180706\n",
      "iteration 2119, dc_loss: 0.02793286368250847, tv_loss: 0.0339619517326355\n",
      "iteration 2120, dc_loss: 0.027919113636016846, tv_loss: 0.03398137167096138\n",
      "iteration 2121, dc_loss: 0.027913816273212433, tv_loss: 0.03397464379668236\n",
      "iteration 2122, dc_loss: 0.027892425656318665, tv_loss: 0.033980078995227814\n",
      "iteration 2123, dc_loss: 0.027898544445633888, tv_loss: 0.03396562114357948\n",
      "iteration 2124, dc_loss: 0.027897536754608154, tv_loss: 0.033950138837099075\n",
      "iteration 2125, dc_loss: 0.027876010164618492, tv_loss: 0.03397578373551369\n",
      "iteration 2126, dc_loss: 0.027892202138900757, tv_loss: 0.03394897282123566\n",
      "iteration 2127, dc_loss: 0.027857711538672447, tv_loss: 0.03398837149143219\n",
      "iteration 2128, dc_loss: 0.0278629120439291, tv_loss: 0.03398226201534271\n",
      "iteration 2129, dc_loss: 0.027851685881614685, tv_loss: 0.033978935331106186\n",
      "iteration 2130, dc_loss: 0.02785409614443779, tv_loss: 0.03395915776491165\n",
      "iteration 2131, dc_loss: 0.027831215411424637, tv_loss: 0.03397827222943306\n",
      "iteration 2132, dc_loss: 0.027844179421663284, tv_loss: 0.03396667167544365\n",
      "iteration 2133, dc_loss: 0.02783135138452053, tv_loss: 0.03400115296244621\n",
      "iteration 2134, dc_loss: 0.027817536145448685, tv_loss: 0.033990297466516495\n",
      "iteration 2135, dc_loss: 0.027822220697999, tv_loss: 0.03396698832511902\n",
      "iteration 2136, dc_loss: 0.027819540351629257, tv_loss: 0.03396616503596306\n",
      "iteration 2137, dc_loss: 0.027788154780864716, tv_loss: 0.03401970490813255\n",
      "iteration 2138, dc_loss: 0.02780858986079693, tv_loss: 0.033970680087804794\n",
      "iteration 2139, dc_loss: 0.02777109295129776, tv_loss: 0.033985916525125504\n",
      "iteration 2140, dc_loss: 0.0277810487896204, tv_loss: 0.033973656594753265\n",
      "iteration 2141, dc_loss: 0.027769843116402626, tv_loss: 0.033990755677223206\n",
      "iteration 2142, dc_loss: 0.027747072279453278, tv_loss: 0.03399099409580231\n",
      "iteration 2143, dc_loss: 0.027750862762331963, tv_loss: 0.03396710753440857\n",
      "iteration 2144, dc_loss: 0.027728969231247902, tv_loss: 0.033979400992393494\n",
      "iteration 2145, dc_loss: 0.027738139033317566, tv_loss: 0.03396892175078392\n",
      "iteration 2146, dc_loss: 0.027728814631700516, tv_loss: 0.03397941216826439\n",
      "iteration 2147, dc_loss: 0.02770623378455639, tv_loss: 0.03399480879306793\n",
      "iteration 2148, dc_loss: 0.027728235349059105, tv_loss: 0.03395513445138931\n",
      "iteration 2149, dc_loss: 0.027692006900906563, tv_loss: 0.03397474065423012\n",
      "iteration 2150, dc_loss: 0.02768980711698532, tv_loss: 0.03396810591220856\n",
      "iteration 2151, dc_loss: 0.02770458534359932, tv_loss: 0.03394616022706032\n",
      "iteration 2152, dc_loss: 0.027665717527270317, tv_loss: 0.03397916629910469\n",
      "iteration 2153, dc_loss: 0.027694648131728172, tv_loss: 0.033951375633478165\n",
      "iteration 2154, dc_loss: 0.027672838419675827, tv_loss: 0.03399603068828583\n",
      "iteration 2155, dc_loss: 0.02765343338251114, tv_loss: 0.033998217433691025\n",
      "iteration 2156, dc_loss: 0.02767253667116165, tv_loss: 0.03395535796880722\n",
      "iteration 2157, dc_loss: 0.027642356231808662, tv_loss: 0.033972762525081635\n",
      "iteration 2158, dc_loss: 0.02763727866113186, tv_loss: 0.03397445008158684\n",
      "iteration 2159, dc_loss: 0.027643969282507896, tv_loss: 0.03396331146359444\n",
      "iteration 2160, dc_loss: 0.027609998360276222, tv_loss: 0.033997710794210434\n",
      "iteration 2161, dc_loss: 0.027633320540189743, tv_loss: 0.03395743668079376\n",
      "iteration 2162, dc_loss: 0.027599945664405823, tv_loss: 0.03398197889328003\n",
      "iteration 2163, dc_loss: 0.02761719562113285, tv_loss: 0.033946141600608826\n",
      "iteration 2164, dc_loss: 0.02758382074534893, tv_loss: 0.03397165983915329\n",
      "iteration 2165, dc_loss: 0.02757871337234974, tv_loss: 0.03397655114531517\n",
      "iteration 2166, dc_loss: 0.02758663333952427, tv_loss: 0.03397990018129349\n",
      "iteration 2167, dc_loss: 0.02755354531109333, tv_loss: 0.03400595486164093\n",
      "iteration 2168, dc_loss: 0.027577612549066544, tv_loss: 0.03395361080765724\n",
      "iteration 2169, dc_loss: 0.027551164850592613, tv_loss: 0.033973053097724915\n",
      "iteration 2170, dc_loss: 0.02754085324704647, tv_loss: 0.03397916257381439\n",
      "iteration 2171, dc_loss: 0.027556506916880608, tv_loss: 0.03396080061793327\n",
      "iteration 2172, dc_loss: 0.027508504688739777, tv_loss: 0.03400972858071327\n",
      "iteration 2173, dc_loss: 0.027537377551198006, tv_loss: 0.033964768052101135\n",
      "iteration 2174, dc_loss: 0.027520284056663513, tv_loss: 0.03396981209516525\n",
      "iteration 2175, dc_loss: 0.02751498855650425, tv_loss: 0.03396298736333847\n",
      "iteration 2176, dc_loss: 0.027507102116942406, tv_loss: 0.033966854214668274\n",
      "iteration 2177, dc_loss: 0.02750057727098465, tv_loss: 0.03396497294306755\n",
      "iteration 2178, dc_loss: 0.027496084570884705, tv_loss: 0.03395889326930046\n",
      "iteration 2179, dc_loss: 0.027491994202136993, tv_loss: 0.03396112099289894\n",
      "iteration 2180, dc_loss: 0.027497880160808563, tv_loss: 0.03395838662981987\n",
      "iteration 2181, dc_loss: 0.027497028931975365, tv_loss: 0.033955901861190796\n",
      "iteration 2182, dc_loss: 0.02747369557619095, tv_loss: 0.03398674353957176\n",
      "iteration 2183, dc_loss: 0.02748272381722927, tv_loss: 0.03396456316113472\n",
      "iteration 2184, dc_loss: 0.02745693549513817, tv_loss: 0.034000858664512634\n",
      "iteration 2185, dc_loss: 0.027472082525491714, tv_loss: 0.03396899625658989\n",
      "iteration 2186, dc_loss: 0.027428291738033295, tv_loss: 0.033986806869506836\n",
      "iteration 2187, dc_loss: 0.027423085644841194, tv_loss: 0.03397015109658241\n",
      "iteration 2188, dc_loss: 0.027424288913607597, tv_loss: 0.03396432474255562\n",
      "iteration 2189, dc_loss: 0.0274024847894907, tv_loss: 0.03397107496857643\n",
      "iteration 2190, dc_loss: 0.02740182727575302, tv_loss: 0.033965494483709335\n",
      "iteration 2191, dc_loss: 0.027400998398661613, tv_loss: 0.03396403789520264\n",
      "iteration 2192, dc_loss: 0.02738557942211628, tv_loss: 0.033977728337049484\n",
      "iteration 2193, dc_loss: 0.02737702988088131, tv_loss: 0.03398090600967407\n",
      "iteration 2194, dc_loss: 0.027385246008634567, tv_loss: 0.03396504744887352\n",
      "iteration 2195, dc_loss: 0.02735995687544346, tv_loss: 0.03397464007139206\n",
      "iteration 2196, dc_loss: 0.0273740291595459, tv_loss: 0.03395625948905945\n",
      "iteration 2197, dc_loss: 0.027356499806046486, tv_loss: 0.033966779708862305\n",
      "iteration 2198, dc_loss: 0.027352482080459595, tv_loss: 0.03396574780344963\n",
      "iteration 2199, dc_loss: 0.02735240012407303, tv_loss: 0.0339568667113781\n",
      "iteration 2200, dc_loss: 0.02733001299202442, tv_loss: 0.0339745432138443\n",
      "iteration 2201, dc_loss: 0.027336835861206055, tv_loss: 0.0339699350297451\n",
      "iteration 2202, dc_loss: 0.027334263548254967, tv_loss: 0.033975303173065186\n",
      "iteration 2203, dc_loss: 0.02729923650622368, tv_loss: 0.03400540351867676\n",
      "iteration 2204, dc_loss: 0.027319246903061867, tv_loss: 0.03396333381533623\n",
      "iteration 2205, dc_loss: 0.027293726801872253, tv_loss: 0.033968765288591385\n",
      "iteration 2206, dc_loss: 0.027291107922792435, tv_loss: 0.03396415337920189\n",
      "iteration 2207, dc_loss: 0.02728363871574402, tv_loss: 0.033969275653362274\n",
      "iteration 2208, dc_loss: 0.027271848171949387, tv_loss: 0.03398760035634041\n",
      "iteration 2209, dc_loss: 0.027273377403616905, tv_loss: 0.03397171199321747\n",
      "iteration 2210, dc_loss: 0.027257468551397324, tv_loss: 0.03397565707564354\n",
      "iteration 2211, dc_loss: 0.02725830115377903, tv_loss: 0.033956099301576614\n",
      "iteration 2212, dc_loss: 0.027249112725257874, tv_loss: 0.0339619442820549\n",
      "iteration 2213, dc_loss: 0.027239952236413956, tv_loss: 0.033966176211833954\n",
      "iteration 2214, dc_loss: 0.027238676324486732, tv_loss: 0.03399120643734932\n",
      "iteration 2215, dc_loss: 0.0272175632417202, tv_loss: 0.03400392830371857\n",
      "iteration 2216, dc_loss: 0.02724360302090645, tv_loss: 0.03394828736782074\n",
      "iteration 2217, dc_loss: 0.02719619870185852, tv_loss: 0.033990781754255295\n",
      "iteration 2218, dc_loss: 0.027229860424995422, tv_loss: 0.0339733250439167\n",
      "iteration 2219, dc_loss: 0.027210760861635208, tv_loss: 0.03399117290973663\n",
      "iteration 2220, dc_loss: 0.027195634320378304, tv_loss: 0.03399083763360977\n",
      "iteration 2221, dc_loss: 0.027228020131587982, tv_loss: 0.03395719826221466\n",
      "iteration 2222, dc_loss: 0.027200687676668167, tv_loss: 0.033989522606134415\n",
      "iteration 2223, dc_loss: 0.027202889323234558, tv_loss: 0.03399809077382088\n",
      "iteration 2224, dc_loss: 0.02718527242541313, tv_loss: 0.03397513926029205\n",
      "iteration 2225, dc_loss: 0.027164818719029427, tv_loss: 0.034000612795352936\n",
      "iteration 2226, dc_loss: 0.027203531935811043, tv_loss: 0.03394032269716263\n",
      "iteration 2227, dc_loss: 0.02714865654706955, tv_loss: 0.033996421843767166\n",
      "iteration 2228, dc_loss: 0.027161207050085068, tv_loss: 0.033963143825531006\n",
      "iteration 2229, dc_loss: 0.027127806097269058, tv_loss: 0.033980049192905426\n",
      "iteration 2230, dc_loss: 0.02711823210120201, tv_loss: 0.033975716680288315\n",
      "iteration 2231, dc_loss: 0.0271331537514925, tv_loss: 0.03395993262529373\n",
      "iteration 2232, dc_loss: 0.027104685083031654, tv_loss: 0.03398503363132477\n",
      "iteration 2233, dc_loss: 0.027108611539006233, tv_loss: 0.0339667834341526\n",
      "iteration 2234, dc_loss: 0.02709290012717247, tv_loss: 0.03397412598133087\n",
      "iteration 2235, dc_loss: 0.027085578069090843, tv_loss: 0.03397531434893608\n",
      "iteration 2236, dc_loss: 0.027097104117274284, tv_loss: 0.0339595302939415\n",
      "iteration 2237, dc_loss: 0.02706894464790821, tv_loss: 0.03397878631949425\n",
      "iteration 2238, dc_loss: 0.027080165222287178, tv_loss: 0.03395964205265045\n",
      "iteration 2239, dc_loss: 0.027047626674175262, tv_loss: 0.033983927220106125\n",
      "iteration 2240, dc_loss: 0.027061572298407555, tv_loss: 0.03396998345851898\n",
      "iteration 2241, dc_loss: 0.027059532701969147, tv_loss: 0.03396337106823921\n",
      "iteration 2242, dc_loss: 0.027042552828788757, tv_loss: 0.033976517617702484\n",
      "iteration 2243, dc_loss: 0.027048729360103607, tv_loss: 0.03395900875329971\n",
      "iteration 2244, dc_loss: 0.02703736536204815, tv_loss: 0.03396866098046303\n",
      "iteration 2245, dc_loss: 0.02704235538840294, tv_loss: 0.03395969420671463\n",
      "iteration 2246, dc_loss: 0.0270213782787323, tv_loss: 0.03397132828831673\n",
      "iteration 2247, dc_loss: 0.027041617780923843, tv_loss: 0.03395281359553337\n",
      "iteration 2248, dc_loss: 0.027021894231438637, tv_loss: 0.033987656235694885\n",
      "iteration 2249, dc_loss: 0.0270308218896389, tv_loss: 0.033983346074819565\n",
      "iteration 2250, dc_loss: 0.02699773758649826, tv_loss: 0.03398893401026726\n",
      "iteration 2251, dc_loss: 0.02700611762702465, tv_loss: 0.033960603177547455\n",
      "iteration 2252, dc_loss: 0.026981830596923828, tv_loss: 0.03398306295275688\n",
      "iteration 2253, dc_loss: 0.02700427919626236, tv_loss: 0.03395348787307739\n",
      "iteration 2254, dc_loss: 0.02695777826011181, tv_loss: 0.034008342772722244\n",
      "iteration 2255, dc_loss: 0.026964932680130005, tv_loss: 0.03397892788052559\n",
      "iteration 2256, dc_loss: 0.02696235477924347, tv_loss: 0.033963412046432495\n",
      "iteration 2257, dc_loss: 0.026935841888189316, tv_loss: 0.03398260101675987\n",
      "iteration 2258, dc_loss: 0.02694958634674549, tv_loss: 0.033962029963731766\n",
      "iteration 2259, dc_loss: 0.026921799406409264, tv_loss: 0.03398067131638527\n",
      "iteration 2260, dc_loss: 0.02692583203315735, tv_loss: 0.033962178975343704\n",
      "iteration 2261, dc_loss: 0.02691570296883583, tv_loss: 0.033962398767471313\n",
      "iteration 2262, dc_loss: 0.026907121762633324, tv_loss: 0.033961206674575806\n",
      "iteration 2263, dc_loss: 0.026917926967144012, tv_loss: 0.033947259187698364\n",
      "iteration 2264, dc_loss: 0.026878762990236282, tv_loss: 0.03399069607257843\n",
      "iteration 2265, dc_loss: 0.026905372738838196, tv_loss: 0.03398032486438751\n",
      "iteration 2266, dc_loss: 0.0268714502453804, tv_loss: 0.03398789465427399\n",
      "iteration 2267, dc_loss: 0.026876889169216156, tv_loss: 0.033962398767471313\n",
      "iteration 2268, dc_loss: 0.02687525376677513, tv_loss: 0.03395957499742508\n",
      "iteration 2269, dc_loss: 0.02684868313372135, tv_loss: 0.03400017321109772\n",
      "iteration 2270, dc_loss: 0.02688283659517765, tv_loss: 0.03395119309425354\n",
      "iteration 2271, dc_loss: 0.026831820607185364, tv_loss: 0.033986952155828476\n",
      "iteration 2272, dc_loss: 0.026831066235899925, tv_loss: 0.03397206589579582\n",
      "iteration 2273, dc_loss: 0.0268549881875515, tv_loss: 0.033955350518226624\n",
      "iteration 2274, dc_loss: 0.02682466246187687, tv_loss: 0.03400178253650665\n",
      "iteration 2275, dc_loss: 0.026823945343494415, tv_loss: 0.0339764840900898\n",
      "iteration 2276, dc_loss: 0.026808297261595726, tv_loss: 0.03396790474653244\n",
      "iteration 2277, dc_loss: 0.026813972741365433, tv_loss: 0.033971529453992844\n",
      "iteration 2278, dc_loss: 0.026809467002749443, tv_loss: 0.03398938477039337\n",
      "iteration 2279, dc_loss: 0.02679695375263691, tv_loss: 0.033982258290052414\n",
      "iteration 2280, dc_loss: 0.026780087500810623, tv_loss: 0.03397773206233978\n",
      "iteration 2281, dc_loss: 0.02678561396896839, tv_loss: 0.03397331014275551\n",
      "iteration 2282, dc_loss: 0.026787757873535156, tv_loss: 0.03397686034440994\n",
      "iteration 2283, dc_loss: 0.026798363775014877, tv_loss: 0.03397361934185028\n",
      "iteration 2284, dc_loss: 0.02678738720715046, tv_loss: 0.033985160291194916\n",
      "iteration 2285, dc_loss: 0.026824327185750008, tv_loss: 0.03395976126194\n",
      "iteration 2286, dc_loss: 0.026856038719415665, tv_loss: 0.03398165479302406\n",
      "iteration 2287, dc_loss: 0.026874052360653877, tv_loss: 0.03394995257258415\n",
      "iteration 2288, dc_loss: 0.0268313717097044, tv_loss: 0.03399350494146347\n",
      "iteration 2289, dc_loss: 0.02684011496603489, tv_loss: 0.03394582122564316\n",
      "iteration 2290, dc_loss: 0.026822833344340324, tv_loss: 0.03395356610417366\n",
      "iteration 2291, dc_loss: 0.026758883148431778, tv_loss: 0.033993061631917953\n",
      "iteration 2292, dc_loss: 0.026737431064248085, tv_loss: 0.033966317772865295\n",
      "iteration 2293, dc_loss: 0.026740530505776405, tv_loss: 0.03394018113613129\n",
      "iteration 2294, dc_loss: 0.026680734008550644, tv_loss: 0.03399958461523056\n",
      "iteration 2295, dc_loss: 0.026720097288489342, tv_loss: 0.03395376726984978\n",
      "iteration 2296, dc_loss: 0.026712000370025635, tv_loss: 0.033970095217227936\n",
      "iteration 2297, dc_loss: 0.02672933228313923, tv_loss: 0.033974822610616684\n",
      "iteration 2298, dc_loss: 0.026737842708826065, tv_loss: 0.0339469388127327\n",
      "iteration 2299, dc_loss: 0.026681454852223396, tv_loss: 0.03398561477661133\n",
      "iteration 2300, dc_loss: 0.02670300379395485, tv_loss: 0.03395281732082367\n",
      "iteration 2301, dc_loss: 0.026658302173018456, tv_loss: 0.033974211663007736\n",
      "iteration 2302, dc_loss: 0.02665364369750023, tv_loss: 0.03395979478955269\n",
      "iteration 2303, dc_loss: 0.02665560320019722, tv_loss: 0.03396464139223099\n",
      "iteration 2304, dc_loss: 0.026638075709342957, tv_loss: 0.03397607430815697\n",
      "iteration 2305, dc_loss: 0.026643339544534683, tv_loss: 0.03396925702691078\n",
      "iteration 2306, dc_loss: 0.02665337175130844, tv_loss: 0.03397037461400032\n",
      "iteration 2307, dc_loss: 0.026642978191375732, tv_loss: 0.0339512936770916\n",
      "iteration 2308, dc_loss: 0.026612896472215652, tv_loss: 0.03397168964147568\n",
      "iteration 2309, dc_loss: 0.026635343208909035, tv_loss: 0.03394978120923042\n",
      "iteration 2310, dc_loss: 0.026587801054120064, tv_loss: 0.03399388864636421\n",
      "iteration 2311, dc_loss: 0.026600118726491928, tv_loss: 0.03398096188902855\n",
      "iteration 2312, dc_loss: 0.026587598025798798, tv_loss: 0.033979907631874084\n",
      "iteration 2313, dc_loss: 0.02659006603062153, tv_loss: 0.03396229073405266\n",
      "iteration 2314, dc_loss: 0.026570826768875122, tv_loss: 0.033981285989284515\n",
      "iteration 2315, dc_loss: 0.026570798829197884, tv_loss: 0.03397273272275925\n",
      "iteration 2316, dc_loss: 0.026576634496450424, tv_loss: 0.033961303532123566\n",
      "iteration 2317, dc_loss: 0.026552913710474968, tv_loss: 0.0339822843670845\n",
      "iteration 2318, dc_loss: 0.026571298018097878, tv_loss: 0.03394891321659088\n",
      "iteration 2319, dc_loss: 0.026531817391514778, tv_loss: 0.03398152440786362\n",
      "iteration 2320, dc_loss: 0.026558494195342064, tv_loss: 0.03394576907157898\n",
      "iteration 2321, dc_loss: 0.026498515158891678, tv_loss: 0.03399994969367981\n",
      "iteration 2322, dc_loss: 0.026540393009781837, tv_loss: 0.03394756466150284\n",
      "iteration 2323, dc_loss: 0.026518482714891434, tv_loss: 0.033977434039115906\n",
      "iteration 2324, dc_loss: 0.026501251384615898, tv_loss: 0.033989377319812775\n",
      "iteration 2325, dc_loss: 0.026536639779806137, tv_loss: 0.03394702821969986\n",
      "iteration 2326, dc_loss: 0.02647976391017437, tv_loss: 0.033997345715761185\n",
      "iteration 2327, dc_loss: 0.02651391364634037, tv_loss: 0.033943794667720795\n",
      "iteration 2328, dc_loss: 0.026468273252248764, tv_loss: 0.03398246690630913\n",
      "iteration 2329, dc_loss: 0.026508865877985954, tv_loss: 0.03393775224685669\n",
      "iteration 2330, dc_loss: 0.026468880474567413, tv_loss: 0.03397279605269432\n",
      "iteration 2331, dc_loss: 0.026498524472117424, tv_loss: 0.03393888473510742\n",
      "iteration 2332, dc_loss: 0.02645469829440117, tv_loss: 0.03399841487407684\n",
      "iteration 2333, dc_loss: 0.026473330333828926, tv_loss: 0.03398742526769638\n",
      "iteration 2334, dc_loss: 0.026455948129296303, tv_loss: 0.033979739993810654\n",
      "iteration 2335, dc_loss: 0.026457644999027252, tv_loss: 0.033960066735744476\n",
      "iteration 2336, dc_loss: 0.02644485794007778, tv_loss: 0.03397209197282791\n",
      "iteration 2337, dc_loss: 0.026448054239153862, tv_loss: 0.03398596867918968\n",
      "iteration 2338, dc_loss: 0.02643493562936783, tv_loss: 0.0339980274438858\n",
      "iteration 2339, dc_loss: 0.026435833424329758, tv_loss: 0.03396332263946533\n",
      "iteration 2340, dc_loss: 0.02642260491847992, tv_loss: 0.03398408368229866\n",
      "iteration 2341, dc_loss: 0.026416882872581482, tv_loss: 0.03399054333567619\n",
      "iteration 2342, dc_loss: 0.02642008289694786, tv_loss: 0.03397616744041443\n",
      "iteration 2343, dc_loss: 0.026420047506690025, tv_loss: 0.033947475254535675\n",
      "iteration 2344, dc_loss: 0.026379000395536423, tv_loss: 0.03399532660841942\n",
      "iteration 2345, dc_loss: 0.026398681104183197, tv_loss: 0.03396881744265556\n",
      "iteration 2346, dc_loss: 0.02638295106589794, tv_loss: 0.033985435962677\n",
      "iteration 2347, dc_loss: 0.026390835642814636, tv_loss: 0.033956002444028854\n",
      "iteration 2348, dc_loss: 0.026348743587732315, tv_loss: 0.033986080437898636\n",
      "iteration 2349, dc_loss: 0.026373902335762978, tv_loss: 0.03396068513393402\n",
      "iteration 2350, dc_loss: 0.026359690353274345, tv_loss: 0.033974383026361465\n",
      "iteration 2351, dc_loss: 0.02636021561920643, tv_loss: 0.03396137058734894\n",
      "iteration 2352, dc_loss: 0.026346800848841667, tv_loss: 0.033969007432460785\n",
      "iteration 2353, dc_loss: 0.026340415701270103, tv_loss: 0.0339578241109848\n",
      "iteration 2354, dc_loss: 0.026315324008464813, tv_loss: 0.03397039696574211\n",
      "iteration 2355, dc_loss: 0.02633402682840824, tv_loss: 0.03396036475896835\n",
      "iteration 2356, dc_loss: 0.02631860226392746, tv_loss: 0.03397847339510918\n",
      "iteration 2357, dc_loss: 0.02631586790084839, tv_loss: 0.03397379443049431\n",
      "iteration 2358, dc_loss: 0.02631043642759323, tv_loss: 0.03396366164088249\n",
      "iteration 2359, dc_loss: 0.026307720690965652, tv_loss: 0.033958569169044495\n",
      "iteration 2360, dc_loss: 0.026295432820916176, tv_loss: 0.03396313637495041\n",
      "iteration 2361, dc_loss: 0.026289530098438263, tv_loss: 0.0339692123234272\n",
      "iteration 2362, dc_loss: 0.02629392221570015, tv_loss: 0.03398073837161064\n",
      "iteration 2363, dc_loss: 0.02629457227885723, tv_loss: 0.03397882729768753\n",
      "iteration 2364, dc_loss: 0.02626795321702957, tv_loss: 0.033992670476436615\n",
      "iteration 2365, dc_loss: 0.026302142068743706, tv_loss: 0.03394347056746483\n",
      "iteration 2366, dc_loss: 0.02626798115670681, tv_loss: 0.03398704156279564\n",
      "iteration 2367, dc_loss: 0.02630312740802765, tv_loss: 0.03396507725119591\n",
      "iteration 2368, dc_loss: 0.026264218613505363, tv_loss: 0.033999715000391006\n",
      "iteration 2369, dc_loss: 0.026304196566343307, tv_loss: 0.0339399054646492\n",
      "iteration 2370, dc_loss: 0.02625753916800022, tv_loss: 0.03399059548974037\n",
      "iteration 2371, dc_loss: 0.026286372914910316, tv_loss: 0.033951520919799805\n",
      "iteration 2372, dc_loss: 0.026242347434163094, tv_loss: 0.034004390239715576\n",
      "iteration 2373, dc_loss: 0.026267169043421745, tv_loss: 0.03395118564367294\n",
      "iteration 2374, dc_loss: 0.026212425902485847, tv_loss: 0.033986400812864304\n",
      "iteration 2375, dc_loss: 0.026221483945846558, tv_loss: 0.03395451605319977\n",
      "iteration 2376, dc_loss: 0.02620570920407772, tv_loss: 0.033965256065130234\n",
      "iteration 2377, dc_loss: 0.026194361969828606, tv_loss: 0.033974550664424896\n",
      "iteration 2378, dc_loss: 0.026179993525147438, tv_loss: 0.03398635610938072\n",
      "iteration 2379, dc_loss: 0.02616804838180542, tv_loss: 0.0339764766395092\n",
      "iteration 2380, dc_loss: 0.026189405471086502, tv_loss: 0.03394802659749985\n",
      "iteration 2381, dc_loss: 0.026163354516029358, tv_loss: 0.033976148813962936\n",
      "iteration 2382, dc_loss: 0.02617953158915043, tv_loss: 0.033953212201595306\n",
      "iteration 2383, dc_loss: 0.026149185374379158, tv_loss: 0.0339723564684391\n",
      "iteration 2384, dc_loss: 0.026168975979089737, tv_loss: 0.033943794667720795\n",
      "iteration 2385, dc_loss: 0.02612561173737049, tv_loss: 0.0339910164475441\n",
      "iteration 2386, dc_loss: 0.026183392852544785, tv_loss: 0.03392902389168739\n",
      "iteration 2387, dc_loss: 0.026124441996216774, tv_loss: 0.033985886722803116\n",
      "iteration 2388, dc_loss: 0.02615043707191944, tv_loss: 0.03396272286772728\n",
      "iteration 2389, dc_loss: 0.02612048014998436, tv_loss: 0.03397944197058678\n",
      "iteration 2390, dc_loss: 0.026137778535485268, tv_loss: 0.0339561328291893\n",
      "iteration 2391, dc_loss: 0.026113707572221756, tv_loss: 0.03396749123930931\n",
      "iteration 2392, dc_loss: 0.026128074154257774, tv_loss: 0.03394690901041031\n",
      "iteration 2393, dc_loss: 0.02609087899327278, tv_loss: 0.033976130187511444\n",
      "iteration 2394, dc_loss: 0.026134664192795753, tv_loss: 0.033930324018001556\n",
      "iteration 2395, dc_loss: 0.026086999103426933, tv_loss: 0.0339733324944973\n",
      "iteration 2396, dc_loss: 0.02610110491514206, tv_loss: 0.03397119790315628\n",
      "iteration 2397, dc_loss: 0.02609020657837391, tv_loss: 0.03397916629910469\n",
      "iteration 2398, dc_loss: 0.026079382747411728, tv_loss: 0.03397858142852783\n",
      "iteration 2399, dc_loss: 0.02609182894229889, tv_loss: 0.033948589116334915\n",
      "iteration 2400, dc_loss: 0.02605479396879673, tv_loss: 0.03397946432232857\n",
      "iteration 2401, dc_loss: 0.02607344277203083, tv_loss: 0.03396615758538246\n",
      "iteration 2402, dc_loss: 0.026039259508252144, tv_loss: 0.033971257507801056\n",
      "iteration 2403, dc_loss: 0.02603721246123314, tv_loss: 0.03395130857825279\n",
      "iteration 2404, dc_loss: 0.026053443551063538, tv_loss: 0.03394939377903938\n",
      "iteration 2405, dc_loss: 0.026025986298918724, tv_loss: 0.03396518528461456\n",
      "iteration 2406, dc_loss: 0.026020705699920654, tv_loss: 0.033959709107875824\n",
      "iteration 2407, dc_loss: 0.02603358030319214, tv_loss: 0.03393358737230301\n",
      "iteration 2408, dc_loss: 0.026014944538474083, tv_loss: 0.03396555036306381\n",
      "iteration 2409, dc_loss: 0.02600322663784027, tv_loss: 0.03397016227245331\n",
      "iteration 2410, dc_loss: 0.02601620741188526, tv_loss: 0.03393898531794548\n",
      "iteration 2411, dc_loss: 0.025995822623372078, tv_loss: 0.0339561365544796\n",
      "iteration 2412, dc_loss: 0.026001818478107452, tv_loss: 0.03394521027803421\n",
      "iteration 2413, dc_loss: 0.025998029857873917, tv_loss: 0.03394561633467674\n",
      "iteration 2414, dc_loss: 0.025978079065680504, tv_loss: 0.033959269523620605\n",
      "iteration 2415, dc_loss: 0.02599479816854, tv_loss: 0.033940572291612625\n",
      "iteration 2416, dc_loss: 0.02597956173121929, tv_loss: 0.0339391864836216\n",
      "iteration 2417, dc_loss: 0.025956952944397926, tv_loss: 0.03395779803395271\n",
      "iteration 2418, dc_loss: 0.025977831333875656, tv_loss: 0.03393780067563057\n",
      "iteration 2419, dc_loss: 0.02596150152385235, tv_loss: 0.03393867611885071\n",
      "iteration 2420, dc_loss: 0.025948062539100647, tv_loss: 0.03395228460431099\n",
      "iteration 2421, dc_loss: 0.025964220985770226, tv_loss: 0.033930275589227676\n",
      "iteration 2422, dc_loss: 0.025949254631996155, tv_loss: 0.033939190208911896\n",
      "iteration 2423, dc_loss: 0.025942625477910042, tv_loss: 0.0339432917535305\n",
      "iteration 2424, dc_loss: 0.025941917672753334, tv_loss: 0.03394218906760216\n",
      "iteration 2425, dc_loss: 0.025929586961865425, tv_loss: 0.033956289291381836\n",
      "iteration 2426, dc_loss: 0.0259215347468853, tv_loss: 0.03396926075220108\n",
      "iteration 2427, dc_loss: 0.025937844067811966, tv_loss: 0.03393765911459923\n",
      "iteration 2428, dc_loss: 0.025915440171957016, tv_loss: 0.03395313024520874\n",
      "iteration 2429, dc_loss: 0.02591850236058235, tv_loss: 0.03394239395856857\n",
      "iteration 2430, dc_loss: 0.025914093479514122, tv_loss: 0.033949315547943115\n",
      "iteration 2431, dc_loss: 0.025896945968270302, tv_loss: 0.03397267311811447\n",
      "iteration 2432, dc_loss: 0.02591070905327797, tv_loss: 0.03394068032503128\n",
      "iteration 2433, dc_loss: 0.02589372731745243, tv_loss: 0.03394949063658714\n",
      "iteration 2434, dc_loss: 0.025882909074425697, tv_loss: 0.03395312651991844\n",
      "iteration 2435, dc_loss: 0.025903278961777687, tv_loss: 0.03392792493104935\n",
      "iteration 2436, dc_loss: 0.025881541892886162, tv_loss: 0.0339580662548542\n",
      "iteration 2437, dc_loss: 0.025874201208353043, tv_loss: 0.033956680446863174\n",
      "iteration 2438, dc_loss: 0.025881262496113777, tv_loss: 0.033941250294446945\n",
      "iteration 2439, dc_loss: 0.025867534801363945, tv_loss: 0.03394734114408493\n",
      "iteration 2440, dc_loss: 0.025864161550998688, tv_loss: 0.03395906090736389\n",
      "iteration 2441, dc_loss: 0.025871464982628822, tv_loss: 0.033940162509679794\n",
      "iteration 2442, dc_loss: 0.025855902582406998, tv_loss: 0.03395839035511017\n",
      "iteration 2443, dc_loss: 0.025843802839517593, tv_loss: 0.03395012393593788\n",
      "iteration 2444, dc_loss: 0.02585119567811489, tv_loss: 0.03394445776939392\n",
      "iteration 2445, dc_loss: 0.025846272706985474, tv_loss: 0.033960312604904175\n",
      "iteration 2446, dc_loss: 0.02583392523229122, tv_loss: 0.033962052315473557\n",
      "iteration 2447, dc_loss: 0.02583906799554825, tv_loss: 0.03393922001123428\n",
      "iteration 2448, dc_loss: 0.025829678401350975, tv_loss: 0.03394300490617752\n",
      "iteration 2449, dc_loss: 0.025825221091508865, tv_loss: 0.033956050872802734\n",
      "iteration 2450, dc_loss: 0.025827279314398766, tv_loss: 0.03395157679915428\n",
      "iteration 2451, dc_loss: 0.02581307291984558, tv_loss: 0.033952172845602036\n",
      "iteration 2452, dc_loss: 0.025813085958361626, tv_loss: 0.03393819183111191\n",
      "iteration 2453, dc_loss: 0.02580675669014454, tv_loss: 0.03394163399934769\n",
      "iteration 2454, dc_loss: 0.0258009135723114, tv_loss: 0.033956337720155716\n",
      "iteration 2455, dc_loss: 0.02580195665359497, tv_loss: 0.03395843878388405\n",
      "iteration 2456, dc_loss: 0.02578672021627426, tv_loss: 0.0339558906853199\n",
      "iteration 2457, dc_loss: 0.025803271681070328, tv_loss: 0.033925674855709076\n",
      "iteration 2458, dc_loss: 0.0257814209908247, tv_loss: 0.03394215926527977\n",
      "iteration 2459, dc_loss: 0.025771938264369965, tv_loss: 0.03396543860435486\n",
      "iteration 2460, dc_loss: 0.025787780061364174, tv_loss: 0.03395991772413254\n",
      "iteration 2461, dc_loss: 0.025767024606466293, tv_loss: 0.033953264355659485\n",
      "iteration 2462, dc_loss: 0.025768477469682693, tv_loss: 0.03394113481044769\n",
      "iteration 2463, dc_loss: 0.025769807398319244, tv_loss: 0.033947017043828964\n",
      "iteration 2464, dc_loss: 0.02574916183948517, tv_loss: 0.03398435562849045\n",
      "iteration 2465, dc_loss: 0.025760255753993988, tv_loss: 0.03394410386681557\n",
      "iteration 2466, dc_loss: 0.025747613981366158, tv_loss: 0.03394841030240059\n",
      "iteration 2467, dc_loss: 0.02574213407933712, tv_loss: 0.033969447016716\n",
      "iteration 2468, dc_loss: 0.02575351670384407, tv_loss: 0.03395523875951767\n",
      "iteration 2469, dc_loss: 0.02572772093117237, tv_loss: 0.033957552164793015\n",
      "iteration 2470, dc_loss: 0.025732889771461487, tv_loss: 0.03395039588212967\n",
      "iteration 2471, dc_loss: 0.02573234587907791, tv_loss: 0.033953968435525894\n",
      "iteration 2472, dc_loss: 0.02571381814777851, tv_loss: 0.03395966812968254\n",
      "iteration 2473, dc_loss: 0.02572510577738285, tv_loss: 0.03393884748220444\n",
      "iteration 2474, dc_loss: 0.025723623111844063, tv_loss: 0.033933695405721664\n",
      "iteration 2475, dc_loss: 0.025704586878418922, tv_loss: 0.03395170718431473\n",
      "iteration 2476, dc_loss: 0.0257108174264431, tv_loss: 0.03394858166575432\n",
      "iteration 2477, dc_loss: 0.025695495307445526, tv_loss: 0.03395697847008705\n",
      "iteration 2478, dc_loss: 0.025702867656946182, tv_loss: 0.03393750265240669\n",
      "iteration 2479, dc_loss: 0.025696635246276855, tv_loss: 0.03393162786960602\n",
      "iteration 2480, dc_loss: 0.025679979473352432, tv_loss: 0.03394497185945511\n",
      "iteration 2481, dc_loss: 0.025688812136650085, tv_loss: 0.033930469304323196\n",
      "iteration 2482, dc_loss: 0.025677360594272614, tv_loss: 0.033934637904167175\n",
      "iteration 2483, dc_loss: 0.025672610849142075, tv_loss: 0.0339360311627388\n",
      "iteration 2484, dc_loss: 0.025680815801024437, tv_loss: 0.03391926735639572\n",
      "iteration 2485, dc_loss: 0.02566390670835972, tv_loss: 0.03393477573990822\n",
      "iteration 2486, dc_loss: 0.02566271275281906, tv_loss: 0.03393261134624481\n",
      "iteration 2487, dc_loss: 0.025661703199148178, tv_loss: 0.03393689915537834\n",
      "iteration 2488, dc_loss: 0.025645816698670387, tv_loss: 0.033961158245801926\n",
      "iteration 2489, dc_loss: 0.025654109194874763, tv_loss: 0.03395452722907066\n",
      "iteration 2490, dc_loss: 0.025655405595898628, tv_loss: 0.03393075242638588\n",
      "iteration 2491, dc_loss: 0.02562200278043747, tv_loss: 0.03395380824804306\n",
      "iteration 2492, dc_loss: 0.025640049949288368, tv_loss: 0.03393549472093582\n",
      "iteration 2493, dc_loss: 0.025645701214671135, tv_loss: 0.03392621502280235\n",
      "iteration 2494, dc_loss: 0.02561243064701557, tv_loss: 0.03395920991897583\n",
      "iteration 2495, dc_loss: 0.025626586750149727, tv_loss: 0.03393901512026787\n",
      "iteration 2496, dc_loss: 0.025628073140978813, tv_loss: 0.0339401476085186\n",
      "iteration 2497, dc_loss: 0.02561534009873867, tv_loss: 0.03393947705626488\n",
      "iteration 2498, dc_loss: 0.025612596422433853, tv_loss: 0.033939845860004425\n",
      "iteration 2499, dc_loss: 0.025607125833630562, tv_loss: 0.03393975645303726\n",
      "iteration 2500, dc_loss: 0.02559649758040905, tv_loss: 0.03394079953432083\n",
      "iteration 2501, dc_loss: 0.02560056746006012, tv_loss: 0.033930882811546326\n",
      "iteration 2502, dc_loss: 0.025588586926460266, tv_loss: 0.03394453227519989\n",
      "iteration 2503, dc_loss: 0.025589032098650932, tv_loss: 0.03393473103642464\n",
      "iteration 2504, dc_loss: 0.025592772290110588, tv_loss: 0.0339275561273098\n",
      "iteration 2505, dc_loss: 0.02557326853275299, tv_loss: 0.03394244983792305\n",
      "iteration 2506, dc_loss: 0.025574877858161926, tv_loss: 0.033943451941013336\n",
      "iteration 2507, dc_loss: 0.02557765319943428, tv_loss: 0.033926934003829956\n",
      "iteration 2508, dc_loss: 0.025561895221471786, tv_loss: 0.03393742814660072\n",
      "iteration 2509, dc_loss: 0.025564854964613914, tv_loss: 0.033959951251745224\n",
      "iteration 2510, dc_loss: 0.0255690086632967, tv_loss: 0.03395604342222214\n",
      "iteration 2511, dc_loss: 0.025549253448843956, tv_loss: 0.03393976390361786\n",
      "iteration 2512, dc_loss: 0.02554580383002758, tv_loss: 0.03395025432109833\n",
      "iteration 2513, dc_loss: 0.025550104677677155, tv_loss: 0.0339612178504467\n",
      "iteration 2514, dc_loss: 0.025539560243487358, tv_loss: 0.033961813896894455\n",
      "iteration 2515, dc_loss: 0.025539444759488106, tv_loss: 0.033934999257326126\n",
      "iteration 2516, dc_loss: 0.02553202584385872, tv_loss: 0.033939797431230545\n",
      "iteration 2517, dc_loss: 0.025536159053444862, tv_loss: 0.03394172713160515\n",
      "iteration 2518, dc_loss: 0.025527428835630417, tv_loss: 0.03395385295152664\n",
      "iteration 2519, dc_loss: 0.025512948632240295, tv_loss: 0.033951401710510254\n",
      "iteration 2520, dc_loss: 0.025522859767079353, tv_loss: 0.03393152356147766\n",
      "iteration 2521, dc_loss: 0.025509532541036606, tv_loss: 0.03393950313329697\n",
      "iteration 2522, dc_loss: 0.025507545098662376, tv_loss: 0.03395074978470802\n",
      "iteration 2523, dc_loss: 0.02551012672483921, tv_loss: 0.033943239599466324\n",
      "iteration 2524, dc_loss: 0.025491535663604736, tv_loss: 0.033956706523895264\n",
      "iteration 2525, dc_loss: 0.025496268644928932, tv_loss: 0.033938124775886536\n",
      "iteration 2526, dc_loss: 0.025496352463960648, tv_loss: 0.033931881189346313\n",
      "iteration 2527, dc_loss: 0.02548040822148323, tv_loss: 0.0339520126581192\n",
      "iteration 2528, dc_loss: 0.025483451783657074, tv_loss: 0.03394441679120064\n",
      "iteration 2529, dc_loss: 0.025476176291704178, tv_loss: 0.0339389443397522\n",
      "iteration 2530, dc_loss: 0.025479381904006004, tv_loss: 0.03393048048019409\n",
      "iteration 2531, dc_loss: 0.025471292436122894, tv_loss: 0.033931635320186615\n",
      "iteration 2532, dc_loss: 0.02546345442533493, tv_loss: 0.03393792733550072\n",
      "iteration 2533, dc_loss: 0.02546110562980175, tv_loss: 0.033937033265829086\n",
      "iteration 2534, dc_loss: 0.02545526623725891, tv_loss: 0.03394823521375656\n",
      "iteration 2535, dc_loss: 0.02546059526503086, tv_loss: 0.03393646329641342\n",
      "iteration 2536, dc_loss: 0.02544689178466797, tv_loss: 0.03393897786736488\n",
      "iteration 2537, dc_loss: 0.02543318085372448, tv_loss: 0.033944811671972275\n",
      "iteration 2538, dc_loss: 0.025449352338910103, tv_loss: 0.03392573073506355\n",
      "iteration 2539, dc_loss: 0.02543279156088829, tv_loss: 0.033931050449609756\n",
      "iteration 2540, dc_loss: 0.02542625553905964, tv_loss: 0.03393379971385002\n",
      "iteration 2541, dc_loss: 0.025433994829654694, tv_loss: 0.033926934003829956\n",
      "iteration 2542, dc_loss: 0.025421123951673508, tv_loss: 0.033937834203243256\n",
      "iteration 2543, dc_loss: 0.02541872300207615, tv_loss: 0.03394662216305733\n",
      "iteration 2544, dc_loss: 0.025411168113350868, tv_loss: 0.03394607454538345\n",
      "iteration 2545, dc_loss: 0.02541278302669525, tv_loss: 0.03393641486763954\n",
      "iteration 2546, dc_loss: 0.025408394634723663, tv_loss: 0.033928461372852325\n",
      "iteration 2547, dc_loss: 0.02540021762251854, tv_loss: 0.03393169119954109\n",
      "iteration 2548, dc_loss: 0.02539466693997383, tv_loss: 0.03394196555018425\n",
      "iteration 2549, dc_loss: 0.025390025228261948, tv_loss: 0.0339435413479805\n",
      "iteration 2550, dc_loss: 0.025399064645171165, tv_loss: 0.03393922373652458\n",
      "iteration 2551, dc_loss: 0.025385435670614243, tv_loss: 0.033935196697711945\n",
      "iteration 2552, dc_loss: 0.02537379413843155, tv_loss: 0.03394373878836632\n",
      "iteration 2553, dc_loss: 0.02537543512880802, tv_loss: 0.03393534943461418\n",
      "iteration 2554, dc_loss: 0.02537708543241024, tv_loss: 0.03393125534057617\n",
      "iteration 2555, dc_loss: 0.025362590327858925, tv_loss: 0.033957045525312424\n",
      "iteration 2556, dc_loss: 0.025359442457556725, tv_loss: 0.03395240753889084\n",
      "iteration 2557, dc_loss: 0.02537437342107296, tv_loss: 0.033928532153367996\n",
      "iteration 2558, dc_loss: 0.02535087987780571, tv_loss: 0.03393600508570671\n",
      "iteration 2559, dc_loss: 0.025350123643875122, tv_loss: 0.03394710645079613\n",
      "iteration 2560, dc_loss: 0.02535853162407875, tv_loss: 0.033926595002412796\n",
      "iteration 2561, dc_loss: 0.025343695655465126, tv_loss: 0.03394417464733124\n",
      "iteration 2562, dc_loss: 0.025342291221022606, tv_loss: 0.033945973962545395\n",
      "iteration 2563, dc_loss: 0.025354310870170593, tv_loss: 0.033929940313100815\n",
      "iteration 2564, dc_loss: 0.025360573083162308, tv_loss: 0.03394342586398125\n",
      "iteration 2565, dc_loss: 0.02537251077592373, tv_loss: 0.03391378000378609\n",
      "iteration 2566, dc_loss: 0.02533602900803089, tv_loss: 0.033951349556446075\n",
      "iteration 2567, dc_loss: 0.025354579091072083, tv_loss: 0.03391239419579506\n",
      "iteration 2568, dc_loss: 0.025326360017061234, tv_loss: 0.03393034264445305\n",
      "iteration 2569, dc_loss: 0.025318220257759094, tv_loss: 0.033925723284482956\n",
      "iteration 2570, dc_loss: 0.025307856500148773, tv_loss: 0.03393131494522095\n",
      "iteration 2571, dc_loss: 0.02530638873577118, tv_loss: 0.033945366740226746\n",
      "iteration 2572, dc_loss: 0.02529698796570301, tv_loss: 0.03395603969693184\n",
      "iteration 2573, dc_loss: 0.02529394067823887, tv_loss: 0.03393542394042015\n",
      "iteration 2574, dc_loss: 0.025291506201028824, tv_loss: 0.033929355442523956\n",
      "iteration 2575, dc_loss: 0.025296755135059357, tv_loss: 0.033935125917196274\n",
      "iteration 2576, dc_loss: 0.025280948728322983, tv_loss: 0.03394567221403122\n",
      "iteration 2577, dc_loss: 0.025272734463214874, tv_loss: 0.03395959362387657\n",
      "iteration 2578, dc_loss: 0.02529909834265709, tv_loss: 0.03390916809439659\n",
      "iteration 2579, dc_loss: 0.025253787636756897, tv_loss: 0.0339590460062027\n",
      "iteration 2580, dc_loss: 0.02526521496474743, tv_loss: 0.03393172845244408\n",
      "iteration 2581, dc_loss: 0.025273289531469345, tv_loss: 0.03392709046602249\n",
      "iteration 2582, dc_loss: 0.025243373587727547, tv_loss: 0.03394577279686928\n",
      "iteration 2583, dc_loss: 0.02525952272117138, tv_loss: 0.033925190567970276\n",
      "iteration 2584, dc_loss: 0.025252701714634895, tv_loss: 0.03392971307039261\n",
      "iteration 2585, dc_loss: 0.02524653449654579, tv_loss: 0.03393186628818512\n",
      "iteration 2586, dc_loss: 0.02523980289697647, tv_loss: 0.03393154218792915\n",
      "iteration 2587, dc_loss: 0.02523892931640148, tv_loss: 0.03392648696899414\n",
      "iteration 2588, dc_loss: 0.025237342342734337, tv_loss: 0.03392678126692772\n",
      "iteration 2589, dc_loss: 0.02522958815097809, tv_loss: 0.03392770141363144\n",
      "iteration 2590, dc_loss: 0.025225704535841942, tv_loss: 0.033928267657756805\n",
      "iteration 2591, dc_loss: 0.02522129938006401, tv_loss: 0.03393010422587395\n",
      "iteration 2592, dc_loss: 0.025206923484802246, tv_loss: 0.033942777663469315\n",
      "iteration 2593, dc_loss: 0.025215335190296173, tv_loss: 0.03393416479229927\n",
      "iteration 2594, dc_loss: 0.025206640362739563, tv_loss: 0.033940110355615616\n",
      "iteration 2595, dc_loss: 0.02519999071955681, tv_loss: 0.03393542766571045\n",
      "iteration 2596, dc_loss: 0.02520734816789627, tv_loss: 0.03392407298088074\n",
      "iteration 2597, dc_loss: 0.02519400604069233, tv_loss: 0.03392712399363518\n",
      "iteration 2598, dc_loss: 0.025195855647325516, tv_loss: 0.033924397081136703\n",
      "iteration 2599, dc_loss: 0.02518131397664547, tv_loss: 0.03393670544028282\n",
      "iteration 2600, dc_loss: 0.025188889354467392, tv_loss: 0.03392361104488373\n",
      "iteration 2601, dc_loss: 0.02518036589026451, tv_loss: 0.03393924608826637\n",
      "iteration 2602, dc_loss: 0.02518085576593876, tv_loss: 0.03394054248929024\n",
      "iteration 2603, dc_loss: 0.025178730487823486, tv_loss: 0.03393899276852608\n",
      "iteration 2604, dc_loss: 0.02516734041273594, tv_loss: 0.033931296318769455\n",
      "iteration 2605, dc_loss: 0.02516932040452957, tv_loss: 0.0339348129928112\n",
      "iteration 2606, dc_loss: 0.02516903541982174, tv_loss: 0.03393363952636719\n",
      "iteration 2607, dc_loss: 0.025149645283818245, tv_loss: 0.03395594283938408\n",
      "iteration 2608, dc_loss: 0.02517680823802948, tv_loss: 0.03392135724425316\n",
      "iteration 2609, dc_loss: 0.025146743282675743, tv_loss: 0.033946260809898376\n",
      "iteration 2610, dc_loss: 0.025156427174806595, tv_loss: 0.03392430394887924\n",
      "iteration 2611, dc_loss: 0.025148935616016388, tv_loss: 0.03392942249774933\n",
      "iteration 2612, dc_loss: 0.025145666673779488, tv_loss: 0.03393338620662689\n",
      "iteration 2613, dc_loss: 0.02514147199690342, tv_loss: 0.033939942717552185\n",
      "iteration 2614, dc_loss: 0.02514210157096386, tv_loss: 0.033939097076654434\n",
      "iteration 2615, dc_loss: 0.02513159066438675, tv_loss: 0.03393477201461792\n",
      "iteration 2616, dc_loss: 0.025116246193647385, tv_loss: 0.03393452987074852\n",
      "iteration 2617, dc_loss: 0.025110524147748947, tv_loss: 0.03394053131341934\n",
      "iteration 2618, dc_loss: 0.025126708671450615, tv_loss: 0.03391699492931366\n",
      "iteration 2619, dc_loss: 0.025097981095314026, tv_loss: 0.0339399054646492\n",
      "iteration 2620, dc_loss: 0.02511335350573063, tv_loss: 0.03392154723405838\n",
      "iteration 2621, dc_loss: 0.025102250277996063, tv_loss: 0.03392850607633591\n",
      "iteration 2622, dc_loss: 0.025080256164073944, tv_loss: 0.03394266217947006\n",
      "iteration 2623, dc_loss: 0.025093084201216698, tv_loss: 0.03392874822020531\n",
      "iteration 2624, dc_loss: 0.025100674480199814, tv_loss: 0.03391434997320175\n",
      "iteration 2625, dc_loss: 0.025078514590859413, tv_loss: 0.03393518924713135\n",
      "iteration 2626, dc_loss: 0.025077514350414276, tv_loss: 0.03393169492483139\n",
      "iteration 2627, dc_loss: 0.02508087083697319, tv_loss: 0.03392844647169113\n",
      "iteration 2628, dc_loss: 0.025070244446396828, tv_loss: 0.033945340663194656\n",
      "iteration 2629, dc_loss: 0.025059910491108894, tv_loss: 0.03394568711519241\n",
      "iteration 2630, dc_loss: 0.025067362934350967, tv_loss: 0.033929165452718735\n",
      "iteration 2631, dc_loss: 0.02506575547158718, tv_loss: 0.03391919657588005\n",
      "iteration 2632, dc_loss: 0.025055967271327972, tv_loss: 0.03393281251192093\n",
      "iteration 2633, dc_loss: 0.025051360949873924, tv_loss: 0.033930838108062744\n",
      "iteration 2634, dc_loss: 0.025044770911335945, tv_loss: 0.03394373506307602\n",
      "iteration 2635, dc_loss: 0.025060685351490974, tv_loss: 0.033932097256183624\n",
      "iteration 2636, dc_loss: 0.02503245882689953, tv_loss: 0.033954888582229614\n",
      "iteration 2637, dc_loss: 0.025052400305867195, tv_loss: 0.03391973301768303\n",
      "iteration 2638, dc_loss: 0.025042416527867317, tv_loss: 0.03392866998910904\n",
      "iteration 2639, dc_loss: 0.02505333535373211, tv_loss: 0.03392437472939491\n",
      "iteration 2640, dc_loss: 0.02502880059182644, tv_loss: 0.03395203873515129\n",
      "iteration 2641, dc_loss: 0.02503957599401474, tv_loss: 0.033945489674806595\n",
      "iteration 2642, dc_loss: 0.025036334991455078, tv_loss: 0.03393145650625229\n",
      "iteration 2643, dc_loss: 0.025030875578522682, tv_loss: 0.03392384573817253\n",
      "iteration 2644, dc_loss: 0.025007734075188637, tv_loss: 0.033935435116291046\n",
      "iteration 2645, dc_loss: 0.025014063343405724, tv_loss: 0.03393228352069855\n",
      "iteration 2646, dc_loss: 0.02499617263674736, tv_loss: 0.03394266590476036\n",
      "iteration 2647, dc_loss: 0.025008168071508408, tv_loss: 0.03392169997096062\n",
      "iteration 2648, dc_loss: 0.024980813264846802, tv_loss: 0.033936094492673874\n",
      "iteration 2649, dc_loss: 0.024996323511004448, tv_loss: 0.03391505032777786\n",
      "iteration 2650, dc_loss: 0.024985868483781815, tv_loss: 0.033936113119125366\n",
      "iteration 2651, dc_loss: 0.024974413216114044, tv_loss: 0.03395741805434227\n",
      "iteration 2652, dc_loss: 0.02499201148748398, tv_loss: 0.033924348652362823\n",
      "iteration 2653, dc_loss: 0.024960488080978394, tv_loss: 0.033938173204660416\n",
      "iteration 2654, dc_loss: 0.024984028190374374, tv_loss: 0.033911872655153275\n",
      "iteration 2655, dc_loss: 0.02495861053466797, tv_loss: 0.033938050270080566\n",
      "iteration 2656, dc_loss: 0.024972666054964066, tv_loss: 0.03392370417714119\n",
      "iteration 2657, dc_loss: 0.024963146075606346, tv_loss: 0.033930957317352295\n",
      "iteration 2658, dc_loss: 0.024946942925453186, tv_loss: 0.033950064331293106\n",
      "iteration 2659, dc_loss: 0.02495737001299858, tv_loss: 0.03392805904150009\n",
      "iteration 2660, dc_loss: 0.02494785748422146, tv_loss: 0.033928271383047104\n",
      "iteration 2661, dc_loss: 0.0249403715133667, tv_loss: 0.03392137587070465\n",
      "iteration 2662, dc_loss: 0.024947814643383026, tv_loss: 0.03391488641500473\n",
      "iteration 2663, dc_loss: 0.024922553449869156, tv_loss: 0.033936597406864166\n",
      "iteration 2664, dc_loss: 0.024950820952653885, tv_loss: 0.033918749541044235\n",
      "iteration 2665, dc_loss: 0.02491680160164833, tv_loss: 0.033959656953811646\n",
      "iteration 2666, dc_loss: 0.024927055463194847, tv_loss: 0.03393106535077095\n",
      "iteration 2667, dc_loss: 0.02491649053990841, tv_loss: 0.033927008509635925\n",
      "iteration 2668, dc_loss: 0.024918198585510254, tv_loss: 0.033929117023944855\n",
      "iteration 2669, dc_loss: 0.024905329570174217, tv_loss: 0.033946529030799866\n",
      "iteration 2670, dc_loss: 0.024913934990763664, tv_loss: 0.033928945660591125\n",
      "iteration 2671, dc_loss: 0.024917474016547203, tv_loss: 0.03392168506979942\n",
      "iteration 2672, dc_loss: 0.024896269664168358, tv_loss: 0.03393018990755081\n",
      "iteration 2673, dc_loss: 0.02489943616092205, tv_loss: 0.03392055258154869\n",
      "iteration 2674, dc_loss: 0.024909362196922302, tv_loss: 0.033911868929862976\n",
      "iteration 2675, dc_loss: 0.024884480983018875, tv_loss: 0.033942773938179016\n",
      "iteration 2676, dc_loss: 0.024909313768148422, tv_loss: 0.033923011273145676\n",
      "iteration 2677, dc_loss: 0.02487264573574066, tv_loss: 0.0339687243103981\n",
      "iteration 2678, dc_loss: 0.024912917986512184, tv_loss: 0.03390851616859436\n",
      "iteration 2679, dc_loss: 0.02487891912460327, tv_loss: 0.03393406420946121\n",
      "iteration 2680, dc_loss: 0.02489473484456539, tv_loss: 0.033925872296094894\n",
      "iteration 2681, dc_loss: 0.02488439530134201, tv_loss: 0.033946581184864044\n",
      "iteration 2682, dc_loss: 0.024889666587114334, tv_loss: 0.03392593190073967\n",
      "iteration 2683, dc_loss: 0.02487710863351822, tv_loss: 0.03392219915986061\n",
      "iteration 2684, dc_loss: 0.024848874658346176, tv_loss: 0.03394125774502754\n",
      "iteration 2685, dc_loss: 0.024856703355908394, tv_loss: 0.03393349424004555\n",
      "iteration 2686, dc_loss: 0.024857496842741966, tv_loss: 0.03393174335360527\n",
      "iteration 2687, dc_loss: 0.024823665618896484, tv_loss: 0.03395324945449829\n",
      "iteration 2688, dc_loss: 0.024855367839336395, tv_loss: 0.03391111642122269\n",
      "iteration 2689, dc_loss: 0.02482895739376545, tv_loss: 0.03393438830971718\n",
      "iteration 2690, dc_loss: 0.024828819558024406, tv_loss: 0.03393244370818138\n",
      "iteration 2691, dc_loss: 0.024834509938955307, tv_loss: 0.033929869532585144\n",
      "iteration 2692, dc_loss: 0.024817058816552162, tv_loss: 0.03393658623099327\n",
      "iteration 2693, dc_loss: 0.02483300492167473, tv_loss: 0.03392092511057854\n",
      "iteration 2694, dc_loss: 0.02480417862534523, tv_loss: 0.033940184861421585\n",
      "iteration 2695, dc_loss: 0.02483217418193817, tv_loss: 0.03390545770525932\n",
      "iteration 2696, dc_loss: 0.024799851700663567, tv_loss: 0.033934637904167175\n",
      "iteration 2697, dc_loss: 0.024821169674396515, tv_loss: 0.033911362290382385\n",
      "iteration 2698, dc_loss: 0.02480241283774376, tv_loss: 0.03392539545893669\n",
      "iteration 2699, dc_loss: 0.024794483557343483, tv_loss: 0.03393452614545822\n",
      "iteration 2700, dc_loss: 0.024798698723316193, tv_loss: 0.033923227339982986\n",
      "iteration 2701, dc_loss: 0.024794336408376694, tv_loss: 0.03392118960618973\n",
      "iteration 2702, dc_loss: 0.024781784042716026, tv_loss: 0.03392722085118294\n",
      "iteration 2703, dc_loss: 0.024799762293696404, tv_loss: 0.033902693539857864\n",
      "iteration 2704, dc_loss: 0.024762390181422234, tv_loss: 0.03393546864390373\n",
      "iteration 2705, dc_loss: 0.024780884385108948, tv_loss: 0.03391733393073082\n",
      "iteration 2706, dc_loss: 0.02477346919476986, tv_loss: 0.03391499072313309\n",
      "iteration 2707, dc_loss: 0.024764101952314377, tv_loss: 0.033922914415597916\n",
      "iteration 2708, dc_loss: 0.024766409769654274, tv_loss: 0.033934690058231354\n",
      "iteration 2709, dc_loss: 0.024764232337474823, tv_loss: 0.033937156200408936\n",
      "iteration 2710, dc_loss: 0.024753868579864502, tv_loss: 0.03393161669373512\n",
      "iteration 2711, dc_loss: 0.024738552048802376, tv_loss: 0.033934418112039566\n",
      "iteration 2712, dc_loss: 0.024757785722613335, tv_loss: 0.03392479941248894\n",
      "iteration 2713, dc_loss: 0.024741068482398987, tv_loss: 0.03395732119679451\n",
      "iteration 2714, dc_loss: 0.024734223261475563, tv_loss: 0.0339331179857254\n",
      "iteration 2715, dc_loss: 0.024748440831899643, tv_loss: 0.033920057117938995\n",
      "iteration 2716, dc_loss: 0.024724382907152176, tv_loss: 0.033942755311727524\n",
      "iteration 2717, dc_loss: 0.024740802124142647, tv_loss: 0.03393447399139404\n",
      "iteration 2718, dc_loss: 0.024718983098864555, tv_loss: 0.03393701836466789\n",
      "iteration 2719, dc_loss: 0.024724680930376053, tv_loss: 0.03392456844449043\n",
      "iteration 2720, dc_loss: 0.024725183844566345, tv_loss: 0.03394417092204094\n",
      "iteration 2721, dc_loss: 0.02473430708050728, tv_loss: 0.03392791002988815\n",
      "iteration 2722, dc_loss: 0.024720607325434685, tv_loss: 0.03393583372235298\n",
      "iteration 2723, dc_loss: 0.024727838113904, tv_loss: 0.03392405062913895\n",
      "iteration 2724, dc_loss: 0.024714890867471695, tv_loss: 0.033955201506614685\n",
      "iteration 2725, dc_loss: 0.024752382189035416, tv_loss: 0.03391214832663536\n",
      "iteration 2726, dc_loss: 0.02471817657351494, tv_loss: 0.03394222632050514\n",
      "iteration 2727, dc_loss: 0.02473970130085945, tv_loss: 0.033903446048498154\n",
      "iteration 2728, dc_loss: 0.02470388077199459, tv_loss: 0.03394628316164017\n",
      "iteration 2729, dc_loss: 0.02472812682390213, tv_loss: 0.0339135117828846\n",
      "iteration 2730, dc_loss: 0.024684973061084747, tv_loss: 0.033955950289964676\n",
      "iteration 2731, dc_loss: 0.024705788120627403, tv_loss: 0.03391822800040245\n",
      "iteration 2732, dc_loss: 0.024680940434336662, tv_loss: 0.03392152860760689\n",
      "iteration 2733, dc_loss: 0.02468564547598362, tv_loss: 0.03390521556138992\n",
      "iteration 2734, dc_loss: 0.024661177769303322, tv_loss: 0.03392229601740837\n",
      "iteration 2735, dc_loss: 0.02466687001287937, tv_loss: 0.03390946984291077\n",
      "iteration 2736, dc_loss: 0.024650542065501213, tv_loss: 0.03392712026834488\n",
      "iteration 2737, dc_loss: 0.02465873211622238, tv_loss: 0.03391476720571518\n",
      "iteration 2738, dc_loss: 0.024661023169755936, tv_loss: 0.03390836343169212\n",
      "iteration 2739, dc_loss: 0.02464483119547367, tv_loss: 0.03392896056175232\n",
      "iteration 2740, dc_loss: 0.02466582879424095, tv_loss: 0.03392941132187843\n",
      "iteration 2741, dc_loss: 0.02463875524699688, tv_loss: 0.03396357595920563\n",
      "iteration 2742, dc_loss: 0.024657653644680977, tv_loss: 0.03390393406152725\n",
      "iteration 2743, dc_loss: 0.02462315931916237, tv_loss: 0.03394083306193352\n",
      "iteration 2744, dc_loss: 0.024645933881402016, tv_loss: 0.03393649682402611\n",
      "iteration 2745, dc_loss: 0.0246224794536829, tv_loss: 0.03395108878612518\n",
      "iteration 2746, dc_loss: 0.02461496740579605, tv_loss: 0.03393106907606125\n",
      "iteration 2747, dc_loss: 0.024624967947602272, tv_loss: 0.033932045102119446\n",
      "iteration 2748, dc_loss: 0.024608442559838295, tv_loss: 0.033961303532123566\n",
      "iteration 2749, dc_loss: 0.024617867544293404, tv_loss: 0.033921368420124054\n",
      "iteration 2750, dc_loss: 0.02459910325706005, tv_loss: 0.033947721123695374\n",
      "iteration 2751, dc_loss: 0.02460256777703762, tv_loss: 0.03394704684615135\n",
      "iteration 2752, dc_loss: 0.024603765457868576, tv_loss: 0.033935993909835815\n",
      "iteration 2753, dc_loss: 0.024608971551060677, tv_loss: 0.03391655907034874\n",
      "iteration 2754, dc_loss: 0.02459115907549858, tv_loss: 0.03393689915537834\n",
      "iteration 2755, dc_loss: 0.024601943790912628, tv_loss: 0.03393246978521347\n",
      "iteration 2756, dc_loss: 0.024589188396930695, tv_loss: 0.03392253816127777\n",
      "iteration 2757, dc_loss: 0.024593571200966835, tv_loss: 0.03391357511281967\n",
      "iteration 2758, dc_loss: 0.024573685601353645, tv_loss: 0.033933449536561966\n",
      "iteration 2759, dc_loss: 0.024585895240306854, tv_loss: 0.03392534703016281\n",
      "iteration 2760, dc_loss: 0.024566268548369408, tv_loss: 0.03393475338816643\n",
      "iteration 2761, dc_loss: 0.024578245356678963, tv_loss: 0.03391791507601738\n",
      "iteration 2762, dc_loss: 0.024557696655392647, tv_loss: 0.0339253768324852\n",
      "iteration 2763, dc_loss: 0.024570032954216003, tv_loss: 0.03390857204794884\n",
      "iteration 2764, dc_loss: 0.024551553651690483, tv_loss: 0.033917274326086044\n",
      "iteration 2765, dc_loss: 0.024557679891586304, tv_loss: 0.03390520066022873\n",
      "iteration 2766, dc_loss: 0.0245415847748518, tv_loss: 0.03392515704035759\n",
      "iteration 2767, dc_loss: 0.024560365825891495, tv_loss: 0.03390807658433914\n",
      "iteration 2768, dc_loss: 0.024536868557333946, tv_loss: 0.03393539413809776\n",
      "iteration 2769, dc_loss: 0.024540606886148453, tv_loss: 0.033942900598049164\n",
      "iteration 2770, dc_loss: 0.02452775090932846, tv_loss: 0.03393276035785675\n",
      "iteration 2771, dc_loss: 0.024552274495363235, tv_loss: 0.033907100558280945\n",
      "iteration 2772, dc_loss: 0.02451319992542267, tv_loss: 0.033941224217414856\n",
      "iteration 2773, dc_loss: 0.02455548569560051, tv_loss: 0.0338987372815609\n",
      "iteration 2774, dc_loss: 0.0245148204267025, tv_loss: 0.033941514790058136\n",
      "iteration 2775, dc_loss: 0.024546118453145027, tv_loss: 0.033908236771821976\n",
      "iteration 2776, dc_loss: 0.024532416835427284, tv_loss: 0.033920321613550186\n",
      "iteration 2777, dc_loss: 0.02453620173037052, tv_loss: 0.03391238674521446\n",
      "iteration 2778, dc_loss: 0.02452387660741806, tv_loss: 0.03393193334341049\n",
      "iteration 2779, dc_loss: 0.024562252685427666, tv_loss: 0.03388582542538643\n",
      "iteration 2780, dc_loss: 0.024518851190805435, tv_loss: 0.0339370034635067\n",
      "iteration 2781, dc_loss: 0.024556439369916916, tv_loss: 0.03389713168144226\n",
      "iteration 2782, dc_loss: 0.024519996717572212, tv_loss: 0.033940501511096954\n",
      "iteration 2783, dc_loss: 0.024531474336981773, tv_loss: 0.03391648828983307\n",
      "iteration 2784, dc_loss: 0.02448723278939724, tv_loss: 0.03394416719675064\n",
      "iteration 2785, dc_loss: 0.024523386731743813, tv_loss: 0.03388703614473343\n",
      "iteration 2786, dc_loss: 0.024463150650262833, tv_loss: 0.033936887979507446\n",
      "iteration 2787, dc_loss: 0.024479670450091362, tv_loss: 0.03391928970813751\n",
      "iteration 2788, dc_loss: 0.02447178028523922, tv_loss: 0.03392656147480011\n",
      "iteration 2789, dc_loss: 0.024460013955831528, tv_loss: 0.03392871841788292\n",
      "iteration 2790, dc_loss: 0.024488529190421104, tv_loss: 0.03389989957213402\n",
      "iteration 2791, dc_loss: 0.024449585005640984, tv_loss: 0.03393444046378136\n",
      "iteration 2792, dc_loss: 0.024480115622282028, tv_loss: 0.03390448912978172\n",
      "iteration 2793, dc_loss: 0.0244548711925745, tv_loss: 0.03392826393246651\n",
      "iteration 2794, dc_loss: 0.024489745497703552, tv_loss: 0.03388446569442749\n",
      "iteration 2795, dc_loss: 0.024432234466075897, tv_loss: 0.03394191339612007\n",
      "iteration 2796, dc_loss: 0.024464517831802368, tv_loss: 0.033901240676641464\n",
      "iteration 2797, dc_loss: 0.024438904598355293, tv_loss: 0.033927544951438904\n",
      "iteration 2798, dc_loss: 0.024425184354186058, tv_loss: 0.033935677260160446\n",
      "iteration 2799, dc_loss: 0.02443716488778591, tv_loss: 0.033928778022527695\n",
      "iteration 2800, dc_loss: 0.0244267787784338, tv_loss: 0.03392036259174347\n",
      "iteration 2801, dc_loss: 0.02443082630634308, tv_loss: 0.03390895947813988\n",
      "iteration 2802, dc_loss: 0.024421263486146927, tv_loss: 0.03391079977154732\n",
      "iteration 2803, dc_loss: 0.02441442757844925, tv_loss: 0.03391266241669655\n",
      "iteration 2804, dc_loss: 0.024413343518972397, tv_loss: 0.03391081094741821\n",
      "iteration 2805, dc_loss: 0.02441113069653511, tv_loss: 0.03390514850616455\n",
      "iteration 2806, dc_loss: 0.024409718811511993, tv_loss: 0.03390603885054588\n",
      "iteration 2807, dc_loss: 0.02440432645380497, tv_loss: 0.03390829637646675\n",
      "iteration 2808, dc_loss: 0.024405140429735184, tv_loss: 0.03390497714281082\n",
      "iteration 2809, dc_loss: 0.024408182129263878, tv_loss: 0.03390171006321907\n",
      "iteration 2810, dc_loss: 0.02439900115132332, tv_loss: 0.03390520438551903\n",
      "iteration 2811, dc_loss: 0.02438599430024624, tv_loss: 0.033910442143678665\n",
      "iteration 2812, dc_loss: 0.024392979219555855, tv_loss: 0.0339014008641243\n",
      "iteration 2813, dc_loss: 0.02439028024673462, tv_loss: 0.03389883041381836\n",
      "iteration 2814, dc_loss: 0.02438483014702797, tv_loss: 0.03390243276953697\n",
      "iteration 2815, dc_loss: 0.02438688464462757, tv_loss: 0.03390214219689369\n",
      "iteration 2816, dc_loss: 0.024387871846556664, tv_loss: 0.033905480057001114\n",
      "iteration 2817, dc_loss: 0.02437383309006691, tv_loss: 0.03391636908054352\n",
      "iteration 2818, dc_loss: 0.024369677528738976, tv_loss: 0.03391269966959953\n",
      "iteration 2819, dc_loss: 0.02437187172472477, tv_loss: 0.0338987372815609\n",
      "iteration 2820, dc_loss: 0.024371927604079247, tv_loss: 0.03389691188931465\n",
      "iteration 2821, dc_loss: 0.024362793192267418, tv_loss: 0.033901579678058624\n",
      "iteration 2822, dc_loss: 0.02436606027185917, tv_loss: 0.03390377387404442\n",
      "iteration 2823, dc_loss: 0.024371955543756485, tv_loss: 0.03390004113316536\n",
      "iteration 2824, dc_loss: 0.02435430884361267, tv_loss: 0.03391782566905022\n",
      "iteration 2825, dc_loss: 0.024351296946406364, tv_loss: 0.03391052037477493\n",
      "iteration 2826, dc_loss: 0.02435203827917576, tv_loss: 0.03390751779079437\n",
      "iteration 2827, dc_loss: 0.024344105273485184, tv_loss: 0.033907201141119\n",
      "iteration 2828, dc_loss: 0.02434379979968071, tv_loss: 0.03390693664550781\n",
      "iteration 2829, dc_loss: 0.02435527741909027, tv_loss: 0.03389007970690727\n",
      "iteration 2830, dc_loss: 0.024339713156223297, tv_loss: 0.03390080854296684\n",
      "iteration 2831, dc_loss: 0.024328531697392464, tv_loss: 0.033909354358911514\n",
      "iteration 2832, dc_loss: 0.024345511570572853, tv_loss: 0.033887773752212524\n",
      "iteration 2833, dc_loss: 0.024334480985999107, tv_loss: 0.03389669209718704\n",
      "iteration 2834, dc_loss: 0.024322740733623505, tv_loss: 0.033903393894433975\n",
      "iteration 2835, dc_loss: 0.0243370421230793, tv_loss: 0.03388577327132225\n",
      "iteration 2836, dc_loss: 0.024321923032402992, tv_loss: 0.03390522673726082\n",
      "iteration 2837, dc_loss: 0.024314166978001595, tv_loss: 0.033910658210515976\n",
      "iteration 2838, dc_loss: 0.024325644597411156, tv_loss: 0.03390701860189438\n",
      "iteration 2839, dc_loss: 0.024316133931279182, tv_loss: 0.033913370221853256\n",
      "iteration 2840, dc_loss: 0.024314887821674347, tv_loss: 0.03390376642346382\n",
      "iteration 2841, dc_loss: 0.024312149733304977, tv_loss: 0.03390369936823845\n",
      "iteration 2842, dc_loss: 0.024304216727614403, tv_loss: 0.03390156105160713\n",
      "iteration 2843, dc_loss: 0.024305099621415138, tv_loss: 0.03390061482787132\n",
      "iteration 2844, dc_loss: 0.02430087700486183, tv_loss: 0.03390422835946083\n",
      "iteration 2845, dc_loss: 0.024300679564476013, tv_loss: 0.0339023657143116\n",
      "iteration 2846, dc_loss: 0.024297697469592094, tv_loss: 0.03390369191765785\n",
      "iteration 2847, dc_loss: 0.024294255301356316, tv_loss: 0.03390644118189812\n",
      "iteration 2848, dc_loss: 0.024287482723593712, tv_loss: 0.03391003981232643\n",
      "iteration 2849, dc_loss: 0.024294594302773476, tv_loss: 0.03389719873666763\n",
      "iteration 2850, dc_loss: 0.024286337196826935, tv_loss: 0.03390209376811981\n",
      "iteration 2851, dc_loss: 0.024279417470097542, tv_loss: 0.033906545490026474\n",
      "iteration 2852, dc_loss: 0.024286095052957535, tv_loss: 0.03389313071966171\n",
      "iteration 2853, dc_loss: 0.02427738718688488, tv_loss: 0.033897433429956436\n",
      "iteration 2854, dc_loss: 0.024269474670290947, tv_loss: 0.03390621766448021\n",
      "iteration 2855, dc_loss: 0.024277113378047943, tv_loss: 0.033891353756189346\n",
      "iteration 2856, dc_loss: 0.024270862340927124, tv_loss: 0.03390612453222275\n",
      "iteration 2857, dc_loss: 0.024261729791760445, tv_loss: 0.03391192480921745\n",
      "iteration 2858, dc_loss: 0.024271788075566292, tv_loss: 0.03390617296099663\n",
      "iteration 2859, dc_loss: 0.024263251572847366, tv_loss: 0.03389967232942581\n",
      "iteration 2860, dc_loss: 0.024252433329820633, tv_loss: 0.03390481323003769\n",
      "iteration 2861, dc_loss: 0.02425595000386238, tv_loss: 0.033907610923051834\n",
      "iteration 2862, dc_loss: 0.024255219846963882, tv_loss: 0.03390752151608467\n",
      "iteration 2863, dc_loss: 0.02425427734851837, tv_loss: 0.03390640765428543\n",
      "iteration 2864, dc_loss: 0.024246791377663612, tv_loss: 0.03390005975961685\n",
      "iteration 2865, dc_loss: 0.024245746433734894, tv_loss: 0.033908095210790634\n",
      "iteration 2866, dc_loss: 0.02424362488090992, tv_loss: 0.03390660881996155\n",
      "iteration 2867, dc_loss: 0.024234533309936523, tv_loss: 0.03391613811254501\n",
      "iteration 2868, dc_loss: 0.0242376159876585, tv_loss: 0.033899467438459396\n",
      "iteration 2869, dc_loss: 0.024240784347057343, tv_loss: 0.03389916568994522\n",
      "iteration 2870, dc_loss: 0.02422875352203846, tv_loss: 0.0339057594537735\n",
      "iteration 2871, dc_loss: 0.02423080988228321, tv_loss: 0.03390234708786011\n",
      "iteration 2872, dc_loss: 0.024230318143963814, tv_loss: 0.033897534012794495\n",
      "iteration 2873, dc_loss: 0.024220474064350128, tv_loss: 0.033901579678058624\n",
      "iteration 2874, dc_loss: 0.024224117398262024, tv_loss: 0.033892907202243805\n",
      "iteration 2875, dc_loss: 0.024218173697590828, tv_loss: 0.03390747308731079\n",
      "iteration 2876, dc_loss: 0.024212172254920006, tv_loss: 0.03392034024000168\n",
      "iteration 2877, dc_loss: 0.024216562509536743, tv_loss: 0.03390628099441528\n",
      "iteration 2878, dc_loss: 0.024218926206231117, tv_loss: 0.03388916328549385\n",
      "iteration 2879, dc_loss: 0.0241991113871336, tv_loss: 0.03390691056847572\n",
      "iteration 2880, dc_loss: 0.024199455976486206, tv_loss: 0.03389979153871536\n",
      "iteration 2881, dc_loss: 0.02420983649790287, tv_loss: 0.03388988599181175\n",
      "iteration 2882, dc_loss: 0.02420174703001976, tv_loss: 0.033895932137966156\n",
      "iteration 2883, dc_loss: 0.024197328835725784, tv_loss: 0.033908139914274216\n",
      "iteration 2884, dc_loss: 0.024193745106458664, tv_loss: 0.03391999006271362\n",
      "iteration 2885, dc_loss: 0.024194883182644844, tv_loss: 0.03390663117170334\n",
      "iteration 2886, dc_loss: 0.02418520115315914, tv_loss: 0.033900484442710876\n",
      "iteration 2887, dc_loss: 0.024191459640860558, tv_loss: 0.03389666602015495\n",
      "iteration 2888, dc_loss: 0.024182185530662537, tv_loss: 0.03391950950026512\n",
      "iteration 2889, dc_loss: 0.02417709492146969, tv_loss: 0.0339224711060524\n",
      "iteration 2890, dc_loss: 0.02418495900928974, tv_loss: 0.03389229252934456\n",
      "iteration 2891, dc_loss: 0.024173468351364136, tv_loss: 0.03390992060303688\n",
      "iteration 2892, dc_loss: 0.024165945127606392, tv_loss: 0.03392519801855087\n",
      "iteration 2893, dc_loss: 0.024173596873879433, tv_loss: 0.03391052782535553\n",
      "iteration 2894, dc_loss: 0.024168819189071655, tv_loss: 0.03390124812722206\n",
      "iteration 2895, dc_loss: 0.02417086809873581, tv_loss: 0.033910080790519714\n",
      "iteration 2896, dc_loss: 0.02416262775659561, tv_loss: 0.03391256555914879\n",
      "iteration 2897, dc_loss: 0.024155011400580406, tv_loss: 0.03390764445066452\n",
      "iteration 2898, dc_loss: 0.02416260913014412, tv_loss: 0.033891912549734116\n",
      "iteration 2899, dc_loss: 0.024155063554644585, tv_loss: 0.0339062437415123\n",
      "iteration 2900, dc_loss: 0.024142980575561523, tv_loss: 0.0339260958135128\n",
      "iteration 2901, dc_loss: 0.024154549464583397, tv_loss: 0.0339014045894146\n",
      "iteration 2902, dc_loss: 0.024151407182216644, tv_loss: 0.033893242478370667\n",
      "iteration 2903, dc_loss: 0.024138685315847397, tv_loss: 0.03390895947813988\n",
      "iteration 2904, dc_loss: 0.024145258590579033, tv_loss: 0.033908192068338394\n",
      "iteration 2905, dc_loss: 0.02414257824420929, tv_loss: 0.03390885144472122\n",
      "iteration 2906, dc_loss: 0.024135800078511238, tv_loss: 0.03390299156308174\n",
      "iteration 2907, dc_loss: 0.024129532277584076, tv_loss: 0.033899012953042984\n",
      "iteration 2908, dc_loss: 0.024130886420607567, tv_loss: 0.033904969692230225\n",
      "iteration 2909, dc_loss: 0.024131694808602333, tv_loss: 0.03390999883413315\n",
      "iteration 2910, dc_loss: 0.024119384586811066, tv_loss: 0.03391402214765549\n",
      "iteration 2911, dc_loss: 0.02412542700767517, tv_loss: 0.03389344736933708\n",
      "iteration 2912, dc_loss: 0.024124475196003914, tv_loss: 0.03389480337500572\n",
      "iteration 2913, dc_loss: 0.024119123816490173, tv_loss: 0.03390250727534294\n",
      "iteration 2914, dc_loss: 0.024111535400152206, tv_loss: 0.03390294313430786\n",
      "iteration 2915, dc_loss: 0.02411104366183281, tv_loss: 0.033904120326042175\n",
      "iteration 2916, dc_loss: 0.0241137333214283, tv_loss: 0.03388959541916847\n",
      "iteration 2917, dc_loss: 0.024103008210659027, tv_loss: 0.033898141235113144\n",
      "iteration 2918, dc_loss: 0.024105241522192955, tv_loss: 0.03389299660921097\n",
      "iteration 2919, dc_loss: 0.02410753257572651, tv_loss: 0.03389173001050949\n",
      "iteration 2920, dc_loss: 0.02409336529672146, tv_loss: 0.03390797600150108\n",
      "iteration 2921, dc_loss: 0.02409283071756363, tv_loss: 0.03390182927250862\n",
      "iteration 2922, dc_loss: 0.02409977838397026, tv_loss: 0.03389487788081169\n",
      "iteration 2923, dc_loss: 0.02408880554139614, tv_loss: 0.0339076891541481\n",
      "iteration 2924, dc_loss: 0.024090884253382683, tv_loss: 0.033901311457157135\n",
      "iteration 2925, dc_loss: 0.024087147787213326, tv_loss: 0.03389809653162956\n",
      "iteration 2926, dc_loss: 0.02407967671751976, tv_loss: 0.03390289098024368\n",
      "iteration 2927, dc_loss: 0.02408301644027233, tv_loss: 0.033899810165166855\n",
      "iteration 2928, dc_loss: 0.024075057357549667, tv_loss: 0.03391777351498604\n",
      "iteration 2929, dc_loss: 0.02407342754304409, tv_loss: 0.03391285985708237\n",
      "iteration 2930, dc_loss: 0.02407398261129856, tv_loss: 0.033896930515766144\n",
      "iteration 2931, dc_loss: 0.024076346307992935, tv_loss: 0.03389383479952812\n",
      "iteration 2932, dc_loss: 0.02406413108110428, tv_loss: 0.03390933573246002\n",
      "iteration 2933, dc_loss: 0.02405988983809948, tv_loss: 0.03391445800662041\n",
      "iteration 2934, dc_loss: 0.024074245244264603, tv_loss: 0.03389183431863785\n",
      "iteration 2935, dc_loss: 0.0240572951734066, tv_loss: 0.03389615938067436\n",
      "iteration 2936, dc_loss: 0.02405238337814808, tv_loss: 0.033900726586580276\n",
      "iteration 2937, dc_loss: 0.02406247705221176, tv_loss: 0.03388926014304161\n",
      "iteration 2938, dc_loss: 0.024048535153269768, tv_loss: 0.03390081971883774\n",
      "iteration 2939, dc_loss: 0.024045860394835472, tv_loss: 0.03389783203601837\n",
      "iteration 2940, dc_loss: 0.024043584242463112, tv_loss: 0.0338977575302124\n",
      "iteration 2941, dc_loss: 0.024046285077929497, tv_loss: 0.03389374539256096\n",
      "iteration 2942, dc_loss: 0.02404433861374855, tv_loss: 0.03389917314052582\n",
      "iteration 2943, dc_loss: 0.024040628224611282, tv_loss: 0.033903028815984726\n",
      "iteration 2944, dc_loss: 0.024033404886722565, tv_loss: 0.03390493988990784\n",
      "iteration 2945, dc_loss: 0.024041645228862762, tv_loss: 0.033886805176734924\n",
      "iteration 2946, dc_loss: 0.02402365766465664, tv_loss: 0.03390040993690491\n",
      "iteration 2947, dc_loss: 0.0240285936743021, tv_loss: 0.033892955631017685\n",
      "iteration 2948, dc_loss: 0.024032339453697205, tv_loss: 0.033884864300489426\n",
      "iteration 2949, dc_loss: 0.024026334285736084, tv_loss: 0.03389057517051697\n",
      "iteration 2950, dc_loss: 0.0240198764950037, tv_loss: 0.03390752524137497\n",
      "iteration 2951, dc_loss: 0.024022536352276802, tv_loss: 0.03391295671463013\n",
      "iteration 2952, dc_loss: 0.024012336507439613, tv_loss: 0.03390035033226013\n",
      "iteration 2953, dc_loss: 0.024011291563510895, tv_loss: 0.03389469161629677\n",
      "iteration 2954, dc_loss: 0.024010827764868736, tv_loss: 0.03390613570809364\n",
      "iteration 2955, dc_loss: 0.0240032821893692, tv_loss: 0.03391794487833977\n",
      "iteration 2956, dc_loss: 0.02400621771812439, tv_loss: 0.03390263393521309\n",
      "iteration 2957, dc_loss: 0.024014780297875404, tv_loss: 0.03388385847210884\n",
      "iteration 2958, dc_loss: 0.023998867720365524, tv_loss: 0.0339021235704422\n",
      "iteration 2959, dc_loss: 0.023986928164958954, tv_loss: 0.03391044959425926\n",
      "iteration 2960, dc_loss: 0.0240029189735651, tv_loss: 0.03389083966612816\n",
      "iteration 2961, dc_loss: 0.02398955263197422, tv_loss: 0.033898863941431046\n",
      "iteration 2962, dc_loss: 0.02398410253226757, tv_loss: 0.03390713036060333\n",
      "iteration 2963, dc_loss: 0.02399805746972561, tv_loss: 0.03388768061995506\n",
      "iteration 2964, dc_loss: 0.023992450907826424, tv_loss: 0.03389346972107887\n",
      "iteration 2965, dc_loss: 0.023972539231181145, tv_loss: 0.03390580415725708\n",
      "iteration 2966, dc_loss: 0.02398517355322838, tv_loss: 0.03388577327132225\n",
      "iteration 2967, dc_loss: 0.023981375619769096, tv_loss: 0.033893365412950516\n",
      "iteration 2968, dc_loss: 0.023974765092134476, tv_loss: 0.03389338403940201\n",
      "iteration 2969, dc_loss: 0.023970460519194603, tv_loss: 0.033890075981616974\n",
      "iteration 2970, dc_loss: 0.023980820551514626, tv_loss: 0.03388657048344612\n",
      "iteration 2971, dc_loss: 0.023966191336512566, tv_loss: 0.03390805423259735\n",
      "iteration 2972, dc_loss: 0.02396474778652191, tv_loss: 0.03391064703464508\n",
      "iteration 2973, dc_loss: 0.02396477200090885, tv_loss: 0.033899493515491486\n",
      "iteration 2974, dc_loss: 0.02396811917424202, tv_loss: 0.03388514369726181\n",
      "iteration 2975, dc_loss: 0.023960139602422714, tv_loss: 0.03389519453048706\n",
      "iteration 2976, dc_loss: 0.023965518921613693, tv_loss: 0.03390033543109894\n",
      "iteration 2977, dc_loss: 0.023945089429616928, tv_loss: 0.03392769396305084\n",
      "iteration 2978, dc_loss: 0.02396491728723049, tv_loss: 0.03388325870037079\n",
      "iteration 2979, dc_loss: 0.023944422602653503, tv_loss: 0.03390384092926979\n",
      "iteration 2980, dc_loss: 0.023947345092892647, tv_loss: 0.03390560671687126\n",
      "iteration 2981, dc_loss: 0.023947643116116524, tv_loss: 0.033911868929862976\n",
      "iteration 2982, dc_loss: 0.023940758779644966, tv_loss: 0.033897314220666885\n",
      "iteration 2983, dc_loss: 0.023935547098517418, tv_loss: 0.03389780595898628\n",
      "iteration 2984, dc_loss: 0.023939529433846474, tv_loss: 0.03391306474804878\n",
      "iteration 2985, dc_loss: 0.023925891146063805, tv_loss: 0.03392006456851959\n",
      "iteration 2986, dc_loss: 0.023935332894325256, tv_loss: 0.03389546647667885\n",
      "iteration 2987, dc_loss: 0.023923292756080627, tv_loss: 0.033904798328876495\n",
      "iteration 2988, dc_loss: 0.023922579362988472, tv_loss: 0.03391756862401962\n",
      "iteration 2989, dc_loss: 0.023936159908771515, tv_loss: 0.033885519951581955\n",
      "iteration 2990, dc_loss: 0.023904455825686455, tv_loss: 0.03391768038272858\n",
      "iteration 2991, dc_loss: 0.023915240541100502, tv_loss: 0.0339055061340332\n",
      "iteration 2992, dc_loss: 0.023932307958602905, tv_loss: 0.03388434648513794\n",
      "iteration 2993, dc_loss: 0.02390815131366253, tv_loss: 0.03390670195221901\n",
      "iteration 2994, dc_loss: 0.02390691265463829, tv_loss: 0.03389832004904747\n",
      "iteration 2995, dc_loss: 0.023921698331832886, tv_loss: 0.03388466686010361\n",
      "iteration 2996, dc_loss: 0.023898009210824966, tv_loss: 0.03390764817595482\n",
      "iteration 2997, dc_loss: 0.02390497550368309, tv_loss: 0.03389878571033478\n",
      "iteration 2998, dc_loss: 0.023899994790554047, tv_loss: 0.03389474004507065\n",
      "iteration 2999, dc_loss: 0.02389904484152794, tv_loss: 0.03388868644833565\n",
      "iteration 3000, dc_loss: 0.023892581462860107, tv_loss: 0.03389674052596092\n",
      "iteration 3001, dc_loss: 0.023892490193247795, tv_loss: 0.03389623761177063\n",
      "iteration 3002, dc_loss: 0.023896493017673492, tv_loss: 0.033895932137966156\n",
      "iteration 3003, dc_loss: 0.02388245426118374, tv_loss: 0.03390282019972801\n",
      "iteration 3004, dc_loss: 0.023882528766989708, tv_loss: 0.03389567881822586\n",
      "iteration 3005, dc_loss: 0.023885536938905716, tv_loss: 0.03388531133532524\n",
      "iteration 3006, dc_loss: 0.02387598343193531, tv_loss: 0.0338968001306057\n",
      "iteration 3007, dc_loss: 0.023878024891018867, tv_loss: 0.03389151394367218\n",
      "iteration 3008, dc_loss: 0.023878194391727448, tv_loss: 0.03390110284090042\n",
      "iteration 3009, dc_loss: 0.023870034143328667, tv_loss: 0.03390629217028618\n",
      "iteration 3010, dc_loss: 0.023874210193753242, tv_loss: 0.033890318125486374\n",
      "iteration 3011, dc_loss: 0.02386591024696827, tv_loss: 0.03389723598957062\n",
      "iteration 3012, dc_loss: 0.02386646345257759, tv_loss: 0.03389163315296173\n",
      "iteration 3013, dc_loss: 0.023862095549702644, tv_loss: 0.033898357301950455\n",
      "iteration 3014, dc_loss: 0.023858778178691864, tv_loss: 0.03389361873269081\n",
      "iteration 3015, dc_loss: 0.023854296654462814, tv_loss: 0.03389497101306915\n",
      "iteration 3016, dc_loss: 0.02387164533138275, tv_loss: 0.03387855365872383\n",
      "iteration 3017, dc_loss: 0.0238491240888834, tv_loss: 0.03389890491962433\n",
      "iteration 3018, dc_loss: 0.02386135421693325, tv_loss: 0.03388800844550133\n",
      "iteration 3019, dc_loss: 0.023856578394770622, tv_loss: 0.0338854044675827\n",
      "iteration 3020, dc_loss: 0.023863136768341064, tv_loss: 0.033880408853292465\n",
      "iteration 3021, dc_loss: 0.023847194388508797, tv_loss: 0.03390268608927727\n",
      "iteration 3022, dc_loss: 0.023864680901169777, tv_loss: 0.03387109190225601\n",
      "iteration 3023, dc_loss: 0.023831214755773544, tv_loss: 0.03390178456902504\n",
      "iteration 3024, dc_loss: 0.023854363709688187, tv_loss: 0.03388434648513794\n",
      "iteration 3025, dc_loss: 0.02383718453347683, tv_loss: 0.033890336751937866\n",
      "iteration 3026, dc_loss: 0.023822683840990067, tv_loss: 0.033898379653692245\n",
      "iteration 3027, dc_loss: 0.02383006364107132, tv_loss: 0.03390161693096161\n",
      "iteration 3028, dc_loss: 0.02383880503475666, tv_loss: 0.033885739743709564\n",
      "iteration 3029, dc_loss: 0.02381896786391735, tv_loss: 0.033902984112501144\n",
      "iteration 3030, dc_loss: 0.02381197363138199, tv_loss: 0.03390224277973175\n",
      "iteration 3031, dc_loss: 0.023840321227908134, tv_loss: 0.033887095749378204\n",
      "iteration 3032, dc_loss: 0.02381070889532566, tv_loss: 0.03390859439969063\n",
      "iteration 3033, dc_loss: 0.023817602545022964, tv_loss: 0.033891577273607254\n",
      "iteration 3034, dc_loss: 0.023812850937247276, tv_loss: 0.033894751220941544\n",
      "iteration 3035, dc_loss: 0.02381088212132454, tv_loss: 0.033907450735569\n",
      "iteration 3036, dc_loss: 0.023805854842066765, tv_loss: 0.033902429044246674\n",
      "iteration 3037, dc_loss: 0.02380165085196495, tv_loss: 0.03389634191989899\n",
      "iteration 3038, dc_loss: 0.02380233444273472, tv_loss: 0.03389778733253479\n",
      "iteration 3039, dc_loss: 0.023803113028407097, tv_loss: 0.033899303525686264\n",
      "iteration 3040, dc_loss: 0.023801729083061218, tv_loss: 0.033890724182128906\n",
      "iteration 3041, dc_loss: 0.02378867194056511, tv_loss: 0.0338997021317482\n",
      "iteration 3042, dc_loss: 0.023792603984475136, tv_loss: 0.033892832696437836\n",
      "iteration 3043, dc_loss: 0.023792404681444168, tv_loss: 0.033897943794727325\n",
      "iteration 3044, dc_loss: 0.02378794178366661, tv_loss: 0.03389286622405052\n",
      "iteration 3045, dc_loss: 0.023775191977620125, tv_loss: 0.033901285380125046\n",
      "iteration 3046, dc_loss: 0.02378985844552517, tv_loss: 0.03387819603085518\n",
      "iteration 3047, dc_loss: 0.02379157394170761, tv_loss: 0.03388102352619171\n",
      "iteration 3048, dc_loss: 0.023767314851284027, tv_loss: 0.03390217199921608\n",
      "iteration 3049, dc_loss: 0.02377774752676487, tv_loss: 0.0338955894112587\n",
      "iteration 3050, dc_loss: 0.02377045713365078, tv_loss: 0.0338907316327095\n",
      "iteration 3051, dc_loss: 0.02377050369977951, tv_loss: 0.03388690948486328\n",
      "iteration 3052, dc_loss: 0.023763548582792282, tv_loss: 0.03389008343219757\n",
      "iteration 3053, dc_loss: 0.023770589381456375, tv_loss: 0.03388424217700958\n",
      "iteration 3054, dc_loss: 0.023763883858919144, tv_loss: 0.033895764499902725\n",
      "iteration 3055, dc_loss: 0.02375919558107853, tv_loss: 0.03389964625239372\n",
      "iteration 3056, dc_loss: 0.02375464327633381, tv_loss: 0.033895812928676605\n",
      "iteration 3057, dc_loss: 0.02374250628054142, tv_loss: 0.03389731049537659\n",
      "iteration 3058, dc_loss: 0.023763447999954224, tv_loss: 0.03388799726963043\n",
      "iteration 3059, dc_loss: 0.023758213967084885, tv_loss: 0.03388861194252968\n",
      "iteration 3060, dc_loss: 0.02373793162405491, tv_loss: 0.03390306234359741\n",
      "iteration 3061, dc_loss: 0.023745477199554443, tv_loss: 0.03388966619968414\n",
      "iteration 3062, dc_loss: 0.023753060027956963, tv_loss: 0.03388628736138344\n",
      "iteration 3063, dc_loss: 0.023731650784611702, tv_loss: 0.033920370042324066\n",
      "iteration 3064, dc_loss: 0.023743484169244766, tv_loss: 0.03389403969049454\n",
      "iteration 3065, dc_loss: 0.02372860535979271, tv_loss: 0.03390177711844444\n",
      "iteration 3066, dc_loss: 0.023741809651255608, tv_loss: 0.03389648348093033\n",
      "iteration 3067, dc_loss: 0.023734761402010918, tv_loss: 0.03389975056052208\n",
      "iteration 3068, dc_loss: 0.02373022586107254, tv_loss: 0.03389500454068184\n",
      "iteration 3069, dc_loss: 0.02372516505420208, tv_loss: 0.033900439739227295\n",
      "iteration 3070, dc_loss: 0.023739803582429886, tv_loss: 0.033881548792123795\n",
      "iteration 3071, dc_loss: 0.023718245327472687, tv_loss: 0.03391023725271225\n",
      "iteration 3072, dc_loss: 0.023738255724310875, tv_loss: 0.03387756273150444\n",
      "iteration 3073, dc_loss: 0.023711688816547394, tv_loss: 0.03390360251069069\n",
      "iteration 3074, dc_loss: 0.02373684011399746, tv_loss: 0.033878393471241\n",
      "iteration 3075, dc_loss: 0.023720860481262207, tv_loss: 0.0338936410844326\n",
      "iteration 3076, dc_loss: 0.02371765673160553, tv_loss: 0.03388962522149086\n",
      "iteration 3077, dc_loss: 0.0237042848020792, tv_loss: 0.033896129578351974\n",
      "iteration 3078, dc_loss: 0.023721812292933464, tv_loss: 0.03387245535850525\n",
      "iteration 3079, dc_loss: 0.02369806170463562, tv_loss: 0.03388843312859535\n",
      "iteration 3080, dc_loss: 0.023705150932073593, tv_loss: 0.03387787193059921\n",
      "iteration 3081, dc_loss: 0.023701520636677742, tv_loss: 0.03387753665447235\n",
      "iteration 3082, dc_loss: 0.023695625364780426, tv_loss: 0.03388350084424019\n",
      "iteration 3083, dc_loss: 0.023691099137067795, tv_loss: 0.0338822677731514\n",
      "iteration 3084, dc_loss: 0.02368227019906044, tv_loss: 0.03388824313879013\n",
      "iteration 3085, dc_loss: 0.02369585447013378, tv_loss: 0.03387627378106117\n",
      "iteration 3086, dc_loss: 0.023687943816184998, tv_loss: 0.033885546028614044\n",
      "iteration 3087, dc_loss: 0.02368389442563057, tv_loss: 0.03388963267207146\n",
      "iteration 3088, dc_loss: 0.023678697645664215, tv_loss: 0.03390135616064072\n",
      "iteration 3089, dc_loss: 0.023685473948717117, tv_loss: 0.03388029709458351\n",
      "iteration 3090, dc_loss: 0.023675663396716118, tv_loss: 0.03388708084821701\n",
      "iteration 3091, dc_loss: 0.02366425283253193, tv_loss: 0.03389232978224754\n",
      "iteration 3092, dc_loss: 0.02367374859750271, tv_loss: 0.03387565538287163\n",
      "iteration 3093, dc_loss: 0.02367074228823185, tv_loss: 0.0338907428085804\n",
      "iteration 3094, dc_loss: 0.02365971729159355, tv_loss: 0.033909644931554794\n",
      "iteration 3095, dc_loss: 0.023673800751566887, tv_loss: 0.033885758370161057\n",
      "iteration 3096, dc_loss: 0.023657670244574547, tv_loss: 0.03388429060578346\n",
      "iteration 3097, dc_loss: 0.023656105622649193, tv_loss: 0.03389253094792366\n",
      "iteration 3098, dc_loss: 0.023660769686102867, tv_loss: 0.03389163315296173\n",
      "iteration 3099, dc_loss: 0.023646336048841476, tv_loss: 0.033908963203430176\n",
      "iteration 3100, dc_loss: 0.023658843711018562, tv_loss: 0.03388034924864769\n",
      "iteration 3101, dc_loss: 0.023648716509342194, tv_loss: 0.03388972952961922\n",
      "iteration 3102, dc_loss: 0.02365179918706417, tv_loss: 0.03388415649533272\n",
      "iteration 3103, dc_loss: 0.023647764697670937, tv_loss: 0.03388845920562744\n",
      "iteration 3104, dc_loss: 0.02363474667072296, tv_loss: 0.03389579802751541\n",
      "iteration 3105, dc_loss: 0.02364567667245865, tv_loss: 0.033877260982990265\n",
      "iteration 3106, dc_loss: 0.023643100634217262, tv_loss: 0.0338759608566761\n",
      "iteration 3107, dc_loss: 0.023626253008842468, tv_loss: 0.03388965129852295\n",
      "iteration 3108, dc_loss: 0.02364281937479973, tv_loss: 0.033871423453092575\n",
      "iteration 3109, dc_loss: 0.02363504283130169, tv_loss: 0.03387562930583954\n",
      "iteration 3110, dc_loss: 0.02361758053302765, tv_loss: 0.03388776257634163\n",
      "iteration 3111, dc_loss: 0.023633534088730812, tv_loss: 0.03387343883514404\n",
      "iteration 3112, dc_loss: 0.023637795820832253, tv_loss: 0.033873509615659714\n",
      "iteration 3113, dc_loss: 0.023605827242136, tv_loss: 0.03391101956367493\n",
      "iteration 3114, dc_loss: 0.023634687066078186, tv_loss: 0.033884406089782715\n",
      "iteration 3115, dc_loss: 0.02361695095896721, tv_loss: 0.033893883228302\n",
      "iteration 3116, dc_loss: 0.023640519008040428, tv_loss: 0.03386908397078514\n",
      "iteration 3117, dc_loss: 0.023615116253495216, tv_loss: 0.03389649838209152\n",
      "iteration 3118, dc_loss: 0.023644262924790382, tv_loss: 0.03386591374874115\n",
      "iteration 3119, dc_loss: 0.023628318682312965, tv_loss: 0.033887118101119995\n",
      "iteration 3120, dc_loss: 0.023644765838980675, tv_loss: 0.03386101871728897\n",
      "iteration 3121, dc_loss: 0.02361016720533371, tv_loss: 0.033897772431373596\n",
      "iteration 3122, dc_loss: 0.023644540458917618, tv_loss: 0.033851493149995804\n",
      "iteration 3123, dc_loss: 0.023589711636304855, tv_loss: 0.0338968001306057\n",
      "iteration 3124, dc_loss: 0.023614119738340378, tv_loss: 0.033872656524181366\n",
      "iteration 3125, dc_loss: 0.02359350398182869, tv_loss: 0.03389598801732063\n",
      "iteration 3126, dc_loss: 0.023582477122545242, tv_loss: 0.033893950283527374\n",
      "iteration 3127, dc_loss: 0.023597249761223793, tv_loss: 0.03388107940554619\n",
      "iteration 3128, dc_loss: 0.023594461381435394, tv_loss: 0.03388392925262451\n",
      "iteration 3129, dc_loss: 0.023591401055455208, tv_loss: 0.033889833837747574\n",
      "iteration 3130, dc_loss: 0.02358032763004303, tv_loss: 0.03390063717961311\n",
      "iteration 3131, dc_loss: 0.023606808856129646, tv_loss: 0.03386155888438225\n",
      "iteration 3132, dc_loss: 0.02356071211397648, tv_loss: 0.03389947861433029\n",
      "iteration 3133, dc_loss: 0.023582177236676216, tv_loss: 0.033884111791849136\n",
      "iteration 3134, dc_loss: 0.02358376234769821, tv_loss: 0.03386738896369934\n",
      "iteration 3135, dc_loss: 0.023559506982564926, tv_loss: 0.03388803079724312\n",
      "iteration 3136, dc_loss: 0.02358076348900795, tv_loss: 0.03387126326560974\n",
      "iteration 3137, dc_loss: 0.023572487756609917, tv_loss: 0.03387563303112984\n",
      "iteration 3138, dc_loss: 0.023554159328341484, tv_loss: 0.03388513624668121\n",
      "iteration 3139, dc_loss: 0.023565623909235, tv_loss: 0.03388121351599693\n",
      "iteration 3140, dc_loss: 0.023572852835059166, tv_loss: 0.03388356789946556\n",
      "iteration 3141, dc_loss: 0.023553093895316124, tv_loss: 0.033899612724781036\n",
      "iteration 3142, dc_loss: 0.02355623058974743, tv_loss: 0.03388383612036705\n",
      "iteration 3143, dc_loss: 0.023555465042591095, tv_loss: 0.033877547830343246\n",
      "iteration 3144, dc_loss: 0.023552710190415382, tv_loss: 0.033875878900289536\n",
      "iteration 3145, dc_loss: 0.02354941889643669, tv_loss: 0.03388288617134094\n",
      "iteration 3146, dc_loss: 0.023545844480395317, tv_loss: 0.03388853371143341\n",
      "iteration 3147, dc_loss: 0.023539863526821136, tv_loss: 0.0338829904794693\n",
      "iteration 3148, dc_loss: 0.023541226983070374, tv_loss: 0.033883653581142426\n",
      "iteration 3149, dc_loss: 0.02354808710515499, tv_loss: 0.03386964648962021\n",
      "iteration 3150, dc_loss: 0.02352876402437687, tv_loss: 0.03388695418834686\n",
      "iteration 3151, dc_loss: 0.02354329079389572, tv_loss: 0.033874284476041794\n",
      "iteration 3152, dc_loss: 0.02352750673890114, tv_loss: 0.03389442712068558\n",
      "iteration 3153, dc_loss: 0.02353249303996563, tv_loss: 0.03388805687427521\n",
      "iteration 3154, dc_loss: 0.023525524884462357, tv_loss: 0.0338822603225708\n",
      "iteration 3155, dc_loss: 0.023530947044491768, tv_loss: 0.03387192264199257\n",
      "iteration 3156, dc_loss: 0.023518048226833344, tv_loss: 0.03387864679098129\n",
      "iteration 3157, dc_loss: 0.02352265641093254, tv_loss: 0.03387559577822685\n",
      "iteration 3158, dc_loss: 0.02352205477654934, tv_loss: 0.033879995346069336\n",
      "iteration 3159, dc_loss: 0.02350488305091858, tv_loss: 0.0338919572532177\n",
      "iteration 3160, dc_loss: 0.023518657311797142, tv_loss: 0.03388344496488571\n",
      "iteration 3161, dc_loss: 0.02351592853665352, tv_loss: 0.03387751802802086\n",
      "iteration 3162, dc_loss: 0.023511512205004692, tv_loss: 0.03387828171253204\n",
      "iteration 3163, dc_loss: 0.02349843643605709, tv_loss: 0.03389031067490578\n",
      "iteration 3164, dc_loss: 0.02350779063999653, tv_loss: 0.03387463092803955\n",
      "iteration 3165, dc_loss: 0.023505013436079025, tv_loss: 0.033873673528432846\n",
      "iteration 3166, dc_loss: 0.023504698649048805, tv_loss: 0.03387110307812691\n",
      "iteration 3167, dc_loss: 0.0234988983720541, tv_loss: 0.033879153430461884\n",
      "iteration 3168, dc_loss: 0.023488705977797508, tv_loss: 0.03388061001896858\n",
      "iteration 3169, dc_loss: 0.023495495319366455, tv_loss: 0.03387312591075897\n",
      "iteration 3170, dc_loss: 0.023494984954595566, tv_loss: 0.03387853130698204\n",
      "iteration 3171, dc_loss: 0.023484013974666595, tv_loss: 0.033888302743434906\n",
      "iteration 3172, dc_loss: 0.023497266694903374, tv_loss: 0.03386861830949783\n",
      "iteration 3173, dc_loss: 0.02348080649971962, tv_loss: 0.03388537839055061\n",
      "iteration 3174, dc_loss: 0.023494713008403778, tv_loss: 0.033868275582790375\n",
      "iteration 3175, dc_loss: 0.02348105050623417, tv_loss: 0.03388004004955292\n",
      "iteration 3176, dc_loss: 0.023489635437726974, tv_loss: 0.033879250288009644\n",
      "iteration 3177, dc_loss: 0.02348269894719124, tv_loss: 0.033906806260347366\n",
      "iteration 3178, dc_loss: 0.023513682186603546, tv_loss: 0.03386400267481804\n",
      "iteration 3179, dc_loss: 0.023468313738703728, tv_loss: 0.033913467079401016\n",
      "iteration 3180, dc_loss: 0.023517729714512825, tv_loss: 0.03386543318629265\n",
      "iteration 3181, dc_loss: 0.023493312299251556, tv_loss: 0.033899612724781036\n",
      "iteration 3182, dc_loss: 0.023502914234995842, tv_loss: 0.03388189896941185\n",
      "iteration 3183, dc_loss: 0.023467712104320526, tv_loss: 0.03389953449368477\n",
      "iteration 3184, dc_loss: 0.023507010191679, tv_loss: 0.03385329991579056\n",
      "iteration 3185, dc_loss: 0.023447595536708832, tv_loss: 0.033906858414411545\n",
      "iteration 3186, dc_loss: 0.023462239652872086, tv_loss: 0.03387795761227608\n",
      "iteration 3187, dc_loss: 0.02345726080238819, tv_loss: 0.03388114273548126\n",
      "iteration 3188, dc_loss: 0.023450041189789772, tv_loss: 0.03388005122542381\n",
      "iteration 3189, dc_loss: 0.023457441478967667, tv_loss: 0.033883631229400635\n",
      "iteration 3190, dc_loss: 0.023449277505278587, tv_loss: 0.033895064145326614\n",
      "iteration 3191, dc_loss: 0.02347618155181408, tv_loss: 0.03386067971587181\n",
      "iteration 3192, dc_loss: 0.02342366799712181, tv_loss: 0.03390173986554146\n",
      "iteration 3193, dc_loss: 0.02346160262823105, tv_loss: 0.03385842591524124\n",
      "iteration 3194, dc_loss: 0.023440897464752197, tv_loss: 0.0338875986635685\n",
      "iteration 3195, dc_loss: 0.023426013067364693, tv_loss: 0.033901508897542953\n",
      "iteration 3196, dc_loss: 0.023450275883078575, tv_loss: 0.0338568240404129\n",
      "iteration 3197, dc_loss: 0.023425716906785965, tv_loss: 0.033879127353429794\n",
      "iteration 3198, dc_loss: 0.023441849276423454, tv_loss: 0.03386395424604416\n",
      "iteration 3199, dc_loss: 0.023420026525855064, tv_loss: 0.033888865262269974\n",
      "iteration 3200, dc_loss: 0.02343936450779438, tv_loss: 0.033879827708005905\n",
      "iteration 3201, dc_loss: 0.02342921867966652, tv_loss: 0.03388381376862526\n",
      "iteration 3202, dc_loss: 0.0234150979667902, tv_loss: 0.033875249326229095\n",
      "iteration 3203, dc_loss: 0.02341822162270546, tv_loss: 0.03386811539530754\n",
      "iteration 3204, dc_loss: 0.023421749472618103, tv_loss: 0.03387322649359703\n",
      "iteration 3205, dc_loss: 0.023413296788930893, tv_loss: 0.03387269750237465\n",
      "iteration 3206, dc_loss: 0.02340894751250744, tv_loss: 0.03387359902262688\n",
      "iteration 3207, dc_loss: 0.023416420444846153, tv_loss: 0.03386562690138817\n",
      "iteration 3208, dc_loss: 0.023413721472024918, tv_loss: 0.033863432705402374\n",
      "iteration 3209, dc_loss: 0.02340560220181942, tv_loss: 0.03387050703167915\n",
      "iteration 3210, dc_loss: 0.02340320497751236, tv_loss: 0.0338679701089859\n",
      "iteration 3211, dc_loss: 0.02340473048388958, tv_loss: 0.0338631235063076\n",
      "iteration 3212, dc_loss: 0.02340279333293438, tv_loss: 0.03386170044541359\n",
      "iteration 3213, dc_loss: 0.023402037099003792, tv_loss: 0.033861540257930756\n",
      "iteration 3214, dc_loss: 0.023394513875246048, tv_loss: 0.033867936581373215\n",
      "iteration 3215, dc_loss: 0.023395469412207603, tv_loss: 0.03386269882321358\n",
      "iteration 3216, dc_loss: 0.023402975872159004, tv_loss: 0.033857643604278564\n",
      "iteration 3217, dc_loss: 0.023391902446746826, tv_loss: 0.03386843577027321\n",
      "iteration 3218, dc_loss: 0.023385072126984596, tv_loss: 0.033873558044433594\n",
      "iteration 3219, dc_loss: 0.023393956944346428, tv_loss: 0.033859703689813614\n",
      "iteration 3220, dc_loss: 0.023389985784888268, tv_loss: 0.0338582769036293\n",
      "iteration 3221, dc_loss: 0.0233776792883873, tv_loss: 0.03387066349387169\n",
      "iteration 3222, dc_loss: 0.023385601118206978, tv_loss: 0.03385714069008827\n",
      "iteration 3223, dc_loss: 0.02338438294827938, tv_loss: 0.033860109746456146\n",
      "iteration 3224, dc_loss: 0.02337663248181343, tv_loss: 0.03387215733528137\n",
      "iteration 3225, dc_loss: 0.02338210493326187, tv_loss: 0.03386707231402397\n",
      "iteration 3226, dc_loss: 0.023377617821097374, tv_loss: 0.033866703510284424\n",
      "iteration 3227, dc_loss: 0.02337379939854145, tv_loss: 0.03386153653264046\n",
      "iteration 3228, dc_loss: 0.023376481607556343, tv_loss: 0.033857088536024094\n",
      "iteration 3229, dc_loss: 0.023368123918771744, tv_loss: 0.03386062756180763\n",
      "iteration 3230, dc_loss: 0.023365765810012817, tv_loss: 0.03385988995432854\n",
      "iteration 3231, dc_loss: 0.023375101387500763, tv_loss: 0.033853061497211456\n",
      "iteration 3232, dc_loss: 0.023365052416920662, tv_loss: 0.03386397659778595\n",
      "iteration 3233, dc_loss: 0.023359855636954308, tv_loss: 0.03387732803821564\n",
      "iteration 3234, dc_loss: 0.02336849644780159, tv_loss: 0.033868949860334396\n",
      "iteration 3235, dc_loss: 0.02335783652961254, tv_loss: 0.033865638077259064\n",
      "iteration 3236, dc_loss: 0.023358510807156563, tv_loss: 0.03385896235704422\n",
      "iteration 3237, dc_loss: 0.023362435400485992, tv_loss: 0.033854350447654724\n",
      "iteration 3238, dc_loss: 0.023350797593593597, tv_loss: 0.03386790305376053\n",
      "iteration 3239, dc_loss: 0.023351997137069702, tv_loss: 0.03387037292122841\n",
      "iteration 3240, dc_loss: 0.02335241250693798, tv_loss: 0.03386834263801575\n",
      "iteration 3241, dc_loss: 0.023349178954958916, tv_loss: 0.033864062279462814\n",
      "iteration 3242, dc_loss: 0.023347901180386543, tv_loss: 0.033863186836242676\n",
      "iteration 3243, dc_loss: 0.023354507982730865, tv_loss: 0.03384974226355553\n",
      "iteration 3244, dc_loss: 0.023343872278928757, tv_loss: 0.03386012092232704\n",
      "iteration 3245, dc_loss: 0.02333744987845421, tv_loss: 0.03387414291501045\n",
      "iteration 3246, dc_loss: 0.023347796872258186, tv_loss: 0.03387025371193886\n",
      "iteration 3247, dc_loss: 0.02333860471844673, tv_loss: 0.03386682644486427\n",
      "iteration 3248, dc_loss: 0.023337677121162415, tv_loss: 0.033857010304927826\n",
      "iteration 3249, dc_loss: 0.02333512343466282, tv_loss: 0.03386113420128822\n",
      "iteration 3250, dc_loss: 0.023329414427280426, tv_loss: 0.033865850418806076\n",
      "iteration 3251, dc_loss: 0.023337170481681824, tv_loss: 0.03385698422789574\n",
      "iteration 3252, dc_loss: 0.02333408035337925, tv_loss: 0.03386033698916435\n",
      "iteration 3253, dc_loss: 0.023325592279434204, tv_loss: 0.03386957198381424\n",
      "iteration 3254, dc_loss: 0.023326314985752106, tv_loss: 0.03386153653264046\n",
      "iteration 3255, dc_loss: 0.023330148309469223, tv_loss: 0.03385559469461441\n",
      "iteration 3256, dc_loss: 0.023322319611907005, tv_loss: 0.03385871276259422\n",
      "iteration 3257, dc_loss: 0.023322654888033867, tv_loss: 0.033855944871902466\n",
      "iteration 3258, dc_loss: 0.02332419902086258, tv_loss: 0.03385945409536362\n",
      "iteration 3259, dc_loss: 0.023310868069529533, tv_loss: 0.03388643264770508\n",
      "iteration 3260, dc_loss: 0.02331804670393467, tv_loss: 0.033869244158267975\n",
      "iteration 3261, dc_loss: 0.02332218736410141, tv_loss: 0.03385055065155029\n",
      "iteration 3262, dc_loss: 0.023310665041208267, tv_loss: 0.03387299180030823\n",
      "iteration 3263, dc_loss: 0.02330840565264225, tv_loss: 0.033887725323438644\n",
      "iteration 3264, dc_loss: 0.023311179131269455, tv_loss: 0.03386484831571579\n",
      "iteration 3265, dc_loss: 0.02331107296049595, tv_loss: 0.03386347368359566\n",
      "iteration 3266, dc_loss: 0.023305458948016167, tv_loss: 0.03388230875134468\n",
      "iteration 3267, dc_loss: 0.023301320150494576, tv_loss: 0.033875320106744766\n",
      "iteration 3268, dc_loss: 0.023305175825953484, tv_loss: 0.03386625275015831\n",
      "iteration 3269, dc_loss: 0.0233052559196949, tv_loss: 0.03387356549501419\n",
      "iteration 3270, dc_loss: 0.02329958975315094, tv_loss: 0.033880703151226044\n",
      "iteration 3271, dc_loss: 0.02329869754612446, tv_loss: 0.033864494413137436\n",
      "iteration 3272, dc_loss: 0.023295065388083458, tv_loss: 0.033884257078170776\n",
      "iteration 3273, dc_loss: 0.02328861691057682, tv_loss: 0.03387448191642761\n",
      "iteration 3274, dc_loss: 0.02329503558576107, tv_loss: 0.03386491537094116\n",
      "iteration 3275, dc_loss: 0.023297930136322975, tv_loss: 0.033871185034513474\n",
      "iteration 3276, dc_loss: 0.02328716404736042, tv_loss: 0.033872779458761215\n",
      "iteration 3277, dc_loss: 0.023287838324904442, tv_loss: 0.03386479243636131\n",
      "iteration 3278, dc_loss: 0.023288613185286522, tv_loss: 0.033867739140987396\n",
      "iteration 3279, dc_loss: 0.0232793428003788, tv_loss: 0.03387679159641266\n",
      "iteration 3280, dc_loss: 0.023284079506993294, tv_loss: 0.03386390581727028\n",
      "iteration 3281, dc_loss: 0.023288164287805557, tv_loss: 0.03385898843407631\n",
      "iteration 3282, dc_loss: 0.023270709440112114, tv_loss: 0.033877551555633545\n",
      "iteration 3283, dc_loss: 0.023277640342712402, tv_loss: 0.0338665209710598\n",
      "iteration 3284, dc_loss: 0.023286616429686546, tv_loss: 0.033848512917757034\n",
      "iteration 3285, dc_loss: 0.023272376507520676, tv_loss: 0.03386436775326729\n",
      "iteration 3286, dc_loss: 0.023270370438694954, tv_loss: 0.03387510031461716\n",
      "iteration 3287, dc_loss: 0.023275820538401604, tv_loss: 0.033862680196762085\n",
      "iteration 3288, dc_loss: 0.023273158818483353, tv_loss: 0.033854540437459946\n",
      "iteration 3289, dc_loss: 0.023258334025740623, tv_loss: 0.03386906534433365\n",
      "iteration 3290, dc_loss: 0.023261187598109245, tv_loss: 0.03387890383601189\n",
      "iteration 3291, dc_loss: 0.02327071875333786, tv_loss: 0.03386245295405388\n",
      "iteration 3292, dc_loss: 0.02325940690934658, tv_loss: 0.0338604599237442\n",
      "iteration 3293, dc_loss: 0.02326034940779209, tv_loss: 0.033861663192510605\n",
      "iteration 3294, dc_loss: 0.023268332704901695, tv_loss: 0.0338614359498024\n",
      "iteration 3295, dc_loss: 0.023253846913576126, tv_loss: 0.03386891633272171\n",
      "iteration 3296, dc_loss: 0.023257020860910416, tv_loss: 0.03385606408119202\n",
      "iteration 3297, dc_loss: 0.023255012929439545, tv_loss: 0.03385331481695175\n",
      "iteration 3298, dc_loss: 0.023246649652719498, tv_loss: 0.03386485576629639\n",
      "iteration 3299, dc_loss: 0.023251932114362717, tv_loss: 0.033870797604322433\n",
      "iteration 3300, dc_loss: 0.023250849917531013, tv_loss: 0.03386393561959267\n",
      "iteration 3301, dc_loss: 0.023245662450790405, tv_loss: 0.033854372799396515\n",
      "iteration 3302, dc_loss: 0.023245321586728096, tv_loss: 0.0338573195040226\n",
      "iteration 3303, dc_loss: 0.023245662450790405, tv_loss: 0.03386634215712547\n",
      "iteration 3304, dc_loss: 0.02324148267507553, tv_loss: 0.03386618196964264\n",
      "iteration 3305, dc_loss: 0.02323731780052185, tv_loss: 0.03385848179459572\n",
      "iteration 3306, dc_loss: 0.02324162982404232, tv_loss: 0.03384985402226448\n",
      "iteration 3307, dc_loss: 0.023232240229845047, tv_loss: 0.03385908901691437\n",
      "iteration 3308, dc_loss: 0.023233648389577866, tv_loss: 0.033863600343465805\n",
      "iteration 3309, dc_loss: 0.02324039489030838, tv_loss: 0.03386212885379791\n",
      "iteration 3310, dc_loss: 0.023232173174619675, tv_loss: 0.03385759890079498\n",
      "iteration 3311, dc_loss: 0.023227330297231674, tv_loss: 0.033855415880680084\n",
      "iteration 3312, dc_loss: 0.023227816447615623, tv_loss: 0.03386497497558594\n",
      "iteration 3313, dc_loss: 0.023225875571370125, tv_loss: 0.033873360604047775\n",
      "iteration 3314, dc_loss: 0.02322554402053356, tv_loss: 0.033862434327602386\n",
      "iteration 3315, dc_loss: 0.02322239801287651, tv_loss: 0.0338539257645607\n",
      "iteration 3316, dc_loss: 0.02322530932724476, tv_loss: 0.03386252000927925\n",
      "iteration 3317, dc_loss: 0.023220930248498917, tv_loss: 0.03386858105659485\n",
      "iteration 3318, dc_loss: 0.02321229688823223, tv_loss: 0.03386586531996727\n",
      "iteration 3319, dc_loss: 0.023219088092446327, tv_loss: 0.03385266289114952\n",
      "iteration 3320, dc_loss: 0.02321738563477993, tv_loss: 0.0338640995323658\n",
      "iteration 3321, dc_loss: 0.023210227489471436, tv_loss: 0.033871784806251526\n",
      "iteration 3322, dc_loss: 0.02321120910346508, tv_loss: 0.033858008682727814\n",
      "iteration 3323, dc_loss: 0.023211749270558357, tv_loss: 0.033853672444820404\n",
      "iteration 3324, dc_loss: 0.02320956438779831, tv_loss: 0.033872928470373154\n",
      "iteration 3325, dc_loss: 0.023204931989312172, tv_loss: 0.03386586159467697\n",
      "iteration 3326, dc_loss: 0.023199377581477165, tv_loss: 0.03385878726840019\n",
      "iteration 3327, dc_loss: 0.02320392243564129, tv_loss: 0.03386176750063896\n",
      "iteration 3328, dc_loss: 0.023208798840641975, tv_loss: 0.033865295350551605\n",
      "iteration 3329, dc_loss: 0.023193297907710075, tv_loss: 0.03386707603931427\n",
      "iteration 3330, dc_loss: 0.023195216432213783, tv_loss: 0.033856261521577835\n",
      "iteration 3331, dc_loss: 0.02320248633623123, tv_loss: 0.03385593742132187\n",
      "iteration 3332, dc_loss: 0.023192575201392174, tv_loss: 0.03387190401554108\n",
      "iteration 3333, dc_loss: 0.0231906957924366, tv_loss: 0.033861901611089706\n",
      "iteration 3334, dc_loss: 0.02319491282105446, tv_loss: 0.03384774550795555\n",
      "iteration 3335, dc_loss: 0.02319408766925335, tv_loss: 0.03385664150118828\n",
      "iteration 3336, dc_loss: 0.02318437024950981, tv_loss: 0.03386610373854637\n",
      "iteration 3337, dc_loss: 0.02318032644689083, tv_loss: 0.033863671123981476\n",
      "iteration 3338, dc_loss: 0.023190295323729515, tv_loss: 0.0338456891477108\n",
      "iteration 3339, dc_loss: 0.023179946467280388, tv_loss: 0.03385718911886215\n",
      "iteration 3340, dc_loss: 0.023179154843091965, tv_loss: 0.03386528417468071\n",
      "iteration 3341, dc_loss: 0.02318784035742283, tv_loss: 0.03385324031114578\n",
      "iteration 3342, dc_loss: 0.023173779249191284, tv_loss: 0.03385951370000839\n",
      "iteration 3343, dc_loss: 0.02317233569920063, tv_loss: 0.033855728805065155\n",
      "iteration 3344, dc_loss: 0.023180630058050156, tv_loss: 0.03384888917207718\n",
      "iteration 3345, dc_loss: 0.023172788321971893, tv_loss: 0.033857326954603195\n",
      "iteration 3346, dc_loss: 0.02317116968333721, tv_loss: 0.03386089205741882\n",
      "iteration 3347, dc_loss: 0.023173216730356216, tv_loss: 0.033847589045763016\n",
      "iteration 3348, dc_loss: 0.0231617521494627, tv_loss: 0.03385799378156662\n",
      "iteration 3349, dc_loss: 0.023162269964814186, tv_loss: 0.03385438397526741\n",
      "iteration 3350, dc_loss: 0.02316751517355442, tv_loss: 0.03385790437459946\n",
      "iteration 3351, dc_loss: 0.023158710449934006, tv_loss: 0.033867768943309784\n",
      "iteration 3352, dc_loss: 0.023161668330430984, tv_loss: 0.03385539352893829\n",
      "iteration 3353, dc_loss: 0.02317061647772789, tv_loss: 0.03383876010775566\n",
      "iteration 3354, dc_loss: 0.023153211921453476, tv_loss: 0.03386366367340088\n",
      "iteration 3355, dc_loss: 0.023154277354478836, tv_loss: 0.03386601433157921\n",
      "iteration 3356, dc_loss: 0.023158399388194084, tv_loss: 0.03385303169488907\n",
      "iteration 3357, dc_loss: 0.023146046325564384, tv_loss: 0.033858783543109894\n",
      "iteration 3358, dc_loss: 0.023150332272052765, tv_loss: 0.03385290503501892\n",
      "iteration 3359, dc_loss: 0.023157238960266113, tv_loss: 0.03385147824883461\n",
      "iteration 3360, dc_loss: 0.023145100101828575, tv_loss: 0.033859606832265854\n",
      "iteration 3361, dc_loss: 0.02314523421227932, tv_loss: 0.03385476768016815\n",
      "iteration 3362, dc_loss: 0.023148884996771812, tv_loss: 0.0338478609919548\n",
      "iteration 3363, dc_loss: 0.023138584569096565, tv_loss: 0.03385898843407631\n",
      "iteration 3364, dc_loss: 0.02314123883843422, tv_loss: 0.03386050462722778\n",
      "iteration 3365, dc_loss: 0.023142319172620773, tv_loss: 0.03385148569941521\n",
      "iteration 3366, dc_loss: 0.02314198762178421, tv_loss: 0.033846665173769\n",
      "iteration 3367, dc_loss: 0.02312895469367504, tv_loss: 0.03386002779006958\n",
      "iteration 3368, dc_loss: 0.02313765324652195, tv_loss: 0.03384692594408989\n",
      "iteration 3369, dc_loss: 0.023137807846069336, tv_loss: 0.03385457769036293\n",
      "iteration 3370, dc_loss: 0.02312709577381611, tv_loss: 0.0338565818965435\n",
      "iteration 3371, dc_loss: 0.023127131164073944, tv_loss: 0.03385705500841141\n",
      "iteration 3372, dc_loss: 0.02312801592051983, tv_loss: 0.03384821489453316\n",
      "iteration 3373, dc_loss: 0.023129506036639214, tv_loss: 0.03384830057621002\n",
      "iteration 3374, dc_loss: 0.023129427805542946, tv_loss: 0.03385113924741745\n",
      "iteration 3375, dc_loss: 0.023119129240512848, tv_loss: 0.03385553136467934\n",
      "iteration 3376, dc_loss: 0.02312154695391655, tv_loss: 0.033847738057374954\n",
      "iteration 3377, dc_loss: 0.023124374449253082, tv_loss: 0.03384503722190857\n",
      "iteration 3378, dc_loss: 0.023109883069992065, tv_loss: 0.0338592603802681\n",
      "iteration 3379, dc_loss: 0.023115236312150955, tv_loss: 0.03385547921061516\n",
      "iteration 3380, dc_loss: 0.023122070357203484, tv_loss: 0.033842191100120544\n",
      "iteration 3381, dc_loss: 0.02310819737613201, tv_loss: 0.033855777233839035\n",
      "iteration 3382, dc_loss: 0.02310974895954132, tv_loss: 0.03385075554251671\n",
      "iteration 3383, dc_loss: 0.023120110854506493, tv_loss: 0.03383924439549446\n",
      "iteration 3384, dc_loss: 0.023107051849365234, tv_loss: 0.033853624016046524\n",
      "iteration 3385, dc_loss: 0.023096635937690735, tv_loss: 0.03386535868048668\n",
      "iteration 3386, dc_loss: 0.023107612505555153, tv_loss: 0.033853914588689804\n",
      "iteration 3387, dc_loss: 0.02310905233025551, tv_loss: 0.03384140878915787\n",
      "iteration 3388, dc_loss: 0.023097781464457512, tv_loss: 0.033858075737953186\n",
      "iteration 3389, dc_loss: 0.023094935342669487, tv_loss: 0.03385226055979729\n",
      "iteration 3390, dc_loss: 0.023105204105377197, tv_loss: 0.03384517505764961\n",
      "iteration 3391, dc_loss: 0.023097675293684006, tv_loss: 0.03384508937597275\n",
      "iteration 3392, dc_loss: 0.023090418428182602, tv_loss: 0.03384804725646973\n",
      "iteration 3393, dc_loss: 0.023096848279237747, tv_loss: 0.03384483605623245\n",
      "iteration 3394, dc_loss: 0.023095883429050446, tv_loss: 0.03384588286280632\n",
      "iteration 3395, dc_loss: 0.023079518228769302, tv_loss: 0.033858705312013626\n",
      "iteration 3396, dc_loss: 0.023092415183782578, tv_loss: 0.03385293111205101\n",
      "iteration 3397, dc_loss: 0.02309316396713257, tv_loss: 0.033843182027339935\n",
      "iteration 3398, dc_loss: 0.02307789772748947, tv_loss: 0.03385617956519127\n",
      "iteration 3399, dc_loss: 0.02308250404894352, tv_loss: 0.03385047987103462\n",
      "iteration 3400, dc_loss: 0.0230938158929348, tv_loss: 0.03384103998541832\n",
      "iteration 3401, dc_loss: 0.023075252771377563, tv_loss: 0.03385774791240692\n",
      "iteration 3402, dc_loss: 0.02308025397360325, tv_loss: 0.033847376704216\n",
      "iteration 3403, dc_loss: 0.02308330498635769, tv_loss: 0.03384128957986832\n",
      "iteration 3404, dc_loss: 0.023066719993948936, tv_loss: 0.03385460004210472\n",
      "iteration 3405, dc_loss: 0.02306678332388401, tv_loss: 0.033853039145469666\n",
      "iteration 3406, dc_loss: 0.023089615628123283, tv_loss: 0.03383222967386246\n",
      "iteration 3407, dc_loss: 0.0230674110352993, tv_loss: 0.03386395797133446\n",
      "iteration 3408, dc_loss: 0.023071765899658203, tv_loss: 0.03385106101632118\n",
      "iteration 3409, dc_loss: 0.02307184599339962, tv_loss: 0.03384355083107948\n",
      "iteration 3410, dc_loss: 0.023069677874445915, tv_loss: 0.033845074474811554\n",
      "iteration 3411, dc_loss: 0.023059258237481117, tv_loss: 0.033864110708236694\n",
      "iteration 3412, dc_loss: 0.02307007648050785, tv_loss: 0.03385128080844879\n",
      "iteration 3413, dc_loss: 0.023060904815793037, tv_loss: 0.03385816887021065\n",
      "iteration 3414, dc_loss: 0.023070257157087326, tv_loss: 0.033842530101537704\n",
      "iteration 3415, dc_loss: 0.023057814687490463, tv_loss: 0.033847905695438385\n",
      "iteration 3416, dc_loss: 0.0230672936886549, tv_loss: 0.033841267228126526\n",
      "iteration 3417, dc_loss: 0.023054609075188637, tv_loss: 0.03384769707918167\n",
      "iteration 3418, dc_loss: 0.023055696859955788, tv_loss: 0.033847346901893616\n",
      "iteration 3419, dc_loss: 0.023047275841236115, tv_loss: 0.03385135903954506\n",
      "iteration 3420, dc_loss: 0.023049967363476753, tv_loss: 0.03384236618876457\n",
      "iteration 3421, dc_loss: 0.023046158254146576, tv_loss: 0.03384591266512871\n",
      "iteration 3422, dc_loss: 0.023050978779792786, tv_loss: 0.033839717507362366\n",
      "iteration 3423, dc_loss: 0.02304946631193161, tv_loss: 0.03383983299136162\n",
      "iteration 3424, dc_loss: 0.02303571254014969, tv_loss: 0.033861201256513596\n",
      "iteration 3425, dc_loss: 0.023048441857099533, tv_loss: 0.033848151564598083\n",
      "iteration 3426, dc_loss: 0.023037385195493698, tv_loss: 0.03385390713810921\n",
      "iteration 3427, dc_loss: 0.02304403856396675, tv_loss: 0.033843379467725754\n",
      "iteration 3428, dc_loss: 0.023034725338220596, tv_loss: 0.033845946192741394\n",
      "iteration 3429, dc_loss: 0.023036882281303406, tv_loss: 0.03384111449122429\n",
      "iteration 3430, dc_loss: 0.023031018674373627, tv_loss: 0.033848270773887634\n",
      "iteration 3431, dc_loss: 0.023029960691928864, tv_loss: 0.033848028630018234\n",
      "iteration 3432, dc_loss: 0.023036833852529526, tv_loss: 0.03384627401828766\n",
      "iteration 3433, dc_loss: 0.023030629381537437, tv_loss: 0.03384576365351677\n",
      "iteration 3434, dc_loss: 0.023025374859571457, tv_loss: 0.03385273367166519\n",
      "iteration 3435, dc_loss: 0.023025376722216606, tv_loss: 0.033842120319604874\n",
      "iteration 3436, dc_loss: 0.023027069866657257, tv_loss: 0.03383874148130417\n",
      "iteration 3437, dc_loss: 0.023016635328531265, tv_loss: 0.033854056149721146\n",
      "iteration 3438, dc_loss: 0.023025648668408394, tv_loss: 0.0338391549885273\n",
      "iteration 3439, dc_loss: 0.023018499836325645, tv_loss: 0.03384413570165634\n",
      "iteration 3440, dc_loss: 0.023019731044769287, tv_loss: 0.03383965045213699\n",
      "iteration 3441, dc_loss: 0.023014530539512634, tv_loss: 0.033840861171483994\n",
      "iteration 3442, dc_loss: 0.02301015518605709, tv_loss: 0.03384409099817276\n",
      "iteration 3443, dc_loss: 0.02301529422402382, tv_loss: 0.033841513097286224\n",
      "iteration 3444, dc_loss: 0.023012030869722366, tv_loss: 0.03384382650256157\n",
      "iteration 3445, dc_loss: 0.02300650253891945, tv_loss: 0.03385248780250549\n",
      "iteration 3446, dc_loss: 0.02300783433020115, tv_loss: 0.03385166823863983\n",
      "iteration 3447, dc_loss: 0.023012882098555565, tv_loss: 0.03384597599506378\n",
      "iteration 3448, dc_loss: 0.02300049178302288, tv_loss: 0.03384961932897568\n",
      "iteration 3449, dc_loss: 0.023012682795524597, tv_loss: 0.03383299335837364\n",
      "iteration 3450, dc_loss: 0.02299155294895172, tv_loss: 0.03385152295231819\n",
      "iteration 3451, dc_loss: 0.02300022915005684, tv_loss: 0.03384018689393997\n",
      "iteration 3452, dc_loss: 0.023004762828350067, tv_loss: 0.033837564289569855\n",
      "iteration 3453, dc_loss: 0.023001648485660553, tv_loss: 0.03384094312787056\n",
      "iteration 3454, dc_loss: 0.022993052378296852, tv_loss: 0.033854492008686066\n",
      "iteration 3455, dc_loss: 0.02299908548593521, tv_loss: 0.033845894038677216\n",
      "iteration 3456, dc_loss: 0.022992128506302834, tv_loss: 0.03384515270590782\n",
      "iteration 3457, dc_loss: 0.022991791367530823, tv_loss: 0.03383729234337807\n",
      "iteration 3458, dc_loss: 0.022977665066719055, tv_loss: 0.03384937718510628\n",
      "iteration 3459, dc_loss: 0.02298903837800026, tv_loss: 0.033837977796792984\n",
      "iteration 3460, dc_loss: 0.022992832586169243, tv_loss: 0.033832017332315445\n",
      "iteration 3461, dc_loss: 0.02298067696392536, tv_loss: 0.033851489424705505\n",
      "iteration 3462, dc_loss: 0.022977981716394424, tv_loss: 0.03385088965296745\n",
      "iteration 3463, dc_loss: 0.022986536845564842, tv_loss: 0.03383754566311836\n",
      "iteration 3464, dc_loss: 0.022977132350206375, tv_loss: 0.033844102174043655\n",
      "iteration 3465, dc_loss: 0.022969117388129234, tv_loss: 0.03384752944111824\n",
      "iteration 3466, dc_loss: 0.022979125380516052, tv_loss: 0.033834826201200485\n",
      "iteration 3467, dc_loss: 0.022977132350206375, tv_loss: 0.03384093567728996\n",
      "iteration 3468, dc_loss: 0.022970782592892647, tv_loss: 0.03384219482541084\n",
      "iteration 3469, dc_loss: 0.022968050092458725, tv_loss: 0.03384655714035034\n",
      "iteration 3470, dc_loss: 0.022975008934736252, tv_loss: 0.03383436053991318\n",
      "iteration 3471, dc_loss: 0.02295893430709839, tv_loss: 0.033848028630018234\n",
      "iteration 3472, dc_loss: 0.022968806326389313, tv_loss: 0.033836815506219864\n",
      "iteration 3473, dc_loss: 0.022971272468566895, tv_loss: 0.033836837857961655\n",
      "iteration 3474, dc_loss: 0.022959500551223755, tv_loss: 0.03384922072291374\n",
      "iteration 3475, dc_loss: 0.022959010675549507, tv_loss: 0.03385775536298752\n",
      "iteration 3476, dc_loss: 0.022962206974625587, tv_loss: 0.03384110704064369\n",
      "iteration 3477, dc_loss: 0.022959435358643532, tv_loss: 0.0338471457362175\n",
      "iteration 3478, dc_loss: 0.022959277033805847, tv_loss: 0.03383830562233925\n",
      "iteration 3479, dc_loss: 0.022953584790229797, tv_loss: 0.03385290876030922\n",
      "iteration 3480, dc_loss: 0.022960128262639046, tv_loss: 0.03383902832865715\n",
      "iteration 3481, dc_loss: 0.022949611768126488, tv_loss: 0.03384849801659584\n",
      "iteration 3482, dc_loss: 0.022955873981118202, tv_loss: 0.03383892402052879\n",
      "iteration 3483, dc_loss: 0.022942638024687767, tv_loss: 0.03386005014181137\n",
      "iteration 3484, dc_loss: 0.022958196699619293, tv_loss: 0.033846415579319\n",
      "iteration 3485, dc_loss: 0.022954856976866722, tv_loss: 0.03384048864245415\n",
      "iteration 3486, dc_loss: 0.022956086322665215, tv_loss: 0.033838193863630295\n",
      "iteration 3487, dc_loss: 0.02293279767036438, tv_loss: 0.03386346623301506\n",
      "iteration 3488, dc_loss: 0.022956782951951027, tv_loss: 0.03383566811680794\n",
      "iteration 3489, dc_loss: 0.022941110655665398, tv_loss: 0.03384413570165634\n",
      "iteration 3490, dc_loss: 0.022943826392292976, tv_loss: 0.03383670747280121\n",
      "iteration 3491, dc_loss: 0.02293027937412262, tv_loss: 0.033850595355033875\n",
      "iteration 3492, dc_loss: 0.02294125221669674, tv_loss: 0.03383516147732735\n",
      "iteration 3493, dc_loss: 0.02293752133846283, tv_loss: 0.033837832510471344\n",
      "iteration 3494, dc_loss: 0.022922107949852943, tv_loss: 0.03384767472743988\n",
      "iteration 3495, dc_loss: 0.022929349914193153, tv_loss: 0.033844757825136185\n",
      "iteration 3496, dc_loss: 0.022933365777134895, tv_loss: 0.03383299335837364\n",
      "iteration 3497, dc_loss: 0.02293451689183712, tv_loss: 0.03383222594857216\n",
      "iteration 3498, dc_loss: 0.02291875146329403, tv_loss: 0.03384576737880707\n",
      "iteration 3499, dc_loss: 0.022932322695851326, tv_loss: 0.03382888436317444\n",
      "iteration 3500, dc_loss: 0.02291896939277649, tv_loss: 0.033849652856588364\n",
      "iteration 3501, dc_loss: 0.022921552881598473, tv_loss: 0.03383701294660568\n",
      "iteration 3502, dc_loss: 0.02292133867740631, tv_loss: 0.033839285373687744\n",
      "iteration 3503, dc_loss: 0.02291988581418991, tv_loss: 0.033832672983407974\n",
      "iteration 3504, dc_loss: 0.022913305088877678, tv_loss: 0.03384222835302353\n",
      "iteration 3505, dc_loss: 0.022912446409463882, tv_loss: 0.033837299793958664\n",
      "iteration 3506, dc_loss: 0.022914456203579903, tv_loss: 0.03383916616439819\n",
      "iteration 3507, dc_loss: 0.022911032661795616, tv_loss: 0.033842865377664566\n",
      "iteration 3508, dc_loss: 0.02291211485862732, tv_loss: 0.03383558616042137\n",
      "iteration 3509, dc_loss: 0.022905845195055008, tv_loss: 0.033839840441942215\n",
      "iteration 3510, dc_loss: 0.022915851324796677, tv_loss: 0.033827539533376694\n",
      "iteration 3511, dc_loss: 0.02289845608174801, tv_loss: 0.033842723816633224\n",
      "iteration 3512, dc_loss: 0.0228984747081995, tv_loss: 0.033846497535705566\n",
      "iteration 3513, dc_loss: 0.02290322817862034, tv_loss: 0.03383870795369148\n",
      "iteration 3514, dc_loss: 0.022904926910996437, tv_loss: 0.03383926674723625\n",
      "iteration 3515, dc_loss: 0.02289540134370327, tv_loss: 0.03383960574865341\n",
      "iteration 3516, dc_loss: 0.02289636619389057, tv_loss: 0.03383887559175491\n",
      "iteration 3517, dc_loss: 0.022900491952896118, tv_loss: 0.03383328765630722\n",
      "iteration 3518, dc_loss: 0.02288726717233658, tv_loss: 0.03385576605796814\n",
      "iteration 3519, dc_loss: 0.022890208289027214, tv_loss: 0.03384252265095711\n",
      "iteration 3520, dc_loss: 0.022890402004122734, tv_loss: 0.03384024649858475\n",
      "iteration 3521, dc_loss: 0.022896703332662582, tv_loss: 0.03383300080895424\n",
      "iteration 3522, dc_loss: 0.02288489229977131, tv_loss: 0.033841218799352646\n",
      "iteration 3523, dc_loss: 0.022887052968144417, tv_loss: 0.03383984789252281\n",
      "iteration 3524, dc_loss: 0.022882862016558647, tv_loss: 0.033838484436273575\n",
      "iteration 3525, dc_loss: 0.02288075163960457, tv_loss: 0.03383611887693405\n",
      "iteration 3526, dc_loss: 0.022883132100105286, tv_loss: 0.033836480230093\n",
      "iteration 3527, dc_loss: 0.022884642705321312, tv_loss: 0.03382943943142891\n",
      "iteration 3528, dc_loss: 0.02287229150533676, tv_loss: 0.033844590187072754\n",
      "iteration 3529, dc_loss: 0.022880464792251587, tv_loss: 0.03383272513747215\n",
      "iteration 3530, dc_loss: 0.022874776273965836, tv_loss: 0.033838964998722076\n",
      "iteration 3531, dc_loss: 0.022876789793372154, tv_loss: 0.033831920474767685\n",
      "iteration 3532, dc_loss: 0.02286786027252674, tv_loss: 0.033841609954833984\n",
      "iteration 3533, dc_loss: 0.022881098091602325, tv_loss: 0.033825602382421494\n",
      "iteration 3534, dc_loss: 0.02286292426288128, tv_loss: 0.03384228050708771\n",
      "iteration 3535, dc_loss: 0.022880464792251587, tv_loss: 0.03382500261068344\n",
      "iteration 3536, dc_loss: 0.022870155051350594, tv_loss: 0.033837929368019104\n",
      "iteration 3537, dc_loss: 0.02287040278315544, tv_loss: 0.03383985161781311\n",
      "iteration 3538, dc_loss: 0.02285933680832386, tv_loss: 0.033851247280836105\n",
      "iteration 3539, dc_loss: 0.02287948690354824, tv_loss: 0.033823516219854355\n",
      "iteration 3540, dc_loss: 0.02285449020564556, tv_loss: 0.03384506702423096\n",
      "iteration 3541, dc_loss: 0.022866997867822647, tv_loss: 0.03383195400238037\n",
      "iteration 3542, dc_loss: 0.022855745628476143, tv_loss: 0.033838339149951935\n",
      "iteration 3543, dc_loss: 0.02286073938012123, tv_loss: 0.03383621200919151\n",
      "iteration 3544, dc_loss: 0.02284928783774376, tv_loss: 0.03383644297719002\n",
      "iteration 3545, dc_loss: 0.022855712100863457, tv_loss: 0.03383113071322441\n",
      "iteration 3546, dc_loss: 0.02284999005496502, tv_loss: 0.033831074833869934\n",
      "iteration 3547, dc_loss: 0.02284218557178974, tv_loss: 0.033838529139757156\n",
      "iteration 3548, dc_loss: 0.022854739800095558, tv_loss: 0.03382555767893791\n",
      "iteration 3549, dc_loss: 0.022843312472105026, tv_loss: 0.03384184464812279\n",
      "iteration 3550, dc_loss: 0.0228451918810606, tv_loss: 0.03383302688598633\n",
      "iteration 3551, dc_loss: 0.02284107357263565, tv_loss: 0.03383997455239296\n",
      "iteration 3552, dc_loss: 0.022847935557365417, tv_loss: 0.03382755443453789\n",
      "iteration 3553, dc_loss: 0.022830385714769363, tv_loss: 0.033837102353572845\n",
      "iteration 3554, dc_loss: 0.022844333201646805, tv_loss: 0.03382865712046623\n",
      "iteration 3555, dc_loss: 0.022835223004221916, tv_loss: 0.033829834312200546\n",
      "iteration 3556, dc_loss: 0.022838452830910683, tv_loss: 0.03382522240281105\n",
      "iteration 3557, dc_loss: 0.022829975932836533, tv_loss: 0.033836446702480316\n",
      "iteration 3558, dc_loss: 0.022830545902252197, tv_loss: 0.03382840380072594\n",
      "iteration 3559, dc_loss: 0.022829430177807808, tv_loss: 0.0338311605155468\n",
      "iteration 3560, dc_loss: 0.022826654836535454, tv_loss: 0.03383321315050125\n",
      "iteration 3561, dc_loss: 0.022832069545984268, tv_loss: 0.033823274075984955\n",
      "iteration 3562, dc_loss: 0.022817425429821014, tv_loss: 0.033835191279649734\n",
      "iteration 3563, dc_loss: 0.022825701162219048, tv_loss: 0.03383282944560051\n",
      "iteration 3564, dc_loss: 0.022823937237262726, tv_loss: 0.03383469581604004\n",
      "iteration 3565, dc_loss: 0.02282334864139557, tv_loss: 0.033832889050245285\n",
      "iteration 3566, dc_loss: 0.022816086187958717, tv_loss: 0.03383888304233551\n",
      "iteration 3567, dc_loss: 0.022825466468930244, tv_loss: 0.03382953628897667\n",
      "iteration 3568, dc_loss: 0.022806694731116295, tv_loss: 0.03384154662489891\n",
      "iteration 3569, dc_loss: 0.02281402423977852, tv_loss: 0.03382869064807892\n",
      "iteration 3570, dc_loss: 0.022815968841314316, tv_loss: 0.033832188695669174\n",
      "iteration 3571, dc_loss: 0.022808749228715897, tv_loss: 0.033833812922239304\n",
      "iteration 3572, dc_loss: 0.0228151585906744, tv_loss: 0.033825185149908066\n",
      "iteration 3573, dc_loss: 0.02281356230378151, tv_loss: 0.033824048936367035\n",
      "iteration 3574, dc_loss: 0.02279483713209629, tv_loss: 0.03383868932723999\n",
      "iteration 3575, dc_loss: 0.022816557437181473, tv_loss: 0.03382566198706627\n",
      "iteration 3576, dc_loss: 0.022800687700510025, tv_loss: 0.03385007381439209\n",
      "iteration 3577, dc_loss: 0.0227997787296772, tv_loss: 0.03384662792086601\n",
      "iteration 3578, dc_loss: 0.02280975878238678, tv_loss: 0.033826109021902084\n",
      "iteration 3579, dc_loss: 0.02279745601117611, tv_loss: 0.03383094444870949\n",
      "iteration 3580, dc_loss: 0.022791940718889236, tv_loss: 0.03384600952267647\n",
      "iteration 3581, dc_loss: 0.022809498012065887, tv_loss: 0.033830273896455765\n",
      "iteration 3582, dc_loss: 0.0227825827896595, tv_loss: 0.03385500982403755\n",
      "iteration 3583, dc_loss: 0.022804243490099907, tv_loss: 0.03382263705134392\n",
      "iteration 3584, dc_loss: 0.022794464603066444, tv_loss: 0.033832550048828125\n",
      "iteration 3585, dc_loss: 0.022799182683229446, tv_loss: 0.03382689878344536\n",
      "iteration 3586, dc_loss: 0.022794121876358986, tv_loss: 0.03383481130003929\n",
      "iteration 3587, dc_loss: 0.022801680490374565, tv_loss: 0.03382933512330055\n",
      "iteration 3588, dc_loss: 0.022777115926146507, tv_loss: 0.03385208919644356\n",
      "iteration 3589, dc_loss: 0.02279680036008358, tv_loss: 0.0338260643184185\n",
      "iteration 3590, dc_loss: 0.02278568409383297, tv_loss: 0.03382890298962593\n",
      "iteration 3591, dc_loss: 0.02278793975710869, tv_loss: 0.03382444009184837\n",
      "iteration 3592, dc_loss: 0.02277643047273159, tv_loss: 0.03384137153625488\n",
      "iteration 3593, dc_loss: 0.022790588438510895, tv_loss: 0.0338318757712841\n",
      "iteration 3594, dc_loss: 0.022764991968870163, tv_loss: 0.03385813161730766\n",
      "iteration 3595, dc_loss: 0.022780301049351692, tv_loss: 0.03382744640111923\n",
      "iteration 3596, dc_loss: 0.02277308702468872, tv_loss: 0.03382724151015282\n",
      "iteration 3597, dc_loss: 0.022767554968595505, tv_loss: 0.033836450427770615\n",
      "iteration 3598, dc_loss: 0.02277502790093422, tv_loss: 0.03383828327059746\n",
      "iteration 3599, dc_loss: 0.02276911959052086, tv_loss: 0.033844783902168274\n",
      "iteration 3600, dc_loss: 0.022772151976823807, tv_loss: 0.03382594883441925\n",
      "iteration 3601, dc_loss: 0.022764449939131737, tv_loss: 0.03383207321166992\n",
      "iteration 3602, dc_loss: 0.022756898775696754, tv_loss: 0.033840034157037735\n",
      "iteration 3603, dc_loss: 0.022759350016713142, tv_loss: 0.03383180871605873\n",
      "iteration 3604, dc_loss: 0.022765425965189934, tv_loss: 0.033820990473032\n",
      "iteration 3605, dc_loss: 0.022762613371014595, tv_loss: 0.033827897161245346\n",
      "iteration 3606, dc_loss: 0.02275584451854229, tv_loss: 0.033830415457487106\n",
      "iteration 3607, dc_loss: 0.02275945618748665, tv_loss: 0.03382089361548424\n",
      "iteration 3608, dc_loss: 0.022763581946492195, tv_loss: 0.03381626680493355\n",
      "iteration 3609, dc_loss: 0.022754359990358353, tv_loss: 0.03382702171802521\n",
      "iteration 3610, dc_loss: 0.02274978719651699, tv_loss: 0.03383331000804901\n",
      "iteration 3611, dc_loss: 0.02275351621210575, tv_loss: 0.03382309898734093\n",
      "iteration 3612, dc_loss: 0.02274949476122856, tv_loss: 0.033824026584625244\n",
      "iteration 3613, dc_loss: 0.022748637944459915, tv_loss: 0.03382157161831856\n",
      "iteration 3614, dc_loss: 0.022752750664949417, tv_loss: 0.03382343426346779\n",
      "iteration 3615, dc_loss: 0.022745780646800995, tv_loss: 0.03383195027709007\n",
      "iteration 3616, dc_loss: 0.022745812311768532, tv_loss: 0.0338268019258976\n",
      "iteration 3617, dc_loss: 0.022747155278921127, tv_loss: 0.0338154174387455\n",
      "iteration 3618, dc_loss: 0.02274470403790474, tv_loss: 0.03382163122296333\n",
      "iteration 3619, dc_loss: 0.022741053253412247, tv_loss: 0.0338200144469738\n",
      "iteration 3620, dc_loss: 0.022743232548236847, tv_loss: 0.033822156488895416\n",
      "iteration 3621, dc_loss: 0.022745436057448387, tv_loss: 0.033817511051893234\n",
      "iteration 3622, dc_loss: 0.022737594321370125, tv_loss: 0.03382615000009537\n",
      "iteration 3623, dc_loss: 0.022731706500053406, tv_loss: 0.03382737562060356\n",
      "iteration 3624, dc_loss: 0.02273801527917385, tv_loss: 0.03381779044866562\n",
      "iteration 3625, dc_loss: 0.02274210937321186, tv_loss: 0.03381110727787018\n",
      "iteration 3626, dc_loss: 0.02273157797753811, tv_loss: 0.03382343053817749\n",
      "iteration 3627, dc_loss: 0.022732025012373924, tv_loss: 0.03381766378879547\n",
      "iteration 3628, dc_loss: 0.022734051570296288, tv_loss: 0.03381851688027382\n",
      "iteration 3629, dc_loss: 0.022727062925696373, tv_loss: 0.03382497653365135\n",
      "iteration 3630, dc_loss: 0.02272893860936165, tv_loss: 0.033821020275354385\n",
      "iteration 3631, dc_loss: 0.022729625925421715, tv_loss: 0.03381860628724098\n",
      "iteration 3632, dc_loss: 0.02272677794098854, tv_loss: 0.03381801396608353\n",
      "iteration 3633, dc_loss: 0.022726640105247498, tv_loss: 0.03381824120879173\n",
      "iteration 3634, dc_loss: 0.022724727168679237, tv_loss: 0.03381775692105293\n",
      "iteration 3635, dc_loss: 0.022722937166690826, tv_loss: 0.03381513059139252\n",
      "iteration 3636, dc_loss: 0.022721799090504646, tv_loss: 0.033815644681453705\n",
      "iteration 3637, dc_loss: 0.022720497101545334, tv_loss: 0.03381836786866188\n",
      "iteration 3638, dc_loss: 0.022720852866768837, tv_loss: 0.03381665050983429\n",
      "iteration 3639, dc_loss: 0.022719699889421463, tv_loss: 0.03381464257836342\n",
      "iteration 3640, dc_loss: 0.02271515317261219, tv_loss: 0.03381798043847084\n",
      "iteration 3641, dc_loss: 0.022718023508787155, tv_loss: 0.03381918743252754\n",
      "iteration 3642, dc_loss: 0.022713232785463333, tv_loss: 0.03381728753447533\n",
      "iteration 3643, dc_loss: 0.022710762917995453, tv_loss: 0.03381684049963951\n",
      "iteration 3644, dc_loss: 0.022717339918017387, tv_loss: 0.03381314501166344\n",
      "iteration 3645, dc_loss: 0.022708628326654434, tv_loss: 0.033822741359472275\n",
      "iteration 3646, dc_loss: 0.02270694263279438, tv_loss: 0.033822204917669296\n",
      "iteration 3647, dc_loss: 0.022713808342814445, tv_loss: 0.033814385533332825\n",
      "iteration 3648, dc_loss: 0.022707421332597733, tv_loss: 0.03382449969649315\n",
      "iteration 3649, dc_loss: 0.022702261805534363, tv_loss: 0.03382192924618721\n",
      "iteration 3650, dc_loss: 0.0227088313549757, tv_loss: 0.033817484974861145\n",
      "iteration 3651, dc_loss: 0.0227042343467474, tv_loss: 0.033814702183008194\n",
      "iteration 3652, dc_loss: 0.022697191685438156, tv_loss: 0.03382369875907898\n",
      "iteration 3653, dc_loss: 0.022704899311065674, tv_loss: 0.033816900104284286\n",
      "iteration 3654, dc_loss: 0.0227048322558403, tv_loss: 0.03381802886724472\n",
      "iteration 3655, dc_loss: 0.022695347666740417, tv_loss: 0.033826954662799835\n",
      "iteration 3656, dc_loss: 0.022692600265145302, tv_loss: 0.03381940722465515\n",
      "iteration 3657, dc_loss: 0.0227033868432045, tv_loss: 0.03381098806858063\n",
      "iteration 3658, dc_loss: 0.022694315761327744, tv_loss: 0.033815667033195496\n",
      "iteration 3659, dc_loss: 0.022687768563628197, tv_loss: 0.03382589668035507\n",
      "iteration 3660, dc_loss: 0.022701609879732132, tv_loss: 0.03381635248661041\n",
      "iteration 3661, dc_loss: 0.02269335836172104, tv_loss: 0.033818695694208145\n",
      "iteration 3662, dc_loss: 0.022682663053274155, tv_loss: 0.033827364444732666\n",
      "iteration 3663, dc_loss: 0.022690581157803535, tv_loss: 0.033812765032052994\n",
      "iteration 3664, dc_loss: 0.02269606478512287, tv_loss: 0.033807553350925446\n",
      "iteration 3665, dc_loss: 0.022683661431074142, tv_loss: 0.03381917253136635\n",
      "iteration 3666, dc_loss: 0.022684330120682716, tv_loss: 0.03382163867354393\n",
      "iteration 3667, dc_loss: 0.022688699886202812, tv_loss: 0.033823411911726\n",
      "iteration 3668, dc_loss: 0.0226791650056839, tv_loss: 0.033828843384981155\n",
      "iteration 3669, dc_loss: 0.022679327055811882, tv_loss: 0.03381998464465141\n",
      "iteration 3670, dc_loss: 0.022685257717967033, tv_loss: 0.033811893314123154\n",
      "iteration 3671, dc_loss: 0.022679833695292473, tv_loss: 0.03381938859820366\n",
      "iteration 3672, dc_loss: 0.022678274661302567, tv_loss: 0.03382379189133644\n",
      "iteration 3673, dc_loss: 0.022678323090076447, tv_loss: 0.03382384032011032\n",
      "iteration 3674, dc_loss: 0.022675761952996254, tv_loss: 0.03381473198533058\n",
      "iteration 3675, dc_loss: 0.022675054147839546, tv_loss: 0.03381775692105293\n",
      "iteration 3676, dc_loss: 0.022674469277262688, tv_loss: 0.033815648406744\n",
      "iteration 3677, dc_loss: 0.02267196960747242, tv_loss: 0.033816855400800705\n",
      "iteration 3678, dc_loss: 0.022670406848192215, tv_loss: 0.03382071107625961\n",
      "iteration 3679, dc_loss: 0.022672947496175766, tv_loss: 0.033814139664173126\n",
      "iteration 3680, dc_loss: 0.022667279466986656, tv_loss: 0.03381988778710365\n",
      "iteration 3681, dc_loss: 0.02266673929989338, tv_loss: 0.03381604328751564\n",
      "iteration 3682, dc_loss: 0.022668922320008278, tv_loss: 0.03381214290857315\n",
      "iteration 3683, dc_loss: 0.022664284333586693, tv_loss: 0.03381555154919624\n",
      "iteration 3684, dc_loss: 0.022664383053779602, tv_loss: 0.033810898661613464\n",
      "iteration 3685, dc_loss: 0.022664573043584824, tv_loss: 0.03381281718611717\n",
      "iteration 3686, dc_loss: 0.022662047296762466, tv_loss: 0.03381165862083435\n",
      "iteration 3687, dc_loss: 0.022661117836833, tv_loss: 0.033813782036304474\n",
      "iteration 3688, dc_loss: 0.022660603746771812, tv_loss: 0.03381429240107536\n",
      "iteration 3689, dc_loss: 0.022657200694084167, tv_loss: 0.03381045162677765\n",
      "iteration 3690, dc_loss: 0.02265494503080845, tv_loss: 0.03381884843111038\n",
      "iteration 3691, dc_loss: 0.02266019769012928, tv_loss: 0.033810555934906006\n",
      "iteration 3692, dc_loss: 0.02265608124434948, tv_loss: 0.03381584957242012\n",
      "iteration 3693, dc_loss: 0.022649435326457024, tv_loss: 0.03382003307342529\n",
      "iteration 3694, dc_loss: 0.022653626278042793, tv_loss: 0.033812399953603745\n",
      "iteration 3695, dc_loss: 0.022653555497527122, tv_loss: 0.03381006792187691\n",
      "iteration 3696, dc_loss: 0.02264869399368763, tv_loss: 0.03381529450416565\n",
      "iteration 3697, dc_loss: 0.02265227772295475, tv_loss: 0.03380752354860306\n",
      "iteration 3698, dc_loss: 0.022646544501185417, tv_loss: 0.033821769058704376\n",
      "iteration 3699, dc_loss: 0.02264346182346344, tv_loss: 0.03382468596100807\n",
      "iteration 3700, dc_loss: 0.022645821794867516, tv_loss: 0.03381883725523949\n",
      "iteration 3701, dc_loss: 0.022649480029940605, tv_loss: 0.033805906772613525\n",
      "iteration 3702, dc_loss: 0.022638767957687378, tv_loss: 0.033817946910858154\n",
      "iteration 3703, dc_loss: 0.02264047972857952, tv_loss: 0.03381894901394844\n",
      "iteration 3704, dc_loss: 0.022645898163318634, tv_loss: 0.03382290527224541\n",
      "iteration 3705, dc_loss: 0.022635342553257942, tv_loss: 0.033825330436229706\n",
      "iteration 3706, dc_loss: 0.022637929767370224, tv_loss: 0.03381795808672905\n",
      "iteration 3707, dc_loss: 0.02263963595032692, tv_loss: 0.03381621465086937\n",
      "iteration 3708, dc_loss: 0.022633511573076248, tv_loss: 0.033827103674411774\n",
      "iteration 3709, dc_loss: 0.022635390982031822, tv_loss: 0.033818814903497696\n",
      "iteration 3710, dc_loss: 0.022638509050011635, tv_loss: 0.03380999341607094\n",
      "iteration 3711, dc_loss: 0.022629663348197937, tv_loss: 0.03382083773612976\n",
      "iteration 3712, dc_loss: 0.022623449563980103, tv_loss: 0.033830828964710236\n",
      "iteration 3713, dc_loss: 0.022632092237472534, tv_loss: 0.033816494047641754\n",
      "iteration 3714, dc_loss: 0.022638235241174698, tv_loss: 0.03380381315946579\n",
      "iteration 3715, dc_loss: 0.02262318879365921, tv_loss: 0.03382551670074463\n",
      "iteration 3716, dc_loss: 0.022620443254709244, tv_loss: 0.03382376208901405\n",
      "iteration 3717, dc_loss: 0.022629166021943092, tv_loss: 0.03381354361772537\n",
      "iteration 3718, dc_loss: 0.022624971345067024, tv_loss: 0.033810265362262726\n",
      "iteration 3719, dc_loss: 0.02262108586728573, tv_loss: 0.03381989523768425\n",
      "iteration 3720, dc_loss: 0.02262735366821289, tv_loss: 0.03381506726145744\n",
      "iteration 3721, dc_loss: 0.02262365259230137, tv_loss: 0.033816900104284286\n",
      "iteration 3722, dc_loss: 0.02261471189558506, tv_loss: 0.033817365765571594\n",
      "iteration 3723, dc_loss: 0.022615987807512283, tv_loss: 0.0338156633079052\n",
      "iteration 3724, dc_loss: 0.02261795848608017, tv_loss: 0.03381180390715599\n",
      "iteration 3725, dc_loss: 0.022615019232034683, tv_loss: 0.033822569996118546\n",
      "iteration 3726, dc_loss: 0.022617733106017113, tv_loss: 0.03381021320819855\n",
      "iteration 3727, dc_loss: 0.02261444739997387, tv_loss: 0.033811718225479126\n",
      "iteration 3728, dc_loss: 0.02260820008814335, tv_loss: 0.03381581977009773\n",
      "iteration 3729, dc_loss: 0.022613493725657463, tv_loss: 0.03381400927901268\n",
      "iteration 3730, dc_loss: 0.022615015506744385, tv_loss: 0.03381846472620964\n",
      "iteration 3731, dc_loss: 0.02260332927107811, tv_loss: 0.03382141888141632\n",
      "iteration 3732, dc_loss: 0.02260422706604004, tv_loss: 0.03381795063614845\n",
      "iteration 3733, dc_loss: 0.02261538803577423, tv_loss: 0.03380318731069565\n",
      "iteration 3734, dc_loss: 0.022604988887906075, tv_loss: 0.03381365165114403\n",
      "iteration 3735, dc_loss: 0.022598639130592346, tv_loss: 0.03382788971066475\n",
      "iteration 3736, dc_loss: 0.022603515535593033, tv_loss: 0.033814914524555206\n",
      "iteration 3737, dc_loss: 0.02261110208928585, tv_loss: 0.033805325627326965\n",
      "iteration 3738, dc_loss: 0.022598128765821457, tv_loss: 0.0338146798312664\n",
      "iteration 3739, dc_loss: 0.02259352058172226, tv_loss: 0.033820703625679016\n",
      "iteration 3740, dc_loss: 0.02260548062622547, tv_loss: 0.0338115319609642\n",
      "iteration 3741, dc_loss: 0.02259927988052368, tv_loss: 0.03381253778934479\n",
      "iteration 3742, dc_loss: 0.022589685395359993, tv_loss: 0.03381956368684769\n",
      "iteration 3743, dc_loss: 0.022595103830099106, tv_loss: 0.033811554312705994\n",
      "iteration 3744, dc_loss: 0.022597884759306908, tv_loss: 0.033813126385211945\n",
      "iteration 3745, dc_loss: 0.02258908376097679, tv_loss: 0.03382379561662674\n",
      "iteration 3746, dc_loss: 0.022588282823562622, tv_loss: 0.03381679952144623\n",
      "iteration 3747, dc_loss: 0.022596752271056175, tv_loss: 0.0338054783642292\n",
      "iteration 3748, dc_loss: 0.022590065374970436, tv_loss: 0.03380962833762169\n",
      "iteration 3749, dc_loss: 0.02258184552192688, tv_loss: 0.03381911292672157\n",
      "iteration 3750, dc_loss: 0.022592835128307343, tv_loss: 0.03381514921784401\n",
      "iteration 3751, dc_loss: 0.022589746862649918, tv_loss: 0.03381248563528061\n",
      "iteration 3752, dc_loss: 0.022578099742531776, tv_loss: 0.03381922468543053\n",
      "iteration 3753, dc_loss: 0.022581715136766434, tv_loss: 0.033811312168836594\n",
      "iteration 3754, dc_loss: 0.02259211800992489, tv_loss: 0.03380335494875908\n",
      "iteration 3755, dc_loss: 0.0225763451308012, tv_loss: 0.033822573721408844\n",
      "iteration 3756, dc_loss: 0.02257443033158779, tv_loss: 0.03382354974746704\n",
      "iteration 3757, dc_loss: 0.022590167820453644, tv_loss: 0.0337991788983345\n",
      "iteration 3758, dc_loss: 0.022577546536922455, tv_loss: 0.03380646929144859\n",
      "iteration 3759, dc_loss: 0.022563325241208076, tv_loss: 0.03382721170783043\n",
      "iteration 3760, dc_loss: 0.022582387551665306, tv_loss: 0.033805880695581436\n",
      "iteration 3761, dc_loss: 0.02257886715233326, tv_loss: 0.033804137259721756\n",
      "iteration 3762, dc_loss: 0.02256390079855919, tv_loss: 0.033817969262599945\n",
      "iteration 3763, dc_loss: 0.02257705107331276, tv_loss: 0.033803101629018784\n",
      "iteration 3764, dc_loss: 0.022575676441192627, tv_loss: 0.03380396217107773\n",
      "iteration 3765, dc_loss: 0.02256082557141781, tv_loss: 0.03381620720028877\n",
      "iteration 3766, dc_loss: 0.022569777444005013, tv_loss: 0.033806413412094116\n",
      "iteration 3767, dc_loss: 0.02257305383682251, tv_loss: 0.033804185688495636\n",
      "iteration 3768, dc_loss: 0.022560294717550278, tv_loss: 0.033812470734119415\n",
      "iteration 3769, dc_loss: 0.02256147190928459, tv_loss: 0.03381209075450897\n",
      "iteration 3770, dc_loss: 0.022568069398403168, tv_loss: 0.03380381688475609\n",
      "iteration 3771, dc_loss: 0.022562555968761444, tv_loss: 0.03380627930164337\n",
      "iteration 3772, dc_loss: 0.0225578211247921, tv_loss: 0.03380788117647171\n",
      "iteration 3773, dc_loss: 0.022562570869922638, tv_loss: 0.03380483388900757\n",
      "iteration 3774, dc_loss: 0.02255989797413349, tv_loss: 0.03381100669503212\n",
      "iteration 3775, dc_loss: 0.022556807845830917, tv_loss: 0.0338200218975544\n",
      "iteration 3776, dc_loss: 0.0225551538169384, tv_loss: 0.033819109201431274\n",
      "iteration 3777, dc_loss: 0.022554652765393257, tv_loss: 0.033810805529356\n",
      "iteration 3778, dc_loss: 0.0225527286529541, tv_loss: 0.03381258249282837\n",
      "iteration 3779, dc_loss: 0.022555939853191376, tv_loss: 0.033811330795288086\n",
      "iteration 3780, dc_loss: 0.022551631554961205, tv_loss: 0.033816516399383545\n",
      "iteration 3781, dc_loss: 0.022547826170921326, tv_loss: 0.033813897520303726\n",
      "iteration 3782, dc_loss: 0.022549770772457123, tv_loss: 0.033809904009103775\n",
      "iteration 3783, dc_loss: 0.02255292795598507, tv_loss: 0.03381579741835594\n",
      "iteration 3784, dc_loss: 0.022544926032423973, tv_loss: 0.03382405266165733\n",
      "iteration 3785, dc_loss: 0.02254049852490425, tv_loss: 0.03382060304284096\n",
      "iteration 3786, dc_loss: 0.022550897672772408, tv_loss: 0.0338054858148098\n",
      "iteration 3787, dc_loss: 0.02254647947847843, tv_loss: 0.033817898482084274\n",
      "iteration 3788, dc_loss: 0.022534441202878952, tv_loss: 0.03382527455687523\n",
      "iteration 3789, dc_loss: 0.022543935105204582, tv_loss: 0.033811066299676895\n",
      "iteration 3790, dc_loss: 0.022546498104929924, tv_loss: 0.033808302134275436\n",
      "iteration 3791, dc_loss: 0.02253422513604164, tv_loss: 0.033826809376478195\n",
      "iteration 3792, dc_loss: 0.022535227239131927, tv_loss: 0.03381655737757683\n",
      "iteration 3793, dc_loss: 0.02254313789308071, tv_loss: 0.03380376473069191\n",
      "iteration 3794, dc_loss: 0.02253478765487671, tv_loss: 0.03381676226854324\n",
      "iteration 3795, dc_loss: 0.022528521716594696, tv_loss: 0.03382513299584389\n",
      "iteration 3796, dc_loss: 0.02253558114171028, tv_loss: 0.03380642458796501\n",
      "iteration 3797, dc_loss: 0.022535618394613266, tv_loss: 0.03380800038576126\n",
      "iteration 3798, dc_loss: 0.022528618574142456, tv_loss: 0.033819954842329025\n",
      "iteration 3799, dc_loss: 0.022532330825924873, tv_loss: 0.03381168097257614\n",
      "iteration 3800, dc_loss: 0.022533686831593513, tv_loss: 0.03380553796887398\n",
      "iteration 3801, dc_loss: 0.02252250164747238, tv_loss: 0.033816300332546234\n",
      "iteration 3802, dc_loss: 0.02252274937927723, tv_loss: 0.03381458669900894\n",
      "iteration 3803, dc_loss: 0.022529983893036842, tv_loss: 0.03380874544382095\n",
      "iteration 3804, dc_loss: 0.022519832476973534, tv_loss: 0.03381194546818733\n",
      "iteration 3805, dc_loss: 0.0225212424993515, tv_loss: 0.03380604460835457\n",
      "iteration 3806, dc_loss: 0.022528842091560364, tv_loss: 0.03380103036761284\n",
      "iteration 3807, dc_loss: 0.022514577955007553, tv_loss: 0.033815063536167145\n",
      "iteration 3808, dc_loss: 0.02251717634499073, tv_loss: 0.033808063715696335\n",
      "iteration 3809, dc_loss: 0.022526433691382408, tv_loss: 0.033799346536397934\n",
      "iteration 3810, dc_loss: 0.022516200318932533, tv_loss: 0.03380943834781647\n",
      "iteration 3811, dc_loss: 0.022507932037115097, tv_loss: 0.033813897520303726\n",
      "iteration 3812, dc_loss: 0.022523296996951103, tv_loss: 0.033799875527620316\n",
      "iteration 3813, dc_loss: 0.022512521594762802, tv_loss: 0.033806074410676956\n",
      "iteration 3814, dc_loss: 0.022506656125187874, tv_loss: 0.03380823880434036\n",
      "iteration 3815, dc_loss: 0.022514842450618744, tv_loss: 0.03379964083433151\n",
      "iteration 3816, dc_loss: 0.022510666400194168, tv_loss: 0.033808641135692596\n",
      "iteration 3817, dc_loss: 0.02250627987086773, tv_loss: 0.0338120274245739\n",
      "iteration 3818, dc_loss: 0.022515758872032166, tv_loss: 0.03380115330219269\n",
      "iteration 3819, dc_loss: 0.022504957392811775, tv_loss: 0.03381101414561272\n",
      "iteration 3820, dc_loss: 0.02250167541205883, tv_loss: 0.03381020575761795\n",
      "iteration 3821, dc_loss: 0.022503219544887543, tv_loss: 0.033806268125772476\n",
      "iteration 3822, dc_loss: 0.022510742768645287, tv_loss: 0.03380046412348747\n",
      "iteration 3823, dc_loss: 0.02250310406088829, tv_loss: 0.033803850412368774\n",
      "iteration 3824, dc_loss: 0.02250591665506363, tv_loss: 0.03379952535033226\n",
      "iteration 3825, dc_loss: 0.022498875856399536, tv_loss: 0.03380674868822098\n",
      "iteration 3826, dc_loss: 0.022500723600387573, tv_loss: 0.033806294202804565\n",
      "iteration 3827, dc_loss: 0.022496752440929413, tv_loss: 0.033810507506132126\n",
      "iteration 3828, dc_loss: 0.02251219004392624, tv_loss: 0.03379129618406296\n",
      "iteration 3829, dc_loss: 0.022494062781333923, tv_loss: 0.033806901425123215\n",
      "iteration 3830, dc_loss: 0.02250050939619541, tv_loss: 0.03380141779780388\n",
      "iteration 3831, dc_loss: 0.02249605767428875, tv_loss: 0.03380062058568001\n",
      "iteration 3832, dc_loss: 0.022491756826639175, tv_loss: 0.03380265086889267\n",
      "iteration 3833, dc_loss: 0.022485872730612755, tv_loss: 0.033814702183008194\n",
      "iteration 3834, dc_loss: 0.02249240316450596, tv_loss: 0.03381113335490227\n",
      "iteration 3835, dc_loss: 0.02248469553887844, tv_loss: 0.03381424769759178\n",
      "iteration 3836, dc_loss: 0.02248542197048664, tv_loss: 0.03380456939339638\n",
      "iteration 3837, dc_loss: 0.02249649167060852, tv_loss: 0.033792756497859955\n",
      "iteration 3838, dc_loss: 0.022482069209218025, tv_loss: 0.033814653754234314\n",
      "iteration 3839, dc_loss: 0.02248556725680828, tv_loss: 0.03381552919745445\n",
      "iteration 3840, dc_loss: 0.02248086966574192, tv_loss: 0.03380928561091423\n",
      "iteration 3841, dc_loss: 0.022483618929982185, tv_loss: 0.033800896257162094\n",
      "iteration 3842, dc_loss: 0.02247779257595539, tv_loss: 0.0338112935423851\n",
      "iteration 3843, dc_loss: 0.0224797073751688, tv_loss: 0.03380981460213661\n",
      "iteration 3844, dc_loss: 0.022480856627225876, tv_loss: 0.033807139843702316\n",
      "iteration 3845, dc_loss: 0.022475294768810272, tv_loss: 0.03380296379327774\n",
      "iteration 3846, dc_loss: 0.02247672528028488, tv_loss: 0.033804282546043396\n",
      "iteration 3847, dc_loss: 0.02247307077050209, tv_loss: 0.03380397707223892\n",
      "iteration 3848, dc_loss: 0.022472873330116272, tv_loss: 0.03380507603287697\n",
      "iteration 3849, dc_loss: 0.022473536431789398, tv_loss: 0.03380560129880905\n",
      "iteration 3850, dc_loss: 0.022478237748146057, tv_loss: 0.03379853814840317\n",
      "iteration 3851, dc_loss: 0.022467242553830147, tv_loss: 0.03380805626511574\n",
      "iteration 3852, dc_loss: 0.02246478945016861, tv_loss: 0.033806707710027695\n",
      "iteration 3853, dc_loss: 0.022472485899925232, tv_loss: 0.033796604722738266\n",
      "iteration 3854, dc_loss: 0.022466661408543587, tv_loss: 0.03379925340414047\n",
      "iteration 3855, dc_loss: 0.022461924701929092, tv_loss: 0.03380374610424042\n",
      "iteration 3856, dc_loss: 0.022468537092208862, tv_loss: 0.033795032650232315\n",
      "iteration 3857, dc_loss: 0.022463100031018257, tv_loss: 0.03379938006401062\n",
      "iteration 3858, dc_loss: 0.022455625236034393, tv_loss: 0.03380672261118889\n",
      "iteration 3859, dc_loss: 0.0224680844694376, tv_loss: 0.033793866634368896\n",
      "iteration 3860, dc_loss: 0.022460278123617172, tv_loss: 0.03380795568227768\n",
      "iteration 3861, dc_loss: 0.02245345339179039, tv_loss: 0.03381715342402458\n",
      "iteration 3862, dc_loss: 0.0224592387676239, tv_loss: 0.03380689397454262\n",
      "iteration 3863, dc_loss: 0.02245897799730301, tv_loss: 0.033798132091760635\n",
      "iteration 3864, dc_loss: 0.022453943267464638, tv_loss: 0.033801209181547165\n",
      "iteration 3865, dc_loss: 0.0224542748183012, tv_loss: 0.03380054235458374\n",
      "iteration 3866, dc_loss: 0.02245020680129528, tv_loss: 0.03380220755934715\n",
      "iteration 3867, dc_loss: 0.022450370714068413, tv_loss: 0.03380092605948448\n",
      "iteration 3868, dc_loss: 0.022454552352428436, tv_loss: 0.03380102291703224\n",
      "iteration 3869, dc_loss: 0.02244546450674534, tv_loss: 0.033807530999183655\n",
      "iteration 3870, dc_loss: 0.02244982123374939, tv_loss: 0.03380560129880905\n",
      "iteration 3871, dc_loss: 0.022451050579547882, tv_loss: 0.033803027123212814\n",
      "iteration 3872, dc_loss: 0.022444678470492363, tv_loss: 0.033803462982177734\n",
      "iteration 3873, dc_loss: 0.022444942966103554, tv_loss: 0.033800482749938965\n",
      "iteration 3874, dc_loss: 0.022446805611252785, tv_loss: 0.033798087388277054\n",
      "iteration 3875, dc_loss: 0.022440144792199135, tv_loss: 0.033803701400756836\n",
      "iteration 3876, dc_loss: 0.022438645362854004, tv_loss: 0.03380195423960686\n",
      "iteration 3877, dc_loss: 0.022440230473876, tv_loss: 0.03379720449447632\n",
      "iteration 3878, dc_loss: 0.02243698760867119, tv_loss: 0.03379976376891136\n",
      "iteration 3879, dc_loss: 0.022441381588578224, tv_loss: 0.03379364311695099\n",
      "iteration 3880, dc_loss: 0.022439507767558098, tv_loss: 0.033794865012168884\n",
      "iteration 3881, dc_loss: 0.022432789206504822, tv_loss: 0.0337972454726696\n",
      "iteration 3882, dc_loss: 0.02243669703602791, tv_loss: 0.03379271551966667\n",
      "iteration 3883, dc_loss: 0.022435301914811134, tv_loss: 0.03380260616540909\n",
      "iteration 3884, dc_loss: 0.022429248318076134, tv_loss: 0.033818140625953674\n",
      "iteration 3885, dc_loss: 0.022434521466493607, tv_loss: 0.03380589932203293\n",
      "iteration 3886, dc_loss: 0.0224306583404541, tv_loss: 0.03379913419485092\n",
      "iteration 3887, dc_loss: 0.022426022216677666, tv_loss: 0.03380051627755165\n",
      "iteration 3888, dc_loss: 0.022430358454585075, tv_loss: 0.033802226185798645\n",
      "iteration 3889, dc_loss: 0.022431574761867523, tv_loss: 0.033814750611782074\n",
      "iteration 3890, dc_loss: 0.02241699956357479, tv_loss: 0.03381679952144623\n",
      "iteration 3891, dc_loss: 0.022431474179029465, tv_loss: 0.03379487991333008\n",
      "iteration 3892, dc_loss: 0.02242983505129814, tv_loss: 0.03380632400512695\n",
      "iteration 3893, dc_loss: 0.022421495988965034, tv_loss: 0.03381837159395218\n",
      "iteration 3894, dc_loss: 0.022418303415179253, tv_loss: 0.03381028026342392\n",
      "iteration 3895, dc_loss: 0.022429194301366806, tv_loss: 0.03379218652844429\n",
      "iteration 3896, dc_loss: 0.02241833135485649, tv_loss: 0.033815108239650726\n",
      "iteration 3897, dc_loss: 0.022424310445785522, tv_loss: 0.03380458429455757\n",
      "iteration 3898, dc_loss: 0.022415246814489365, tv_loss: 0.03381171077489853\n",
      "iteration 3899, dc_loss: 0.022431207820773125, tv_loss: 0.03379292041063309\n",
      "iteration 3900, dc_loss: 0.022418122738599777, tv_loss: 0.03380730375647545\n",
      "iteration 3901, dc_loss: 0.022415146231651306, tv_loss: 0.03381146490573883\n",
      "iteration 3902, dc_loss: 0.022409379482269287, tv_loss: 0.03380636125802994\n",
      "iteration 3903, dc_loss: 0.02241704612970352, tv_loss: 0.033799801021814346\n",
      "iteration 3904, dc_loss: 0.022405562922358513, tv_loss: 0.03381427749991417\n",
      "iteration 3905, dc_loss: 0.022409971803426743, tv_loss: 0.03380174934864044\n",
      "iteration 3906, dc_loss: 0.022414041683077812, tv_loss: 0.033795882016420364\n",
      "iteration 3907, dc_loss: 0.022404491901397705, tv_loss: 0.03380962461233139\n",
      "iteration 3908, dc_loss: 0.02240701951086521, tv_loss: 0.033805668354034424\n",
      "iteration 3909, dc_loss: 0.02240452915430069, tv_loss: 0.03379980847239494\n",
      "iteration 3910, dc_loss: 0.022410348057746887, tv_loss: 0.033794060349464417\n",
      "iteration 3911, dc_loss: 0.022395608946681023, tv_loss: 0.033807769417762756\n",
      "iteration 3912, dc_loss: 0.022403081879019737, tv_loss: 0.033806342631578445\n",
      "iteration 3913, dc_loss: 0.022406071424484253, tv_loss: 0.033802032470703125\n",
      "iteration 3914, dc_loss: 0.02239338681101799, tv_loss: 0.033801544457674026\n",
      "iteration 3915, dc_loss: 0.022390667349100113, tv_loss: 0.03380913287401199\n",
      "iteration 3916, dc_loss: 0.022403933107852936, tv_loss: 0.03379461169242859\n",
      "iteration 3917, dc_loss: 0.02239796333014965, tv_loss: 0.03379608690738678\n",
      "iteration 3918, dc_loss: 0.022388780489563942, tv_loss: 0.033806461840867996\n",
      "iteration 3919, dc_loss: 0.02239828184247017, tv_loss: 0.03379233926534653\n",
      "iteration 3920, dc_loss: 0.022392617538571358, tv_loss: 0.03379441052675247\n",
      "iteration 3921, dc_loss: 0.022388070821762085, tv_loss: 0.03380168229341507\n",
      "iteration 3922, dc_loss: 0.022391466423869133, tv_loss: 0.0337957926094532\n",
      "iteration 3923, dc_loss: 0.022389132529497147, tv_loss: 0.03379766270518303\n",
      "iteration 3924, dc_loss: 0.02238708920776844, tv_loss: 0.03379925712943077\n",
      "iteration 3925, dc_loss: 0.022391771897673607, tv_loss: 0.033791884779930115\n",
      "iteration 3926, dc_loss: 0.022380324080586433, tv_loss: 0.033805616199970245\n",
      "iteration 3927, dc_loss: 0.022379720583558083, tv_loss: 0.03380000591278076\n",
      "iteration 3928, dc_loss: 0.022388866171240807, tv_loss: 0.03378788009285927\n",
      "iteration 3929, dc_loss: 0.02237887866795063, tv_loss: 0.03379792720079422\n",
      "iteration 3930, dc_loss: 0.022380605340003967, tv_loss: 0.033798232674598694\n",
      "iteration 3931, dc_loss: 0.0223837960511446, tv_loss: 0.03378751501441002\n",
      "iteration 3932, dc_loss: 0.02237795479595661, tv_loss: 0.03379424661397934\n",
      "iteration 3933, dc_loss: 0.02237161435186863, tv_loss: 0.03380204364657402\n",
      "iteration 3934, dc_loss: 0.022381799295544624, tv_loss: 0.03379165381193161\n",
      "iteration 3935, dc_loss: 0.022373899817466736, tv_loss: 0.03379135578870773\n",
      "iteration 3936, dc_loss: 0.022372271865606308, tv_loss: 0.03379650413990021\n",
      "iteration 3937, dc_loss: 0.022380342707037926, tv_loss: 0.03379037231206894\n",
      "iteration 3938, dc_loss: 0.022369977086782455, tv_loss: 0.03379840403795242\n",
      "iteration 3939, dc_loss: 0.022366685792803764, tv_loss: 0.033802829682826996\n",
      "iteration 3940, dc_loss: 0.022370945662260056, tv_loss: 0.033796049654483795\n",
      "iteration 3941, dc_loss: 0.02236819826066494, tv_loss: 0.0337991937994957\n",
      "iteration 3942, dc_loss: 0.022367598488926888, tv_loss: 0.0337972417473793\n",
      "iteration 3943, dc_loss: 0.022368622943758965, tv_loss: 0.03379083052277565\n",
      "iteration 3944, dc_loss: 0.022363530471920967, tv_loss: 0.033794477581977844\n",
      "iteration 3945, dc_loss: 0.02236100658774376, tv_loss: 0.03379922732710838\n",
      "iteration 3946, dc_loss: 0.0223693884909153, tv_loss: 0.03378672897815704\n",
      "iteration 3947, dc_loss: 0.022363198921084404, tv_loss: 0.03379523754119873\n",
      "iteration 3948, dc_loss: 0.02235412783920765, tv_loss: 0.0338064469397068\n",
      "iteration 3949, dc_loss: 0.02236287109553814, tv_loss: 0.03379230573773384\n",
      "iteration 3950, dc_loss: 0.02236095443367958, tv_loss: 0.033796217292547226\n",
      "iteration 3951, dc_loss: 0.022349784150719643, tv_loss: 0.03380025923252106\n",
      "iteration 3952, dc_loss: 0.022361421957612038, tv_loss: 0.03378516435623169\n",
      "iteration 3953, dc_loss: 0.022365044802427292, tv_loss: 0.03378463536500931\n",
      "iteration 3954, dc_loss: 0.022344840690493584, tv_loss: 0.03380008786916733\n",
      "iteration 3955, dc_loss: 0.022354643791913986, tv_loss: 0.03378691524267197\n",
      "iteration 3956, dc_loss: 0.022358747199177742, tv_loss: 0.03378552943468094\n",
      "iteration 3957, dc_loss: 0.02234065718948841, tv_loss: 0.0338028147816658\n",
      "iteration 3958, dc_loss: 0.022354496642947197, tv_loss: 0.033783040940761566\n",
      "iteration 3959, dc_loss: 0.022352926433086395, tv_loss: 0.03378685563802719\n",
      "iteration 3960, dc_loss: 0.02233933098614216, tv_loss: 0.03380133584141731\n",
      "iteration 3961, dc_loss: 0.02235097996890545, tv_loss: 0.03378474712371826\n",
      "iteration 3962, dc_loss: 0.022345978766679764, tv_loss: 0.03378981351852417\n",
      "iteration 3963, dc_loss: 0.022343680262565613, tv_loss: 0.03379521518945694\n",
      "iteration 3964, dc_loss: 0.02234824188053608, tv_loss: 0.033793844282627106\n",
      "iteration 3965, dc_loss: 0.022337883710861206, tv_loss: 0.03380826488137245\n",
      "iteration 3966, dc_loss: 0.022341566160321236, tv_loss: 0.033798132091760635\n",
      "iteration 3967, dc_loss: 0.02234014868736267, tv_loss: 0.033795975148677826\n",
      "iteration 3968, dc_loss: 0.022343231365084648, tv_loss: 0.033788617700338364\n",
      "iteration 3969, dc_loss: 0.022335534915328026, tv_loss: 0.03379277512431145\n",
      "iteration 3970, dc_loss: 0.022334475070238113, tv_loss: 0.0337996631860733\n",
      "iteration 3971, dc_loss: 0.022335823625326157, tv_loss: 0.033799923956394196\n",
      "iteration 3972, dc_loss: 0.022351974621415138, tv_loss: 0.03378824517130852\n",
      "iteration 3973, dc_loss: 0.022335244342684746, tv_loss: 0.03380369767546654\n",
      "iteration 3974, dc_loss: 0.022348225116729736, tv_loss: 0.03378867730498314\n",
      "iteration 3975, dc_loss: 0.022354915738105774, tv_loss: 0.03379783779382706\n",
      "iteration 3976, dc_loss: 0.022362036630511284, tv_loss: 0.03378446400165558\n",
      "iteration 3977, dc_loss: 0.022328924387693405, tv_loss: 0.03381587564945221\n",
      "iteration 3978, dc_loss: 0.022348955273628235, tv_loss: 0.033783286809921265\n",
      "iteration 3979, dc_loss: 0.022327950224280357, tv_loss: 0.033795084804296494\n",
      "iteration 3980, dc_loss: 0.02232166938483715, tv_loss: 0.033792827278375626\n",
      "iteration 3981, dc_loss: 0.02233748883008957, tv_loss: 0.03378380089998245\n",
      "iteration 3982, dc_loss: 0.02232719026505947, tv_loss: 0.03380018472671509\n",
      "iteration 3983, dc_loss: 0.022330129519104958, tv_loss: 0.033792052417993546\n",
      "iteration 3984, dc_loss: 0.02232440747320652, tv_loss: 0.03379233554005623\n",
      "iteration 3985, dc_loss: 0.022325748577713966, tv_loss: 0.03379350155591965\n",
      "iteration 3986, dc_loss: 0.02231295220553875, tv_loss: 0.033799201250076294\n",
      "iteration 3987, dc_loss: 0.022325290367007256, tv_loss: 0.03378704562783241\n",
      "iteration 3988, dc_loss: 0.02232755348086357, tv_loss: 0.03378884866833687\n",
      "iteration 3989, dc_loss: 0.022311940789222717, tv_loss: 0.03379631042480469\n",
      "iteration 3990, dc_loss: 0.022317107766866684, tv_loss: 0.03379150852560997\n",
      "iteration 3991, dc_loss: 0.02231782302260399, tv_loss: 0.03379184007644653\n",
      "iteration 3992, dc_loss: 0.02231161668896675, tv_loss: 0.03379614278674126\n",
      "iteration 3993, dc_loss: 0.02231461927294731, tv_loss: 0.033789440989494324\n",
      "iteration 3994, dc_loss: 0.02231491170823574, tv_loss: 0.03378749266266823\n",
      "iteration 3995, dc_loss: 0.022307822480797768, tv_loss: 0.03378977254033089\n",
      "iteration 3996, dc_loss: 0.02231244370341301, tv_loss: 0.03378221392631531\n",
      "iteration 3997, dc_loss: 0.02230473980307579, tv_loss: 0.033793047070503235\n",
      "iteration 3998, dc_loss: 0.022309012711048126, tv_loss: 0.03378859534859657\n",
      "iteration 3999, dc_loss: 0.022307774052023888, tv_loss: 0.03378436341881752\n",
      "iteration 4000, dc_loss: 0.022299151867628098, tv_loss: 0.03379298746585846\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params = dict(fi: dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 3.005403518676758, tv_loss: 0.0005004759877920151\n",
      "iteration 2, dc_loss: 2.8783397674560547, tv_loss: 0.0033010724000632763\n",
      "iteration 3, dc_loss: 2.8058218955993652, tv_loss: 0.005649450235068798\n",
      "iteration 4, dc_loss: 2.7543725967407227, tv_loss: 0.007261334452778101\n",
      "iteration 5, dc_loss: 2.715136766433716, tv_loss: 0.008402793668210506\n",
      "iteration 6, dc_loss: 2.6857385635375977, tv_loss: 0.009193306788802147\n",
      "iteration 7, dc_loss: 2.664520263671875, tv_loss: 0.009729486890137196\n",
      "iteration 8, dc_loss: 2.647268533706665, tv_loss: 0.010113631375133991\n",
      "iteration 9, dc_loss: 2.6308071613311768, tv_loss: 0.010357657447457314\n",
      "iteration 10, dc_loss: 2.614577054977417, tv_loss: 0.010506504215300083\n",
      "iteration 11, dc_loss: 2.5979108810424805, tv_loss: 0.010666261427104473\n",
      "iteration 12, dc_loss: 2.582097291946411, tv_loss: 0.010824855417013168\n",
      "iteration 13, dc_loss: 2.567143678665161, tv_loss: 0.010946669615805149\n",
      "iteration 14, dc_loss: 2.552508592605591, tv_loss: 0.011023906990885735\n",
      "iteration 15, dc_loss: 2.5382063388824463, tv_loss: 0.011045851744711399\n",
      "iteration 16, dc_loss: 2.524513006210327, tv_loss: 0.010988466441631317\n",
      "iteration 17, dc_loss: 2.511547088623047, tv_loss: 0.010830056853592396\n",
      "iteration 18, dc_loss: 2.4989452362060547, tv_loss: 0.010592401959002018\n",
      "iteration 19, dc_loss: 2.4861350059509277, tv_loss: 0.010356188751757145\n",
      "iteration 20, dc_loss: 2.4726688861846924, tv_loss: 0.010141187347471714\n",
      "iteration 21, dc_loss: 2.4590635299682617, tv_loss: 0.009822860360145569\n",
      "iteration 22, dc_loss: 2.4456140995025635, tv_loss: 0.009373526088893414\n",
      "iteration 23, dc_loss: 2.4321444034576416, tv_loss: 0.008904878050088882\n",
      "iteration 24, dc_loss: 2.418938398361206, tv_loss: 0.008352904580533504\n",
      "iteration 25, dc_loss: 2.4058876037597656, tv_loss: 0.007716033142060041\n",
      "iteration 26, dc_loss: 2.392751932144165, tv_loss: 0.007261931896209717\n",
      "iteration 27, dc_loss: 2.379934549331665, tv_loss: 0.007037212606519461\n",
      "iteration 28, dc_loss: 2.367453098297119, tv_loss: 0.0070050559006631374\n",
      "iteration 29, dc_loss: 2.355013132095337, tv_loss: 0.007128631696105003\n",
      "iteration 30, dc_loss: 2.342592239379883, tv_loss: 0.007321435492485762\n",
      "iteration 31, dc_loss: 2.3300955295562744, tv_loss: 0.00751302158460021\n",
      "iteration 32, dc_loss: 2.3178024291992188, tv_loss: 0.007651710417121649\n",
      "iteration 33, dc_loss: 2.305851936340332, tv_loss: 0.00773429311811924\n",
      "iteration 34, dc_loss: 2.293928623199463, tv_loss: 0.0077711972407996655\n",
      "iteration 35, dc_loss: 2.2820894718170166, tv_loss: 0.0077624088153243065\n",
      "iteration 36, dc_loss: 2.2705109119415283, tv_loss: 0.007742073852568865\n",
      "iteration 37, dc_loss: 2.259014844894409, tv_loss: 0.007750635500997305\n",
      "iteration 38, dc_loss: 2.2476377487182617, tv_loss: 0.007723807357251644\n",
      "iteration 39, dc_loss: 2.236121416091919, tv_loss: 0.00782078132033348\n",
      "iteration 40, dc_loss: 2.2245209217071533, tv_loss: 0.007823939435184002\n",
      "iteration 41, dc_loss: 2.2128746509552, tv_loss: 0.007967411540448666\n",
      "iteration 42, dc_loss: 2.2016680240631104, tv_loss: 0.00806622114032507\n",
      "iteration 43, dc_loss: 2.190800189971924, tv_loss: 0.008129526861011982\n",
      "iteration 44, dc_loss: 2.1798417568206787, tv_loss: 0.008316927589476109\n",
      "iteration 45, dc_loss: 2.1689305305480957, tv_loss: 0.008353465236723423\n",
      "iteration 46, dc_loss: 2.157749891281128, tv_loss: 0.00853655394166708\n",
      "iteration 47, dc_loss: 2.1467056274414062, tv_loss: 0.00859930831938982\n",
      "iteration 48, dc_loss: 2.135918140411377, tv_loss: 0.00870150700211525\n",
      "iteration 49, dc_loss: 2.1253373622894287, tv_loss: 0.00882942508906126\n",
      "iteration 50, dc_loss: 2.1149332523345947, tv_loss: 0.008877679705619812\n",
      "iteration 51, dc_loss: 2.104421615600586, tv_loss: 0.00905125867575407\n",
      "iteration 52, dc_loss: 2.0939207077026367, tv_loss: 0.00909622386097908\n",
      "iteration 53, dc_loss: 2.0832865238189697, tv_loss: 0.009254984557628632\n",
      "iteration 54, dc_loss: 2.072852849960327, tv_loss: 0.009327978827059269\n",
      "iteration 55, dc_loss: 2.0625970363616943, tv_loss: 0.00943055097013712\n",
      "iteration 56, dc_loss: 2.0524656772613525, tv_loss: 0.009565934538841248\n",
      "iteration 57, dc_loss: 2.042552947998047, tv_loss: 0.009625429287552834\n",
      "iteration 58, dc_loss: 2.0326385498046875, tv_loss: 0.009817948564887047\n",
      "iteration 59, dc_loss: 2.022890329360962, tv_loss: 0.00982617773115635\n",
      "iteration 60, dc_loss: 2.0129663944244385, tv_loss: 0.010039460845291615\n",
      "iteration 61, dc_loss: 2.002916097640991, tv_loss: 0.01004334632307291\n",
      "iteration 62, dc_loss: 1.9927213191986084, tv_loss: 0.010204512625932693\n",
      "iteration 63, dc_loss: 1.9828613996505737, tv_loss: 0.010287965647876263\n",
      "iteration 64, dc_loss: 1.9733631610870361, tv_loss: 0.010366247966885567\n",
      "iteration 65, dc_loss: 1.963962435722351, tv_loss: 0.010544816963374615\n",
      "iteration 66, dc_loss: 1.9546414613723755, tv_loss: 0.010569468140602112\n",
      "iteration 67, dc_loss: 1.945040225982666, tv_loss: 0.010768059641122818\n",
      "iteration 68, dc_loss: 1.9354273080825806, tv_loss: 0.010785525664687157\n",
      "iteration 69, dc_loss: 1.9258224964141846, tv_loss: 0.010919236578047276\n",
      "iteration 70, dc_loss: 1.9165265560150146, tv_loss: 0.01101002935320139\n",
      "iteration 71, dc_loss: 1.9074729681015015, tv_loss: 0.01108019519597292\n",
      "iteration 72, dc_loss: 1.8984297513961792, tv_loss: 0.011259776540100574\n",
      "iteration 73, dc_loss: 1.8894779682159424, tv_loss: 0.011291788890957832\n",
      "iteration 74, dc_loss: 1.8803437948226929, tv_loss: 0.011474397964775562\n",
      "iteration 75, dc_loss: 1.8712505102157593, tv_loss: 0.011483488604426384\n",
      "iteration 76, dc_loss: 1.8620706796646118, tv_loss: 0.011628254316747189\n",
      "iteration 77, dc_loss: 1.853108525276184, tv_loss: 0.011677813716232777\n",
      "iteration 78, dc_loss: 1.8442695140838623, tv_loss: 0.011776057071983814\n",
      "iteration 79, dc_loss: 1.8355531692504883, tv_loss: 0.011901166290044785\n",
      "iteration 80, dc_loss: 1.8270262479782104, tv_loss: 0.011984449811279774\n",
      "iteration 81, dc_loss: 1.8185871839523315, tv_loss: 0.01219018455594778\n",
      "iteration 82, dc_loss: 1.8104021549224854, tv_loss: 0.012179543264210224\n",
      "iteration 83, dc_loss: 1.8021148443222046, tv_loss: 0.01236309576779604\n",
      "iteration 84, dc_loss: 1.7935969829559326, tv_loss: 0.012319532223045826\n",
      "iteration 85, dc_loss: 1.784569501876831, tv_loss: 0.012548830360174179\n",
      "iteration 86, dc_loss: 1.7760268449783325, tv_loss: 0.012509261257946491\n",
      "iteration 87, dc_loss: 1.7678667306900024, tv_loss: 0.012583468109369278\n",
      "iteration 88, dc_loss: 1.75963294506073, tv_loss: 0.012736240401864052\n",
      "iteration 89, dc_loss: 1.7511365413665771, tv_loss: 0.012834398075938225\n",
      "iteration 90, dc_loss: 1.742692232131958, tv_loss: 0.01298576407134533\n",
      "iteration 91, dc_loss: 1.734651803970337, tv_loss: 0.012937679886817932\n",
      "iteration 92, dc_loss: 1.7263877391815186, tv_loss: 0.01300707645714283\n",
      "iteration 93, dc_loss: 1.7181439399719238, tv_loss: 0.013128429651260376\n",
      "iteration 94, dc_loss: 1.7103757858276367, tv_loss: 0.013246490620076656\n",
      "iteration 95, dc_loss: 1.7025095224380493, tv_loss: 0.013412289321422577\n",
      "iteration 96, dc_loss: 1.694434404373169, tv_loss: 0.013352216221392155\n",
      "iteration 97, dc_loss: 1.6863347291946411, tv_loss: 0.013465364463627338\n",
      "iteration 98, dc_loss: 1.6785880327224731, tv_loss: 0.013541872613132\n",
      "iteration 99, dc_loss: 1.6708968877792358, tv_loss: 0.013691895641386509\n",
      "iteration 100, dc_loss: 1.6630510091781616, tv_loss: 0.013808371499180794\n",
      "iteration 101, dc_loss: 1.6553620100021362, tv_loss: 0.013756364583969116\n",
      "iteration 102, dc_loss: 1.6475884914398193, tv_loss: 0.013928675092756748\n",
      "iteration 103, dc_loss: 1.6400529146194458, tv_loss: 0.013900280930101871\n",
      "iteration 104, dc_loss: 1.6324186325073242, tv_loss: 0.014055321924388409\n",
      "iteration 105, dc_loss: 1.625010371208191, tv_loss: 0.014080815948545933\n",
      "iteration 106, dc_loss: 1.6176027059555054, tv_loss: 0.01423618383705616\n",
      "iteration 107, dc_loss: 1.6103630065917969, tv_loss: 0.014273865148425102\n",
      "iteration 108, dc_loss: 1.6029890775680542, tv_loss: 0.014360334724187851\n",
      "iteration 109, dc_loss: 1.5954391956329346, tv_loss: 0.014419502578675747\n",
      "iteration 110, dc_loss: 1.5878472328186035, tv_loss: 0.014465061016380787\n",
      "iteration 111, dc_loss: 1.5804126262664795, tv_loss: 0.014545287005603313\n",
      "iteration 112, dc_loss: 1.5732548236846924, tv_loss: 0.014566315338015556\n",
      "iteration 113, dc_loss: 1.566192865371704, tv_loss: 0.014741674065589905\n",
      "iteration 114, dc_loss: 1.5595623254776, tv_loss: 0.014776666648685932\n",
      "iteration 115, dc_loss: 1.5531286001205444, tv_loss: 0.014948510564863682\n",
      "iteration 116, dc_loss: 1.5464223623275757, tv_loss: 0.014823324047029018\n",
      "iteration 117, dc_loss: 1.5388710498809814, tv_loss: 0.015112305991351604\n",
      "iteration 118, dc_loss: 1.5311915874481201, tv_loss: 0.014898843131959438\n",
      "iteration 119, dc_loss: 1.5241702795028687, tv_loss: 0.015012809075415134\n",
      "iteration 120, dc_loss: 1.517244577407837, tv_loss: 0.015134935267269611\n",
      "iteration 121, dc_loss: 1.5101407766342163, tv_loss: 0.015055486932396889\n",
      "iteration 122, dc_loss: 1.503316044807434, tv_loss: 0.015276466496288776\n",
      "iteration 123, dc_loss: 1.4965318441390991, tv_loss: 0.015387831255793571\n",
      "iteration 124, dc_loss: 1.4894176721572876, tv_loss: 0.015529824420809746\n",
      "iteration 125, dc_loss: 1.4827123880386353, tv_loss: 0.015440675429999828\n",
      "iteration 126, dc_loss: 1.4758938550949097, tv_loss: 0.01541148405522108\n",
      "iteration 127, dc_loss: 1.4689854383468628, tv_loss: 0.015570922754704952\n",
      "iteration 128, dc_loss: 1.462563395500183, tv_loss: 0.015731163322925568\n",
      "iteration 129, dc_loss: 1.455909013748169, tv_loss: 0.01576746068894863\n",
      "iteration 130, dc_loss: 1.449059009552002, tv_loss: 0.01587323285639286\n",
      "iteration 131, dc_loss: 1.4426453113555908, tv_loss: 0.015769928693771362\n",
      "iteration 132, dc_loss: 1.4359660148620605, tv_loss: 0.01595558412373066\n",
      "iteration 133, dc_loss: 1.429456353187561, tv_loss: 0.015959596261382103\n",
      "iteration 134, dc_loss: 1.4230118989944458, tv_loss: 0.016082387417554855\n",
      "iteration 135, dc_loss: 1.4165593385696411, tv_loss: 0.01611614041030407\n",
      "iteration 136, dc_loss: 1.4101848602294922, tv_loss: 0.016160540282726288\n",
      "iteration 137, dc_loss: 1.4037748575210571, tv_loss: 0.016232717782258987\n",
      "iteration 138, dc_loss: 1.3973842859268188, tv_loss: 0.01626322790980339\n",
      "iteration 139, dc_loss: 1.3910738229751587, tv_loss: 0.016363101080060005\n",
      "iteration 140, dc_loss: 1.3848809003829956, tv_loss: 0.016405019909143448\n",
      "iteration 141, dc_loss: 1.3785409927368164, tv_loss: 0.016453489661216736\n",
      "iteration 142, dc_loss: 1.372255802154541, tv_loss: 0.016525965183973312\n",
      "iteration 143, dc_loss: 1.3661624193191528, tv_loss: 0.016532287001609802\n",
      "iteration 144, dc_loss: 1.3599177598953247, tv_loss: 0.016642291098833084\n",
      "iteration 145, dc_loss: 1.3538119792938232, tv_loss: 0.016652898862957954\n",
      "iteration 146, dc_loss: 1.3478095531463623, tv_loss: 0.016754774376749992\n",
      "iteration 147, dc_loss: 1.3418272733688354, tv_loss: 0.016808301210403442\n",
      "iteration 148, dc_loss: 1.3357828855514526, tv_loss: 0.01689063012599945\n",
      "iteration 149, dc_loss: 1.329879641532898, tv_loss: 0.016859091818332672\n",
      "iteration 150, dc_loss: 1.3237829208374023, tv_loss: 0.017038004472851753\n",
      "iteration 151, dc_loss: 1.3179805278778076, tv_loss: 0.016911102458834648\n",
      "iteration 152, dc_loss: 1.3119581937789917, tv_loss: 0.01715726964175701\n",
      "iteration 153, dc_loss: 1.3064186573028564, tv_loss: 0.016964141279459\n",
      "iteration 154, dc_loss: 1.3004711866378784, tv_loss: 0.01731112413108349\n",
      "iteration 155, dc_loss: 1.2951616048812866, tv_loss: 0.016978729516267776\n",
      "iteration 156, dc_loss: 1.2891371250152588, tv_loss: 0.017455441877245903\n",
      "iteration 157, dc_loss: 1.2833364009857178, tv_loss: 0.017126014456152916\n",
      "iteration 158, dc_loss: 1.2771469354629517, tv_loss: 0.017361750826239586\n",
      "iteration 159, dc_loss: 1.2713886499404907, tv_loss: 0.017550088465213776\n",
      "iteration 160, dc_loss: 1.2662516832351685, tv_loss: 0.017397036775946617\n",
      "iteration 161, dc_loss: 1.2607218027114868, tv_loss: 0.01775270327925682\n",
      "iteration 162, dc_loss: 1.2554887533187866, tv_loss: 0.017436912283301353\n",
      "iteration 163, dc_loss: 1.2492820024490356, tv_loss: 0.01778622902929783\n",
      "iteration 164, dc_loss: 1.2434130907058716, tv_loss: 0.017624584957957268\n",
      "iteration 165, dc_loss: 1.2378236055374146, tv_loss: 0.017637699842453003\n",
      "iteration 166, dc_loss: 1.2323238849639893, tv_loss: 0.01785130426287651\n",
      "iteration 167, dc_loss: 1.227060079574585, tv_loss: 0.017646992579102516\n",
      "iteration 168, dc_loss: 1.221268653869629, tv_loss: 0.017935248091816902\n",
      "iteration 169, dc_loss: 1.2159792184829712, tv_loss: 0.01790730282664299\n",
      "iteration 170, dc_loss: 1.2107163667678833, tv_loss: 0.01797000877559185\n",
      "iteration 171, dc_loss: 1.2052546739578247, tv_loss: 0.018132109194993973\n",
      "iteration 172, dc_loss: 1.1999653577804565, tv_loss: 0.017973758280277252\n",
      "iteration 173, dc_loss: 1.1943162679672241, tv_loss: 0.018243536353111267\n",
      "iteration 174, dc_loss: 1.1892261505126953, tv_loss: 0.018074266612529755\n",
      "iteration 175, dc_loss: 1.1838465929031372, tv_loss: 0.018226319923996925\n",
      "iteration 176, dc_loss: 1.1785087585449219, tv_loss: 0.01825176551938057\n",
      "iteration 177, dc_loss: 1.1732226610183716, tv_loss: 0.018262552097439766\n",
      "iteration 178, dc_loss: 1.167986512184143, tv_loss: 0.01837797649204731\n",
      "iteration 179, dc_loss: 1.1628960371017456, tv_loss: 0.018409013748168945\n",
      "iteration 180, dc_loss: 1.1577332019805908, tv_loss: 0.01847030408680439\n",
      "iteration 181, dc_loss: 1.152590274810791, tv_loss: 0.018503345549106598\n",
      "iteration 182, dc_loss: 1.1474727392196655, tv_loss: 0.01852433942258358\n",
      "iteration 183, dc_loss: 1.1423008441925049, tv_loss: 0.01860795170068741\n",
      "iteration 184, dc_loss: 1.1372288465499878, tv_loss: 0.018648294731974602\n",
      "iteration 185, dc_loss: 1.1322089433670044, tv_loss: 0.01872371882200241\n",
      "iteration 186, dc_loss: 1.1272966861724854, tv_loss: 0.018708860501646996\n",
      "iteration 187, dc_loss: 1.1222162246704102, tv_loss: 0.018854817375540733\n",
      "iteration 188, dc_loss: 1.117443323135376, tv_loss: 0.018710685893893242\n",
      "iteration 189, dc_loss: 1.1123573780059814, tv_loss: 0.019039830192923546\n",
      "iteration 190, dc_loss: 1.1079797744750977, tv_loss: 0.018711093813180923\n",
      "iteration 191, dc_loss: 1.1030186414718628, tv_loss: 0.019250789657235146\n",
      "iteration 192, dc_loss: 1.0986254215240479, tv_loss: 0.01878228411078453\n",
      "iteration 193, dc_loss: 1.0934041738510132, tv_loss: 0.01925741136074066\n",
      "iteration 194, dc_loss: 1.0885562896728516, tv_loss: 0.01897464133799076\n",
      "iteration 195, dc_loss: 1.0834580659866333, tv_loss: 0.0191066712141037\n",
      "iteration 196, dc_loss: 1.0785419940948486, tv_loss: 0.019225727766752243\n",
      "iteration 197, dc_loss: 1.0740888118743896, tv_loss: 0.01907375082373619\n",
      "iteration 198, dc_loss: 1.0691462755203247, tv_loss: 0.019517652690410614\n",
      "iteration 199, dc_loss: 1.0648282766342163, tv_loss: 0.019139889627695084\n",
      "iteration 200, dc_loss: 1.0599626302719116, tv_loss: 0.019476469606161118\n",
      "iteration 201, dc_loss: 1.055393099784851, tv_loss: 0.019382232800126076\n",
      "iteration 202, dc_loss: 1.0505398511886597, tv_loss: 0.019470136612653732\n",
      "iteration 203, dc_loss: 1.045594334602356, tv_loss: 0.019566036760807037\n",
      "iteration 204, dc_loss: 1.0411442518234253, tv_loss: 0.01942955143749714\n",
      "iteration 205, dc_loss: 1.036547303199768, tv_loss: 0.01970558986067772\n",
      "iteration 206, dc_loss: 1.0321552753448486, tv_loss: 0.01950974389910698\n",
      "iteration 207, dc_loss: 1.0273120403289795, tv_loss: 0.019765079021453857\n",
      "iteration 208, dc_loss: 1.022825002670288, tv_loss: 0.01966291479766369\n",
      "iteration 209, dc_loss: 1.0183329582214355, tv_loss: 0.019778450950980186\n",
      "iteration 210, dc_loss: 1.013831377029419, tv_loss: 0.019837720319628716\n",
      "iteration 211, dc_loss: 1.009272813796997, tv_loss: 0.019802339375019073\n",
      "iteration 212, dc_loss: 1.0046441555023193, tv_loss: 0.019963892176747322\n",
      "iteration 213, dc_loss: 1.0004932880401611, tv_loss: 0.019811030477285385\n",
      "iteration 214, dc_loss: 0.996013343334198, tv_loss: 0.020048798993229866\n",
      "iteration 215, dc_loss: 0.9918909668922424, tv_loss: 0.01984354294836521\n",
      "iteration 216, dc_loss: 0.9872995615005493, tv_loss: 0.020206734538078308\n",
      "iteration 217, dc_loss: 0.9832471013069153, tv_loss: 0.019966475665569305\n",
      "iteration 218, dc_loss: 0.9786314964294434, tv_loss: 0.020286057144403458\n",
      "iteration 219, dc_loss: 0.974484920501709, tv_loss: 0.01999567449092865\n",
      "iteration 220, dc_loss: 0.9698579907417297, tv_loss: 0.02034885063767433\n",
      "iteration 221, dc_loss: 0.9659214615821838, tv_loss: 0.020075524225831032\n",
      "iteration 222, dc_loss: 0.9613738059997559, tv_loss: 0.020396249368786812\n",
      "iteration 223, dc_loss: 0.9572553634643555, tv_loss: 0.020209142938256264\n",
      "iteration 224, dc_loss: 0.95277339220047, tv_loss: 0.02047055959701538\n",
      "iteration 225, dc_loss: 0.9486726522445679, tv_loss: 0.02041606791317463\n",
      "iteration 226, dc_loss: 0.9446002244949341, tv_loss: 0.020418815314769745\n",
      "iteration 227, dc_loss: 0.9403740763664246, tv_loss: 0.020636681467294693\n",
      "iteration 228, dc_loss: 0.9367607235908508, tv_loss: 0.020349081605672836\n",
      "iteration 229, dc_loss: 0.9328663945198059, tv_loss: 0.02093450538814068\n",
      "iteration 230, dc_loss: 0.9299612045288086, tv_loss: 0.02022765390574932\n",
      "iteration 231, dc_loss: 0.9261857867240906, tv_loss: 0.021135609596967697\n",
      "iteration 232, dc_loss: 0.9214450716972351, tv_loss: 0.020411070436239243\n",
      "iteration 233, dc_loss: 0.9166246056556702, tv_loss: 0.020567724481225014\n",
      "iteration 234, dc_loss: 0.9125335216522217, tv_loss: 0.02105671353638172\n",
      "iteration 235, dc_loss: 0.9092344045639038, tv_loss: 0.020454544574022293\n",
      "iteration 236, dc_loss: 0.9046246409416199, tv_loss: 0.020970234647393227\n",
      "iteration 237, dc_loss: 0.900380551815033, tv_loss: 0.020834889262914658\n",
      "iteration 238, dc_loss: 0.8966975212097168, tv_loss: 0.020746255293488503\n",
      "iteration 239, dc_loss: 0.8925750851631165, tv_loss: 0.02118174359202385\n",
      "iteration 240, dc_loss: 0.8888165950775146, tv_loss: 0.02083803154528141\n",
      "iteration 241, dc_loss: 0.8846184015274048, tv_loss: 0.0210141409188509\n",
      "iteration 242, dc_loss: 0.880340039730072, tv_loss: 0.021135238930583\n",
      "iteration 243, dc_loss: 0.8767353892326355, tv_loss: 0.020929379388689995\n",
      "iteration 244, dc_loss: 0.8726881742477417, tv_loss: 0.021193360909819603\n",
      "iteration 245, dc_loss: 0.8687121868133545, tv_loss: 0.021113885566592216\n",
      "iteration 246, dc_loss: 0.8648070693016052, tv_loss: 0.02112760953605175\n",
      "iteration 247, dc_loss: 0.8610270023345947, tv_loss: 0.02131566032767296\n",
      "iteration 248, dc_loss: 0.8573339581489563, tv_loss: 0.02122391015291214\n",
      "iteration 249, dc_loss: 0.8532745838165283, tv_loss: 0.0213841013610363\n",
      "iteration 250, dc_loss: 0.8495832085609436, tv_loss: 0.021262764930725098\n",
      "iteration 251, dc_loss: 0.8457815051078796, tv_loss: 0.021322285756468773\n",
      "iteration 252, dc_loss: 0.8418087363243103, tv_loss: 0.021510642021894455\n",
      "iteration 253, dc_loss: 0.8384112119674683, tv_loss: 0.021293478086590767\n",
      "iteration 254, dc_loss: 0.8344690799713135, tv_loss: 0.021546723321080208\n",
      "iteration 255, dc_loss: 0.8306807279586792, tv_loss: 0.02153681218624115\n",
      "iteration 256, dc_loss: 0.8273089528083801, tv_loss: 0.02146558277308941\n",
      "iteration 257, dc_loss: 0.8234493732452393, tv_loss: 0.02171640843153\n",
      "iteration 258, dc_loss: 0.8198685646057129, tv_loss: 0.021538009867072105\n",
      "iteration 259, dc_loss: 0.8162729144096375, tv_loss: 0.021671170368790627\n",
      "iteration 260, dc_loss: 0.8124876618385315, tv_loss: 0.021695997565984726\n",
      "iteration 261, dc_loss: 0.8089806437492371, tv_loss: 0.02166789211332798\n",
      "iteration 262, dc_loss: 0.8052861094474792, tv_loss: 0.021781381219625473\n",
      "iteration 263, dc_loss: 0.8018301129341125, tv_loss: 0.021681345999240875\n",
      "iteration 264, dc_loss: 0.7984043955802917, tv_loss: 0.02192900888621807\n",
      "iteration 265, dc_loss: 0.795011579990387, tv_loss: 0.02180948667228222\n",
      "iteration 266, dc_loss: 0.7916921973228455, tv_loss: 0.02196485921740532\n",
      "iteration 267, dc_loss: 0.7885310649871826, tv_loss: 0.021747509017586708\n",
      "iteration 268, dc_loss: 0.7852965593338013, tv_loss: 0.02204534411430359\n",
      "iteration 269, dc_loss: 0.782201886177063, tv_loss: 0.021759742870926857\n",
      "iteration 270, dc_loss: 0.7782312631607056, tv_loss: 0.022049522027373314\n",
      "iteration 271, dc_loss: 0.7741706371307373, tv_loss: 0.022229250520467758\n",
      "iteration 272, dc_loss: 0.7703126668930054, tv_loss: 0.02203483134508133\n",
      "iteration 273, dc_loss: 0.766907811164856, tv_loss: 0.022058259695768356\n",
      "iteration 274, dc_loss: 0.7637953162193298, tv_loss: 0.02209249697625637\n",
      "iteration 275, dc_loss: 0.7604510188102722, tv_loss: 0.022200696170330048\n",
      "iteration 276, dc_loss: 0.7568337321281433, tv_loss: 0.02219722606241703\n",
      "iteration 277, dc_loss: 0.7530860304832458, tv_loss: 0.022300366312265396\n",
      "iteration 278, dc_loss: 0.7499779462814331, tv_loss: 0.022241337224841118\n",
      "iteration 279, dc_loss: 0.7468104362487793, tv_loss: 0.02239340916275978\n",
      "iteration 280, dc_loss: 0.7436285614967346, tv_loss: 0.022421635687351227\n",
      "iteration 281, dc_loss: 0.7402727007865906, tv_loss: 0.022303881123661995\n",
      "iteration 282, dc_loss: 0.7366854548454285, tv_loss: 0.02264772541821003\n",
      "iteration 283, dc_loss: 0.7340108752250671, tv_loss: 0.022217649966478348\n",
      "iteration 284, dc_loss: 0.7304501533508301, tv_loss: 0.022821558639407158\n",
      "iteration 285, dc_loss: 0.7273252606391907, tv_loss: 0.022308122366666794\n",
      "iteration 286, dc_loss: 0.7234156727790833, tv_loss: 0.022626884281635284\n",
      "iteration 287, dc_loss: 0.7200124859809875, tv_loss: 0.022639427334070206\n",
      "iteration 288, dc_loss: 0.717269778251648, tv_loss: 0.022460486739873886\n",
      "iteration 289, dc_loss: 0.7137654423713684, tv_loss: 0.02277315780520439\n",
      "iteration 290, dc_loss: 0.7106868028640747, tv_loss: 0.022529657930135727\n",
      "iteration 291, dc_loss: 0.7071821093559265, tv_loss: 0.02278107777237892\n",
      "iteration 292, dc_loss: 0.7039965391159058, tv_loss: 0.022799644619226456\n",
      "iteration 293, dc_loss: 0.7011311650276184, tv_loss: 0.022651707753539085\n",
      "iteration 294, dc_loss: 0.6976535320281982, tv_loss: 0.022867100313305855\n",
      "iteration 295, dc_loss: 0.6944730281829834, tv_loss: 0.02278945781290531\n",
      "iteration 296, dc_loss: 0.6911917924880981, tv_loss: 0.02284979447722435\n",
      "iteration 297, dc_loss: 0.6881709694862366, tv_loss: 0.02289765514433384\n",
      "iteration 298, dc_loss: 0.6852702498435974, tv_loss: 0.02297462709248066\n",
      "iteration 299, dc_loss: 0.6826455593109131, tv_loss: 0.022832218557596207\n",
      "iteration 300, dc_loss: 0.6803318858146667, tv_loss: 0.023265346884727478\n",
      "iteration 301, dc_loss: 0.6776609420776367, tv_loss: 0.02266945131123066\n",
      "iteration 302, dc_loss: 0.674042284488678, tv_loss: 0.023355398327112198\n",
      "iteration 303, dc_loss: 0.6700510382652283, tv_loss: 0.022838685661554337\n",
      "iteration 304, dc_loss: 0.6671869158744812, tv_loss: 0.023069478571414948\n",
      "iteration 305, dc_loss: 0.6641255021095276, tv_loss: 0.023292237892746925\n",
      "iteration 306, dc_loss: 0.6616090536117554, tv_loss: 0.02281114086508751\n",
      "iteration 307, dc_loss: 0.6575706601142883, tv_loss: 0.023304156959056854\n",
      "iteration 308, dc_loss: 0.6544464230537415, tv_loss: 0.023115286603569984\n",
      "iteration 309, dc_loss: 0.6516391634941101, tv_loss: 0.023130930960178375\n",
      "iteration 310, dc_loss: 0.6487203240394592, tv_loss: 0.023507675155997276\n",
      "iteration 311, dc_loss: 0.6459830403327942, tv_loss: 0.023112649098038673\n",
      "iteration 312, dc_loss: 0.642372190952301, tv_loss: 0.023349814116954803\n",
      "iteration 313, dc_loss: 0.6393250823020935, tv_loss: 0.023419149219989777\n",
      "iteration 314, dc_loss: 0.6367491483688354, tv_loss: 0.023242143914103508\n",
      "iteration 315, dc_loss: 0.6335317492485046, tv_loss: 0.023529548197984695\n",
      "iteration 316, dc_loss: 0.6308183670043945, tv_loss: 0.023358158767223358\n",
      "iteration 317, dc_loss: 0.6277329325675964, tv_loss: 0.023436710238456726\n",
      "iteration 318, dc_loss: 0.6248636841773987, tv_loss: 0.02367229573428631\n",
      "iteration 319, dc_loss: 0.6223931312561035, tv_loss: 0.023392561823129654\n",
      "iteration 320, dc_loss: 0.6191378831863403, tv_loss: 0.0237012580037117\n",
      "iteration 321, dc_loss: 0.6163160800933838, tv_loss: 0.023591574281454086\n",
      "iteration 322, dc_loss: 0.6134241819381714, tv_loss: 0.023580016568303108\n",
      "iteration 323, dc_loss: 0.6105259656906128, tv_loss: 0.02374640479683876\n",
      "iteration 324, dc_loss: 0.6079265475273132, tv_loss: 0.02359158731997013\n",
      "iteration 325, dc_loss: 0.604989230632782, tv_loss: 0.02381335198879242\n",
      "iteration 326, dc_loss: 0.6023343205451965, tv_loss: 0.023737406358122826\n",
      "iteration 327, dc_loss: 0.5994742512702942, tv_loss: 0.023798972368240356\n",
      "iteration 328, dc_loss: 0.59670090675354, tv_loss: 0.023863179609179497\n",
      "iteration 329, dc_loss: 0.5940899848937988, tv_loss: 0.023814747110009193\n",
      "iteration 330, dc_loss: 0.5913213491439819, tv_loss: 0.02398848906159401\n",
      "iteration 331, dc_loss: 0.5888280868530273, tv_loss: 0.023849770426750183\n",
      "iteration 332, dc_loss: 0.5860340595245361, tv_loss: 0.02404179610311985\n",
      "iteration 333, dc_loss: 0.5835445523262024, tv_loss: 0.02389807254076004\n",
      "iteration 334, dc_loss: 0.5807607173919678, tv_loss: 0.024135148152709007\n",
      "iteration 335, dc_loss: 0.5783073306083679, tv_loss: 0.023953258991241455\n",
      "iteration 336, dc_loss: 0.5755352973937988, tv_loss: 0.024167250841856003\n",
      "iteration 337, dc_loss: 0.5730807185173035, tv_loss: 0.02405879832804203\n",
      "iteration 338, dc_loss: 0.5704137682914734, tv_loss: 0.024190548807382584\n",
      "iteration 339, dc_loss: 0.5678612589836121, tv_loss: 0.0241532064974308\n",
      "iteration 340, dc_loss: 0.5651803612709045, tv_loss: 0.02422359026968479\n",
      "iteration 341, dc_loss: 0.5625304579734802, tv_loss: 0.02422931231558323\n",
      "iteration 342, dc_loss: 0.5599719285964966, tv_loss: 0.024260401725769043\n",
      "iteration 343, dc_loss: 0.5574299693107605, tv_loss: 0.024326348677277565\n",
      "iteration 344, dc_loss: 0.5549740195274353, tv_loss: 0.024337420240044594\n",
      "iteration 345, dc_loss: 0.5525100827217102, tv_loss: 0.024335656315088272\n",
      "iteration 346, dc_loss: 0.5499183535575867, tv_loss: 0.024482423439621925\n",
      "iteration 347, dc_loss: 0.5476990342140198, tv_loss: 0.02428765967488289\n",
      "iteration 348, dc_loss: 0.5451545119285583, tv_loss: 0.024690520018339157\n",
      "iteration 349, dc_loss: 0.543549120426178, tv_loss: 0.024171216413378716\n",
      "iteration 350, dc_loss: 0.5413485169410706, tv_loss: 0.024914033710956573\n",
      "iteration 351, dc_loss: 0.5394159555435181, tv_loss: 0.024125024676322937\n",
      "iteration 352, dc_loss: 0.5365748405456543, tv_loss: 0.02476181462407112\n",
      "iteration 353, dc_loss: 0.5338945388793945, tv_loss: 0.024477267637848854\n",
      "iteration 354, dc_loss: 0.5317359566688538, tv_loss: 0.024665959179401398\n",
      "iteration 355, dc_loss: 0.5288676619529724, tv_loss: 0.02487531304359436\n",
      "iteration 356, dc_loss: 0.5271655917167664, tv_loss: 0.024349957704544067\n",
      "iteration 357, dc_loss: 0.5244577527046204, tv_loss: 0.024869130924344063\n",
      "iteration 358, dc_loss: 0.5224023461341858, tv_loss: 0.024670826271176338\n",
      "iteration 359, dc_loss: 0.519784688949585, tv_loss: 0.024769920855760574\n",
      "iteration 360, dc_loss: 0.5177140235900879, tv_loss: 0.02493409253656864\n",
      "iteration 361, dc_loss: 0.5148301124572754, tv_loss: 0.02469697967171669\n",
      "iteration 362, dc_loss: 0.5126986503601074, tv_loss: 0.02477852813899517\n",
      "iteration 363, dc_loss: 0.510002076625824, tv_loss: 0.024893125519156456\n",
      "iteration 364, dc_loss: 0.507951557636261, tv_loss: 0.024724485352635384\n",
      "iteration 365, dc_loss: 0.5053267478942871, tv_loss: 0.025200840085744858\n",
      "iteration 366, dc_loss: 0.5029956102371216, tv_loss: 0.02491598017513752\n",
      "iteration 367, dc_loss: 0.5009722709655762, tv_loss: 0.02482862025499344\n",
      "iteration 368, dc_loss: 0.49837344884872437, tv_loss: 0.025067027658224106\n",
      "iteration 369, dc_loss: 0.4962204694747925, tv_loss: 0.02489040605723858\n",
      "iteration 370, dc_loss: 0.4939211905002594, tv_loss: 0.02511381171643734\n",
      "iteration 371, dc_loss: 0.49159955978393555, tv_loss: 0.025305695831775665\n",
      "iteration 372, dc_loss: 0.48956775665283203, tv_loss: 0.024994347244501114\n",
      "iteration 373, dc_loss: 0.48713505268096924, tv_loss: 0.025187185034155846\n",
      "iteration 374, dc_loss: 0.4850517809391022, tv_loss: 0.02539362758398056\n",
      "iteration 375, dc_loss: 0.4827711582183838, tv_loss: 0.02520626224577427\n",
      "iteration 376, dc_loss: 0.48065873980522156, tv_loss: 0.025352872908115387\n",
      "iteration 377, dc_loss: 0.4786146879196167, tv_loss: 0.025355946272611618\n",
      "iteration 378, dc_loss: 0.4762111008167267, tv_loss: 0.02535405568778515\n",
      "iteration 379, dc_loss: 0.47421756386756897, tv_loss: 0.025407548993825912\n",
      "iteration 380, dc_loss: 0.47199442982673645, tv_loss: 0.025430817157030106\n",
      "iteration 381, dc_loss: 0.46988657116889954, tv_loss: 0.025336388498544693\n",
      "iteration 382, dc_loss: 0.4678472876548767, tv_loss: 0.02538742497563362\n",
      "iteration 383, dc_loss: 0.4657238721847534, tv_loss: 0.025497032329440117\n",
      "iteration 384, dc_loss: 0.4635680317878723, tv_loss: 0.02548026479780674\n",
      "iteration 385, dc_loss: 0.4615377187728882, tv_loss: 0.025392435491085052\n",
      "iteration 386, dc_loss: 0.45939838886260986, tv_loss: 0.025628473609685898\n",
      "iteration 387, dc_loss: 0.4574820399284363, tv_loss: 0.025553368031978607\n",
      "iteration 388, dc_loss: 0.45532315969467163, tv_loss: 0.025637025013566017\n",
      "iteration 389, dc_loss: 0.45359548926353455, tv_loss: 0.025393741205334663\n",
      "iteration 390, dc_loss: 0.45150187611579895, tv_loss: 0.02572924830019474\n",
      "iteration 391, dc_loss: 0.4501720666885376, tv_loss: 0.025348583236336708\n",
      "iteration 392, dc_loss: 0.4484599232673645, tv_loss: 0.02595681883394718\n",
      "iteration 393, dc_loss: 0.44686785340309143, tv_loss: 0.025479601696133614\n",
      "iteration 394, dc_loss: 0.444295734167099, tv_loss: 0.025960804894566536\n",
      "iteration 395, dc_loss: 0.44193413853645325, tv_loss: 0.025599125772714615\n",
      "iteration 396, dc_loss: 0.43955230712890625, tv_loss: 0.02569705992937088\n",
      "iteration 397, dc_loss: 0.43755772709846497, tv_loss: 0.02585620805621147\n",
      "iteration 398, dc_loss: 0.4360470473766327, tv_loss: 0.025759398937225342\n",
      "iteration 399, dc_loss: 0.4339854419231415, tv_loss: 0.026078056544065475\n",
      "iteration 400, dc_loss: 0.4322294294834137, tv_loss: 0.025609353557229042\n",
      "iteration 401, dc_loss: 0.42973411083221436, tv_loss: 0.025962449610233307\n",
      "iteration 402, dc_loss: 0.42808762192726135, tv_loss: 0.025821944698691368\n",
      "iteration 403, dc_loss: 0.42685121297836304, tv_loss: 0.025711778551340103\n",
      "iteration 404, dc_loss: 0.4250861704349518, tv_loss: 0.026023756712675095\n",
      "iteration 405, dc_loss: 0.42349863052368164, tv_loss: 0.025938358157873154\n",
      "iteration 406, dc_loss: 0.4222545027732849, tv_loss: 0.025883717462420464\n",
      "iteration 407, dc_loss: 0.4205358028411865, tv_loss: 0.026089150458574295\n",
      "iteration 408, dc_loss: 0.41899433732032776, tv_loss: 0.025936974212527275\n",
      "iteration 409, dc_loss: 0.4177263677120209, tv_loss: 0.025867335498332977\n",
      "iteration 410, dc_loss: 0.41603124141693115, tv_loss: 0.0261674951761961\n",
      "iteration 411, dc_loss: 0.4146146774291992, tv_loss: 0.02605346590280533\n",
      "iteration 412, dc_loss: 0.41322171688079834, tv_loss: 0.026023996993899345\n",
      "iteration 413, dc_loss: 0.4116002917289734, tv_loss: 0.02620258927345276\n",
      "iteration 414, dc_loss: 0.41023755073547363, tv_loss: 0.026007073000073433\n",
      "iteration 415, dc_loss: 0.40877896547317505, tv_loss: 0.026066644117236137\n",
      "iteration 416, dc_loss: 0.40720894932746887, tv_loss: 0.0263753030449152\n",
      "iteration 417, dc_loss: 0.40593817830085754, tv_loss: 0.026066500693559647\n",
      "iteration 418, dc_loss: 0.4043922424316406, tv_loss: 0.026114534586668015\n",
      "iteration 419, dc_loss: 0.4028787314891815, tv_loss: 0.02624782733619213\n",
      "iteration 420, dc_loss: 0.40162205696105957, tv_loss: 0.026104869320988655\n",
      "iteration 421, dc_loss: 0.4001004695892334, tv_loss: 0.026280514895915985\n",
      "iteration 422, dc_loss: 0.39863526821136475, tv_loss: 0.026417572051286697\n",
      "iteration 423, dc_loss: 0.39733970165252686, tv_loss: 0.026192205026745796\n",
      "iteration 424, dc_loss: 0.3958355784416199, tv_loss: 0.026287443935871124\n",
      "iteration 425, dc_loss: 0.3944656252861023, tv_loss: 0.02629271149635315\n",
      "iteration 426, dc_loss: 0.3931054472923279, tv_loss: 0.02643416076898575\n",
      "iteration 427, dc_loss: 0.3916378617286682, tv_loss: 0.02639980986714363\n",
      "iteration 428, dc_loss: 0.3903321623802185, tv_loss: 0.026300035417079926\n",
      "iteration 429, dc_loss: 0.38892608880996704, tv_loss: 0.026394430547952652\n",
      "iteration 430, dc_loss: 0.3875178396701813, tv_loss: 0.026595599949359894\n",
      "iteration 431, dc_loss: 0.3862190544605255, tv_loss: 0.026372704654932022\n",
      "iteration 432, dc_loss: 0.3848249018192291, tv_loss: 0.02651360258460045\n",
      "iteration 433, dc_loss: 0.38348618149757385, tv_loss: 0.02666981890797615\n",
      "iteration 434, dc_loss: 0.3821098208427429, tv_loss: 0.026477541774511337\n",
      "iteration 435, dc_loss: 0.3807893693447113, tv_loss: 0.026683460921049118\n",
      "iteration 436, dc_loss: 0.3794597387313843, tv_loss: 0.026536790654063225\n",
      "iteration 437, dc_loss: 0.3780786991119385, tv_loss: 0.026563383638858795\n",
      "iteration 438, dc_loss: 0.37683114409446716, tv_loss: 0.02674851194024086\n",
      "iteration 439, dc_loss: 0.37545621395111084, tv_loss: 0.026556164026260376\n",
      "iteration 440, dc_loss: 0.3741413950920105, tv_loss: 0.026817748323082924\n",
      "iteration 441, dc_loss: 0.3728390634059906, tv_loss: 0.026664480566978455\n",
      "iteration 442, dc_loss: 0.3715129494667053, tv_loss: 0.026615740731358528\n",
      "iteration 443, dc_loss: 0.37023478746414185, tv_loss: 0.02667233720421791\n",
      "iteration 444, dc_loss: 0.3689296543598175, tv_loss: 0.026777736842632294\n",
      "iteration 445, dc_loss: 0.36760666966438293, tv_loss: 0.026699015870690346\n",
      "iteration 446, dc_loss: 0.36631742119789124, tv_loss: 0.026691440492868423\n",
      "iteration 447, dc_loss: 0.36506569385528564, tv_loss: 0.026833008974790573\n",
      "iteration 448, dc_loss: 0.36375322937965393, tv_loss: 0.026813935488462448\n",
      "iteration 449, dc_loss: 0.3624701201915741, tv_loss: 0.026754774153232574\n",
      "iteration 450, dc_loss: 0.3612436354160309, tv_loss: 0.026793383061885834\n",
      "iteration 451, dc_loss: 0.3599274754524231, tv_loss: 0.02693070098757744\n",
      "iteration 452, dc_loss: 0.3586784303188324, tv_loss: 0.026847077533602715\n",
      "iteration 453, dc_loss: 0.3574293851852417, tv_loss: 0.02682088129222393\n",
      "iteration 454, dc_loss: 0.356170654296875, tv_loss: 0.026979604735970497\n",
      "iteration 455, dc_loss: 0.3549632430076599, tv_loss: 0.02688329853117466\n",
      "iteration 456, dc_loss: 0.3537061810493469, tv_loss: 0.026858584955334663\n",
      "iteration 457, dc_loss: 0.3524627387523651, tv_loss: 0.026912467554211617\n",
      "iteration 458, dc_loss: 0.3512111008167267, tv_loss: 0.02705296128988266\n",
      "iteration 459, dc_loss: 0.35001853108406067, tv_loss: 0.026953069493174553\n",
      "iteration 460, dc_loss: 0.3487361967563629, tv_loss: 0.026989469304680824\n",
      "iteration 461, dc_loss: 0.3476111590862274, tv_loss: 0.026994556188583374\n",
      "iteration 462, dc_loss: 0.34632986783981323, tv_loss: 0.027138378471136093\n",
      "iteration 463, dc_loss: 0.34514063596725464, tv_loss: 0.02702534943819046\n",
      "iteration 464, dc_loss: 0.3439435064792633, tv_loss: 0.02704976685345173\n",
      "iteration 465, dc_loss: 0.3427513539791107, tv_loss: 0.027179254218935966\n",
      "iteration 466, dc_loss: 0.34150880575180054, tv_loss: 0.027119802311062813\n",
      "iteration 467, dc_loss: 0.34038665890693665, tv_loss: 0.027048377320170403\n",
      "iteration 468, dc_loss: 0.33913934230804443, tv_loss: 0.027244411408901215\n",
      "iteration 469, dc_loss: 0.3380158543586731, tv_loss: 0.027127735316753387\n",
      "iteration 470, dc_loss: 0.3367888629436493, tv_loss: 0.027140280231833458\n",
      "iteration 471, dc_loss: 0.33566269278526306, tv_loss: 0.027107587084174156\n",
      "iteration 472, dc_loss: 0.33445656299591064, tv_loss: 0.027187442407011986\n",
      "iteration 473, dc_loss: 0.3334084749221802, tv_loss: 0.027188360691070557\n",
      "iteration 474, dc_loss: 0.33219972252845764, tv_loss: 0.02738157846033573\n",
      "iteration 475, dc_loss: 0.3313612937927246, tv_loss: 0.02709774486720562\n",
      "iteration 476, dc_loss: 0.3301549553871155, tv_loss: 0.027418099343776703\n",
      "iteration 477, dc_loss: 0.3298102021217346, tv_loss: 0.02707601897418499\n",
      "iteration 478, dc_loss: 0.328960657119751, tv_loss: 0.02772459201514721\n",
      "iteration 479, dc_loss: 0.32818925380706787, tv_loss: 0.02703140117228031\n",
      "iteration 480, dc_loss: 0.3264233469963074, tv_loss: 0.027541957795619965\n",
      "iteration 481, dc_loss: 0.32458487153053284, tv_loss: 0.027432510629296303\n",
      "iteration 482, dc_loss: 0.323382705450058, tv_loss: 0.027209240943193436\n",
      "iteration 483, dc_loss: 0.32251641154289246, tv_loss: 0.02765880525112152\n",
      "iteration 484, dc_loss: 0.3218713104724884, tv_loss: 0.027317818254232407\n",
      "iteration 485, dc_loss: 0.31991708278656006, tv_loss: 0.02760079875588417\n",
      "iteration 486, dc_loss: 0.3187344968318939, tv_loss: 0.027579640969634056\n",
      "iteration 487, dc_loss: 0.318043053150177, tv_loss: 0.02726837620139122\n",
      "iteration 488, dc_loss: 0.3167625069618225, tv_loss: 0.027719974517822266\n",
      "iteration 489, dc_loss: 0.31589362025260925, tv_loss: 0.0273683350533247\n",
      "iteration 490, dc_loss: 0.31427761912345886, tv_loss: 0.027550779283046722\n",
      "iteration 491, dc_loss: 0.31314393877983093, tv_loss: 0.027604110538959503\n",
      "iteration 492, dc_loss: 0.3125874400138855, tv_loss: 0.02730668894946575\n",
      "iteration 493, dc_loss: 0.31111401319503784, tv_loss: 0.027799196541309357\n",
      "iteration 494, dc_loss: 0.31003832817077637, tv_loss: 0.02749870903789997\n",
      "iteration 495, dc_loss: 0.30897536873817444, tv_loss: 0.027454590424895287\n",
      "iteration 496, dc_loss: 0.30785486102104187, tv_loss: 0.027690712362527847\n",
      "iteration 497, dc_loss: 0.30707794427871704, tv_loss: 0.027369989082217216\n",
      "iteration 498, dc_loss: 0.3056236803531647, tv_loss: 0.02765565738081932\n",
      "iteration 499, dc_loss: 0.3045814037322998, tv_loss: 0.02767406404018402\n",
      "iteration 500, dc_loss: 0.3037645220756531, tv_loss: 0.027601905167102814\n",
      "iteration 501, dc_loss: 0.3024924099445343, tv_loss: 0.02781040221452713\n",
      "iteration 502, dc_loss: 0.30163657665252686, tv_loss: 0.027541520074009895\n",
      "iteration 503, dc_loss: 0.300455242395401, tv_loss: 0.027634914964437485\n",
      "iteration 504, dc_loss: 0.29932910203933716, tv_loss: 0.027738837525248528\n",
      "iteration 505, dc_loss: 0.2985309660434723, tv_loss: 0.027666185051202774\n",
      "iteration 506, dc_loss: 0.2973223626613617, tv_loss: 0.02795039676129818\n",
      "iteration 507, dc_loss: 0.29637661576271057, tv_loss: 0.027677392587065697\n",
      "iteration 508, dc_loss: 0.29530709981918335, tv_loss: 0.02775443345308304\n",
      "iteration 509, dc_loss: 0.2942720949649811, tv_loss: 0.028041528537869453\n",
      "iteration 510, dc_loss: 0.29335111379623413, tv_loss: 0.027728796005249023\n",
      "iteration 511, dc_loss: 0.2922866642475128, tv_loss: 0.028071898967027664\n",
      "iteration 512, dc_loss: 0.2913508713245392, tv_loss: 0.027752693742513657\n",
      "iteration 513, dc_loss: 0.2902000844478607, tv_loss: 0.027871448546648026\n",
      "iteration 514, dc_loss: 0.2892472445964813, tv_loss: 0.0280101727694273\n",
      "iteration 515, dc_loss: 0.28833916783332825, tv_loss: 0.02784297615289688\n",
      "iteration 516, dc_loss: 0.2871938645839691, tv_loss: 0.027978932484984398\n",
      "iteration 517, dc_loss: 0.2864517867565155, tv_loss: 0.027762066572904587\n",
      "iteration 518, dc_loss: 0.2852795422077179, tv_loss: 0.028031421825289726\n",
      "iteration 519, dc_loss: 0.2843787372112274, tv_loss: 0.02802763134241104\n",
      "iteration 520, dc_loss: 0.2834078371524811, tv_loss: 0.027926437556743622\n",
      "iteration 521, dc_loss: 0.28237125277519226, tv_loss: 0.02801978774368763\n",
      "iteration 522, dc_loss: 0.28156760334968567, tv_loss: 0.02806585282087326\n",
      "iteration 523, dc_loss: 0.2805180847644806, tv_loss: 0.02805512212216854\n",
      "iteration 524, dc_loss: 0.2795754373073578, tv_loss: 0.02800724282860756\n",
      "iteration 525, dc_loss: 0.2786467671394348, tv_loss: 0.028052112087607384\n",
      "iteration 526, dc_loss: 0.27768653631210327, tv_loss: 0.028172750025987625\n",
      "iteration 527, dc_loss: 0.2768368124961853, tv_loss: 0.028051065281033516\n",
      "iteration 528, dc_loss: 0.27588769793510437, tv_loss: 0.02813107892870903\n",
      "iteration 529, dc_loss: 0.2751747667789459, tv_loss: 0.02804940566420555\n",
      "iteration 530, dc_loss: 0.27433204650878906, tv_loss: 0.02831677533686161\n",
      "iteration 531, dc_loss: 0.27364063262939453, tv_loss: 0.028064122423529625\n",
      "iteration 532, dc_loss: 0.2726829946041107, tv_loss: 0.028246823698282242\n",
      "iteration 533, dc_loss: 0.27175238728523254, tv_loss: 0.028243569657206535\n",
      "iteration 534, dc_loss: 0.27065640687942505, tv_loss: 0.028191139921545982\n",
      "iteration 535, dc_loss: 0.269368439912796, tv_loss: 0.028278473764657974\n",
      "iteration 536, dc_loss: 0.2685371935367584, tv_loss: 0.02813875675201416\n",
      "iteration 537, dc_loss: 0.26745977997779846, tv_loss: 0.028392555192112923\n",
      "iteration 538, dc_loss: 0.26685193181037903, tv_loss: 0.028134401887655258\n",
      "iteration 539, dc_loss: 0.265790194272995, tv_loss: 0.02833896316587925\n",
      "iteration 540, dc_loss: 0.26498499512672424, tv_loss: 0.028168903663754463\n",
      "iteration 541, dc_loss: 0.2639335095882416, tv_loss: 0.02830631472170353\n",
      "iteration 542, dc_loss: 0.2629075348377228, tv_loss: 0.028408106416463852\n",
      "iteration 543, dc_loss: 0.26217103004455566, tv_loss: 0.02819182351231575\n",
      "iteration 544, dc_loss: 0.26109468936920166, tv_loss: 0.028419354930520058\n",
      "iteration 545, dc_loss: 0.26055389642715454, tv_loss: 0.02818816527724266\n",
      "iteration 546, dc_loss: 0.25946640968322754, tv_loss: 0.02859051339328289\n",
      "iteration 547, dc_loss: 0.25863558053970337, tv_loss: 0.028312556445598602\n",
      "iteration 548, dc_loss: 0.25755393505096436, tv_loss: 0.028394317254424095\n",
      "iteration 549, dc_loss: 0.256665974855423, tv_loss: 0.028563451021909714\n",
      "iteration 550, dc_loss: 0.255980908870697, tv_loss: 0.02832420915365219\n",
      "iteration 551, dc_loss: 0.2549467086791992, tv_loss: 0.028569037094712257\n",
      "iteration 552, dc_loss: 0.254426509141922, tv_loss: 0.028317946940660477\n",
      "iteration 553, dc_loss: 0.253266841173172, tv_loss: 0.0286758653819561\n",
      "iteration 554, dc_loss: 0.2525809109210968, tv_loss: 0.02839481085538864\n",
      "iteration 555, dc_loss: 0.25153928995132446, tv_loss: 0.02851025015115738\n",
      "iteration 556, dc_loss: 0.25073546171188354, tv_loss: 0.028478870168328285\n",
      "iteration 557, dc_loss: 0.2498699277639389, tv_loss: 0.02862050011754036\n",
      "iteration 558, dc_loss: 0.24896985292434692, tv_loss: 0.02859647013247013\n",
      "iteration 559, dc_loss: 0.24821333587169647, tv_loss: 0.02854226343333721\n",
      "iteration 560, dc_loss: 0.24733436107635498, tv_loss: 0.02873840555548668\n",
      "iteration 561, dc_loss: 0.2466878443956375, tv_loss: 0.028481682762503624\n",
      "iteration 562, dc_loss: 0.24563264846801758, tv_loss: 0.02872697450220585\n",
      "iteration 563, dc_loss: 0.24511189758777618, tv_loss: 0.028535209596157074\n",
      "iteration 564, dc_loss: 0.24404381215572357, tv_loss: 0.028756840154528618\n",
      "iteration 565, dc_loss: 0.24349108338356018, tv_loss: 0.028494469821453094\n",
      "iteration 566, dc_loss: 0.24251551926136017, tv_loss: 0.028806516900658607\n",
      "iteration 567, dc_loss: 0.24199044704437256, tv_loss: 0.028579246252775192\n",
      "iteration 568, dc_loss: 0.2409999817609787, tv_loss: 0.028845714405179024\n",
      "iteration 569, dc_loss: 0.24035313725471497, tv_loss: 0.028528008610010147\n",
      "iteration 570, dc_loss: 0.23930490016937256, tv_loss: 0.028805335983633995\n",
      "iteration 571, dc_loss: 0.23857061564922333, tv_loss: 0.028769314289093018\n",
      "iteration 572, dc_loss: 0.23777638375759125, tv_loss: 0.02873106673359871\n",
      "iteration 573, dc_loss: 0.2369116246700287, tv_loss: 0.028811048716306686\n",
      "iteration 574, dc_loss: 0.23640459775924683, tv_loss: 0.028677450492978096\n",
      "iteration 575, dc_loss: 0.23545178771018982, tv_loss: 0.028955016285181046\n",
      "iteration 576, dc_loss: 0.234844371676445, tv_loss: 0.028656961396336555\n",
      "iteration 577, dc_loss: 0.2336849570274353, tv_loss: 0.028911011293530464\n",
      "iteration 578, dc_loss: 0.23303820192813873, tv_loss: 0.028751330450177193\n",
      "iteration 579, dc_loss: 0.23209773004055023, tv_loss: 0.028970517218112946\n",
      "iteration 580, dc_loss: 0.23148280382156372, tv_loss: 0.028805451467633247\n",
      "iteration 581, dc_loss: 0.23068206012248993, tv_loss: 0.028861667960882187\n",
      "iteration 582, dc_loss: 0.22999168932437897, tv_loss: 0.028902851045131683\n",
      "iteration 583, dc_loss: 0.22918111085891724, tv_loss: 0.02895674668252468\n",
      "iteration 584, dc_loss: 0.2283477485179901, tv_loss: 0.02895110845565796\n",
      "iteration 585, dc_loss: 0.22757165133953094, tv_loss: 0.028911355882883072\n",
      "iteration 586, dc_loss: 0.22680692374706268, tv_loss: 0.028923703357577324\n",
      "iteration 587, dc_loss: 0.22597308456897736, tv_loss: 0.02903478778898716\n",
      "iteration 588, dc_loss: 0.2253645658493042, tv_loss: 0.028881333768367767\n",
      "iteration 589, dc_loss: 0.2244812250137329, tv_loss: 0.029068704694509506\n",
      "iteration 590, dc_loss: 0.22403140366077423, tv_loss: 0.02886488102376461\n",
      "iteration 591, dc_loss: 0.22311870753765106, tv_loss: 0.029176142066717148\n",
      "iteration 592, dc_loss: 0.222617506980896, tv_loss: 0.028905192390084267\n",
      "iteration 593, dc_loss: 0.22180026769638062, tv_loss: 0.02913028560578823\n",
      "iteration 594, dc_loss: 0.22147144377231598, tv_loss: 0.02896123193204403\n",
      "iteration 595, dc_loss: 0.22098106145858765, tv_loss: 0.029219303280115128\n",
      "iteration 596, dc_loss: 0.22082076966762543, tv_loss: 0.028894083574414253\n",
      "iteration 597, dc_loss: 0.22019505500793457, tv_loss: 0.029356781393289566\n",
      "iteration 598, dc_loss: 0.21941344439983368, tv_loss: 0.028869152069091797\n",
      "iteration 599, dc_loss: 0.21760372817516327, tv_loss: 0.029277732595801353\n",
      "iteration 600, dc_loss: 0.21682694554328918, tv_loss: 0.02893228456377983\n",
      "iteration 601, dc_loss: 0.2162448614835739, tv_loss: 0.029212910681962967\n",
      "iteration 602, dc_loss: 0.21597230434417725, tv_loss: 0.029155846685171127\n",
      "iteration 603, dc_loss: 0.2149246633052826, tv_loss: 0.02921123243868351\n",
      "iteration 604, dc_loss: 0.2141738086938858, tv_loss: 0.029095662757754326\n",
      "iteration 605, dc_loss: 0.21332359313964844, tv_loss: 0.029252154752612114\n",
      "iteration 606, dc_loss: 0.2126792073249817, tv_loss: 0.02916235662996769\n",
      "iteration 607, dc_loss: 0.21182484924793243, tv_loss: 0.029229197651147842\n",
      "iteration 608, dc_loss: 0.2112085521221161, tv_loss: 0.029168156906962395\n",
      "iteration 609, dc_loss: 0.21059353649616241, tv_loss: 0.02925996668636799\n",
      "iteration 610, dc_loss: 0.20974282920360565, tv_loss: 0.02934895269572735\n",
      "iteration 611, dc_loss: 0.20918476581573486, tv_loss: 0.02916083298623562\n",
      "iteration 612, dc_loss: 0.20828163623809814, tv_loss: 0.029354214668273926\n",
      "iteration 613, dc_loss: 0.20764845609664917, tv_loss: 0.029287496581673622\n",
      "iteration 614, dc_loss: 0.20700682699680328, tv_loss: 0.029291681945323944\n",
      "iteration 615, dc_loss: 0.2063361257314682, tv_loss: 0.029250042513012886\n",
      "iteration 616, dc_loss: 0.20558857917785645, tv_loss: 0.029339101165533066\n",
      "iteration 617, dc_loss: 0.20489896833896637, tv_loss: 0.029425110667943954\n",
      "iteration 618, dc_loss: 0.20429736375808716, tv_loss: 0.029310766607522964\n",
      "iteration 619, dc_loss: 0.2035352885723114, tv_loss: 0.0294494666159153\n",
      "iteration 620, dc_loss: 0.20299692451953888, tv_loss: 0.029328932985663414\n",
      "iteration 621, dc_loss: 0.2021895796060562, tv_loss: 0.029475396499037743\n",
      "iteration 622, dc_loss: 0.20170807838439941, tv_loss: 0.02927817590534687\n",
      "iteration 623, dc_loss: 0.20090395212173462, tv_loss: 0.029456499963998795\n",
      "iteration 624, dc_loss: 0.200342059135437, tv_loss: 0.029402054846286774\n",
      "iteration 625, dc_loss: 0.19959436357021332, tv_loss: 0.029482262209057808\n",
      "iteration 626, dc_loss: 0.199031263589859, tv_loss: 0.029349735006690025\n",
      "iteration 627, dc_loss: 0.19828324019908905, tv_loss: 0.029471447691321373\n",
      "iteration 628, dc_loss: 0.1977395862340927, tv_loss: 0.02946738712489605\n",
      "iteration 629, dc_loss: 0.19703251123428345, tv_loss: 0.029524335637688637\n",
      "iteration 630, dc_loss: 0.19642548263072968, tv_loss: 0.02946154586970806\n",
      "iteration 631, dc_loss: 0.19576594233512878, tv_loss: 0.02948491834104061\n",
      "iteration 632, dc_loss: 0.19514039158821106, tv_loss: 0.029589034616947174\n",
      "iteration 633, dc_loss: 0.19454409182071686, tv_loss: 0.029545310884714127\n",
      "iteration 634, dc_loss: 0.19393783807754517, tv_loss: 0.029542332515120506\n",
      "iteration 635, dc_loss: 0.19333134591579437, tv_loss: 0.029578689485788345\n",
      "iteration 636, dc_loss: 0.19263780117034912, tv_loss: 0.02961781993508339\n",
      "iteration 637, dc_loss: 0.19218119978904724, tv_loss: 0.029489245265722275\n",
      "iteration 638, dc_loss: 0.19147567451000214, tv_loss: 0.029671551659703255\n",
      "iteration 639, dc_loss: 0.1910824477672577, tv_loss: 0.029453184455633163\n",
      "iteration 640, dc_loss: 0.1903482973575592, tv_loss: 0.029705945402383804\n",
      "iteration 641, dc_loss: 0.19016017019748688, tv_loss: 0.02940746210515499\n",
      "iteration 642, dc_loss: 0.18934689462184906, tv_loss: 0.029859434813261032\n",
      "iteration 643, dc_loss: 0.18944503366947174, tv_loss: 0.029268043115735054\n",
      "iteration 644, dc_loss: 0.1884884387254715, tv_loss: 0.030116312205791473\n",
      "iteration 645, dc_loss: 0.1887444704771042, tv_loss: 0.02920057252049446\n",
      "iteration 646, dc_loss: 0.18736770749092102, tv_loss: 0.030114982277154922\n",
      "iteration 647, dc_loss: 0.18708360195159912, tv_loss: 0.029310323297977448\n",
      "iteration 648, dc_loss: 0.18573477864265442, tv_loss: 0.02974809892475605\n",
      "iteration 649, dc_loss: 0.18514855206012726, tv_loss: 0.029805021360516548\n",
      "iteration 650, dc_loss: 0.18502096831798553, tv_loss: 0.02946368418633938\n",
      "iteration 651, dc_loss: 0.1840413361787796, tv_loss: 0.029988033697009087\n",
      "iteration 652, dc_loss: 0.18369060754776, tv_loss: 0.029498282819986343\n",
      "iteration 653, dc_loss: 0.1826842576265335, tv_loss: 0.029701627790927887\n",
      "iteration 654, dc_loss: 0.1820807307958603, tv_loss: 0.029878396540880203\n",
      "iteration 655, dc_loss: 0.18197157979011536, tv_loss: 0.0296369306743145\n",
      "iteration 656, dc_loss: 0.18098527193069458, tv_loss: 0.02993888594210148\n",
      "iteration 657, dc_loss: 0.1805242896080017, tv_loss: 0.029676778241991997\n",
      "iteration 658, dc_loss: 0.17988577485084534, tv_loss: 0.029883990064263344\n",
      "iteration 659, dc_loss: 0.17911668121814728, tv_loss: 0.02992069348692894\n",
      "iteration 660, dc_loss: 0.17886731028556824, tv_loss: 0.029782332479953766\n",
      "iteration 661, dc_loss: 0.17815081775188446, tv_loss: 0.030058743432164192\n",
      "iteration 662, dc_loss: 0.17764827609062195, tv_loss: 0.029780149459838867\n",
      "iteration 663, dc_loss: 0.17704029381275177, tv_loss: 0.029938187450170517\n",
      "iteration 664, dc_loss: 0.17636431753635406, tv_loss: 0.029962068423628807\n",
      "iteration 665, dc_loss: 0.17600026726722717, tv_loss: 0.029763473197817802\n",
      "iteration 666, dc_loss: 0.17528662085533142, tv_loss: 0.030023492872714996\n",
      "iteration 667, dc_loss: 0.17479434609413147, tv_loss: 0.029890136793255806\n",
      "iteration 668, dc_loss: 0.174289733171463, tv_loss: 0.029826974496245384\n",
      "iteration 669, dc_loss: 0.17366662621498108, tv_loss: 0.029956459999084473\n",
      "iteration 670, dc_loss: 0.17322605848312378, tv_loss: 0.029894430190324783\n",
      "iteration 671, dc_loss: 0.17261487245559692, tv_loss: 0.029902292415499687\n",
      "iteration 672, dc_loss: 0.17203283309936523, tv_loss: 0.029934823513031006\n",
      "iteration 673, dc_loss: 0.17158746719360352, tv_loss: 0.02982505038380623\n",
      "iteration 674, dc_loss: 0.17090865969657898, tv_loss: 0.02998983860015869\n",
      "iteration 675, dc_loss: 0.17051799595355988, tv_loss: 0.029893487691879272\n",
      "iteration 676, dc_loss: 0.16992774605751038, tv_loss: 0.030023040249943733\n",
      "iteration 677, dc_loss: 0.1693844348192215, tv_loss: 0.030059101060032845\n",
      "iteration 678, dc_loss: 0.16898512840270996, tv_loss: 0.02989093028008938\n",
      "iteration 679, dc_loss: 0.16829034686088562, tv_loss: 0.030073381960392\n",
      "iteration 680, dc_loss: 0.16797827184200287, tv_loss: 0.02991560474038124\n",
      "iteration 681, dc_loss: 0.16728274524211884, tv_loss: 0.030222931876778603\n",
      "iteration 682, dc_loss: 0.1669624298810959, tv_loss: 0.029965460300445557\n",
      "iteration 683, dc_loss: 0.1662880927324295, tv_loss: 0.030066099017858505\n",
      "iteration 684, dc_loss: 0.1659182608127594, tv_loss: 0.02994660474359989\n",
      "iteration 685, dc_loss: 0.16537365317344666, tv_loss: 0.03017968125641346\n",
      "iteration 686, dc_loss: 0.16500328481197357, tv_loss: 0.030126383528113365\n",
      "iteration 687, dc_loss: 0.16460366547107697, tv_loss: 0.03011331520974636\n",
      "iteration 688, dc_loss: 0.1642305999994278, tv_loss: 0.030017053708434105\n",
      "iteration 689, dc_loss: 0.16374816000461578, tv_loss: 0.030229343101382256\n",
      "iteration 690, dc_loss: 0.16317813098430634, tv_loss: 0.030087560415267944\n",
      "iteration 691, dc_loss: 0.1624031811952591, tv_loss: 0.030217094346880913\n",
      "iteration 692, dc_loss: 0.1621156632900238, tv_loss: 0.02996710315346718\n",
      "iteration 693, dc_loss: 0.16143502295017242, tv_loss: 0.03027527593076229\n",
      "iteration 694, dc_loss: 0.16120217740535736, tv_loss: 0.03009396232664585\n",
      "iteration 695, dc_loss: 0.16039304435253143, tv_loss: 0.03026387095451355\n",
      "iteration 696, dc_loss: 0.159899041056633, tv_loss: 0.030113020911812782\n",
      "iteration 697, dc_loss: 0.15939579904079437, tv_loss: 0.030189884826540947\n",
      "iteration 698, dc_loss: 0.15892843902111053, tv_loss: 0.030375709757208824\n",
      "iteration 699, dc_loss: 0.1586005538702011, tv_loss: 0.03013623133301735\n",
      "iteration 700, dc_loss: 0.15796640515327454, tv_loss: 0.030360758304595947\n",
      "iteration 701, dc_loss: 0.15764856338500977, tv_loss: 0.03023418039083481\n",
      "iteration 702, dc_loss: 0.15690293908119202, tv_loss: 0.03035554103553295\n",
      "iteration 703, dc_loss: 0.15667106211185455, tv_loss: 0.030196061357855797\n",
      "iteration 704, dc_loss: 0.15602977573871613, tv_loss: 0.030419358983635902\n",
      "iteration 705, dc_loss: 0.15561968088150024, tv_loss: 0.030257463455200195\n",
      "iteration 706, dc_loss: 0.15523067116737366, tv_loss: 0.03023555874824524\n",
      "iteration 707, dc_loss: 0.15462429821491241, tv_loss: 0.03047950193285942\n",
      "iteration 708, dc_loss: 0.15425744652748108, tv_loss: 0.030240103602409363\n",
      "iteration 709, dc_loss: 0.15369798243045807, tv_loss: 0.030485982075333595\n",
      "iteration 710, dc_loss: 0.1533547192811966, tv_loss: 0.030287038534879684\n",
      "iteration 711, dc_loss: 0.15279141068458557, tv_loss: 0.030394908040761948\n",
      "iteration 712, dc_loss: 0.15247155725955963, tv_loss: 0.030417274683713913\n",
      "iteration 713, dc_loss: 0.15194670855998993, tv_loss: 0.030353054404258728\n",
      "iteration 714, dc_loss: 0.1515311896800995, tv_loss: 0.03038572520017624\n",
      "iteration 715, dc_loss: 0.15111427009105682, tv_loss: 0.03053201362490654\n",
      "iteration 716, dc_loss: 0.15076200664043427, tv_loss: 0.030321475118398666\n",
      "iteration 717, dc_loss: 0.15026718378067017, tv_loss: 0.030698103830218315\n",
      "iteration 718, dc_loss: 0.15021559596061707, tv_loss: 0.030266789719462395\n",
      "iteration 719, dc_loss: 0.14977163076400757, tv_loss: 0.030639780685305595\n",
      "iteration 720, dc_loss: 0.14993658661842346, tv_loss: 0.0302583035081625\n",
      "iteration 721, dc_loss: 0.1492721289396286, tv_loss: 0.0307143684476614\n",
      "iteration 722, dc_loss: 0.14925752580165863, tv_loss: 0.030276481062173843\n",
      "iteration 723, dc_loss: 0.14791052043437958, tv_loss: 0.030719855800271034\n",
      "iteration 724, dc_loss: 0.14743752777576447, tv_loss: 0.0304111335426569\n",
      "iteration 725, dc_loss: 0.1467069387435913, tv_loss: 0.030596677213907242\n",
      "iteration 726, dc_loss: 0.1465415060520172, tv_loss: 0.030538372695446014\n",
      "iteration 727, dc_loss: 0.1464911848306656, tv_loss: 0.030491283163428307\n",
      "iteration 728, dc_loss: 0.14562322199344635, tv_loss: 0.030719537287950516\n",
      "iteration 729, dc_loss: 0.14535927772521973, tv_loss: 0.030485616996884346\n",
      "iteration 730, dc_loss: 0.1445397287607193, tv_loss: 0.0305978674441576\n",
      "iteration 731, dc_loss: 0.14429375529289246, tv_loss: 0.03076651319861412\n",
      "iteration 732, dc_loss: 0.14412802457809448, tv_loss: 0.030473828315734863\n",
      "iteration 733, dc_loss: 0.14347323775291443, tv_loss: 0.03076835162937641\n",
      "iteration 734, dc_loss: 0.14311537146568298, tv_loss: 0.030490655452013016\n",
      "iteration 735, dc_loss: 0.14244115352630615, tv_loss: 0.030703868716955185\n",
      "iteration 736, dc_loss: 0.14227961003780365, tv_loss: 0.0306283850222826\n",
      "iteration 737, dc_loss: 0.14193780720233917, tv_loss: 0.030490262433886528\n",
      "iteration 738, dc_loss: 0.1413765400648117, tv_loss: 0.03081652894616127\n",
      "iteration 739, dc_loss: 0.1409563571214676, tv_loss: 0.03052699565887451\n",
      "iteration 740, dc_loss: 0.1404661238193512, tv_loss: 0.03068714588880539\n",
      "iteration 741, dc_loss: 0.14014844596385956, tv_loss: 0.03067493438720703\n",
      "iteration 742, dc_loss: 0.1397525668144226, tv_loss: 0.030616646632552147\n",
      "iteration 743, dc_loss: 0.13936814665794373, tv_loss: 0.030811108648777008\n",
      "iteration 744, dc_loss: 0.1388714760541916, tv_loss: 0.030648747459053993\n",
      "iteration 745, dc_loss: 0.13844238221645355, tv_loss: 0.03085187077522278\n",
      "iteration 746, dc_loss: 0.13803306221961975, tv_loss: 0.030738193541765213\n",
      "iteration 747, dc_loss: 0.13780763745307922, tv_loss: 0.030792493373155594\n",
      "iteration 748, dc_loss: 0.1372927725315094, tv_loss: 0.030770815908908844\n",
      "iteration 749, dc_loss: 0.13686184585094452, tv_loss: 0.030903689563274384\n",
      "iteration 750, dc_loss: 0.13650593161582947, tv_loss: 0.03070818819105625\n",
      "iteration 751, dc_loss: 0.13611078262329102, tv_loss: 0.03095012530684471\n",
      "iteration 752, dc_loss: 0.135801762342453, tv_loss: 0.030702246353030205\n",
      "iteration 753, dc_loss: 0.13533727824687958, tv_loss: 0.03106866218149662\n",
      "iteration 754, dc_loss: 0.13499657809734344, tv_loss: 0.03085377626121044\n",
      "iteration 755, dc_loss: 0.13464277982711792, tv_loss: 0.03096102364361286\n",
      "iteration 756, dc_loss: 0.13427530229091644, tv_loss: 0.03079255297780037\n",
      "iteration 757, dc_loss: 0.13385871052742004, tv_loss: 0.031006140634417534\n",
      "iteration 758, dc_loss: 0.13346189260482788, tv_loss: 0.030869213864207268\n",
      "iteration 759, dc_loss: 0.13312183320522308, tv_loss: 0.03099079057574272\n",
      "iteration 760, dc_loss: 0.13276395201683044, tv_loss: 0.03084753267467022\n",
      "iteration 761, dc_loss: 0.13235028088092804, tv_loss: 0.031028369441628456\n",
      "iteration 762, dc_loss: 0.13199307024478912, tv_loss: 0.03092866763472557\n",
      "iteration 763, dc_loss: 0.13160304725170135, tv_loss: 0.030971771106123924\n",
      "iteration 764, dc_loss: 0.13125497102737427, tv_loss: 0.03091001883149147\n",
      "iteration 765, dc_loss: 0.13084788620471954, tv_loss: 0.03099624067544937\n",
      "iteration 766, dc_loss: 0.1304989904165268, tv_loss: 0.030937964096665382\n",
      "iteration 767, dc_loss: 0.1301707774400711, tv_loss: 0.03097262606024742\n",
      "iteration 768, dc_loss: 0.12977007031440735, tv_loss: 0.030958188697695732\n",
      "iteration 769, dc_loss: 0.12939788401126862, tv_loss: 0.031017934903502464\n",
      "iteration 770, dc_loss: 0.12903566658496857, tv_loss: 0.030967721715569496\n",
      "iteration 771, dc_loss: 0.12872087955474854, tv_loss: 0.030979255214333534\n",
      "iteration 772, dc_loss: 0.12833885848522186, tv_loss: 0.030947860330343246\n",
      "iteration 773, dc_loss: 0.12796464562416077, tv_loss: 0.031053194776177406\n",
      "iteration 774, dc_loss: 0.1276031881570816, tv_loss: 0.031009111553430557\n",
      "iteration 775, dc_loss: 0.127360999584198, tv_loss: 0.030986277386546135\n",
      "iteration 776, dc_loss: 0.12692661583423615, tv_loss: 0.031034233048558235\n",
      "iteration 777, dc_loss: 0.12671609222888947, tv_loss: 0.03099703975021839\n",
      "iteration 778, dc_loss: 0.12625767290592194, tv_loss: 0.03112693689763546\n",
      "iteration 779, dc_loss: 0.12635545432567596, tv_loss: 0.030872520059347153\n",
      "iteration 780, dc_loss: 0.1258329153060913, tv_loss: 0.031242219731211662\n",
      "iteration 781, dc_loss: 0.12619845569133759, tv_loss: 0.030814053490757942\n",
      "iteration 782, dc_loss: 0.12561576068401337, tv_loss: 0.03142653778195381\n",
      "iteration 783, dc_loss: 0.12608258426189423, tv_loss: 0.03065718151628971\n",
      "iteration 784, dc_loss: 0.12514041364192963, tv_loss: 0.031380023807287216\n",
      "iteration 785, dc_loss: 0.12507180869579315, tv_loss: 0.03089350089430809\n",
      "iteration 786, dc_loss: 0.12416009604930878, tv_loss: 0.031100573018193245\n",
      "iteration 787, dc_loss: 0.12364021688699722, tv_loss: 0.031239114701747894\n",
      "iteration 788, dc_loss: 0.12359602749347687, tv_loss: 0.03080449253320694\n",
      "iteration 789, dc_loss: 0.12283197045326233, tv_loss: 0.03140755370259285\n",
      "iteration 790, dc_loss: 0.12271876633167267, tv_loss: 0.030934104695916176\n",
      "iteration 791, dc_loss: 0.12227071821689606, tv_loss: 0.03110009990632534\n",
      "iteration 792, dc_loss: 0.12176765501499176, tv_loss: 0.0311775803565979\n",
      "iteration 793, dc_loss: 0.12166760116815567, tv_loss: 0.031015345826745033\n",
      "iteration 794, dc_loss: 0.12096960097551346, tv_loss: 0.031227372586727142\n",
      "iteration 795, dc_loss: 0.12098319083452225, tv_loss: 0.031068997457623482\n",
      "iteration 796, dc_loss: 0.12051279842853546, tv_loss: 0.031014468520879745\n",
      "iteration 797, dc_loss: 0.11996254324913025, tv_loss: 0.031346388161182404\n",
      "iteration 798, dc_loss: 0.11982062458992004, tv_loss: 0.031004412099719048\n",
      "iteration 799, dc_loss: 0.11947428435087204, tv_loss: 0.03125326707959175\n",
      "iteration 800, dc_loss: 0.1190914586186409, tv_loss: 0.0311525147408247\n",
      "iteration 801, dc_loss: 0.11884425580501556, tv_loss: 0.03113033063709736\n",
      "iteration 802, dc_loss: 0.11832574009895325, tv_loss: 0.031243031844496727\n",
      "iteration 803, dc_loss: 0.11819799244403839, tv_loss: 0.031170114874839783\n",
      "iteration 804, dc_loss: 0.11799293756484985, tv_loss: 0.031062938272953033\n",
      "iteration 805, dc_loss: 0.11765975505113602, tv_loss: 0.03113488107919693\n",
      "iteration 806, dc_loss: 0.11735933274030685, tv_loss: 0.031172899529337883\n",
      "iteration 807, dc_loss: 0.11719710379838943, tv_loss: 0.031067144125699997\n",
      "iteration 808, dc_loss: 0.11691462248563766, tv_loss: 0.03111843951046467\n",
      "iteration 809, dc_loss: 0.1165747195482254, tv_loss: 0.031191984191536903\n",
      "iteration 810, dc_loss: 0.11642372608184814, tv_loss: 0.031089169904589653\n",
      "iteration 811, dc_loss: 0.11613739281892776, tv_loss: 0.031167881563305855\n",
      "iteration 812, dc_loss: 0.11585645377635956, tv_loss: 0.031132416799664497\n",
      "iteration 813, dc_loss: 0.11567205935716629, tv_loss: 0.03117360547184944\n",
      "iteration 814, dc_loss: 0.11535516381263733, tv_loss: 0.031152402982115746\n",
      "iteration 815, dc_loss: 0.11512826383113861, tv_loss: 0.03123452514410019\n",
      "iteration 816, dc_loss: 0.1149386391043663, tv_loss: 0.031094398349523544\n",
      "iteration 817, dc_loss: 0.11466053128242493, tv_loss: 0.03123543970286846\n",
      "iteration 818, dc_loss: 0.11437136679887772, tv_loss: 0.03118733875453472\n",
      "iteration 819, dc_loss: 0.11421389877796173, tv_loss: 0.031230393797159195\n",
      "iteration 820, dc_loss: 0.11393469572067261, tv_loss: 0.031162723898887634\n",
      "iteration 821, dc_loss: 0.11369023472070694, tv_loss: 0.031291838735342026\n",
      "iteration 822, dc_loss: 0.11344179511070251, tv_loss: 0.031213944777846336\n",
      "iteration 823, dc_loss: 0.11323682963848114, tv_loss: 0.031287018209695816\n",
      "iteration 824, dc_loss: 0.11298556625843048, tv_loss: 0.031249646097421646\n",
      "iteration 825, dc_loss: 0.11276097595691681, tv_loss: 0.03127817437052727\n",
      "iteration 826, dc_loss: 0.11248931288719177, tv_loss: 0.031293559819459915\n",
      "iteration 827, dc_loss: 0.11228817701339722, tv_loss: 0.03125917166471481\n",
      "iteration 828, dc_loss: 0.11205879598855972, tv_loss: 0.031230716034770012\n",
      "iteration 829, dc_loss: 0.11181814968585968, tv_loss: 0.03128731995820999\n",
      "iteration 830, dc_loss: 0.11156082153320312, tv_loss: 0.03125671297311783\n",
      "iteration 831, dc_loss: 0.11138948798179626, tv_loss: 0.0312945693731308\n",
      "iteration 832, dc_loss: 0.11112090200185776, tv_loss: 0.031260691583156586\n",
      "iteration 833, dc_loss: 0.11089004576206207, tv_loss: 0.03136136010289192\n",
      "iteration 834, dc_loss: 0.11065997928380966, tv_loss: 0.03130905330181122\n",
      "iteration 835, dc_loss: 0.11045646667480469, tv_loss: 0.031332433223724365\n",
      "iteration 836, dc_loss: 0.11020764708518982, tv_loss: 0.03132664039731026\n",
      "iteration 837, dc_loss: 0.10999643802642822, tv_loss: 0.03132207691669464\n",
      "iteration 838, dc_loss: 0.10974881052970886, tv_loss: 0.031321246176958084\n",
      "iteration 839, dc_loss: 0.10955983400344849, tv_loss: 0.03132738173007965\n",
      "iteration 840, dc_loss: 0.10932696610689163, tv_loss: 0.0312887467443943\n",
      "iteration 841, dc_loss: 0.10908296704292297, tv_loss: 0.03139247000217438\n",
      "iteration 842, dc_loss: 0.1088457927107811, tv_loss: 0.031347811222076416\n",
      "iteration 843, dc_loss: 0.10867714136838913, tv_loss: 0.03137033432722092\n",
      "iteration 844, dc_loss: 0.10843220353126526, tv_loss: 0.03134904429316521\n",
      "iteration 845, dc_loss: 0.10820829123258591, tv_loss: 0.031394556164741516\n",
      "iteration 846, dc_loss: 0.10796966403722763, tv_loss: 0.031381867825984955\n",
      "iteration 847, dc_loss: 0.1077902764081955, tv_loss: 0.03137342631816864\n",
      "iteration 848, dc_loss: 0.1075696274638176, tv_loss: 0.03133561834692955\n",
      "iteration 849, dc_loss: 0.10733771324157715, tv_loss: 0.031412649899721146\n",
      "iteration 850, dc_loss: 0.10709597915410995, tv_loss: 0.03138520568609238\n",
      "iteration 851, dc_loss: 0.10693374276161194, tv_loss: 0.03140920028090477\n",
      "iteration 852, dc_loss: 0.10669654607772827, tv_loss: 0.03137660026550293\n",
      "iteration 853, dc_loss: 0.10647276788949966, tv_loss: 0.031449224799871445\n",
      "iteration 854, dc_loss: 0.1062430590391159, tv_loss: 0.03142380714416504\n",
      "iteration 855, dc_loss: 0.10606739670038223, tv_loss: 0.03142661228775978\n",
      "iteration 856, dc_loss: 0.10585141181945801, tv_loss: 0.03139762580394745\n",
      "iteration 857, dc_loss: 0.1056274026632309, tv_loss: 0.03144431486725807\n",
      "iteration 858, dc_loss: 0.10539215058088303, tv_loss: 0.03143101558089256\n",
      "iteration 859, dc_loss: 0.10522935539484024, tv_loss: 0.031439170241355896\n",
      "iteration 860, dc_loss: 0.10500285774469376, tv_loss: 0.03140487149357796\n",
      "iteration 861, dc_loss: 0.10478182882070541, tv_loss: 0.03148917481303215\n",
      "iteration 862, dc_loss: 0.10455800592899323, tv_loss: 0.03145578131079674\n",
      "iteration 863, dc_loss: 0.10438574850559235, tv_loss: 0.031477201730012894\n",
      "iteration 864, dc_loss: 0.1041751280426979, tv_loss: 0.03144362196326256\n",
      "iteration 865, dc_loss: 0.10395834594964981, tv_loss: 0.03148467838764191\n",
      "iteration 866, dc_loss: 0.10372805595397949, tv_loss: 0.03147711604833603\n",
      "iteration 867, dc_loss: 0.10356004536151886, tv_loss: 0.03148406744003296\n",
      "iteration 868, dc_loss: 0.10335142910480499, tv_loss: 0.031449027359485626\n",
      "iteration 869, dc_loss: 0.10313493758440018, tv_loss: 0.031513385474681854\n",
      "iteration 870, dc_loss: 0.1029156818985939, tv_loss: 0.031482916325330734\n",
      "iteration 871, dc_loss: 0.10274845361709595, tv_loss: 0.031515125185251236\n",
      "iteration 872, dc_loss: 0.10254178196191788, tv_loss: 0.031472254544496536\n",
      "iteration 873, dc_loss: 0.10232701152563095, tv_loss: 0.03153754025697708\n",
      "iteration 874, dc_loss: 0.10210337489843369, tv_loss: 0.03152434155344963\n",
      "iteration 875, dc_loss: 0.10193624347448349, tv_loss: 0.03152967989444733\n",
      "iteration 876, dc_loss: 0.1017392948269844, tv_loss: 0.031497035175561905\n",
      "iteration 877, dc_loss: 0.10152920335531235, tv_loss: 0.03154122456908226\n",
      "iteration 878, dc_loss: 0.10130970180034637, tv_loss: 0.03152627870440483\n",
      "iteration 879, dc_loss: 0.10114540159702301, tv_loss: 0.0315464623272419\n",
      "iteration 880, dc_loss: 0.10095297545194626, tv_loss: 0.03149395436048508\n",
      "iteration 881, dc_loss: 0.10073689371347427, tv_loss: 0.03157613426446915\n",
      "iteration 882, dc_loss: 0.10051962733268738, tv_loss: 0.03155091032385826\n",
      "iteration 883, dc_loss: 0.10035741329193115, tv_loss: 0.03157855197787285\n",
      "iteration 884, dc_loss: 0.10016787797212601, tv_loss: 0.03153492510318756\n",
      "iteration 885, dc_loss: 0.09995809942483902, tv_loss: 0.03158826753497124\n",
      "iteration 886, dc_loss: 0.09973981231451035, tv_loss: 0.031580183655023575\n",
      "iteration 887, dc_loss: 0.09958553314208984, tv_loss: 0.03158285468816757\n",
      "iteration 888, dc_loss: 0.09940238296985626, tv_loss: 0.031536590307950974\n",
      "iteration 889, dc_loss: 0.09918555617332458, tv_loss: 0.0316070131957531\n",
      "iteration 890, dc_loss: 0.09897257387638092, tv_loss: 0.031589411199092865\n",
      "iteration 891, dc_loss: 0.09881408512592316, tv_loss: 0.0316118448972702\n",
      "iteration 892, dc_loss: 0.09863532334566116, tv_loss: 0.03156079724431038\n",
      "iteration 893, dc_loss: 0.0984235405921936, tv_loss: 0.031630489975214005\n",
      "iteration 894, dc_loss: 0.0982123389840126, tv_loss: 0.03161453455686569\n",
      "iteration 895, dc_loss: 0.09806666523218155, tv_loss: 0.03162204846739769\n",
      "iteration 896, dc_loss: 0.09788680076599121, tv_loss: 0.03157413750886917\n",
      "iteration 897, dc_loss: 0.09766700863838196, tv_loss: 0.031655073165893555\n",
      "iteration 898, dc_loss: 0.09746216982603073, tv_loss: 0.03163667023181915\n",
      "iteration 899, dc_loss: 0.09731535613536835, tv_loss: 0.031641993671655655\n",
      "iteration 900, dc_loss: 0.09714222699403763, tv_loss: 0.031594760715961456\n",
      "iteration 901, dc_loss: 0.09692437201738358, tv_loss: 0.03167020156979561\n",
      "iteration 902, dc_loss: 0.09672027826309204, tv_loss: 0.031653258949518204\n",
      "iteration 903, dc_loss: 0.09658537060022354, tv_loss: 0.031652841717004776\n",
      "iteration 904, dc_loss: 0.09640520066022873, tv_loss: 0.03160921111702919\n",
      "iteration 905, dc_loss: 0.0961889773607254, tv_loss: 0.03169351816177368\n",
      "iteration 906, dc_loss: 0.09599019587039948, tv_loss: 0.03167227283120155\n",
      "iteration 907, dc_loss: 0.09585423767566681, tv_loss: 0.031674984842538834\n",
      "iteration 908, dc_loss: 0.09567716717720032, tv_loss: 0.03163542598485947\n",
      "iteration 909, dc_loss: 0.0954633355140686, tv_loss: 0.031710460782051086\n",
      "iteration 910, dc_loss: 0.09526733309030533, tv_loss: 0.03168823942542076\n",
      "iteration 911, dc_loss: 0.09514326602220535, tv_loss: 0.03168575093150139\n",
      "iteration 912, dc_loss: 0.09495726227760315, tv_loss: 0.03165121749043465\n",
      "iteration 913, dc_loss: 0.09474906325340271, tv_loss: 0.03173219785094261\n",
      "iteration 914, dc_loss: 0.09455399960279465, tv_loss: 0.03171096742153168\n",
      "iteration 915, dc_loss: 0.09443926066160202, tv_loss: 0.03170013427734375\n",
      "iteration 916, dc_loss: 0.0942486971616745, tv_loss: 0.03168153762817383\n",
      "iteration 917, dc_loss: 0.09406525641679764, tv_loss: 0.031738102436065674\n",
      "iteration 918, dc_loss: 0.09386401623487473, tv_loss: 0.03173689916729927\n",
      "iteration 919, dc_loss: 0.09380537271499634, tv_loss: 0.031699057668447495\n",
      "iteration 920, dc_loss: 0.09362445771694183, tv_loss: 0.03171965852379799\n",
      "iteration 921, dc_loss: 0.09355935454368591, tv_loss: 0.031732212752103806\n",
      "iteration 922, dc_loss: 0.09343724697828293, tv_loss: 0.03180518373847008\n",
      "iteration 923, dc_loss: 0.09356001019477844, tv_loss: 0.03167196363210678\n",
      "iteration 924, dc_loss: 0.09352043271064758, tv_loss: 0.031798746436834335\n",
      "iteration 925, dc_loss: 0.09334450215101242, tv_loss: 0.0317060761153698\n",
      "iteration 926, dc_loss: 0.09287601709365845, tv_loss: 0.031834788620471954\n",
      "iteration 927, dc_loss: 0.09263370931148529, tv_loss: 0.03167487680912018\n",
      "iteration 928, dc_loss: 0.09220552444458008, tv_loss: 0.0317520834505558\n",
      "iteration 929, dc_loss: 0.09208032488822937, tv_loss: 0.031781625002622604\n",
      "iteration 930, dc_loss: 0.09197250753641129, tv_loss: 0.03175162896513939\n",
      "iteration 931, dc_loss: 0.09191890805959702, tv_loss: 0.0317830853164196\n",
      "iteration 932, dc_loss: 0.09171346575021744, tv_loss: 0.031690336763858795\n",
      "iteration 933, dc_loss: 0.09132913500070572, tv_loss: 0.0318925678730011\n",
      "iteration 934, dc_loss: 0.09123887121677399, tv_loss: 0.03171461448073387\n",
      "iteration 935, dc_loss: 0.09105544537305832, tv_loss: 0.03182145580649376\n",
      "iteration 936, dc_loss: 0.09097141772508621, tv_loss: 0.03173397481441498\n",
      "iteration 937, dc_loss: 0.09077751636505127, tv_loss: 0.0317985862493515\n",
      "iteration 938, dc_loss: 0.09048164635896683, tv_loss: 0.03184808790683746\n",
      "iteration 939, dc_loss: 0.09049433469772339, tv_loss: 0.0316925048828125\n",
      "iteration 940, dc_loss: 0.09013336896896362, tv_loss: 0.03185165300965309\n",
      "iteration 941, dc_loss: 0.09006890654563904, tv_loss: 0.031783849000930786\n",
      "iteration 942, dc_loss: 0.08989110589027405, tv_loss: 0.03178144991397858\n",
      "iteration 943, dc_loss: 0.08967217803001404, tv_loss: 0.03184591606259346\n",
      "iteration 944, dc_loss: 0.08961715549230576, tv_loss: 0.031728412955999374\n",
      "iteration 945, dc_loss: 0.08930262923240662, tv_loss: 0.03187454119324684\n",
      "iteration 946, dc_loss: 0.0892619639635086, tv_loss: 0.03173951432108879\n",
      "iteration 947, dc_loss: 0.08904480189085007, tv_loss: 0.03182227537035942\n",
      "iteration 948, dc_loss: 0.0888623520731926, tv_loss: 0.03181533142924309\n",
      "iteration 949, dc_loss: 0.08878667652606964, tv_loss: 0.0317772701382637\n",
      "iteration 950, dc_loss: 0.08849772810935974, tv_loss: 0.03186894208192825\n",
      "iteration 951, dc_loss: 0.08847701549530029, tv_loss: 0.0317750982940197\n",
      "iteration 952, dc_loss: 0.08820436894893646, tv_loss: 0.03185151144862175\n",
      "iteration 953, dc_loss: 0.08812005817890167, tv_loss: 0.0318126417696476\n",
      "iteration 954, dc_loss: 0.08789949119091034, tv_loss: 0.03183303028345108\n",
      "iteration 955, dc_loss: 0.08773225545883179, tv_loss: 0.03188793733716011\n",
      "iteration 956, dc_loss: 0.087651826441288, tv_loss: 0.03178022429347038\n",
      "iteration 957, dc_loss: 0.08741702139377594, tv_loss: 0.031912028789520264\n",
      "iteration 958, dc_loss: 0.08732685446739197, tv_loss: 0.031823307275772095\n",
      "iteration 959, dc_loss: 0.08710739761590958, tv_loss: 0.03189048543572426\n",
      "iteration 960, dc_loss: 0.08701139688491821, tv_loss: 0.03181377798318863\n",
      "iteration 961, dc_loss: 0.08680756390094757, tv_loss: 0.03187503293156624\n",
      "iteration 962, dc_loss: 0.08665051311254501, tv_loss: 0.03186246007680893\n",
      "iteration 963, dc_loss: 0.08652600646018982, tv_loss: 0.031854093074798584\n",
      "iteration 964, dc_loss: 0.08634047955274582, tv_loss: 0.031880054622888565\n",
      "iteration 965, dc_loss: 0.08625772595405579, tv_loss: 0.03182922676205635\n",
      "iteration 966, dc_loss: 0.0860000029206276, tv_loss: 0.03191645070910454\n",
      "iteration 967, dc_loss: 0.08596145361661911, tv_loss: 0.03183075040578842\n",
      "iteration 968, dc_loss: 0.08572525531053543, tv_loss: 0.031892385333776474\n",
      "iteration 969, dc_loss: 0.08563315123319626, tv_loss: 0.03186650201678276\n",
      "iteration 970, dc_loss: 0.08543252944946289, tv_loss: 0.03189259395003319\n",
      "iteration 971, dc_loss: 0.08530903607606888, tv_loss: 0.03189747408032417\n",
      "iteration 972, dc_loss: 0.08517344295978546, tv_loss: 0.031871408224105835\n",
      "iteration 973, dc_loss: 0.08502005785703659, tv_loss: 0.03191015124320984\n",
      "iteration 974, dc_loss: 0.08486874401569366, tv_loss: 0.03190207853913307\n",
      "iteration 975, dc_loss: 0.08475594967603683, tv_loss: 0.03190644085407257\n",
      "iteration 976, dc_loss: 0.08460181951522827, tv_loss: 0.031923793256282806\n",
      "iteration 977, dc_loss: 0.08451589196920395, tv_loss: 0.031896915286779404\n",
      "iteration 978, dc_loss: 0.08434173464775085, tv_loss: 0.03196832165122032\n",
      "iteration 979, dc_loss: 0.08434334397315979, tv_loss: 0.03186006844043732\n",
      "iteration 980, dc_loss: 0.0841563493013382, tv_loss: 0.03199552372097969\n",
      "iteration 981, dc_loss: 0.08421333134174347, tv_loss: 0.03184693679213524\n",
      "iteration 982, dc_loss: 0.0839814618229866, tv_loss: 0.03205413371324539\n",
      "iteration 983, dc_loss: 0.0840359628200531, tv_loss: 0.03182607144117355\n",
      "iteration 984, dc_loss: 0.08370018005371094, tv_loss: 0.032054342329502106\n",
      "iteration 985, dc_loss: 0.08367277681827545, tv_loss: 0.03181346505880356\n",
      "iteration 986, dc_loss: 0.08318586647510529, tv_loss: 0.032079312950372696\n",
      "iteration 987, dc_loss: 0.08315593749284744, tv_loss: 0.03183512017130852\n",
      "iteration 988, dc_loss: 0.08278349041938782, tv_loss: 0.03202323243021965\n",
      "iteration 989, dc_loss: 0.08280012011528015, tv_loss: 0.031880684196949005\n",
      "iteration 990, dc_loss: 0.08261507749557495, tv_loss: 0.03200109302997589\n",
      "iteration 991, dc_loss: 0.08252473175525665, tv_loss: 0.03198898583650589\n",
      "iteration 992, dc_loss: 0.0824311152100563, tv_loss: 0.031917754560709\n",
      "iteration 993, dc_loss: 0.08217500895261765, tv_loss: 0.03199918568134308\n",
      "iteration 994, dc_loss: 0.08207245916128159, tv_loss: 0.03191014751791954\n",
      "iteration 995, dc_loss: 0.08180537074804306, tv_loss: 0.032002151012420654\n",
      "iteration 996, dc_loss: 0.08172279596328735, tv_loss: 0.031938545405864716\n",
      "iteration 997, dc_loss: 0.08158068358898163, tv_loss: 0.03196955472230911\n",
      "iteration 998, dc_loss: 0.08145540207624435, tv_loss: 0.03200686722993851\n",
      "iteration 999, dc_loss: 0.08141263574361801, tv_loss: 0.03193458169698715\n",
      "iteration 1000, dc_loss: 0.08114104717969894, tv_loss: 0.03207052871584892\n",
      "iteration 1001, dc_loss: 0.0811375230550766, tv_loss: 0.03191125765442848\n",
      "iteration 1002, dc_loss: 0.08081454038619995, tv_loss: 0.0320819690823555\n",
      "iteration 1003, dc_loss: 0.08085460960865021, tv_loss: 0.031915318220853806\n",
      "iteration 1004, dc_loss: 0.08057533949613571, tv_loss: 0.03210573270916939\n",
      "iteration 1005, dc_loss: 0.08064545691013336, tv_loss: 0.03194178640842438\n",
      "iteration 1006, dc_loss: 0.08038081973791122, tv_loss: 0.0320974662899971\n",
      "iteration 1007, dc_loss: 0.08041590452194214, tv_loss: 0.03194174915552139\n",
      "iteration 1008, dc_loss: 0.08010359853506088, tv_loss: 0.032102711498737335\n",
      "iteration 1009, dc_loss: 0.08008035272359848, tv_loss: 0.03197052702307701\n",
      "iteration 1010, dc_loss: 0.07977594435214996, tv_loss: 0.03209373727440834\n",
      "iteration 1011, dc_loss: 0.07971702516078949, tv_loss: 0.031981345266103745\n",
      "iteration 1012, dc_loss: 0.0794903114438057, tv_loss: 0.03206424415111542\n",
      "iteration 1013, dc_loss: 0.07940483093261719, tv_loss: 0.032020214945077896\n",
      "iteration 1014, dc_loss: 0.07925700396299362, tv_loss: 0.032045893371105194\n",
      "iteration 1015, dc_loss: 0.07912179082632065, tv_loss: 0.03205681964755058\n",
      "iteration 1016, dc_loss: 0.07900803536176682, tv_loss: 0.032034147530794144\n",
      "iteration 1017, dc_loss: 0.07885473966598511, tv_loss: 0.03204948455095291\n",
      "iteration 1018, dc_loss: 0.07871853560209274, tv_loss: 0.03206738084554672\n",
      "iteration 1019, dc_loss: 0.07863454520702362, tv_loss: 0.03205280378460884\n",
      "iteration 1020, dc_loss: 0.07842868566513062, tv_loss: 0.03211378678679466\n",
      "iteration 1021, dc_loss: 0.07842282950878143, tv_loss: 0.03199389949440956\n",
      "iteration 1022, dc_loss: 0.07816773653030396, tv_loss: 0.03215654194355011\n",
      "iteration 1023, dc_loss: 0.07823295146226883, tv_loss: 0.031977422535419464\n",
      "iteration 1024, dc_loss: 0.07793274521827698, tv_loss: 0.032184626907110214\n",
      "iteration 1025, dc_loss: 0.07808925956487656, tv_loss: 0.03195040300488472\n",
      "iteration 1026, dc_loss: 0.07771961390972137, tv_loss: 0.03228265047073364\n",
      "iteration 1027, dc_loss: 0.07799538969993591, tv_loss: 0.03191274777054787\n",
      "iteration 1028, dc_loss: 0.0775635614991188, tv_loss: 0.03229784592986107\n",
      "iteration 1029, dc_loss: 0.07786872237920761, tv_loss: 0.03190034627914429\n",
      "iteration 1030, dc_loss: 0.07738546282052994, tv_loss: 0.032325293868780136\n",
      "iteration 1031, dc_loss: 0.07756483554840088, tv_loss: 0.03193984180688858\n",
      "iteration 1032, dc_loss: 0.07709681242704391, tv_loss: 0.03223744034767151\n",
      "iteration 1033, dc_loss: 0.07706711441278458, tv_loss: 0.03203928470611572\n",
      "iteration 1034, dc_loss: 0.07675832509994507, tv_loss: 0.0321490541100502\n",
      "iteration 1035, dc_loss: 0.0766119435429573, tv_loss: 0.03213054686784744\n",
      "iteration 1036, dc_loss: 0.07657229900360107, tv_loss: 0.032065100967884064\n",
      "iteration 1037, dc_loss: 0.07629433274269104, tv_loss: 0.03224801644682884\n",
      "iteration 1038, dc_loss: 0.07642216980457306, tv_loss: 0.03199966251850128\n",
      "iteration 1039, dc_loss: 0.07602372765541077, tv_loss: 0.032274242490530014\n",
      "iteration 1040, dc_loss: 0.07614468038082123, tv_loss: 0.032009635120630264\n",
      "iteration 1041, dc_loss: 0.0757722333073616, tv_loss: 0.032240986824035645\n",
      "iteration 1042, dc_loss: 0.0758020430803299, tv_loss: 0.032077573239803314\n",
      "iteration 1043, dc_loss: 0.07559297233819962, tv_loss: 0.03216787427663803\n",
      "iteration 1044, dc_loss: 0.07545918971300125, tv_loss: 0.03218218311667442\n",
      "iteration 1045, dc_loss: 0.07541381567716599, tv_loss: 0.03210671991109848\n",
      "iteration 1046, dc_loss: 0.07518262416124344, tv_loss: 0.03222348168492317\n",
      "iteration 1047, dc_loss: 0.07520197331905365, tv_loss: 0.03208493813872337\n",
      "iteration 1048, dc_loss: 0.07495633512735367, tv_loss: 0.03222404047846794\n",
      "iteration 1049, dc_loss: 0.07493843883275986, tv_loss: 0.03214098885655403\n",
      "iteration 1050, dc_loss: 0.07479534298181534, tv_loss: 0.0321890190243721\n",
      "iteration 1051, dc_loss: 0.07467875629663467, tv_loss: 0.032203637063503265\n",
      "iteration 1052, dc_loss: 0.07465820014476776, tv_loss: 0.03213458135724068\n",
      "iteration 1053, dc_loss: 0.0744486004114151, tv_loss: 0.032253626734018326\n",
      "iteration 1054, dc_loss: 0.07449474930763245, tv_loss: 0.032118480652570724\n",
      "iteration 1055, dc_loss: 0.07420028001070023, tv_loss: 0.03228382021188736\n",
      "iteration 1056, dc_loss: 0.07425088435411453, tv_loss: 0.03211930766701698\n",
      "iteration 1057, dc_loss: 0.07395300269126892, tv_loss: 0.03228835389018059\n",
      "iteration 1058, dc_loss: 0.07394995540380478, tv_loss: 0.03214496001601219\n",
      "iteration 1059, dc_loss: 0.07367891818284988, tv_loss: 0.03226719796657562\n",
      "iteration 1060, dc_loss: 0.07362526655197144, tv_loss: 0.032175738364458084\n",
      "iteration 1061, dc_loss: 0.0734289363026619, tv_loss: 0.03224051743745804\n",
      "iteration 1062, dc_loss: 0.07333515584468842, tv_loss: 0.0322086401283741\n",
      "iteration 1063, dc_loss: 0.07319948822259903, tv_loss: 0.03223245218396187\n",
      "iteration 1064, dc_loss: 0.07310973107814789, tv_loss: 0.03221095725893974\n",
      "iteration 1065, dc_loss: 0.07299164682626724, tv_loss: 0.03222553804516792\n",
      "iteration 1066, dc_loss: 0.07289683818817139, tv_loss: 0.03222076594829559\n",
      "iteration 1067, dc_loss: 0.0727669969201088, tv_loss: 0.032254401594400406\n",
      "iteration 1068, dc_loss: 0.07270723581314087, tv_loss: 0.03222907707095146\n",
      "iteration 1069, dc_loss: 0.07253298908472061, tv_loss: 0.0322854220867157\n",
      "iteration 1070, dc_loss: 0.0725099965929985, tv_loss: 0.0321950800716877\n",
      "iteration 1071, dc_loss: 0.07230179756879807, tv_loss: 0.032318536192178726\n",
      "iteration 1072, dc_loss: 0.072306327521801, tv_loss: 0.032202742993831635\n",
      "iteration 1073, dc_loss: 0.07207315415143967, tv_loss: 0.03232816234230995\n",
      "iteration 1074, dc_loss: 0.07210593670606613, tv_loss: 0.03219452500343323\n",
      "iteration 1075, dc_loss: 0.07187161594629288, tv_loss: 0.032370515167713165\n",
      "iteration 1076, dc_loss: 0.0719732716679573, tv_loss: 0.03218507394194603\n",
      "iteration 1077, dc_loss: 0.07169168442487717, tv_loss: 0.03240063413977623\n",
      "iteration 1078, dc_loss: 0.07191477715969086, tv_loss: 0.03211808204650879\n",
      "iteration 1079, dc_loss: 0.07154545933008194, tv_loss: 0.03248772770166397\n",
      "iteration 1080, dc_loss: 0.07182388752698898, tv_loss: 0.03208335489034653\n",
      "iteration 1081, dc_loss: 0.0713370218873024, tv_loss: 0.03249787539243698\n",
      "iteration 1082, dc_loss: 0.07156091183423996, tv_loss: 0.03210742771625519\n",
      "iteration 1083, dc_loss: 0.07099466770887375, tv_loss: 0.03249317407608032\n",
      "iteration 1084, dc_loss: 0.07109960168600082, tv_loss: 0.03215990588068962\n",
      "iteration 1085, dc_loss: 0.0706990659236908, tv_loss: 0.03240501880645752\n",
      "iteration 1086, dc_loss: 0.07070225477218628, tv_loss: 0.03226882591843605\n",
      "iteration 1087, dc_loss: 0.07059811800718307, tv_loss: 0.03228330612182617\n",
      "iteration 1088, dc_loss: 0.07044976949691772, tv_loss: 0.03238602727651596\n",
      "iteration 1089, dc_loss: 0.07056211680173874, tv_loss: 0.032208070158958435\n",
      "iteration 1090, dc_loss: 0.07026098668575287, tv_loss: 0.0324489027261734\n",
      "iteration 1091, dc_loss: 0.07038507610559464, tv_loss: 0.03219909220933914\n",
      "iteration 1092, dc_loss: 0.07003241032361984, tv_loss: 0.03244325891137123\n",
      "iteration 1093, dc_loss: 0.07004456222057343, tv_loss: 0.032264597713947296\n",
      "iteration 1094, dc_loss: 0.06984014809131622, tv_loss: 0.03235020861029625\n",
      "iteration 1095, dc_loss: 0.06970976293087006, tv_loss: 0.03237604722380638\n",
      "iteration 1096, dc_loss: 0.06973940134048462, tv_loss: 0.03226200118660927\n",
      "iteration 1097, dc_loss: 0.06949145346879959, tv_loss: 0.03245778754353523\n",
      "iteration 1098, dc_loss: 0.0696585550904274, tv_loss: 0.03222205117344856\n",
      "iteration 1099, dc_loss: 0.0693155899643898, tv_loss: 0.03249940276145935\n",
      "iteration 1100, dc_loss: 0.06948935985565186, tv_loss: 0.03221016749739647\n",
      "iteration 1101, dc_loss: 0.06909869611263275, tv_loss: 0.03247839957475662\n",
      "iteration 1102, dc_loss: 0.06914716213941574, tv_loss: 0.03226487338542938\n",
      "iteration 1103, dc_loss: 0.06886400282382965, tv_loss: 0.0324116013944149\n",
      "iteration 1104, dc_loss: 0.0688040554523468, tv_loss: 0.03237224370241165\n",
      "iteration 1105, dc_loss: 0.06871452927589417, tv_loss: 0.03234380856156349\n",
      "iteration 1106, dc_loss: 0.06857267022132874, tv_loss: 0.03240933641791344\n",
      "iteration 1107, dc_loss: 0.06859543919563293, tv_loss: 0.03231048211455345\n",
      "iteration 1108, dc_loss: 0.06834327429533005, tv_loss: 0.032462622970342636\n",
      "iteration 1109, dc_loss: 0.06842344254255295, tv_loss: 0.032275572419166565\n",
      "iteration 1110, dc_loss: 0.06811562180519104, tv_loss: 0.03248954564332962\n",
      "iteration 1111, dc_loss: 0.06818916648626328, tv_loss: 0.032296936959028244\n",
      "iteration 1112, dc_loss: 0.06791026890277863, tv_loss: 0.032464444637298584\n",
      "iteration 1113, dc_loss: 0.06794998794794083, tv_loss: 0.032316904515028\n",
      "iteration 1114, dc_loss: 0.06772158294916153, tv_loss: 0.03245840221643448\n",
      "iteration 1115, dc_loss: 0.06772235035896301, tv_loss: 0.032352011650800705\n",
      "iteration 1116, dc_loss: 0.06754343211650848, tv_loss: 0.03244253247976303\n",
      "iteration 1117, dc_loss: 0.06748167425394058, tv_loss: 0.0324244350194931\n",
      "iteration 1118, dc_loss: 0.06735824793577194, tv_loss: 0.03243327513337135\n",
      "iteration 1119, dc_loss: 0.06726477295160294, tv_loss: 0.03241746500134468\n",
      "iteration 1120, dc_loss: 0.0671815425157547, tv_loss: 0.032402265816926956\n",
      "iteration 1121, dc_loss: 0.06704114377498627, tv_loss: 0.03244036063551903\n",
      "iteration 1122, dc_loss: 0.06702332943677902, tv_loss: 0.03241071477532387\n",
      "iteration 1123, dc_loss: 0.06684960424900055, tv_loss: 0.032485008239746094\n",
      "iteration 1124, dc_loss: 0.06687015295028687, tv_loss: 0.03238215297460556\n",
      "iteration 1125, dc_loss: 0.06667251884937286, tv_loss: 0.03251619637012482\n",
      "iteration 1126, dc_loss: 0.06678512692451477, tv_loss: 0.03234652429819107\n",
      "iteration 1127, dc_loss: 0.06654955446720123, tv_loss: 0.03252813592553139\n",
      "iteration 1128, dc_loss: 0.06668238341808319, tv_loss: 0.03237069770693779\n",
      "iteration 1129, dc_loss: 0.06647153943777084, tv_loss: 0.032535914331674576\n",
      "iteration 1130, dc_loss: 0.06660464406013489, tv_loss: 0.03237161785364151\n",
      "iteration 1131, dc_loss: 0.06636969745159149, tv_loss: 0.032544516026973724\n",
      "iteration 1132, dc_loss: 0.06640863418579102, tv_loss: 0.03243746981024742\n",
      "iteration 1133, dc_loss: 0.06618691235780716, tv_loss: 0.032481156289577484\n",
      "iteration 1134, dc_loss: 0.06603903323411942, tv_loss: 0.03248323127627373\n",
      "iteration 1135, dc_loss: 0.06597522646188736, tv_loss: 0.032399117946624756\n",
      "iteration 1136, dc_loss: 0.06574919819831848, tv_loss: 0.03259632736444473\n",
      "iteration 1137, dc_loss: 0.06602521240711212, tv_loss: 0.032295674085617065\n",
      "iteration 1138, dc_loss: 0.06562274694442749, tv_loss: 0.032744377851486206\n",
      "iteration 1139, dc_loss: 0.06605012714862823, tv_loss: 0.03219424560666084\n",
      "iteration 1140, dc_loss: 0.06537309288978577, tv_loss: 0.032760489732027054\n",
      "iteration 1141, dc_loss: 0.06556978821754456, tv_loss: 0.03227255865931511\n",
      "iteration 1142, dc_loss: 0.06504727154970169, tv_loss: 0.0326014906167984\n",
      "iteration 1143, dc_loss: 0.06501510739326477, tv_loss: 0.03248412907123566\n",
      "iteration 1144, dc_loss: 0.06505852937698364, tv_loss: 0.03240413963794708\n",
      "iteration 1145, dc_loss: 0.06480027735233307, tv_loss: 0.03265288844704628\n",
      "iteration 1146, dc_loss: 0.06502719968557358, tv_loss: 0.032338857650756836\n",
      "iteration 1147, dc_loss: 0.0646006315946579, tv_loss: 0.03264391049742699\n",
      "iteration 1148, dc_loss: 0.06467816233634949, tv_loss: 0.03240944445133209\n",
      "iteration 1149, dc_loss: 0.0644327700138092, tv_loss: 0.03253662586212158\n",
      "iteration 1150, dc_loss: 0.06435441970825195, tv_loss: 0.03256174176931381\n",
      "iteration 1151, dc_loss: 0.06443973630666733, tv_loss: 0.0324152335524559\n",
      "iteration 1152, dc_loss: 0.06416812539100647, tv_loss: 0.032613568007946014\n",
      "iteration 1153, dc_loss: 0.06421024352312088, tv_loss: 0.03245958313345909\n",
      "iteration 1154, dc_loss: 0.06399643421173096, tv_loss: 0.032549988478422165\n",
      "iteration 1155, dc_loss: 0.06391023099422455, tv_loss: 0.03251922130584717\n",
      "iteration 1156, dc_loss: 0.06385598331689835, tv_loss: 0.032488759607076645\n",
      "iteration 1157, dc_loss: 0.06370167434215546, tv_loss: 0.032577890902757645\n",
      "iteration 1158, dc_loss: 0.06372314691543579, tv_loss: 0.03249742463231087\n",
      "iteration 1159, dc_loss: 0.06354338675737381, tv_loss: 0.03260594606399536\n",
      "iteration 1160, dc_loss: 0.06349468976259232, tv_loss: 0.03255156800150871\n",
      "iteration 1161, dc_loss: 0.06341033428907394, tv_loss: 0.032532233744859695\n",
      "iteration 1162, dc_loss: 0.06328727304935455, tv_loss: 0.03258451074361801\n",
      "iteration 1163, dc_loss: 0.06323549151420593, tv_loss: 0.03254188597202301\n",
      "iteration 1164, dc_loss: 0.06309561431407928, tv_loss: 0.03260510787367821\n",
      "iteration 1165, dc_loss: 0.06306174397468567, tv_loss: 0.03254377469420433\n",
      "iteration 1166, dc_loss: 0.06295714527368546, tv_loss: 0.03257601708173752\n",
      "iteration 1167, dc_loss: 0.06289637088775635, tv_loss: 0.0325629860162735\n",
      "iteration 1168, dc_loss: 0.06277967244386673, tv_loss: 0.032589033246040344\n",
      "iteration 1169, dc_loss: 0.06272923946380615, tv_loss: 0.032549548894166946\n",
      "iteration 1170, dc_loss: 0.0625782236456871, tv_loss: 0.03263537958264351\n",
      "iteration 1171, dc_loss: 0.06261702626943588, tv_loss: 0.032510556280612946\n",
      "iteration 1172, dc_loss: 0.062408749014139175, tv_loss: 0.03265516832470894\n",
      "iteration 1173, dc_loss: 0.06252281367778778, tv_loss: 0.0324968658387661\n",
      "iteration 1174, dc_loss: 0.062324345111846924, tv_loss: 0.03269192576408386\n",
      "iteration 1175, dc_loss: 0.06251616030931473, tv_loss: 0.03249990940093994\n",
      "iteration 1176, dc_loss: 0.06238611787557602, tv_loss: 0.03271899372339249\n",
      "iteration 1177, dc_loss: 0.06267186999320984, tv_loss: 0.032473862171173096\n",
      "iteration 1178, dc_loss: 0.06261269748210907, tv_loss: 0.032788604497909546\n",
      "iteration 1179, dc_loss: 0.06286568939685822, tv_loss: 0.0324389673769474\n",
      "iteration 1180, dc_loss: 0.062413375824689865, tv_loss: 0.03282535448670387\n",
      "iteration 1181, dc_loss: 0.06238526105880737, tv_loss: 0.032415036112070084\n",
      "iteration 1182, dc_loss: 0.06168144568800926, tv_loss: 0.032781071960926056\n",
      "iteration 1183, dc_loss: 0.061822559684515, tv_loss: 0.032490722835063934\n",
      "iteration 1184, dc_loss: 0.06169680878520012, tv_loss: 0.03270135074853897\n",
      "iteration 1185, dc_loss: 0.061896372586488724, tv_loss: 0.032586537301540375\n",
      "iteration 1186, dc_loss: 0.06165573373436928, tv_loss: 0.032638803124427795\n",
      "iteration 1187, dc_loss: 0.06136666238307953, tv_loss: 0.03263643756508827\n",
      "iteration 1188, dc_loss: 0.06115177646279335, tv_loss: 0.03261604532599449\n",
      "iteration 1189, dc_loss: 0.06117360666394234, tv_loss: 0.03258461877703667\n",
      "iteration 1190, dc_loss: 0.06114695221185684, tv_loss: 0.032670311629772186\n",
      "iteration 1191, dc_loss: 0.06110252067446709, tv_loss: 0.032619789242744446\n",
      "iteration 1192, dc_loss: 0.06093095242977142, tv_loss: 0.03262679651379585\n",
      "iteration 1193, dc_loss: 0.0607195645570755, tv_loss: 0.03269267454743385\n",
      "iteration 1194, dc_loss: 0.060823697596788406, tv_loss: 0.03255446255207062\n",
      "iteration 1195, dc_loss: 0.060620564967393875, tv_loss: 0.03273237869143486\n",
      "iteration 1196, dc_loss: 0.060717690736055374, tv_loss: 0.03254758566617966\n",
      "iteration 1197, dc_loss: 0.0604107640683651, tv_loss: 0.03274330124258995\n",
      "iteration 1198, dc_loss: 0.06043720245361328, tv_loss: 0.032590676099061966\n",
      "iteration 1199, dc_loss: 0.06026013568043709, tv_loss: 0.03268759325146675\n",
      "iteration 1200, dc_loss: 0.06027178093791008, tv_loss: 0.03262509033083916\n",
      "iteration 1201, dc_loss: 0.06015359237790108, tv_loss: 0.032688651233911514\n",
      "iteration 1202, dc_loss: 0.06005197390913963, tv_loss: 0.032651130110025406\n",
      "iteration 1203, dc_loss: 0.060007061809301376, tv_loss: 0.032644808292388916\n",
      "iteration 1204, dc_loss: 0.05994664877653122, tv_loss: 0.03268691152334213\n",
      "iteration 1205, dc_loss: 0.05986740067601204, tv_loss: 0.03265167027711868\n",
      "iteration 1206, dc_loss: 0.05980310216546059, tv_loss: 0.03266911953687668\n",
      "iteration 1207, dc_loss: 0.059744853526353836, tv_loss: 0.032691169530153275\n",
      "iteration 1208, dc_loss: 0.05969352275133133, tv_loss: 0.0326409712433815\n",
      "iteration 1209, dc_loss: 0.05962157994508743, tv_loss: 0.032662373036146164\n",
      "iteration 1210, dc_loss: 0.05955051630735397, tv_loss: 0.03269163891673088\n",
      "iteration 1211, dc_loss: 0.05949930474162102, tv_loss: 0.032652564346790314\n",
      "iteration 1212, dc_loss: 0.05942603573203087, tv_loss: 0.03267333656549454\n",
      "iteration 1213, dc_loss: 0.05937480553984642, tv_loss: 0.03267889469861984\n",
      "iteration 1214, dc_loss: 0.05931318551301956, tv_loss: 0.03266692906618118\n",
      "iteration 1215, dc_loss: 0.059233199805021286, tv_loss: 0.032685667276382446\n",
      "iteration 1216, dc_loss: 0.059198249131441116, tv_loss: 0.03266538679599762\n",
      "iteration 1217, dc_loss: 0.05911879613995552, tv_loss: 0.03268815949559212\n",
      "iteration 1218, dc_loss: 0.05904895067214966, tv_loss: 0.032692499458789825\n",
      "iteration 1219, dc_loss: 0.05901710316538811, tv_loss: 0.03266815468668938\n",
      "iteration 1220, dc_loss: 0.05893591418862343, tv_loss: 0.03269055113196373\n",
      "iteration 1221, dc_loss: 0.058869704604148865, tv_loss: 0.03271143510937691\n",
      "iteration 1222, dc_loss: 0.05883047357201576, tv_loss: 0.032701194286346436\n",
      "iteration 1223, dc_loss: 0.05875786393880844, tv_loss: 0.03270215168595314\n",
      "iteration 1224, dc_loss: 0.058695029467344284, tv_loss: 0.03269915655255318\n",
      "iteration 1225, dc_loss: 0.05864380672574043, tv_loss: 0.03269219398498535\n",
      "iteration 1226, dc_loss: 0.05856944993138313, tv_loss: 0.03270535543560982\n",
      "iteration 1227, dc_loss: 0.058529775589704514, tv_loss: 0.032703302800655365\n",
      "iteration 1228, dc_loss: 0.058464765548706055, tv_loss: 0.032726678997278214\n",
      "iteration 1229, dc_loss: 0.05838463827967644, tv_loss: 0.032728761434555054\n",
      "iteration 1230, dc_loss: 0.05835612490773201, tv_loss: 0.03269222751259804\n",
      "iteration 1231, dc_loss: 0.058280088007450104, tv_loss: 0.032716210931539536\n",
      "iteration 1232, dc_loss: 0.05822752043604851, tv_loss: 0.032714154571294785\n",
      "iteration 1233, dc_loss: 0.05817379802465439, tv_loss: 0.03272281959652901\n",
      "iteration 1234, dc_loss: 0.058090243488550186, tv_loss: 0.032754085958004\n",
      "iteration 1235, dc_loss: 0.05806352570652962, tv_loss: 0.03271125629544258\n",
      "iteration 1236, dc_loss: 0.057996828109025955, tv_loss: 0.032712746411561966\n",
      "iteration 1237, dc_loss: 0.05792280286550522, tv_loss: 0.03273364156484604\n",
      "iteration 1238, dc_loss: 0.0578918419778347, tv_loss: 0.032708898186683655\n",
      "iteration 1239, dc_loss: 0.057810552418231964, tv_loss: 0.03273361921310425\n",
      "iteration 1240, dc_loss: 0.05776108056306839, tv_loss: 0.0327252633869648\n",
      "iteration 1241, dc_loss: 0.05770950764417648, tv_loss: 0.03274466097354889\n",
      "iteration 1242, dc_loss: 0.05763126537203789, tv_loss: 0.032752007246017456\n",
      "iteration 1243, dc_loss: 0.057600971311330795, tv_loss: 0.0327284038066864\n",
      "iteration 1244, dc_loss: 0.057546455413103104, tv_loss: 0.032723959535360336\n",
      "iteration 1245, dc_loss: 0.05745813250541687, tv_loss: 0.03275619447231293\n",
      "iteration 1246, dc_loss: 0.057426586747169495, tv_loss: 0.032741401344537735\n",
      "iteration 1247, dc_loss: 0.057360030710697174, tv_loss: 0.032753828912973404\n",
      "iteration 1248, dc_loss: 0.05729975923895836, tv_loss: 0.03275582566857338\n",
      "iteration 1249, dc_loss: 0.057264164090156555, tv_loss: 0.032731544226408005\n",
      "iteration 1250, dc_loss: 0.057184696197509766, tv_loss: 0.0327538326382637\n",
      "iteration 1251, dc_loss: 0.057125892490148544, tv_loss: 0.03276051953434944\n",
      "iteration 1252, dc_loss: 0.057087380439043045, tv_loss: 0.03276032954454422\n",
      "iteration 1253, dc_loss: 0.057018619030714035, tv_loss: 0.032769251614809036\n",
      "iteration 1254, dc_loss: 0.05696985125541687, tv_loss: 0.03276694566011429\n",
      "iteration 1255, dc_loss: 0.05691487714648247, tv_loss: 0.0327579490840435\n",
      "iteration 1256, dc_loss: 0.056862231343984604, tv_loss: 0.0327548123896122\n",
      "iteration 1257, dc_loss: 0.056792039424180984, tv_loss: 0.0327763631939888\n",
      "iteration 1258, dc_loss: 0.056744761765003204, tv_loss: 0.03277641907334328\n",
      "iteration 1259, dc_loss: 0.05669570341706276, tv_loss: 0.032765503972768784\n",
      "iteration 1260, dc_loss: 0.056629423052072525, tv_loss: 0.032766323536634445\n",
      "iteration 1261, dc_loss: 0.05659271031618118, tv_loss: 0.03275756165385246\n",
      "iteration 1262, dc_loss: 0.056510671973228455, tv_loss: 0.032786186784505844\n",
      "iteration 1263, dc_loss: 0.05646970495581627, tv_loss: 0.03277480602264404\n",
      "iteration 1264, dc_loss: 0.05642621964216232, tv_loss: 0.032760441303253174\n",
      "iteration 1265, dc_loss: 0.05634529888629913, tv_loss: 0.032797448337078094\n",
      "iteration 1266, dc_loss: 0.05632079765200615, tv_loss: 0.03277656063437462\n",
      "iteration 1267, dc_loss: 0.05624153092503548, tv_loss: 0.03279914706945419\n",
      "iteration 1268, dc_loss: 0.056194983422756195, tv_loss: 0.03279193863272667\n",
      "iteration 1269, dc_loss: 0.056138522922992706, tv_loss: 0.032785866409540176\n",
      "iteration 1270, dc_loss: 0.056085363030433655, tv_loss: 0.0327872559428215\n",
      "iteration 1271, dc_loss: 0.056038159877061844, tv_loss: 0.03277625888586044\n",
      "iteration 1272, dc_loss: 0.055977724492549896, tv_loss: 0.03278496116399765\n",
      "iteration 1273, dc_loss: 0.05593548342585564, tv_loss: 0.032777126878499985\n",
      "iteration 1274, dc_loss: 0.05586232990026474, tv_loss: 0.03279932588338852\n",
      "iteration 1275, dc_loss: 0.055811118334531784, tv_loss: 0.032794371247291565\n",
      "iteration 1276, dc_loss: 0.055770233273506165, tv_loss: 0.03278912976384163\n",
      "iteration 1277, dc_loss: 0.05570852383971214, tv_loss: 0.032808803021907806\n",
      "iteration 1278, dc_loss: 0.055660832673311234, tv_loss: 0.0328066311776638\n",
      "iteration 1279, dc_loss: 0.055599432438611984, tv_loss: 0.03281345218420029\n",
      "iteration 1280, dc_loss: 0.05556318908929825, tv_loss: 0.03279741108417511\n",
      "iteration 1281, dc_loss: 0.05548406392335892, tv_loss: 0.03282308578491211\n",
      "iteration 1282, dc_loss: 0.055442534387111664, tv_loss: 0.032805588096380234\n",
      "iteration 1283, dc_loss: 0.055390432476997375, tv_loss: 0.032802119851112366\n",
      "iteration 1284, dc_loss: 0.05533576011657715, tv_loss: 0.03281015530228615\n",
      "iteration 1285, dc_loss: 0.05529240146279335, tv_loss: 0.0328122153878212\n",
      "iteration 1286, dc_loss: 0.05523194000124931, tv_loss: 0.03281854838132858\n",
      "iteration 1287, dc_loss: 0.05517369881272316, tv_loss: 0.03283229470252991\n",
      "iteration 1288, dc_loss: 0.05514033883810043, tv_loss: 0.032837286591529846\n",
      "iteration 1289, dc_loss: 0.05506633594632149, tv_loss: 0.03284822031855583\n",
      "iteration 1290, dc_loss: 0.055028922855854034, tv_loss: 0.03283071145415306\n",
      "iteration 1291, dc_loss: 0.054964225739240646, tv_loss: 0.032841719686985016\n",
      "iteration 1292, dc_loss: 0.054931119084358215, tv_loss: 0.032843928784132004\n",
      "iteration 1293, dc_loss: 0.054861340671777725, tv_loss: 0.03285321965813637\n",
      "iteration 1294, dc_loss: 0.054825086146593094, tv_loss: 0.03282633051276207\n",
      "iteration 1295, dc_loss: 0.054764438420534134, tv_loss: 0.03284492716193199\n",
      "iteration 1296, dc_loss: 0.05470649525523186, tv_loss: 0.032858606427907944\n",
      "iteration 1297, dc_loss: 0.054662223905324936, tv_loss: 0.03286339342594147\n",
      "iteration 1298, dc_loss: 0.054619111120700836, tv_loss: 0.03283087536692619\n",
      "iteration 1299, dc_loss: 0.05456402897834778, tv_loss: 0.03283355012536049\n",
      "iteration 1300, dc_loss: 0.054495953023433685, tv_loss: 0.03286579251289368\n",
      "iteration 1301, dc_loss: 0.05446482077240944, tv_loss: 0.03285166248679161\n",
      "iteration 1302, dc_loss: 0.05439490079879761, tv_loss: 0.03287310153245926\n",
      "iteration 1303, dc_loss: 0.05436144024133682, tv_loss: 0.03286133334040642\n",
      "iteration 1304, dc_loss: 0.05430765822529793, tv_loss: 0.032843589782714844\n",
      "iteration 1305, dc_loss: 0.05426105484366417, tv_loss: 0.03285063058137894\n",
      "iteration 1306, dc_loss: 0.05420424044132233, tv_loss: 0.03288324922323227\n",
      "iteration 1307, dc_loss: 0.05413774773478508, tv_loss: 0.03288158401846886\n",
      "iteration 1308, dc_loss: 0.05410805344581604, tv_loss: 0.03286369517445564\n",
      "iteration 1309, dc_loss: 0.05405830591917038, tv_loss: 0.03287041187286377\n",
      "iteration 1310, dc_loss: 0.054003212600946426, tv_loss: 0.03287482261657715\n",
      "iteration 1311, dc_loss: 0.05394725501537323, tv_loss: 0.03289042040705681\n",
      "iteration 1312, dc_loss: 0.053911589086055756, tv_loss: 0.032867204397916794\n",
      "iteration 1313, dc_loss: 0.053833238780498505, tv_loss: 0.0329120010137558\n",
      "iteration 1314, dc_loss: 0.053832072764635086, tv_loss: 0.0328545980155468\n",
      "iteration 1315, dc_loss: 0.0537344329059124, tv_loss: 0.032902251929044724\n",
      "iteration 1316, dc_loss: 0.05372607335448265, tv_loss: 0.0328662246465683\n",
      "iteration 1317, dc_loss: 0.053640495985746384, tv_loss: 0.032912492752075195\n",
      "iteration 1318, dc_loss: 0.05362492799758911, tv_loss: 0.032879505306482315\n",
      "iteration 1319, dc_loss: 0.05353249981999397, tv_loss: 0.03290447220206261\n",
      "iteration 1320, dc_loss: 0.053544338792562485, tv_loss: 0.03285958990454674\n",
      "iteration 1321, dc_loss: 0.05343272164463997, tv_loss: 0.03293723613023758\n",
      "iteration 1322, dc_loss: 0.05347288027405739, tv_loss: 0.032850466668605804\n",
      "iteration 1323, dc_loss: 0.053323887288570404, tv_loss: 0.03295733034610748\n",
      "iteration 1324, dc_loss: 0.05342256277799606, tv_loss: 0.032806020230054855\n",
      "iteration 1325, dc_loss: 0.05323753133416176, tv_loss: 0.032972969114780426\n",
      "iteration 1326, dc_loss: 0.05337497591972351, tv_loss: 0.03281901404261589\n",
      "iteration 1327, dc_loss: 0.053161852061748505, tv_loss: 0.03301730006933212\n",
      "iteration 1328, dc_loss: 0.05336059257388115, tv_loss: 0.03277001902461052\n",
      "iteration 1329, dc_loss: 0.053089726716279984, tv_loss: 0.033038750290870667\n",
      "iteration 1330, dc_loss: 0.05331072583794594, tv_loss: 0.0327720008790493\n",
      "iteration 1331, dc_loss: 0.05295370891690254, tv_loss: 0.03306245431303978\n",
      "iteration 1332, dc_loss: 0.053119707852602005, tv_loss: 0.03280026838183403\n",
      "iteration 1333, dc_loss: 0.05284745246171951, tv_loss: 0.032980069518089294\n",
      "iteration 1334, dc_loss: 0.0528830923140049, tv_loss: 0.032882533967494965\n",
      "iteration 1335, dc_loss: 0.05277992784976959, tv_loss: 0.03292839601635933\n",
      "iteration 1336, dc_loss: 0.052709754556417465, tv_loss: 0.03294915705919266\n",
      "iteration 1337, dc_loss: 0.05275119096040726, tv_loss: 0.03286098688840866\n",
      "iteration 1338, dc_loss: 0.05261165276169777, tv_loss: 0.03299301490187645\n",
      "iteration 1339, dc_loss: 0.052707474678754807, tv_loss: 0.03285134583711624\n",
      "iteration 1340, dc_loss: 0.05252929776906967, tv_loss: 0.03298819810152054\n",
      "iteration 1341, dc_loss: 0.05259135738015175, tv_loss: 0.03288853168487549\n",
      "iteration 1342, dc_loss: 0.05240897461771965, tv_loss: 0.03300294280052185\n",
      "iteration 1343, dc_loss: 0.05248786136507988, tv_loss: 0.032866816967725754\n",
      "iteration 1344, dc_loss: 0.052328143268823624, tv_loss: 0.03297007083892822\n",
      "iteration 1345, dc_loss: 0.05233302712440491, tv_loss: 0.03290558606386185\n",
      "iteration 1346, dc_loss: 0.052268192172050476, tv_loss: 0.032949481159448624\n",
      "iteration 1347, dc_loss: 0.052184849977493286, tv_loss: 0.03297032415866852\n",
      "iteration 1348, dc_loss: 0.05221206322312355, tv_loss: 0.03289920091629028\n",
      "iteration 1349, dc_loss: 0.05210372060537338, tv_loss: 0.03297363594174385\n",
      "iteration 1350, dc_loss: 0.052121348679065704, tv_loss: 0.03290993347764015\n",
      "iteration 1351, dc_loss: 0.0520118772983551, tv_loss: 0.0329715721309185\n",
      "iteration 1352, dc_loss: 0.052042704075574875, tv_loss: 0.03289858624339104\n",
      "iteration 1353, dc_loss: 0.051902107894420624, tv_loss: 0.03298630565404892\n",
      "iteration 1354, dc_loss: 0.051945824176073074, tv_loss: 0.032899875193834305\n",
      "iteration 1355, dc_loss: 0.051814425736665726, tv_loss: 0.0329873226583004\n",
      "iteration 1356, dc_loss: 0.0518600232899189, tv_loss: 0.032911162823438644\n",
      "iteration 1357, dc_loss: 0.05172562599182129, tv_loss: 0.03298885002732277\n",
      "iteration 1358, dc_loss: 0.051785100251436234, tv_loss: 0.03289800137281418\n",
      "iteration 1359, dc_loss: 0.05165243148803711, tv_loss: 0.032992053776979446\n",
      "iteration 1360, dc_loss: 0.05168839916586876, tv_loss: 0.032915256917476654\n",
      "iteration 1361, dc_loss: 0.05154569819569588, tv_loss: 0.033014606684446335\n",
      "iteration 1362, dc_loss: 0.05162068083882332, tv_loss: 0.03290848061442375\n",
      "iteration 1363, dc_loss: 0.05146721377968788, tv_loss: 0.03300878778100014\n",
      "iteration 1364, dc_loss: 0.05154182389378548, tv_loss: 0.032889124006032944\n",
      "iteration 1365, dc_loss: 0.05136098340153694, tv_loss: 0.03302731364965439\n",
      "iteration 1366, dc_loss: 0.05147101730108261, tv_loss: 0.032875895500183105\n",
      "iteration 1367, dc_loss: 0.05126428231596947, tv_loss: 0.033044010400772095\n",
      "iteration 1368, dc_loss: 0.05141100287437439, tv_loss: 0.03286658227443695\n",
      "iteration 1369, dc_loss: 0.05118735134601593, tv_loss: 0.03307684510946274\n",
      "iteration 1370, dc_loss: 0.05135833099484444, tv_loss: 0.03287769481539726\n",
      "iteration 1371, dc_loss: 0.05110583081841469, tv_loss: 0.03309333324432373\n",
      "iteration 1372, dc_loss: 0.05131125450134277, tv_loss: 0.0328400693833828\n",
      "iteration 1373, dc_loss: 0.05102745443582535, tv_loss: 0.033092912286520004\n",
      "iteration 1374, dc_loss: 0.051194120198488235, tv_loss: 0.032864443957805634\n",
      "iteration 1375, dc_loss: 0.05091991275548935, tv_loss: 0.033091749995946884\n",
      "iteration 1376, dc_loss: 0.05105186253786087, tv_loss: 0.03288527950644493\n",
      "iteration 1377, dc_loss: 0.05083712562918663, tv_loss: 0.03304765000939369\n",
      "iteration 1378, dc_loss: 0.05088333413004875, tv_loss: 0.032927919179201126\n",
      "iteration 1379, dc_loss: 0.05075184628367424, tv_loss: 0.03300761431455612\n",
      "iteration 1380, dc_loss: 0.050738364458084106, tv_loss: 0.03297050669789314\n",
      "iteration 1381, dc_loss: 0.050703294575214386, tv_loss: 0.03297022357583046\n",
      "iteration 1382, dc_loss: 0.05062362551689148, tv_loss: 0.033009663224220276\n",
      "iteration 1383, dc_loss: 0.0506720170378685, tv_loss: 0.03292335569858551\n",
      "iteration 1384, dc_loss: 0.05051495134830475, tv_loss: 0.03304458037018776\n",
      "iteration 1385, dc_loss: 0.05062609165906906, tv_loss: 0.03290478512644768\n",
      "iteration 1386, dc_loss: 0.050434328615665436, tv_loss: 0.03307991102337837\n",
      "iteration 1387, dc_loss: 0.05058402940630913, tv_loss: 0.03290117532014847\n",
      "iteration 1388, dc_loss: 0.050346408039331436, tv_loss: 0.03310519456863403\n",
      "iteration 1389, dc_loss: 0.05047791078686714, tv_loss: 0.03291280195116997\n",
      "iteration 1390, dc_loss: 0.050265274941921234, tv_loss: 0.03308524936437607\n",
      "iteration 1391, dc_loss: 0.05036172270774841, tv_loss: 0.03292300924658775\n",
      "iteration 1392, dc_loss: 0.05018049478530884, tv_loss: 0.03306210786104202\n",
      "iteration 1393, dc_loss: 0.05024897679686546, tv_loss: 0.032943833619356155\n",
      "iteration 1394, dc_loss: 0.05009884759783745, tv_loss: 0.03305038809776306\n",
      "iteration 1395, dc_loss: 0.05010700970888138, tv_loss: 0.03299752250313759\n",
      "iteration 1396, dc_loss: 0.050016533583402634, tv_loss: 0.03302851691842079\n",
      "iteration 1397, dc_loss: 0.050009533762931824, tv_loss: 0.03299945592880249\n",
      "iteration 1398, dc_loss: 0.049958955496549606, tv_loss: 0.03300165385007858\n",
      "iteration 1399, dc_loss: 0.04989152029156685, tv_loss: 0.03302322328090668\n",
      "iteration 1400, dc_loss: 0.04988543316721916, tv_loss: 0.032995570451021194\n",
      "iteration 1401, dc_loss: 0.04979246109724045, tv_loss: 0.03305453062057495\n",
      "iteration 1402, dc_loss: 0.049827881157398224, tv_loss: 0.032966919243335724\n",
      "iteration 1403, dc_loss: 0.04970742389559746, tv_loss: 0.03306099772453308\n",
      "iteration 1404, dc_loss: 0.049769189208745956, tv_loss: 0.03297439590096474\n",
      "iteration 1405, dc_loss: 0.04963776841759682, tv_loss: 0.033082786947488785\n",
      "iteration 1406, dc_loss: 0.049766696989536285, tv_loss: 0.032931167632341385\n",
      "iteration 1407, dc_loss: 0.049568064510822296, tv_loss: 0.03314170241355896\n",
      "iteration 1408, dc_loss: 0.049802251160144806, tv_loss: 0.03289330378174782\n",
      "iteration 1409, dc_loss: 0.04953509569168091, tv_loss: 0.03316613659262657\n",
      "iteration 1410, dc_loss: 0.049815334379673004, tv_loss: 0.0328645333647728\n",
      "iteration 1411, dc_loss: 0.04947622865438461, tv_loss: 0.03320407494902611\n",
      "iteration 1412, dc_loss: 0.049711257219314575, tv_loss: 0.03286853805184364\n",
      "iteration 1413, dc_loss: 0.04933569207787514, tv_loss: 0.03316565603017807\n",
      "iteration 1414, dc_loss: 0.04945898801088333, tv_loss: 0.03293055295944214\n",
      "iteration 1415, dc_loss: 0.04921623691916466, tv_loss: 0.0331004336476326\n",
      "iteration 1416, dc_loss: 0.04925481975078583, tv_loss: 0.0330209955573082\n",
      "iteration 1417, dc_loss: 0.04921691492199898, tv_loss: 0.03302321955561638\n",
      "iteration 1418, dc_loss: 0.049152132123708725, tv_loss: 0.033066630363464355\n",
      "iteration 1419, dc_loss: 0.0491984486579895, tv_loss: 0.03299146145582199\n",
      "iteration 1420, dc_loss: 0.049042824655771255, tv_loss: 0.03309369832277298\n",
      "iteration 1421, dc_loss: 0.04908445104956627, tv_loss: 0.03298332914710045\n",
      "iteration 1422, dc_loss: 0.04895133152604103, tv_loss: 0.03306742012500763\n",
      "iteration 1423, dc_loss: 0.048945602029561996, tv_loss: 0.03302321955561638\n",
      "iteration 1424, dc_loss: 0.04888005182147026, tv_loss: 0.03304510936141014\n",
      "iteration 1425, dc_loss: 0.048849884420633316, tv_loss: 0.033047113567590714\n",
      "iteration 1426, dc_loss: 0.04882849007844925, tv_loss: 0.033028244972229004\n",
      "iteration 1427, dc_loss: 0.04876365512609482, tv_loss: 0.03304983675479889\n",
      "iteration 1428, dc_loss: 0.04875665903091431, tv_loss: 0.03301757201552391\n",
      "iteration 1429, dc_loss: 0.04869239404797554, tv_loss: 0.03305152431130409\n",
      "iteration 1430, dc_loss: 0.0486336424946785, tv_loss: 0.033080872148275375\n",
      "iteration 1431, dc_loss: 0.04862280562520027, tv_loss: 0.03303162381052971\n",
      "iteration 1432, dc_loss: 0.04852414131164551, tv_loss: 0.0330970473587513\n",
      "iteration 1433, dc_loss: 0.04856939613819122, tv_loss: 0.033033423125743866\n",
      "iteration 1434, dc_loss: 0.048431187868118286, tv_loss: 0.03311535716056824\n",
      "iteration 1435, dc_loss: 0.048480741679668427, tv_loss: 0.03303315490484238\n",
      "iteration 1436, dc_loss: 0.048366524279117584, tv_loss: 0.03311586380004883\n",
      "iteration 1437, dc_loss: 0.04842058941721916, tv_loss: 0.033033426851034164\n",
      "iteration 1438, dc_loss: 0.04829433932900429, tv_loss: 0.033135708421468735\n",
      "iteration 1439, dc_loss: 0.048354532569646835, tv_loss: 0.033030543476343155\n",
      "iteration 1440, dc_loss: 0.04823211953043938, tv_loss: 0.03311944380402565\n",
      "iteration 1441, dc_loss: 0.048332035541534424, tv_loss: 0.033002205193042755\n",
      "iteration 1442, dc_loss: 0.04815695807337761, tv_loss: 0.03318240866065025\n",
      "iteration 1443, dc_loss: 0.04840141534805298, tv_loss: 0.032963238656520844\n",
      "iteration 1444, dc_loss: 0.04817550629377365, tv_loss: 0.033247921615839005\n",
      "iteration 1445, dc_loss: 0.04851716011762619, tv_loss: 0.03289635106921196\n",
      "iteration 1446, dc_loss: 0.048179466277360916, tv_loss: 0.03328538313508034\n",
      "iteration 1447, dc_loss: 0.048501912504434586, tv_loss: 0.0328693650662899\n",
      "iteration 1448, dc_loss: 0.048057205975055695, tv_loss: 0.033297859132289886\n",
      "iteration 1449, dc_loss: 0.04826304689049721, tv_loss: 0.03291897103190422\n",
      "iteration 1450, dc_loss: 0.04784642159938812, tv_loss: 0.03319036588072777\n",
      "iteration 1451, dc_loss: 0.0479089729487896, tv_loss: 0.033026546239852905\n",
      "iteration 1452, dc_loss: 0.04781365767121315, tv_loss: 0.03307940065860748\n",
      "iteration 1453, dc_loss: 0.04776684567332268, tv_loss: 0.0331304557621479\n",
      "iteration 1454, dc_loss: 0.047919582575559616, tv_loss: 0.033001258969306946\n",
      "iteration 1455, dc_loss: 0.047663502395153046, tv_loss: 0.033239733427762985\n",
      "iteration 1456, dc_loss: 0.047867968678474426, tv_loss: 0.0329740084707737\n",
      "iteration 1457, dc_loss: 0.04756031185388565, tv_loss: 0.03319487348198891\n",
      "iteration 1458, dc_loss: 0.04763330519199371, tv_loss: 0.03304173797369003\n",
      "iteration 1459, dc_loss: 0.047541387379169464, tv_loss: 0.03310408070683479\n",
      "iteration 1460, dc_loss: 0.04748492315411568, tv_loss: 0.033126555383205414\n",
      "iteration 1461, dc_loss: 0.04756690934300423, tv_loss: 0.03302062302827835\n",
      "iteration 1462, dc_loss: 0.04735880345106125, tv_loss: 0.033208370208740234\n",
      "iteration 1463, dc_loss: 0.047500140964984894, tv_loss: 0.0330214723944664\n",
      "iteration 1464, dc_loss: 0.04729895293712616, tv_loss: 0.03317719325423241\n",
      "iteration 1465, dc_loss: 0.04736404865980148, tv_loss: 0.03305364400148392\n",
      "iteration 1466, dc_loss: 0.04725935310125351, tv_loss: 0.03310968726873398\n",
      "iteration 1467, dc_loss: 0.04721875116229057, tv_loss: 0.03311515599489212\n",
      "iteration 1468, dc_loss: 0.04720371216535568, tv_loss: 0.033092811703681946\n",
      "iteration 1469, dc_loss: 0.04711199179291725, tv_loss: 0.03316134214401245\n",
      "iteration 1470, dc_loss: 0.0471595861017704, tv_loss: 0.03308436647057533\n",
      "iteration 1471, dc_loss: 0.04705698788166046, tv_loss: 0.0331403948366642\n",
      "iteration 1472, dc_loss: 0.04707951098680496, tv_loss: 0.03306790813803673\n",
      "iteration 1473, dc_loss: 0.046959295868873596, tv_loss: 0.033148858696222305\n",
      "iteration 1474, dc_loss: 0.046967409551143646, tv_loss: 0.03309299051761627\n",
      "iteration 1475, dc_loss: 0.04688997194170952, tv_loss: 0.03313916549086571\n",
      "iteration 1476, dc_loss: 0.04690041020512581, tv_loss: 0.03310355916619301\n",
      "iteration 1477, dc_loss: 0.04683057591319084, tv_loss: 0.03315522521734238\n",
      "iteration 1478, dc_loss: 0.04681069031357765, tv_loss: 0.03313891217112541\n",
      "iteration 1479, dc_loss: 0.04677877947688103, tv_loss: 0.03311625495553017\n",
      "iteration 1480, dc_loss: 0.0467124842107296, tv_loss: 0.033145636320114136\n",
      "iteration 1481, dc_loss: 0.04669513925909996, tv_loss: 0.033124301582574844\n",
      "iteration 1482, dc_loss: 0.04664260894060135, tv_loss: 0.033135611563920975\n",
      "iteration 1483, dc_loss: 0.046631935983896255, tv_loss: 0.03310703858733177\n",
      "iteration 1484, dc_loss: 0.04654959961771965, tv_loss: 0.03315142169594765\n",
      "iteration 1485, dc_loss: 0.04654917120933533, tv_loss: 0.03312460705637932\n",
      "iteration 1486, dc_loss: 0.04649904742836952, tv_loss: 0.03313332423567772\n",
      "iteration 1487, dc_loss: 0.046484071761369705, tv_loss: 0.03311854973435402\n",
      "iteration 1488, dc_loss: 0.046426922082901, tv_loss: 0.03314793109893799\n",
      "iteration 1489, dc_loss: 0.04641680046916008, tv_loss: 0.033129867166280746\n",
      "iteration 1490, dc_loss: 0.04634442552924156, tv_loss: 0.03316657617688179\n",
      "iteration 1491, dc_loss: 0.046376269310712814, tv_loss: 0.033108003437519073\n",
      "iteration 1492, dc_loss: 0.046277932822704315, tv_loss: 0.03317525237798691\n",
      "iteration 1493, dc_loss: 0.04635099694132805, tv_loss: 0.03308588266372681\n",
      "iteration 1494, dc_loss: 0.04620932415127754, tv_loss: 0.033220238983631134\n",
      "iteration 1495, dc_loss: 0.04637099802494049, tv_loss: 0.03304622694849968\n",
      "iteration 1496, dc_loss: 0.04613751545548439, tv_loss: 0.033281441777944565\n",
      "iteration 1497, dc_loss: 0.04638713598251343, tv_loss: 0.03301779180765152\n",
      "iteration 1498, dc_loss: 0.04610050469636917, tv_loss: 0.033319100737571716\n",
      "iteration 1499, dc_loss: 0.04645455256104469, tv_loss: 0.03295523673295975\n",
      "iteration 1500, dc_loss: 0.04608849808573723, tv_loss: 0.03338237106800079\n",
      "iteration 1501, dc_loss: 0.04649181291460991, tv_loss: 0.032923176884651184\n",
      "iteration 1502, dc_loss: 0.04605556279420853, tv_loss: 0.03336786851286888\n",
      "iteration 1503, dc_loss: 0.046315748244524, tv_loss: 0.032966092228889465\n",
      "iteration 1504, dc_loss: 0.045898716896772385, tv_loss: 0.03327670320868492\n",
      "iteration 1505, dc_loss: 0.04593998193740845, tv_loss: 0.033110104501247406\n",
      "iteration 1506, dc_loss: 0.04585929214954376, tv_loss: 0.033133652061223984\n",
      "iteration 1507, dc_loss: 0.04576000198721886, tv_loss: 0.033233970403671265\n",
      "iteration 1508, dc_loss: 0.04595576599240303, tv_loss: 0.03304855152964592\n",
      "iteration 1509, dc_loss: 0.04572305083274841, tv_loss: 0.033300235867500305\n",
      "iteration 1510, dc_loss: 0.04592645913362503, tv_loss: 0.033019550144672394\n",
      "iteration 1511, dc_loss: 0.04560241475701332, tv_loss: 0.03327729180455208\n",
      "iteration 1512, dc_loss: 0.04567219689488411, tv_loss: 0.03310332074761391\n",
      "iteration 1513, dc_loss: 0.045554205775260925, tv_loss: 0.03316628560423851\n",
      "iteration 1514, dc_loss: 0.04549676179885864, tv_loss: 0.033214908093214035\n",
      "iteration 1515, dc_loss: 0.045626670122146606, tv_loss: 0.033072322607040405\n",
      "iteration 1516, dc_loss: 0.04541642218828201, tv_loss: 0.03328236564993858\n",
      "iteration 1517, dc_loss: 0.045583710074424744, tv_loss: 0.03306933119893074\n",
      "iteration 1518, dc_loss: 0.04535871744155884, tv_loss: 0.03324786201119423\n",
      "iteration 1519, dc_loss: 0.04541221633553505, tv_loss: 0.03313503786921501\n",
      "iteration 1520, dc_loss: 0.04529690742492676, tv_loss: 0.03319074958562851\n",
      "iteration 1521, dc_loss: 0.04527739807963371, tv_loss: 0.03318445011973381\n",
      "iteration 1522, dc_loss: 0.04529181122779846, tv_loss: 0.03313995152711868\n",
      "iteration 1523, dc_loss: 0.04518456384539604, tv_loss: 0.03322727978229523\n",
      "iteration 1524, dc_loss: 0.04526733607053757, tv_loss: 0.033113762736320496\n",
      "iteration 1525, dc_loss: 0.045123253017663956, tv_loss: 0.03323425352573395\n",
      "iteration 1526, dc_loss: 0.045169759541749954, tv_loss: 0.0331433042883873\n",
      "iteration 1527, dc_loss: 0.04506244137883186, tv_loss: 0.033209044486284256\n",
      "iteration 1528, dc_loss: 0.0450538694858551, tv_loss: 0.033183347433805466\n",
      "iteration 1529, dc_loss: 0.04502391815185547, tv_loss: 0.03316831216216087\n",
      "iteration 1530, dc_loss: 0.04496745392680168, tv_loss: 0.033199019730091095\n",
      "iteration 1531, dc_loss: 0.044967763125896454, tv_loss: 0.033164747059345245\n",
      "iteration 1532, dc_loss: 0.04490135982632637, tv_loss: 0.033208806067705154\n",
      "iteration 1533, dc_loss: 0.04493609070777893, tv_loss: 0.033148813992738724\n",
      "iteration 1534, dc_loss: 0.04485045373439789, tv_loss: 0.03322172909975052\n",
      "iteration 1535, dc_loss: 0.0449201762676239, tv_loss: 0.03315296396613121\n",
      "iteration 1536, dc_loss: 0.0448615625500679, tv_loss: 0.03322425112128258\n",
      "iteration 1537, dc_loss: 0.044916197657585144, tv_loss: 0.03316102176904678\n",
      "iteration 1538, dc_loss: 0.04480234161019325, tv_loss: 0.033226873725652695\n",
      "iteration 1539, dc_loss: 0.04479339346289635, tv_loss: 0.03316088020801544\n",
      "iteration 1540, dc_loss: 0.04473253712058067, tv_loss: 0.03318672627210617\n",
      "iteration 1541, dc_loss: 0.0446803942322731, tv_loss: 0.03322916850447655\n",
      "iteration 1542, dc_loss: 0.04472867771983147, tv_loss: 0.033202312886714935\n",
      "iteration 1543, dc_loss: 0.04456165060400963, tv_loss: 0.03328782692551613\n",
      "iteration 1544, dc_loss: 0.044661957770586014, tv_loss: 0.03311682119965553\n",
      "iteration 1545, dc_loss: 0.04447430372238159, tv_loss: 0.033290795981884\n",
      "iteration 1546, dc_loss: 0.04462825879454613, tv_loss: 0.03317764401435852\n",
      "iteration 1547, dc_loss: 0.044532593339681625, tv_loss: 0.033275485038757324\n",
      "iteration 1548, dc_loss: 0.04456157609820366, tv_loss: 0.03316812962293625\n",
      "iteration 1549, dc_loss: 0.044410910457372665, tv_loss: 0.03326670452952385\n",
      "iteration 1550, dc_loss: 0.044563621282577515, tv_loss: 0.03314550220966339\n",
      "iteration 1551, dc_loss: 0.04434719681739807, tv_loss: 0.03334710747003555\n",
      "iteration 1552, dc_loss: 0.04452628642320633, tv_loss: 0.0330723412334919\n",
      "iteration 1553, dc_loss: 0.04424440115690231, tv_loss: 0.03333887830376625\n",
      "iteration 1554, dc_loss: 0.04438071325421333, tv_loss: 0.0331558957695961\n",
      "iteration 1555, dc_loss: 0.044259000569581985, tv_loss: 0.03324935957789421\n",
      "iteration 1556, dc_loss: 0.04420929029583931, tv_loss: 0.03323429822921753\n",
      "iteration 1557, dc_loss: 0.04419402405619621, tv_loss: 0.03319530561566353\n",
      "iteration 1558, dc_loss: 0.0441463328897953, tv_loss: 0.03325071930885315\n",
      "iteration 1559, dc_loss: 0.044132255017757416, tv_loss: 0.03321712091565132\n",
      "iteration 1560, dc_loss: 0.044067345559597015, tv_loss: 0.033229485154151917\n",
      "iteration 1561, dc_loss: 0.04406512528657913, tv_loss: 0.03319605812430382\n",
      "iteration 1562, dc_loss: 0.0439378097653389, tv_loss: 0.03330022096633911\n",
      "iteration 1563, dc_loss: 0.044071998447179794, tv_loss: 0.033158447593450546\n",
      "iteration 1564, dc_loss: 0.043910253793001175, tv_loss: 0.03327008709311485\n",
      "iteration 1565, dc_loss: 0.04397926852107048, tv_loss: 0.03316175937652588\n",
      "iteration 1566, dc_loss: 0.04382500424981117, tv_loss: 0.033306654542684555\n",
      "iteration 1567, dc_loss: 0.04392710700631142, tv_loss: 0.033161357045173645\n",
      "iteration 1568, dc_loss: 0.043758559972047806, tv_loss: 0.03328879922628403\n",
      "iteration 1569, dc_loss: 0.04380819946527481, tv_loss: 0.033206842839717865\n",
      "iteration 1570, dc_loss: 0.043691329658031464, tv_loss: 0.033271778374910355\n",
      "iteration 1571, dc_loss: 0.043713320046663284, tv_loss: 0.033211153000593185\n",
      "iteration 1572, dc_loss: 0.04365380108356476, tv_loss: 0.03322568163275719\n",
      "iteration 1573, dc_loss: 0.04361637681722641, tv_loss: 0.03324493020772934\n",
      "iteration 1574, dc_loss: 0.04360467195510864, tv_loss: 0.03322586789727211\n",
      "iteration 1575, dc_loss: 0.04354798421263695, tv_loss: 0.033254869282245636\n",
      "iteration 1576, dc_loss: 0.043547503650188446, tv_loss: 0.033203113824129105\n",
      "iteration 1577, dc_loss: 0.043467793613672256, tv_loss: 0.033245280385017395\n",
      "iteration 1578, dc_loss: 0.04349783435463905, tv_loss: 0.03321220725774765\n",
      "iteration 1579, dc_loss: 0.04340628907084465, tv_loss: 0.03326854109764099\n",
      "iteration 1580, dc_loss: 0.04341856762766838, tv_loss: 0.033206187188625336\n",
      "iteration 1581, dc_loss: 0.043317437171936035, tv_loss: 0.033292870968580246\n",
      "iteration 1582, dc_loss: 0.043385155498981476, tv_loss: 0.03320447355508804\n",
      "iteration 1583, dc_loss: 0.0432593896985054, tv_loss: 0.03329843282699585\n",
      "iteration 1584, dc_loss: 0.043339088559150696, tv_loss: 0.03319764882326126\n",
      "iteration 1585, dc_loss: 0.043207183480262756, tv_loss: 0.033287499099969864\n",
      "iteration 1586, dc_loss: 0.043297868221998215, tv_loss: 0.033184826374053955\n",
      "iteration 1587, dc_loss: 0.04313036799430847, tv_loss: 0.03336359187960625\n",
      "iteration 1588, dc_loss: 0.04330079257488251, tv_loss: 0.03315342217683792\n",
      "iteration 1589, dc_loss: 0.04308361932635307, tv_loss: 0.0333721786737442\n",
      "iteration 1590, dc_loss: 0.043309275060892105, tv_loss: 0.033141616731882095\n",
      "iteration 1591, dc_loss: 0.043071482330560684, tv_loss: 0.03339708223938942\n",
      "iteration 1592, dc_loss: 0.043335895985364914, tv_loss: 0.03312919661402702\n",
      "iteration 1593, dc_loss: 0.04302675649523735, tv_loss: 0.033459439873695374\n",
      "iteration 1594, dc_loss: 0.0433502122759819, tv_loss: 0.03306637331843376\n",
      "iteration 1595, dc_loss: 0.04294608160853386, tv_loss: 0.03343343734741211\n",
      "iteration 1596, dc_loss: 0.04317319393157959, tv_loss: 0.033109020441770554\n",
      "iteration 1597, dc_loss: 0.04282980039715767, tv_loss: 0.033376943320035934\n",
      "iteration 1598, dc_loss: 0.042900972068309784, tv_loss: 0.0332157239317894\n",
      "iteration 1599, dc_loss: 0.04281468316912651, tv_loss: 0.03326043114066124\n",
      "iteration 1600, dc_loss: 0.042732395231723785, tv_loss: 0.033328913152217865\n",
      "iteration 1601, dc_loss: 0.04288751259446144, tv_loss: 0.033166322857141495\n",
      "iteration 1602, dc_loss: 0.04267454892396927, tv_loss: 0.03332098200917244\n",
      "iteration 1603, dc_loss: 0.042675577104091644, tv_loss: 0.03326738625764847\n",
      "iteration 1604, dc_loss: 0.04277372732758522, tv_loss: 0.033184703439474106\n",
      "iteration 1605, dc_loss: 0.04261491075158119, tv_loss: 0.03331591561436653\n",
      "iteration 1606, dc_loss: 0.04262440279126167, tv_loss: 0.033266156911849976\n",
      "iteration 1607, dc_loss: 0.04266820847988129, tv_loss: 0.03321525454521179\n",
      "iteration 1608, dc_loss: 0.04253688454627991, tv_loss: 0.033319927752017975\n",
      "iteration 1609, dc_loss: 0.042570095509290695, tv_loss: 0.033239517360925674\n",
      "iteration 1610, dc_loss: 0.04257320985198021, tv_loss: 0.03322241082787514\n",
      "iteration 1611, dc_loss: 0.04246283695101738, tv_loss: 0.03331777825951576\n",
      "iteration 1612, dc_loss: 0.04250221326947212, tv_loss: 0.033231835812330246\n",
      "iteration 1613, dc_loss: 0.04248928651213646, tv_loss: 0.03322065621614456\n",
      "iteration 1614, dc_loss: 0.04239761829376221, tv_loss: 0.03329930081963539\n",
      "iteration 1615, dc_loss: 0.042426951229572296, tv_loss: 0.03323420137166977\n",
      "iteration 1616, dc_loss: 0.04240080341696739, tv_loss: 0.033248644322156906\n",
      "iteration 1617, dc_loss: 0.042338140308856964, tv_loss: 0.03331002593040466\n",
      "iteration 1618, dc_loss: 0.04236859455704689, tv_loss: 0.03324954956769943\n",
      "iteration 1619, dc_loss: 0.042307958006858826, tv_loss: 0.03326459228992462\n",
      "iteration 1620, dc_loss: 0.0422666035592556, tv_loss: 0.03328494355082512\n",
      "iteration 1621, dc_loss: 0.04229806736111641, tv_loss: 0.03322797268629074\n",
      "iteration 1622, dc_loss: 0.04223429039120674, tv_loss: 0.03326764330267906\n",
      "iteration 1623, dc_loss: 0.04220985993742943, tv_loss: 0.03328050300478935\n",
      "iteration 1624, dc_loss: 0.042215995490550995, tv_loss: 0.03326112776994705\n",
      "iteration 1625, dc_loss: 0.042160212993621826, tv_loss: 0.033290307968854904\n",
      "iteration 1626, dc_loss: 0.04214964434504509, tv_loss: 0.03327289596199989\n",
      "iteration 1627, dc_loss: 0.04213767871260643, tv_loss: 0.03326092287898064\n",
      "iteration 1628, dc_loss: 0.04208408296108246, tv_loss: 0.03329014405608177\n",
      "iteration 1629, dc_loss: 0.04209063947200775, tv_loss: 0.03326703608036041\n",
      "iteration 1630, dc_loss: 0.0420660637319088, tv_loss: 0.03326814994215965\n",
      "iteration 1631, dc_loss: 0.04202267527580261, tv_loss: 0.033291276544332504\n",
      "iteration 1632, dc_loss: 0.04201515018939972, tv_loss: 0.0332779623568058\n",
      "iteration 1633, dc_loss: 0.041986025869846344, tv_loss: 0.03327517956495285\n",
      "iteration 1634, dc_loss: 0.041962698101997375, tv_loss: 0.03327434882521629\n",
      "iteration 1635, dc_loss: 0.04194728657603264, tv_loss: 0.033273618668317795\n",
      "iteration 1636, dc_loss: 0.041914746165275574, tv_loss: 0.033280231058597565\n",
      "iteration 1637, dc_loss: 0.041897837072610855, tv_loss: 0.033269379287958145\n",
      "iteration 1638, dc_loss: 0.04187353327870369, tv_loss: 0.033270739018917084\n",
      "iteration 1639, dc_loss: 0.04184645414352417, tv_loss: 0.033285144716501236\n",
      "iteration 1640, dc_loss: 0.04183436185121536, tv_loss: 0.03326587751507759\n",
      "iteration 1641, dc_loss: 0.041805945336818695, tv_loss: 0.033273693174123764\n",
      "iteration 1642, dc_loss: 0.04178272932767868, tv_loss: 0.033274419605731964\n",
      "iteration 1643, dc_loss: 0.04176971688866615, tv_loss: 0.03326890617609024\n",
      "iteration 1644, dc_loss: 0.04172113165259361, tv_loss: 0.03330446779727936\n",
      "iteration 1645, dc_loss: 0.04171319305896759, tv_loss: 0.03332610800862312\n",
      "iteration 1646, dc_loss: 0.041704125702381134, tv_loss: 0.03330226242542267\n",
      "iteration 1647, dc_loss: 0.04166697338223457, tv_loss: 0.03328360989689827\n",
      "iteration 1648, dc_loss: 0.0416516549885273, tv_loss: 0.033306363970041275\n",
      "iteration 1649, dc_loss: 0.04163126274943352, tv_loss: 0.03331628069281578\n",
      "iteration 1650, dc_loss: 0.041600532829761505, tv_loss: 0.033302903175354004\n",
      "iteration 1651, dc_loss: 0.04158221557736397, tv_loss: 0.03328564763069153\n",
      "iteration 1652, dc_loss: 0.041557811200618744, tv_loss: 0.03329747915267944\n",
      "iteration 1653, dc_loss: 0.04153706133365631, tv_loss: 0.03331030532717705\n",
      "iteration 1654, dc_loss: 0.04151371866464615, tv_loss: 0.03331425040960312\n",
      "iteration 1655, dc_loss: 0.04149094596505165, tv_loss: 0.03330133855342865\n",
      "iteration 1656, dc_loss: 0.041474342346191406, tv_loss: 0.03328549861907959\n",
      "iteration 1657, dc_loss: 0.0414491668343544, tv_loss: 0.03329499810934067\n",
      "iteration 1658, dc_loss: 0.04142599180340767, tv_loss: 0.03331165388226509\n",
      "iteration 1659, dc_loss: 0.04140530154109001, tv_loss: 0.03331628441810608\n",
      "iteration 1660, dc_loss: 0.04138270020484924, tv_loss: 0.03329966589808464\n",
      "iteration 1661, dc_loss: 0.041361089795827866, tv_loss: 0.03329348564147949\n",
      "iteration 1662, dc_loss: 0.04133491590619087, tv_loss: 0.03330034017562866\n",
      "iteration 1663, dc_loss: 0.04131712019443512, tv_loss: 0.0332946740090847\n",
      "iteration 1664, dc_loss: 0.041295550763607025, tv_loss: 0.03330012783408165\n",
      "iteration 1665, dc_loss: 0.04126908630132675, tv_loss: 0.03331061452627182\n",
      "iteration 1666, dc_loss: 0.041250184178352356, tv_loss: 0.03330469876527786\n",
      "iteration 1667, dc_loss: 0.041235920041799545, tv_loss: 0.03329246863722801\n",
      "iteration 1668, dc_loss: 0.04120588302612305, tv_loss: 0.033307626843452454\n",
      "iteration 1669, dc_loss: 0.041188471019268036, tv_loss: 0.033298004418611526\n",
      "iteration 1670, dc_loss: 0.0411592535674572, tv_loss: 0.03330080583691597\n",
      "iteration 1671, dc_loss: 0.04114213213324547, tv_loss: 0.03329508751630783\n",
      "iteration 1672, dc_loss: 0.04112211614847183, tv_loss: 0.03329605609178543\n",
      "iteration 1673, dc_loss: 0.04109609127044678, tv_loss: 0.03329671546816826\n",
      "iteration 1674, dc_loss: 0.041077326983213425, tv_loss: 0.03330029547214508\n",
      "iteration 1675, dc_loss: 0.04106113314628601, tv_loss: 0.03330518305301666\n",
      "iteration 1676, dc_loss: 0.04103042930364609, tv_loss: 0.033350713551044464\n",
      "iteration 1677, dc_loss: 0.04101888835430145, tv_loss: 0.033330995589494705\n",
      "iteration 1678, dc_loss: 0.04098882898688316, tv_loss: 0.03330915793776512\n",
      "iteration 1679, dc_loss: 0.04096450284123421, tv_loss: 0.03333364054560661\n",
      "iteration 1680, dc_loss: 0.04095061495900154, tv_loss: 0.03335953131318092\n",
      "iteration 1681, dc_loss: 0.040922991931438446, tv_loss: 0.033342525362968445\n",
      "iteration 1682, dc_loss: 0.04091301187872887, tv_loss: 0.03332259878516197\n",
      "iteration 1683, dc_loss: 0.040893714874982834, tv_loss: 0.033361852169036865\n",
      "iteration 1684, dc_loss: 0.040851809084415436, tv_loss: 0.03334902599453926\n",
      "iteration 1685, dc_loss: 0.04085060581564903, tv_loss: 0.03333507105708122\n",
      "iteration 1686, dc_loss: 0.04083679988980293, tv_loss: 0.03334443271160126\n",
      "iteration 1687, dc_loss: 0.04079042002558708, tv_loss: 0.0333498977124691\n",
      "iteration 1688, dc_loss: 0.04078632593154907, tv_loss: 0.03333185240626335\n",
      "iteration 1689, dc_loss: 0.04078572988510132, tv_loss: 0.033335864543914795\n",
      "iteration 1690, dc_loss: 0.040731772780418396, tv_loss: 0.03336895629763603\n",
      "iteration 1691, dc_loss: 0.040725227445364, tv_loss: 0.03333767503499985\n",
      "iteration 1692, dc_loss: 0.04073113203048706, tv_loss: 0.03332488611340523\n",
      "iteration 1693, dc_loss: 0.04068267345428467, tv_loss: 0.033348117023706436\n",
      "iteration 1694, dc_loss: 0.040662504732608795, tv_loss: 0.033334117382764816\n",
      "iteration 1695, dc_loss: 0.040637608617544174, tv_loss: 0.033333003520965576\n",
      "iteration 1696, dc_loss: 0.04060981422662735, tv_loss: 0.03334713727235794\n",
      "iteration 1697, dc_loss: 0.040599558502435684, tv_loss: 0.03333447501063347\n",
      "iteration 1698, dc_loss: 0.04056726396083832, tv_loss: 0.03333212435245514\n",
      "iteration 1699, dc_loss: 0.040537212044000626, tv_loss: 0.033337898552417755\n",
      "iteration 1700, dc_loss: 0.040519118309020996, tv_loss: 0.033344391733407974\n",
      "iteration 1701, dc_loss: 0.040517594665288925, tv_loss: 0.033314384520053864\n",
      "iteration 1702, dc_loss: 0.04049033299088478, tv_loss: 0.03332038223743439\n",
      "iteration 1703, dc_loss: 0.040456950664520264, tv_loss: 0.0333375446498394\n",
      "iteration 1704, dc_loss: 0.040449127554893494, tv_loss: 0.03332480043172836\n",
      "iteration 1705, dc_loss: 0.040425971150398254, tv_loss: 0.03332367166876793\n",
      "iteration 1706, dc_loss: 0.04038985073566437, tv_loss: 0.033338986337184906\n",
      "iteration 1707, dc_loss: 0.04038085788488388, tv_loss: 0.03333912417292595\n",
      "iteration 1708, dc_loss: 0.04037294536828995, tv_loss: 0.03333038091659546\n",
      "iteration 1709, dc_loss: 0.04034982621669769, tv_loss: 0.03334445133805275\n",
      "iteration 1710, dc_loss: 0.04032423719763756, tv_loss: 0.03335275873541832\n",
      "iteration 1711, dc_loss: 0.040318239480257034, tv_loss: 0.03334957733750343\n",
      "iteration 1712, dc_loss: 0.04030834138393402, tv_loss: 0.03333383798599243\n",
      "iteration 1713, dc_loss: 0.04027866944670677, tv_loss: 0.0333402119576931\n",
      "iteration 1714, dc_loss: 0.040252797305583954, tv_loss: 0.033333562314510345\n",
      "iteration 1715, dc_loss: 0.04022742062807083, tv_loss: 0.03333393856883049\n",
      "iteration 1716, dc_loss: 0.0401962473988533, tv_loss: 0.03334281966090202\n",
      "iteration 1717, dc_loss: 0.040184758603572845, tv_loss: 0.03333958983421326\n",
      "iteration 1718, dc_loss: 0.04015615954995155, tv_loss: 0.03336217626929283\n",
      "iteration 1719, dc_loss: 0.04013088718056679, tv_loss: 0.033364035189151764\n",
      "iteration 1720, dc_loss: 0.04011773318052292, tv_loss: 0.03334120288491249\n",
      "iteration 1721, dc_loss: 0.040095265954732895, tv_loss: 0.033331580460071564\n",
      "iteration 1722, dc_loss: 0.04006299749016762, tv_loss: 0.03335673734545708\n",
      "iteration 1723, dc_loss: 0.04005458205938339, tv_loss: 0.033365145325660706\n",
      "iteration 1724, dc_loss: 0.04004170000553131, tv_loss: 0.03336295112967491\n",
      "iteration 1725, dc_loss: 0.040004223585128784, tv_loss: 0.03337115794420242\n",
      "iteration 1726, dc_loss: 0.04000415280461311, tv_loss: 0.033337511122226715\n",
      "iteration 1727, dc_loss: 0.03999623283743858, tv_loss: 0.033331479877233505\n",
      "iteration 1728, dc_loss: 0.039963528513908386, tv_loss: 0.03335871919989586\n",
      "iteration 1729, dc_loss: 0.03994292765855789, tv_loss: 0.03337507322430611\n",
      "iteration 1730, dc_loss: 0.03992123529314995, tv_loss: 0.03337451070547104\n",
      "iteration 1731, dc_loss: 0.03989654779434204, tv_loss: 0.03336047753691673\n",
      "iteration 1732, dc_loss: 0.03988143056631088, tv_loss: 0.03335147723555565\n",
      "iteration 1733, dc_loss: 0.03986576199531555, tv_loss: 0.033348891884088516\n",
      "iteration 1734, dc_loss: 0.03983338177204132, tv_loss: 0.03336695209145546\n",
      "iteration 1735, dc_loss: 0.03981298208236694, tv_loss: 0.03336730971932411\n",
      "iteration 1736, dc_loss: 0.03979756310582161, tv_loss: 0.03335229307413101\n",
      "iteration 1737, dc_loss: 0.039764124900102615, tv_loss: 0.03335721045732498\n",
      "iteration 1738, dc_loss: 0.039760734885931015, tv_loss: 0.03334031626582146\n",
      "iteration 1739, dc_loss: 0.039714265614748, tv_loss: 0.03336285054683685\n",
      "iteration 1740, dc_loss: 0.03971545025706291, tv_loss: 0.033344537019729614\n",
      "iteration 1741, dc_loss: 0.03968760743737221, tv_loss: 0.03334575518965721\n",
      "iteration 1742, dc_loss: 0.039663445204496384, tv_loss: 0.03335297480225563\n",
      "iteration 1743, dc_loss: 0.039654336869716644, tv_loss: 0.03335420414805412\n",
      "iteration 1744, dc_loss: 0.039620544761419296, tv_loss: 0.0333823636174202\n",
      "iteration 1745, dc_loss: 0.03961917757987976, tv_loss: 0.03337014093995094\n",
      "iteration 1746, dc_loss: 0.039590831845998764, tv_loss: 0.033366382122039795\n",
      "iteration 1747, dc_loss: 0.039561253041028976, tv_loss: 0.03336778283119202\n",
      "iteration 1748, dc_loss: 0.039538297802209854, tv_loss: 0.033374059945344925\n",
      "iteration 1749, dc_loss: 0.0395459346473217, tv_loss: 0.033344168215990067\n",
      "iteration 1750, dc_loss: 0.03950921818614006, tv_loss: 0.033365584909915924\n",
      "iteration 1751, dc_loss: 0.0394911915063858, tv_loss: 0.03336429223418236\n",
      "iteration 1752, dc_loss: 0.039475709199905396, tv_loss: 0.0333745963871479\n",
      "iteration 1753, dc_loss: 0.03946072980761528, tv_loss: 0.03339068964123726\n",
      "iteration 1754, dc_loss: 0.039464958012104034, tv_loss: 0.03336590528488159\n",
      "iteration 1755, dc_loss: 0.03946401923894882, tv_loss: 0.03338285908102989\n",
      "iteration 1756, dc_loss: 0.03944304212927818, tv_loss: 0.033372655510902405\n",
      "iteration 1757, dc_loss: 0.03943319991230965, tv_loss: 0.03336752951145172\n",
      "iteration 1758, dc_loss: 0.039378341287374496, tv_loss: 0.03338037431240082\n",
      "iteration 1759, dc_loss: 0.03934741020202637, tv_loss: 0.03338086977601051\n",
      "iteration 1760, dc_loss: 0.03930830955505371, tv_loss: 0.03340176120400429\n",
      "iteration 1761, dc_loss: 0.03936275467276573, tv_loss: 0.03333535045385361\n",
      "iteration 1762, dc_loss: 0.039305590093135834, tv_loss: 0.03339720517396927\n",
      "iteration 1763, dc_loss: 0.03932339325547218, tv_loss: 0.03335157409310341\n",
      "iteration 1764, dc_loss: 0.0392456091940403, tv_loss: 0.03339199721813202\n",
      "iteration 1765, dc_loss: 0.03922820836305618, tv_loss: 0.033389151096343994\n",
      "iteration 1766, dc_loss: 0.03924272581934929, tv_loss: 0.033345047384500504\n",
      "iteration 1767, dc_loss: 0.039180684834718704, tv_loss: 0.03341813385486603\n",
      "iteration 1768, dc_loss: 0.039231669157743454, tv_loss: 0.0333501361310482\n",
      "iteration 1769, dc_loss: 0.03915569558739662, tv_loss: 0.033406589180231094\n",
      "iteration 1770, dc_loss: 0.03917737677693367, tv_loss: 0.033349987119436264\n",
      "iteration 1771, dc_loss: 0.03909023106098175, tv_loss: 0.033397890627384186\n",
      "iteration 1772, dc_loss: 0.039079632610082626, tv_loss: 0.033383484929800034\n",
      "iteration 1773, dc_loss: 0.03909742087125778, tv_loss: 0.03336292505264282\n",
      "iteration 1774, dc_loss: 0.03904646262526512, tv_loss: 0.033394575119018555\n",
      "iteration 1775, dc_loss: 0.039070818573236465, tv_loss: 0.033352576196193695\n",
      "iteration 1776, dc_loss: 0.039004914462566376, tv_loss: 0.03340292349457741\n",
      "iteration 1777, dc_loss: 0.03902086243033409, tv_loss: 0.03338829427957535\n",
      "iteration 1778, dc_loss: 0.03896923363208771, tv_loss: 0.033426303416490555\n",
      "iteration 1779, dc_loss: 0.03896051272749901, tv_loss: 0.03337893635034561\n",
      "iteration 1780, dc_loss: 0.038934580981731415, tv_loss: 0.03338826447725296\n",
      "iteration 1781, dc_loss: 0.03891440108418465, tv_loss: 0.033399369567632675\n",
      "iteration 1782, dc_loss: 0.038903072476387024, tv_loss: 0.03340240567922592\n",
      "iteration 1783, dc_loss: 0.038885585963726044, tv_loss: 0.0333956703543663\n",
      "iteration 1784, dc_loss: 0.03887514770030975, tv_loss: 0.03337504714727402\n",
      "iteration 1785, dc_loss: 0.03884095326066017, tv_loss: 0.03339719772338867\n",
      "iteration 1786, dc_loss: 0.03883599489927292, tv_loss: 0.03337690234184265\n",
      "iteration 1787, dc_loss: 0.038802068680524826, tv_loss: 0.033393651247024536\n",
      "iteration 1788, dc_loss: 0.03878113627433777, tv_loss: 0.03340284526348114\n",
      "iteration 1789, dc_loss: 0.03879319131374359, tv_loss: 0.03336595371365547\n",
      "iteration 1790, dc_loss: 0.03872223198413849, tv_loss: 0.03341686725616455\n",
      "iteration 1791, dc_loss: 0.038758985698223114, tv_loss: 0.033377762883901596\n",
      "iteration 1792, dc_loss: 0.03870387747883797, tv_loss: 0.033404167741537094\n",
      "iteration 1793, dc_loss: 0.038744255900382996, tv_loss: 0.03335414454340935\n",
      "iteration 1794, dc_loss: 0.03864428028464317, tv_loss: 0.03344319760799408\n",
      "iteration 1795, dc_loss: 0.038744762539863586, tv_loss: 0.033326517790555954\n",
      "iteration 1796, dc_loss: 0.03861941769719124, tv_loss: 0.03345690667629242\n",
      "iteration 1797, dc_loss: 0.03873342275619507, tv_loss: 0.0333331823348999\n",
      "iteration 1798, dc_loss: 0.038614578545093536, tv_loss: 0.03345925360918045\n",
      "iteration 1799, dc_loss: 0.03873376548290253, tv_loss: 0.03334314376115799\n",
      "iteration 1800, dc_loss: 0.03855428844690323, tv_loss: 0.033501625061035156\n",
      "iteration 1801, dc_loss: 0.038654740899801254, tv_loss: 0.033344343304634094\n",
      "iteration 1802, dc_loss: 0.03850480541586876, tv_loss: 0.03344901651144028\n",
      "iteration 1803, dc_loss: 0.0385490320622921, tv_loss: 0.03336870297789574\n",
      "iteration 1804, dc_loss: 0.038474828004837036, tv_loss: 0.03343130648136139\n",
      "iteration 1805, dc_loss: 0.03847062587738037, tv_loss: 0.03339892625808716\n",
      "iteration 1806, dc_loss: 0.038447946310043335, tv_loss: 0.033411964774131775\n",
      "iteration 1807, dc_loss: 0.03843565285205841, tv_loss: 0.03340497985482216\n",
      "iteration 1808, dc_loss: 0.03844425454735756, tv_loss: 0.03339983522891998\n",
      "iteration 1809, dc_loss: 0.03839351609349251, tv_loss: 0.03346136584877968\n",
      "iteration 1810, dc_loss: 0.038430411368608475, tv_loss: 0.03338688239455223\n",
      "iteration 1811, dc_loss: 0.038347773253917694, tv_loss: 0.0334414504468441\n",
      "iteration 1812, dc_loss: 0.03838082775473595, tv_loss: 0.03340751305222511\n",
      "iteration 1813, dc_loss: 0.038315724581480026, tv_loss: 0.03345310315489769\n",
      "iteration 1814, dc_loss: 0.03834206983447075, tv_loss: 0.03339659422636032\n",
      "iteration 1815, dc_loss: 0.03826019540429115, tv_loss: 0.03345159813761711\n",
      "iteration 1816, dc_loss: 0.0382719412446022, tv_loss: 0.03344615176320076\n",
      "iteration 1817, dc_loss: 0.03826090320944786, tv_loss: 0.033416565507650375\n",
      "iteration 1818, dc_loss: 0.03821796551346779, tv_loss: 0.03344513475894928\n",
      "iteration 1819, dc_loss: 0.0382252037525177, tv_loss: 0.03345496952533722\n",
      "iteration 1820, dc_loss: 0.03819219022989273, tv_loss: 0.03342602774500847\n",
      "iteration 1821, dc_loss: 0.03817256540060043, tv_loss: 0.033447716385126114\n",
      "iteration 1822, dc_loss: 0.03813029080629349, tv_loss: 0.03347904980182648\n",
      "iteration 1823, dc_loss: 0.038188111037015915, tv_loss: 0.033385150134563446\n",
      "iteration 1824, dc_loss: 0.03809487819671631, tv_loss: 0.03347153961658478\n",
      "iteration 1825, dc_loss: 0.03812740370631218, tv_loss: 0.03342166170477867\n",
      "iteration 1826, dc_loss: 0.03808412700891495, tv_loss: 0.03344672545790672\n",
      "iteration 1827, dc_loss: 0.03812265396118164, tv_loss: 0.033395715057849884\n",
      "iteration 1828, dc_loss: 0.03803695738315582, tv_loss: 0.03348428010940552\n",
      "iteration 1829, dc_loss: 0.038131289184093475, tv_loss: 0.033391259610652924\n",
      "iteration 1830, dc_loss: 0.03801882639527321, tv_loss: 0.033479250967502594\n",
      "iteration 1831, dc_loss: 0.03808685392141342, tv_loss: 0.033380039036273956\n",
      "iteration 1832, dc_loss: 0.03797008469700813, tv_loss: 0.03349221497774124\n",
      "iteration 1833, dc_loss: 0.03802488371729851, tv_loss: 0.03339152783155441\n",
      "iteration 1834, dc_loss: 0.037928659468889236, tv_loss: 0.03346579894423485\n",
      "iteration 1835, dc_loss: 0.0379817821085453, tv_loss: 0.033404238522052765\n",
      "iteration 1836, dc_loss: 0.03787629306316376, tv_loss: 0.03348397836089134\n",
      "iteration 1837, dc_loss: 0.037954289466142654, tv_loss: 0.03340084105730057\n",
      "iteration 1838, dc_loss: 0.0378948412835598, tv_loss: 0.03344554081559181\n",
      "iteration 1839, dc_loss: 0.03792406991124153, tv_loss: 0.033404309302568436\n",
      "iteration 1840, dc_loss: 0.037850189954042435, tv_loss: 0.03348315507173538\n",
      "iteration 1841, dc_loss: 0.037899550050497055, tv_loss: 0.033387716859579086\n",
      "iteration 1842, dc_loss: 0.03778940811753273, tv_loss: 0.0334772951900959\n",
      "iteration 1843, dc_loss: 0.03785316273570061, tv_loss: 0.03340804576873779\n",
      "iteration 1844, dc_loss: 0.03774845227599144, tv_loss: 0.033475473523139954\n",
      "iteration 1845, dc_loss: 0.03780355677008629, tv_loss: 0.03340376913547516\n",
      "iteration 1846, dc_loss: 0.037710100412368774, tv_loss: 0.033474184572696686\n",
      "iteration 1847, dc_loss: 0.037759844213724136, tv_loss: 0.03341035917401314\n",
      "iteration 1848, dc_loss: 0.03768492117524147, tv_loss: 0.03346775844693184\n",
      "iteration 1849, dc_loss: 0.03772193565964699, tv_loss: 0.03339463099837303\n",
      "iteration 1850, dc_loss: 0.037632476538419724, tv_loss: 0.033473823219537735\n",
      "iteration 1851, dc_loss: 0.03768020123243332, tv_loss: 0.033411309123039246\n",
      "iteration 1852, dc_loss: 0.0376160629093647, tv_loss: 0.03345893695950508\n",
      "iteration 1853, dc_loss: 0.037644267082214355, tv_loss: 0.0334073044359684\n",
      "iteration 1854, dc_loss: 0.037575628608465195, tv_loss: 0.03347153216600418\n",
      "iteration 1855, dc_loss: 0.03762960806488991, tv_loss: 0.033391356468200684\n",
      "iteration 1856, dc_loss: 0.03756806626915932, tv_loss: 0.03345399722456932\n",
      "iteration 1857, dc_loss: 0.03758932277560234, tv_loss: 0.033423446118831635\n",
      "iteration 1858, dc_loss: 0.037529364228248596, tv_loss: 0.03348419442772865\n",
      "iteration 1859, dc_loss: 0.03757128864526749, tv_loss: 0.03341899812221527\n",
      "iteration 1860, dc_loss: 0.03749594837427139, tv_loss: 0.03346720710396767\n",
      "iteration 1861, dc_loss: 0.03754623606801033, tv_loss: 0.03339860215783119\n",
      "iteration 1862, dc_loss: 0.03743177652359009, tv_loss: 0.03350578621029854\n",
      "iteration 1863, dc_loss: 0.0375126376748085, tv_loss: 0.03340717777609825\n",
      "iteration 1864, dc_loss: 0.03739561140537262, tv_loss: 0.03349902480840683\n",
      "iteration 1865, dc_loss: 0.03748588636517525, tv_loss: 0.0333826020359993\n",
      "iteration 1866, dc_loss: 0.03735538572072983, tv_loss: 0.03351388871669769\n",
      "iteration 1867, dc_loss: 0.0374910905957222, tv_loss: 0.033368952572345734\n",
      "iteration 1868, dc_loss: 0.037338901311159134, tv_loss: 0.03352149948477745\n",
      "iteration 1869, dc_loss: 0.037490375339984894, tv_loss: 0.033363040536642075\n",
      "iteration 1870, dc_loss: 0.03733311593532562, tv_loss: 0.033543217927217484\n",
      "iteration 1871, dc_loss: 0.0374998040497303, tv_loss: 0.03335348889231682\n",
      "iteration 1872, dc_loss: 0.037297118455171585, tv_loss: 0.03355421498417854\n",
      "iteration 1873, dc_loss: 0.03745628148317337, tv_loss: 0.033344071358442307\n",
      "iteration 1874, dc_loss: 0.03723990172147751, tv_loss: 0.033517006784677505\n",
      "iteration 1875, dc_loss: 0.03731848672032356, tv_loss: 0.03338657692074776\n",
      "iteration 1876, dc_loss: 0.03719756007194519, tv_loss: 0.0334717258810997\n",
      "iteration 1877, dc_loss: 0.037203919142484665, tv_loss: 0.03343375772237778\n",
      "iteration 1878, dc_loss: 0.03718789666891098, tv_loss: 0.033427465707063675\n",
      "iteration 1879, dc_loss: 0.037137556821107864, tv_loss: 0.03347253426909447\n",
      "iteration 1880, dc_loss: 0.037202950567007065, tv_loss: 0.033414360135793686\n",
      "iteration 1881, dc_loss: 0.03711598366498947, tv_loss: 0.033509302884340286\n",
      "iteration 1882, dc_loss: 0.03718894347548485, tv_loss: 0.03341908007860184\n",
      "iteration 1883, dc_loss: 0.03706802800297737, tv_loss: 0.03350861370563507\n",
      "iteration 1884, dc_loss: 0.03714991360902786, tv_loss: 0.0333981029689312\n",
      "iteration 1885, dc_loss: 0.037038207054138184, tv_loss: 0.03350019454956055\n",
      "iteration 1886, dc_loss: 0.03708723187446594, tv_loss: 0.03342783451080322\n",
      "iteration 1887, dc_loss: 0.03702494874596596, tv_loss: 0.033474430441856384\n",
      "iteration 1888, dc_loss: 0.037018414586782455, tv_loss: 0.03346780687570572\n",
      "iteration 1889, dc_loss: 0.03702213242650032, tv_loss: 0.03344457969069481\n",
      "iteration 1890, dc_loss: 0.03697705268859863, tv_loss: 0.03348725661635399\n",
      "iteration 1891, dc_loss: 0.03703621029853821, tv_loss: 0.033430181443691254\n",
      "iteration 1892, dc_loss: 0.03695531561970711, tv_loss: 0.03350795432925224\n",
      "iteration 1893, dc_loss: 0.03703581541776657, tv_loss: 0.033398617058992386\n",
      "iteration 1894, dc_loss: 0.03687199577689171, tv_loss: 0.033539898693561554\n",
      "iteration 1895, dc_loss: 0.03699794411659241, tv_loss: 0.03338845819234848\n",
      "iteration 1896, dc_loss: 0.036869391798973083, tv_loss: 0.03351539373397827\n",
      "iteration 1897, dc_loss: 0.03696458414196968, tv_loss: 0.03341960534453392\n",
      "iteration 1898, dc_loss: 0.03685633838176727, tv_loss: 0.03351694345474243\n",
      "iteration 1899, dc_loss: 0.03694215416908264, tv_loss: 0.03342575579881668\n",
      "iteration 1900, dc_loss: 0.03682824596762657, tv_loss: 0.03350689262151718\n",
      "iteration 1901, dc_loss: 0.03690434992313385, tv_loss: 0.03341226652264595\n",
      "iteration 1902, dc_loss: 0.03678601607680321, tv_loss: 0.033513981848955154\n",
      "iteration 1903, dc_loss: 0.03683709353208542, tv_loss: 0.033443890511989594\n",
      "iteration 1904, dc_loss: 0.036742594093084335, tv_loss: 0.03352329134941101\n",
      "iteration 1905, dc_loss: 0.03678470104932785, tv_loss: 0.03343707695603371\n",
      "iteration 1906, dc_loss: 0.03669676557183266, tv_loss: 0.033489521592855453\n",
      "iteration 1907, dc_loss: 0.03672821819782257, tv_loss: 0.033442240208387375\n",
      "iteration 1908, dc_loss: 0.03666852414608002, tv_loss: 0.03348499536514282\n",
      "iteration 1909, dc_loss: 0.03666811063885689, tv_loss: 0.033481307327747345\n",
      "iteration 1910, dc_loss: 0.03667629882693291, tv_loss: 0.03347938880324364\n",
      "iteration 1911, dc_loss: 0.036650266498327255, tv_loss: 0.03348805010318756\n",
      "iteration 1912, dc_loss: 0.03665202856063843, tv_loss: 0.03347857668995857\n",
      "iteration 1913, dc_loss: 0.03663752228021622, tv_loss: 0.03346254676580429\n",
      "iteration 1914, dc_loss: 0.036621447652578354, tv_loss: 0.033468812704086304\n",
      "iteration 1915, dc_loss: 0.036597952246665955, tv_loss: 0.033466316759586334\n",
      "iteration 1916, dc_loss: 0.03657171502709389, tv_loss: 0.03346581012010574\n",
      "iteration 1917, dc_loss: 0.036539461463689804, tv_loss: 0.03347478061914444\n",
      "iteration 1918, dc_loss: 0.036549944430589676, tv_loss: 0.03344366326928139\n",
      "iteration 1919, dc_loss: 0.03648447245359421, tv_loss: 0.03349654749035835\n",
      "iteration 1920, dc_loss: 0.03650354593992233, tv_loss: 0.033486828207969666\n",
      "iteration 1921, dc_loss: 0.03645613417029381, tv_loss: 0.03351552411913872\n",
      "iteration 1922, dc_loss: 0.03648124262690544, tv_loss: 0.03346467390656471\n",
      "iteration 1923, dc_loss: 0.03643961250782013, tv_loss: 0.03348662704229355\n",
      "iteration 1924, dc_loss: 0.036460455507040024, tv_loss: 0.03345396742224693\n",
      "iteration 1925, dc_loss: 0.03639443591237068, tv_loss: 0.03351238742470741\n",
      "iteration 1926, dc_loss: 0.036429356783628464, tv_loss: 0.033462949097156525\n",
      "iteration 1927, dc_loss: 0.03637082502245903, tv_loss: 0.033500466495752335\n",
      "iteration 1928, dc_loss: 0.03641215339303017, tv_loss: 0.03344361111521721\n",
      "iteration 1929, dc_loss: 0.03632853180170059, tv_loss: 0.03351140767335892\n",
      "iteration 1930, dc_loss: 0.03638162463903427, tv_loss: 0.03344130143523216\n",
      "iteration 1931, dc_loss: 0.03628868609666824, tv_loss: 0.03351474553346634\n",
      "iteration 1932, dc_loss: 0.03636625409126282, tv_loss: 0.033427901566028595\n",
      "iteration 1933, dc_loss: 0.03625830262899399, tv_loss: 0.033528950065374374\n",
      "iteration 1934, dc_loss: 0.03636869788169861, tv_loss: 0.03342186659574509\n",
      "iteration 1935, dc_loss: 0.03624008223414421, tv_loss: 0.03358004242181778\n",
      "iteration 1936, dc_loss: 0.036407146602869034, tv_loss: 0.0334172323346138\n",
      "iteration 1937, dc_loss: 0.03624245896935463, tv_loss: 0.03359886631369591\n",
      "iteration 1938, dc_loss: 0.03649546951055527, tv_loss: 0.033353351056575775\n",
      "iteration 1939, dc_loss: 0.03626098856329918, tv_loss: 0.033644989132881165\n",
      "iteration 1940, dc_loss: 0.036596570163965225, tv_loss: 0.03333193063735962\n",
      "iteration 1941, dc_loss: 0.03627876564860344, tv_loss: 0.03367355838418007\n",
      "iteration 1942, dc_loss: 0.03654003515839577, tv_loss: 0.03333768621087074\n",
      "iteration 1943, dc_loss: 0.03619784116744995, tv_loss: 0.03359696641564369\n",
      "iteration 1944, dc_loss: 0.03626788407564163, tv_loss: 0.0334208682179451\n",
      "iteration 1945, dc_loss: 0.03609734773635864, tv_loss: 0.03351932018995285\n",
      "iteration 1946, dc_loss: 0.036082297563552856, tv_loss: 0.03350507840514183\n",
      "iteration 1947, dc_loss: 0.036160342395305634, tv_loss: 0.03342769294977188\n",
      "iteration 1948, dc_loss: 0.03604225069284439, tv_loss: 0.033580947667360306\n",
      "iteration 1949, dc_loss: 0.036241594702005386, tv_loss: 0.033384352922439575\n",
      "iteration 1950, dc_loss: 0.03600647673010826, tv_loss: 0.03361103683710098\n",
      "iteration 1951, dc_loss: 0.036130160093307495, tv_loss: 0.03341999277472496\n",
      "iteration 1952, dc_loss: 0.03596845269203186, tv_loss: 0.0335240513086319\n",
      "iteration 1953, dc_loss: 0.035981517285108566, tv_loss: 0.033489007502794266\n",
      "iteration 1954, dc_loss: 0.03599349036812782, tv_loss: 0.03345958888530731\n",
      "iteration 1955, dc_loss: 0.03590799868106842, tv_loss: 0.033549774438142776\n",
      "iteration 1956, dc_loss: 0.03605277091264725, tv_loss: 0.033417366445064545\n",
      "iteration 1957, dc_loss: 0.03588838875293732, tv_loss: 0.033566106110811234\n",
      "iteration 1958, dc_loss: 0.0359976701438427, tv_loss: 0.033424172550439835\n",
      "iteration 1959, dc_loss: 0.035856928676366806, tv_loss: 0.03356393054127693\n",
      "iteration 1960, dc_loss: 0.03588526323437691, tv_loss: 0.033508818596601486\n",
      "iteration 1961, dc_loss: 0.03586626425385475, tv_loss: 0.03349961340427399\n",
      "iteration 1962, dc_loss: 0.035815414041280746, tv_loss: 0.03353200480341911\n",
      "iteration 1963, dc_loss: 0.03588493913412094, tv_loss: 0.03348774090409279\n",
      "iteration 1964, dc_loss: 0.0358172208070755, tv_loss: 0.0335485115647316\n",
      "iteration 1965, dc_loss: 0.03586936369538307, tv_loss: 0.033483777195215225\n",
      "iteration 1966, dc_loss: 0.03581603243947029, tv_loss: 0.03353525698184967\n",
      "iteration 1967, dc_loss: 0.03582793101668358, tv_loss: 0.03351372107863426\n",
      "iteration 1968, dc_loss: 0.0357612781226635, tv_loss: 0.03352314233779907\n",
      "iteration 1969, dc_loss: 0.03575507923960686, tv_loss: 0.03349674493074417\n",
      "iteration 1970, dc_loss: 0.0357549712061882, tv_loss: 0.03348785266280174\n",
      "iteration 1971, dc_loss: 0.03570222854614258, tv_loss: 0.03355349972844124\n",
      "iteration 1972, dc_loss: 0.03575562685728073, tv_loss: 0.03348582610487938\n",
      "iteration 1973, dc_loss: 0.0356629304587841, tv_loss: 0.03354644402861595\n",
      "iteration 1974, dc_loss: 0.03570200502872467, tv_loss: 0.03347349539399147\n",
      "iteration 1975, dc_loss: 0.03565070405602455, tv_loss: 0.033500947058200836\n",
      "iteration 1976, dc_loss: 0.03562730923295021, tv_loss: 0.03350622206926346\n",
      "iteration 1977, dc_loss: 0.03563254699110985, tv_loss: 0.033479243516922\n",
      "iteration 1978, dc_loss: 0.03557881340384483, tv_loss: 0.0335325226187706\n",
      "iteration 1979, dc_loss: 0.03564052656292915, tv_loss: 0.03346535563468933\n",
      "iteration 1980, dc_loss: 0.035530395805835724, tv_loss: 0.03357355296611786\n",
      "iteration 1981, dc_loss: 0.03561728447675705, tv_loss: 0.03348112851381302\n",
      "iteration 1982, dc_loss: 0.03549826145172119, tv_loss: 0.03356276825070381\n",
      "iteration 1983, dc_loss: 0.03555871918797493, tv_loss: 0.033472690731287\n",
      "iteration 1984, dc_loss: 0.035496123135089874, tv_loss: 0.03353315591812134\n",
      "iteration 1985, dc_loss: 0.035500310361385345, tv_loss: 0.033507321029901505\n",
      "iteration 1986, dc_loss: 0.03547609597444534, tv_loss: 0.03352870047092438\n",
      "iteration 1987, dc_loss: 0.03546253591775894, tv_loss: 0.033525872975587845\n",
      "iteration 1988, dc_loss: 0.03545201197266579, tv_loss: 0.033503785729408264\n",
      "iteration 1989, dc_loss: 0.03544863313436508, tv_loss: 0.03350098803639412\n",
      "iteration 1990, dc_loss: 0.03541835397481918, tv_loss: 0.033517878502607346\n",
      "iteration 1991, dc_loss: 0.035388898104429245, tv_loss: 0.033545371145009995\n",
      "iteration 1992, dc_loss: 0.035400763154029846, tv_loss: 0.03352021053433418\n",
      "iteration 1993, dc_loss: 0.03538788855075836, tv_loss: 0.033518217504024506\n",
      "iteration 1994, dc_loss: 0.035394471138715744, tv_loss: 0.03351782634854317\n",
      "iteration 1995, dc_loss: 0.03539654612541199, tv_loss: 0.033519547432661057\n",
      "iteration 1996, dc_loss: 0.03545406460762024, tv_loss: 0.03350990265607834\n",
      "iteration 1997, dc_loss: 0.035406701266765594, tv_loss: 0.033548370003700256\n",
      "iteration 1998, dc_loss: 0.035405900329351425, tv_loss: 0.0335204154253006\n",
      "iteration 1999, dc_loss: 0.03534436970949173, tv_loss: 0.03354540839791298\n",
      "iteration 2000, dc_loss: 0.03537807986140251, tv_loss: 0.03350260108709335\n",
      "iteration 2001, dc_loss: 0.035331644117832184, tv_loss: 0.03353621065616608\n",
      "iteration 2002, dc_loss: 0.03530164435505867, tv_loss: 0.033507611602544785\n",
      "iteration 2003, dc_loss: 0.035247188061475754, tv_loss: 0.033500805497169495\n",
      "iteration 2004, dc_loss: 0.035268384963274, tv_loss: 0.03352503851056099\n",
      "iteration 2005, dc_loss: 0.03527506813406944, tv_loss: 0.03349357470870018\n",
      "iteration 2006, dc_loss: 0.03520110622048378, tv_loss: 0.03351808339357376\n",
      "iteration 2007, dc_loss: 0.035210225731134415, tv_loss: 0.033513475209474564\n",
      "iteration 2008, dc_loss: 0.03524310886859894, tv_loss: 0.033498384058475494\n",
      "iteration 2009, dc_loss: 0.03517228737473488, tv_loss: 0.033535219728946686\n",
      "iteration 2010, dc_loss: 0.0351596400141716, tv_loss: 0.0335240438580513\n",
      "iteration 2011, dc_loss: 0.035184334963560104, tv_loss: 0.03350590169429779\n",
      "iteration 2012, dc_loss: 0.03514659032225609, tv_loss: 0.03351471200585365\n",
      "iteration 2013, dc_loss: 0.03512910380959511, tv_loss: 0.03350260108709335\n",
      "iteration 2014, dc_loss: 0.03513350710272789, tv_loss: 0.03350236266851425\n",
      "iteration 2015, dc_loss: 0.03511165827512741, tv_loss: 0.03351633623242378\n",
      "iteration 2016, dc_loss: 0.03509720787405968, tv_loss: 0.033497750759124756\n",
      "iteration 2017, dc_loss: 0.03508346155285835, tv_loss: 0.03350505232810974\n",
      "iteration 2018, dc_loss: 0.03507177159190178, tv_loss: 0.033514101058244705\n",
      "iteration 2019, dc_loss: 0.0350663885474205, tv_loss: 0.03349391743540764\n",
      "iteration 2020, dc_loss: 0.03503488004207611, tv_loss: 0.03352447599172592\n",
      "iteration 2021, dc_loss: 0.035032324492931366, tv_loss: 0.03352905809879303\n",
      "iteration 2022, dc_loss: 0.03504001349210739, tv_loss: 0.03351595252752304\n",
      "iteration 2023, dc_loss: 0.034994155168533325, tv_loss: 0.033536557108163834\n",
      "iteration 2024, dc_loss: 0.03499512001872063, tv_loss: 0.0335221029818058\n",
      "iteration 2025, dc_loss: 0.034999508410692215, tv_loss: 0.03350929543375969\n",
      "iteration 2026, dc_loss: 0.03496107459068298, tv_loss: 0.03354176506400108\n",
      "iteration 2027, dc_loss: 0.03496135026216507, tv_loss: 0.0335344634950161\n",
      "iteration 2028, dc_loss: 0.034953679889440536, tv_loss: 0.03352176025509834\n",
      "iteration 2029, dc_loss: 0.03492678329348564, tv_loss: 0.033528149127960205\n",
      "iteration 2030, dc_loss: 0.03493684530258179, tv_loss: 0.0335257314145565\n",
      "iteration 2031, dc_loss: 0.03491239994764328, tv_loss: 0.03353026881814003\n",
      "iteration 2032, dc_loss: 0.034890953451395035, tv_loss: 0.03353555500507355\n",
      "iteration 2033, dc_loss: 0.03490590676665306, tv_loss: 0.03351199999451637\n",
      "iteration 2034, dc_loss: 0.034873709082603455, tv_loss: 0.03353460878133774\n",
      "iteration 2035, dc_loss: 0.03485807031393051, tv_loss: 0.0335451103746891\n",
      "iteration 2036, dc_loss: 0.03486870601773262, tv_loss: 0.033511508256196976\n",
      "iteration 2037, dc_loss: 0.03483712300658226, tv_loss: 0.033529527485370636\n",
      "iteration 2038, dc_loss: 0.03483104333281517, tv_loss: 0.03352571651339531\n",
      "iteration 2039, dc_loss: 0.03484116494655609, tv_loss: 0.03351633995771408\n",
      "iteration 2040, dc_loss: 0.034802794456481934, tv_loss: 0.03353486582636833\n",
      "iteration 2041, dc_loss: 0.03478977829217911, tv_loss: 0.033537574112415314\n",
      "iteration 2042, dc_loss: 0.034800976514816284, tv_loss: 0.03351578116416931\n",
      "iteration 2043, dc_loss: 0.0347716324031353, tv_loss: 0.033522360026836395\n",
      "iteration 2044, dc_loss: 0.034758757799863815, tv_loss: 0.03353195637464523\n",
      "iteration 2045, dc_loss: 0.03476160019636154, tv_loss: 0.03352797403931618\n",
      "iteration 2046, dc_loss: 0.03473908454179764, tv_loss: 0.0335325188934803\n",
      "iteration 2047, dc_loss: 0.03473304212093353, tv_loss: 0.033534105867147446\n",
      "iteration 2048, dc_loss: 0.034726642072200775, tv_loss: 0.033526502549648285\n",
      "iteration 2049, dc_loss: 0.034707508981227875, tv_loss: 0.033528883010149\n",
      "iteration 2050, dc_loss: 0.0346948467195034, tv_loss: 0.03353481739759445\n",
      "iteration 2051, dc_loss: 0.03469424322247505, tv_loss: 0.033518049865961075\n",
      "iteration 2052, dc_loss: 0.03467605635523796, tv_loss: 0.03353327512741089\n",
      "iteration 2053, dc_loss: 0.034663282334804535, tv_loss: 0.03352903202176094\n",
      "iteration 2054, dc_loss: 0.03465493395924568, tv_loss: 0.03352009877562523\n",
      "iteration 2055, dc_loss: 0.034637752920389175, tv_loss: 0.033532947301864624\n",
      "iteration 2056, dc_loss: 0.03463631123304367, tv_loss: 0.033530618995428085\n",
      "iteration 2057, dc_loss: 0.03462383523583412, tv_loss: 0.03353032097220421\n",
      "iteration 2058, dc_loss: 0.034605689346790314, tv_loss: 0.03355161100625992\n",
      "iteration 2059, dc_loss: 0.034599266946315765, tv_loss: 0.03353884443640709\n",
      "iteration 2060, dc_loss: 0.03458382561802864, tv_loss: 0.03354014828801155\n",
      "iteration 2061, dc_loss: 0.03458639606833458, tv_loss: 0.03353118523955345\n",
      "iteration 2062, dc_loss: 0.03457207605242729, tv_loss: 0.03353683277964592\n",
      "iteration 2063, dc_loss: 0.034538742154836655, tv_loss: 0.03356721252202988\n",
      "iteration 2064, dc_loss: 0.03454569727182388, tv_loss: 0.033530257642269135\n",
      "iteration 2065, dc_loss: 0.034547049552202225, tv_loss: 0.03352762758731842\n",
      "iteration 2066, dc_loss: 0.0345156267285347, tv_loss: 0.033547159284353256\n",
      "iteration 2067, dc_loss: 0.03450712934136391, tv_loss: 0.03354058414697647\n",
      "iteration 2068, dc_loss: 0.03450813144445419, tv_loss: 0.03353714942932129\n",
      "iteration 2069, dc_loss: 0.03448823094367981, tv_loss: 0.03353602811694145\n",
      "iteration 2070, dc_loss: 0.03447231277823448, tv_loss: 0.03355388343334198\n",
      "iteration 2071, dc_loss: 0.03446822240948677, tv_loss: 0.03354082256555557\n",
      "iteration 2072, dc_loss: 0.03446562960743904, tv_loss: 0.033534497022628784\n",
      "iteration 2073, dc_loss: 0.0344441719353199, tv_loss: 0.033545754849910736\n",
      "iteration 2074, dc_loss: 0.03443374112248421, tv_loss: 0.03353840857744217\n",
      "iteration 2075, dc_loss: 0.034427471458911896, tv_loss: 0.03353758156299591\n",
      "iteration 2076, dc_loss: 0.03440706059336662, tv_loss: 0.033553674817085266\n",
      "iteration 2077, dc_loss: 0.03440796956419945, tv_loss: 0.03352699056267738\n",
      "iteration 2078, dc_loss: 0.03440310060977936, tv_loss: 0.033544786274433136\n",
      "iteration 2079, dc_loss: 0.034371115267276764, tv_loss: 0.0335438996553421\n",
      "iteration 2080, dc_loss: 0.0343620628118515, tv_loss: 0.03355052322149277\n",
      "iteration 2081, dc_loss: 0.03436866030097008, tv_loss: 0.033539723604917526\n",
      "iteration 2082, dc_loss: 0.03435244783759117, tv_loss: 0.03353412076830864\n",
      "iteration 2083, dc_loss: 0.03433333709836006, tv_loss: 0.033542852848768234\n",
      "iteration 2084, dc_loss: 0.0343344546854496, tv_loss: 0.0335410051047802\n",
      "iteration 2085, dc_loss: 0.034320589154958725, tv_loss: 0.03353622928261757\n",
      "iteration 2086, dc_loss: 0.034302953630685806, tv_loss: 0.03354042395949364\n",
      "iteration 2087, dc_loss: 0.03429051861166954, tv_loss: 0.033548951148986816\n",
      "iteration 2088, dc_loss: 0.03429242968559265, tv_loss: 0.03355364128947258\n",
      "iteration 2089, dc_loss: 0.034275706857442856, tv_loss: 0.0335640124976635\n",
      "iteration 2090, dc_loss: 0.0342547670006752, tv_loss: 0.033563729375600815\n",
      "iteration 2091, dc_loss: 0.034256964921951294, tv_loss: 0.033537548035383224\n",
      "iteration 2092, dc_loss: 0.03424989432096481, tv_loss: 0.033540621399879456\n",
      "iteration 2093, dc_loss: 0.0342276468873024, tv_loss: 0.033556509763002396\n",
      "iteration 2094, dc_loss: 0.03422129899263382, tv_loss: 0.03355754166841507\n",
      "iteration 2095, dc_loss: 0.03421136736869812, tv_loss: 0.03355736657977104\n",
      "iteration 2096, dc_loss: 0.03419990837574005, tv_loss: 0.033555030822753906\n",
      "iteration 2097, dc_loss: 0.03418978676199913, tv_loss: 0.033557992428541183\n",
      "iteration 2098, dc_loss: 0.03417888283729553, tv_loss: 0.033550966531038284\n",
      "iteration 2099, dc_loss: 0.03415970504283905, tv_loss: 0.03355434536933899\n",
      "iteration 2100, dc_loss: 0.03416189178824425, tv_loss: 0.03353981301188469\n",
      "iteration 2101, dc_loss: 0.03415486589074135, tv_loss: 0.033544693142175674\n",
      "iteration 2102, dc_loss: 0.03413193300366402, tv_loss: 0.0335596464574337\n",
      "iteration 2103, dc_loss: 0.03412378579378128, tv_loss: 0.03355422243475914\n",
      "iteration 2104, dc_loss: 0.03412346541881561, tv_loss: 0.033552445471286774\n",
      "iteration 2105, dc_loss: 0.03410303592681885, tv_loss: 0.03355543687939644\n",
      "iteration 2106, dc_loss: 0.03408681973814964, tv_loss: 0.03356291353702545\n",
      "iteration 2107, dc_loss: 0.0340871699154377, tv_loss: 0.03355120122432709\n",
      "iteration 2108, dc_loss: 0.03408481925725937, tv_loss: 0.03354494273662567\n",
      "iteration 2109, dc_loss: 0.03405969217419624, tv_loss: 0.03355824202299118\n",
      "iteration 2110, dc_loss: 0.03405812755227089, tv_loss: 0.03355056047439575\n",
      "iteration 2111, dc_loss: 0.03405129536986351, tv_loss: 0.033555638045072556\n",
      "iteration 2112, dc_loss: 0.03405659273266792, tv_loss: 0.03354659304022789\n",
      "iteration 2113, dc_loss: 0.034046806395053864, tv_loss: 0.033556219190359116\n",
      "iteration 2114, dc_loss: 0.03405432403087616, tv_loss: 0.03354645520448685\n",
      "iteration 2115, dc_loss: 0.03402657061815262, tv_loss: 0.033579375594854355\n",
      "iteration 2116, dc_loss: 0.034018903970718384, tv_loss: 0.033562205731868744\n",
      "iteration 2117, dc_loss: 0.033982615917921066, tv_loss: 0.03356659784913063\n",
      "iteration 2118, dc_loss: 0.03396549075841904, tv_loss: 0.03356277942657471\n",
      "iteration 2119, dc_loss: 0.03396105021238327, tv_loss: 0.03356276825070381\n",
      "iteration 2120, dc_loss: 0.0339573472738266, tv_loss: 0.03355644643306732\n",
      "iteration 2121, dc_loss: 0.03396528214216232, tv_loss: 0.03354279324412346\n",
      "iteration 2122, dc_loss: 0.03393257036805153, tv_loss: 0.03356499597430229\n",
      "iteration 2123, dc_loss: 0.03392712026834488, tv_loss: 0.033553607761859894\n",
      "iteration 2124, dc_loss: 0.03390716016292572, tv_loss: 0.03355853632092476\n",
      "iteration 2125, dc_loss: 0.03389868512749672, tv_loss: 0.03356098383665085\n",
      "iteration 2126, dc_loss: 0.03389863297343254, tv_loss: 0.033560480922460556\n",
      "iteration 2127, dc_loss: 0.033875416964292526, tv_loss: 0.03358057513833046\n",
      "iteration 2128, dc_loss: 0.03388042002916336, tv_loss: 0.033567238599061966\n",
      "iteration 2129, dc_loss: 0.03386025130748749, tv_loss: 0.03355984017252922\n",
      "iteration 2130, dc_loss: 0.03384541720151901, tv_loss: 0.033567529171705246\n",
      "iteration 2131, dc_loss: 0.033839814364910126, tv_loss: 0.03356410190463066\n",
      "iteration 2132, dc_loss: 0.03381853178143501, tv_loss: 0.03358515352010727\n",
      "iteration 2133, dc_loss: 0.033834509551525116, tv_loss: 0.03355187922716141\n",
      "iteration 2134, dc_loss: 0.033798426389694214, tv_loss: 0.03357701376080513\n",
      "iteration 2135, dc_loss: 0.03379740193486214, tv_loss: 0.03355956822633743\n",
      "iteration 2136, dc_loss: 0.03379261493682861, tv_loss: 0.033555515110492706\n",
      "iteration 2137, dc_loss: 0.03376273065805435, tv_loss: 0.03357403352856636\n",
      "iteration 2138, dc_loss: 0.03377269580960274, tv_loss: 0.03355471417307854\n",
      "iteration 2139, dc_loss: 0.033750563859939575, tv_loss: 0.03357885032892227\n",
      "iteration 2140, dc_loss: 0.03375006467103958, tv_loss: 0.03357793390750885\n",
      "iteration 2141, dc_loss: 0.03373630344867706, tv_loss: 0.03357535973191261\n",
      "iteration 2142, dc_loss: 0.033718667924404144, tv_loss: 0.03357545658946037\n",
      "iteration 2143, dc_loss: 0.03372073546051979, tv_loss: 0.0335564948618412\n",
      "iteration 2144, dc_loss: 0.03369057551026344, tv_loss: 0.03358219563961029\n",
      "iteration 2145, dc_loss: 0.0336960144340992, tv_loss: 0.03357553854584694\n",
      "iteration 2146, dc_loss: 0.033686328679323196, tv_loss: 0.03358341008424759\n",
      "iteration 2147, dc_loss: 0.03367604687809944, tv_loss: 0.03357529267668724\n",
      "iteration 2148, dc_loss: 0.03366434574127197, tv_loss: 0.03356826677918434\n",
      "iteration 2149, dc_loss: 0.03364570066332817, tv_loss: 0.033571481704711914\n",
      "iteration 2150, dc_loss: 0.033641040325164795, tv_loss: 0.033579155802726746\n",
      "iteration 2151, dc_loss: 0.03363616019487381, tv_loss: 0.03358384966850281\n",
      "iteration 2152, dc_loss: 0.03362235054373741, tv_loss: 0.033579859882593155\n",
      "iteration 2153, dc_loss: 0.033603254705667496, tv_loss: 0.03358578681945801\n",
      "iteration 2154, dc_loss: 0.03360990434885025, tv_loss: 0.03356621041893959\n",
      "iteration 2155, dc_loss: 0.03359517082571983, tv_loss: 0.033579498529434204\n",
      "iteration 2156, dc_loss: 0.03358202800154686, tv_loss: 0.03358665853738785\n",
      "iteration 2157, dc_loss: 0.033560965210199356, tv_loss: 0.033592455089092255\n",
      "iteration 2158, dc_loss: 0.03356257081031799, tv_loss: 0.0335766077041626\n",
      "iteration 2159, dc_loss: 0.03355132043361664, tv_loss: 0.033571843057870865\n",
      "iteration 2160, dc_loss: 0.03353854641318321, tv_loss: 0.03358364850282669\n",
      "iteration 2161, dc_loss: 0.03354327753186226, tv_loss: 0.033574361354112625\n",
      "iteration 2162, dc_loss: 0.03351522237062454, tv_loss: 0.03359160199761391\n",
      "iteration 2163, dc_loss: 0.033503804355859756, tv_loss: 0.03358730301260948\n",
      "iteration 2164, dc_loss: 0.033505070954561234, tv_loss: 0.03356948122382164\n",
      "iteration 2165, dc_loss: 0.033497247844934464, tv_loss: 0.03357095643877983\n",
      "iteration 2166, dc_loss: 0.033477067947387695, tv_loss: 0.03358425199985504\n",
      "iteration 2167, dc_loss: 0.03346928581595421, tv_loss: 0.033589210361242294\n",
      "iteration 2168, dc_loss: 0.03346056863665581, tv_loss: 0.033584415912628174\n",
      "iteration 2169, dc_loss: 0.03345201537013054, tv_loss: 0.033579833805561066\n",
      "iteration 2170, dc_loss: 0.03343654423952103, tv_loss: 0.03358133137226105\n",
      "iteration 2171, dc_loss: 0.033437881618738174, tv_loss: 0.03357086703181267\n",
      "iteration 2172, dc_loss: 0.033418621867895126, tv_loss: 0.03358175978064537\n",
      "iteration 2173, dc_loss: 0.03341491147875786, tv_loss: 0.03359093889594078\n",
      "iteration 2174, dc_loss: 0.033404022455215454, tv_loss: 0.03357762470841408\n",
      "iteration 2175, dc_loss: 0.03339601680636406, tv_loss: 0.033576637506484985\n",
      "iteration 2176, dc_loss: 0.03339844197034836, tv_loss: 0.03358478099107742\n",
      "iteration 2177, dc_loss: 0.03341229259967804, tv_loss: 0.03357511758804321\n",
      "iteration 2178, dc_loss: 0.03341322019696236, tv_loss: 0.0336022712290287\n",
      "iteration 2179, dc_loss: 0.03341871500015259, tv_loss: 0.03357848897576332\n",
      "iteration 2180, dc_loss: 0.03339811787009239, tv_loss: 0.033586595207452774\n",
      "iteration 2181, dc_loss: 0.03336873650550842, tv_loss: 0.03357044234871864\n",
      "iteration 2182, dc_loss: 0.03333885595202446, tv_loss: 0.03356846794486046\n",
      "iteration 2183, dc_loss: 0.03331761807203293, tv_loss: 0.03358973562717438\n",
      "iteration 2184, dc_loss: 0.03335009887814522, tv_loss: 0.033560533076524734\n",
      "iteration 2185, dc_loss: 0.03330199047923088, tv_loss: 0.03360914811491966\n",
      "iteration 2186, dc_loss: 0.03331568092107773, tv_loss: 0.03355860337615013\n",
      "iteration 2187, dc_loss: 0.03328036144375801, tv_loss: 0.033573612570762634\n",
      "iteration 2188, dc_loss: 0.03327209874987602, tv_loss: 0.03358495980501175\n",
      "iteration 2189, dc_loss: 0.03331579267978668, tv_loss: 0.0335395485162735\n",
      "iteration 2190, dc_loss: 0.03323669359087944, tv_loss: 0.033612292259931564\n",
      "iteration 2191, dc_loss: 0.03327043727040291, tv_loss: 0.03356391191482544\n",
      "iteration 2192, dc_loss: 0.033215027302503586, tv_loss: 0.03361821547150612\n",
      "iteration 2193, dc_loss: 0.0332305021584034, tv_loss: 0.033607419580221176\n",
      "iteration 2194, dc_loss: 0.03323987126350403, tv_loss: 0.033576615154743195\n",
      "iteration 2195, dc_loss: 0.033184509724378586, tv_loss: 0.0336025170981884\n",
      "iteration 2196, dc_loss: 0.03321978822350502, tv_loss: 0.033559203147888184\n",
      "iteration 2197, dc_loss: 0.03315960243344307, tv_loss: 0.03364855796098709\n",
      "iteration 2198, dc_loss: 0.03319116681814194, tv_loss: 0.03359806537628174\n",
      "iteration 2199, dc_loss: 0.03316733241081238, tv_loss: 0.033587388694286346\n",
      "iteration 2200, dc_loss: 0.03313374146819115, tv_loss: 0.033616166561841965\n",
      "iteration 2201, dc_loss: 0.03317174315452576, tv_loss: 0.03358667716383934\n",
      "iteration 2202, dc_loss: 0.03312453255057335, tv_loss: 0.03361840173602104\n",
      "iteration 2203, dc_loss: 0.03312738612294197, tv_loss: 0.03359454497694969\n",
      "iteration 2204, dc_loss: 0.03311190754175186, tv_loss: 0.03360726684331894\n",
      "iteration 2205, dc_loss: 0.03310180827975273, tv_loss: 0.033620014786720276\n",
      "iteration 2206, dc_loss: 0.033097051084041595, tv_loss: 0.033602047711610794\n",
      "iteration 2207, dc_loss: 0.033091556280851364, tv_loss: 0.03359047323465347\n",
      "iteration 2208, dc_loss: 0.03307942673563957, tv_loss: 0.033606406301259995\n",
      "iteration 2209, dc_loss: 0.033053141087293625, tv_loss: 0.03361310809850693\n",
      "iteration 2210, dc_loss: 0.03305681049823761, tv_loss: 0.03359486535191536\n",
      "iteration 2211, dc_loss: 0.03305517137050629, tv_loss: 0.0335899256169796\n",
      "iteration 2212, dc_loss: 0.03302933648228645, tv_loss: 0.03359859809279442\n",
      "iteration 2213, dc_loss: 0.03302505612373352, tv_loss: 0.03359229117631912\n",
      "iteration 2214, dc_loss: 0.03301785886287689, tv_loss: 0.033593956381082535\n",
      "iteration 2215, dc_loss: 0.03300352022051811, tv_loss: 0.033597011119127274\n",
      "iteration 2216, dc_loss: 0.032994549721479416, tv_loss: 0.03359271213412285\n",
      "iteration 2217, dc_loss: 0.032989513128995895, tv_loss: 0.03359271213412285\n",
      "iteration 2218, dc_loss: 0.03297998383641243, tv_loss: 0.03359810262918472\n",
      "iteration 2219, dc_loss: 0.0329660139977932, tv_loss: 0.033600639551877975\n",
      "iteration 2220, dc_loss: 0.032974451780319214, tv_loss: 0.033591609448194504\n",
      "iteration 2221, dc_loss: 0.032934293150901794, tv_loss: 0.03362919017672539\n",
      "iteration 2222, dc_loss: 0.032939665019512177, tv_loss: 0.0335988812148571\n",
      "iteration 2223, dc_loss: 0.032932817935943604, tv_loss: 0.03360506519675255\n",
      "iteration 2224, dc_loss: 0.032921113073825836, tv_loss: 0.03360842168331146\n",
      "iteration 2225, dc_loss: 0.032914429903030396, tv_loss: 0.03360190615057945\n",
      "iteration 2226, dc_loss: 0.03291056677699089, tv_loss: 0.03359944000840187\n",
      "iteration 2227, dc_loss: 0.03289332613348961, tv_loss: 0.03361052647233009\n",
      "iteration 2228, dc_loss: 0.0329054519534111, tv_loss: 0.03358620032668114\n",
      "iteration 2229, dc_loss: 0.03287183493375778, tv_loss: 0.033611588180065155\n",
      "iteration 2230, dc_loss: 0.032889075577259064, tv_loss: 0.033589743077754974\n",
      "iteration 2231, dc_loss: 0.03285400941967964, tv_loss: 0.03361193835735321\n",
      "iteration 2232, dc_loss: 0.03287596255540848, tv_loss: 0.03357802703976631\n",
      "iteration 2233, dc_loss: 0.03283052146434784, tv_loss: 0.03360980004072189\n",
      "iteration 2234, dc_loss: 0.032853029668331146, tv_loss: 0.03358108550310135\n",
      "iteration 2235, dc_loss: 0.0328044593334198, tv_loss: 0.03362198546528816\n",
      "iteration 2236, dc_loss: 0.032806042581796646, tv_loss: 0.03361018747091293\n",
      "iteration 2237, dc_loss: 0.03279656171798706, tv_loss: 0.03359968215227127\n",
      "iteration 2238, dc_loss: 0.0327882245182991, tv_loss: 0.033604465425014496\n",
      "iteration 2239, dc_loss: 0.03278798609972, tv_loss: 0.03358810022473335\n",
      "iteration 2240, dc_loss: 0.03275705501437187, tv_loss: 0.033616531640291214\n",
      "iteration 2241, dc_loss: 0.03278053551912308, tv_loss: 0.03358962759375572\n",
      "iteration 2242, dc_loss: 0.032749611884355545, tv_loss: 0.033617306500673294\n",
      "iteration 2243, dc_loss: 0.03275790810585022, tv_loss: 0.033603496849536896\n",
      "iteration 2244, dc_loss: 0.0327230803668499, tv_loss: 0.03361861780285835\n",
      "iteration 2245, dc_loss: 0.03274225816130638, tv_loss: 0.033590372651815414\n",
      "iteration 2246, dc_loss: 0.03271256759762764, tv_loss: 0.03360725939273834\n",
      "iteration 2247, dc_loss: 0.03271685168147087, tv_loss: 0.03359386697411537\n",
      "iteration 2248, dc_loss: 0.03269905596971512, tv_loss: 0.03359945863485336\n",
      "iteration 2249, dc_loss: 0.0326797254383564, tv_loss: 0.03361545130610466\n",
      "iteration 2250, dc_loss: 0.03268597275018692, tv_loss: 0.03359800577163696\n",
      "iteration 2251, dc_loss: 0.03267775475978851, tv_loss: 0.03360489010810852\n",
      "iteration 2252, dc_loss: 0.03266213834285736, tv_loss: 0.0336204394698143\n",
      "iteration 2253, dc_loss: 0.032644208520650864, tv_loss: 0.03362192586064339\n",
      "iteration 2254, dc_loss: 0.03264673426747322, tv_loss: 0.03360015153884888\n",
      "iteration 2255, dc_loss: 0.032628633081912994, tv_loss: 0.033610906451940536\n",
      "iteration 2256, dc_loss: 0.03262585774064064, tv_loss: 0.03360193595290184\n",
      "iteration 2257, dc_loss: 0.032615795731544495, tv_loss: 0.0335964635014534\n",
      "iteration 2258, dc_loss: 0.03259824216365814, tv_loss: 0.03360684961080551\n",
      "iteration 2259, dc_loss: 0.032587986439466476, tv_loss: 0.033608727157115936\n",
      "iteration 2260, dc_loss: 0.03259831294417381, tv_loss: 0.03359201177954674\n",
      "iteration 2261, dc_loss: 0.03258547931909561, tv_loss: 0.03360060229897499\n",
      "iteration 2262, dc_loss: 0.032560378313064575, tv_loss: 0.03362075611948967\n",
      "iteration 2263, dc_loss: 0.032567474991083145, tv_loss: 0.03361450135707855\n",
      "iteration 2264, dc_loss: 0.032550837844610214, tv_loss: 0.033632270991802216\n",
      "iteration 2265, dc_loss: 0.03256592899560928, tv_loss: 0.033608682453632355\n",
      "iteration 2266, dc_loss: 0.03253091126680374, tv_loss: 0.03362853080034256\n",
      "iteration 2267, dc_loss: 0.03255605697631836, tv_loss: 0.033591657876968384\n",
      "iteration 2268, dc_loss: 0.03250804543495178, tv_loss: 0.03363413363695145\n",
      "iteration 2269, dc_loss: 0.03255346789956093, tv_loss: 0.03358915448188782\n",
      "iteration 2270, dc_loss: 0.03251048922538757, tv_loss: 0.033624544739723206\n",
      "iteration 2271, dc_loss: 0.03254058212041855, tv_loss: 0.03358062729239464\n",
      "iteration 2272, dc_loss: 0.03246976435184479, tv_loss: 0.03364007547497749\n",
      "iteration 2273, dc_loss: 0.032520733773708344, tv_loss: 0.03357864171266556\n",
      "iteration 2274, dc_loss: 0.032444801181554794, tv_loss: 0.0336405448615551\n",
      "iteration 2275, dc_loss: 0.03245854005217552, tv_loss: 0.033616531640291214\n",
      "iteration 2276, dc_loss: 0.03244200721383095, tv_loss: 0.033622946590185165\n",
      "iteration 2277, dc_loss: 0.03242884948849678, tv_loss: 0.03362547233700752\n",
      "iteration 2278, dc_loss: 0.03242836147546768, tv_loss: 0.03360714763402939\n",
      "iteration 2279, dc_loss: 0.032409556210041046, tv_loss: 0.03361902013421059\n",
      "iteration 2280, dc_loss: 0.032416265457868576, tv_loss: 0.0336136519908905\n",
      "iteration 2281, dc_loss: 0.03238082677125931, tv_loss: 0.033636048436164856\n",
      "iteration 2282, dc_loss: 0.03242125362157822, tv_loss: 0.03358633816242218\n",
      "iteration 2283, dc_loss: 0.03237596154212952, tv_loss: 0.03363504260778427\n",
      "iteration 2284, dc_loss: 0.032396458089351654, tv_loss: 0.03360229358077049\n",
      "iteration 2285, dc_loss: 0.032357946038246155, tv_loss: 0.033631566911935806\n",
      "iteration 2286, dc_loss: 0.03237701579928398, tv_loss: 0.03362555056810379\n",
      "iteration 2287, dc_loss: 0.03234269097447395, tv_loss: 0.033637259155511856\n",
      "iteration 2288, dc_loss: 0.03234870731830597, tv_loss: 0.033617328852415085\n",
      "iteration 2289, dc_loss: 0.032314643263816833, tv_loss: 0.033636465668678284\n",
      "iteration 2290, dc_loss: 0.03232676908373833, tv_loss: 0.033611055463552475\n",
      "iteration 2291, dc_loss: 0.03229004517197609, tv_loss: 0.03365879878401756\n",
      "iteration 2292, dc_loss: 0.03231377899646759, tv_loss: 0.03360074758529663\n",
      "iteration 2293, dc_loss: 0.03228578343987465, tv_loss: 0.03362857550382614\n",
      "iteration 2294, dc_loss: 0.03227424621582031, tv_loss: 0.03362780436873436\n",
      "iteration 2295, dc_loss: 0.032276201993227005, tv_loss: 0.0336172841489315\n",
      "iteration 2296, dc_loss: 0.032267771661281586, tv_loss: 0.03364265710115433\n",
      "iteration 2297, dc_loss: 0.03226333111524582, tv_loss: 0.03363264352083206\n",
      "iteration 2298, dc_loss: 0.032247282564640045, tv_loss: 0.033638231456279755\n",
      "iteration 2299, dc_loss: 0.03225390240550041, tv_loss: 0.0336175300180912\n",
      "iteration 2300, dc_loss: 0.032231178134679794, tv_loss: 0.03364600986242294\n",
      "iteration 2301, dc_loss: 0.032242301851511, tv_loss: 0.033629417419433594\n",
      "iteration 2302, dc_loss: 0.03221547231078148, tv_loss: 0.0336410216987133\n",
      "iteration 2303, dc_loss: 0.0322304293513298, tv_loss: 0.03360506519675255\n",
      "iteration 2304, dc_loss: 0.03219430893659592, tv_loss: 0.03364355117082596\n",
      "iteration 2305, dc_loss: 0.03221110627055168, tv_loss: 0.033630095422267914\n",
      "iteration 2306, dc_loss: 0.03217323124408722, tv_loss: 0.03363986685872078\n",
      "iteration 2307, dc_loss: 0.032193414866924286, tv_loss: 0.03361992910504341\n",
      "iteration 2308, dc_loss: 0.032147567719221115, tv_loss: 0.03363918513059616\n",
      "iteration 2309, dc_loss: 0.032170794904232025, tv_loss: 0.03361142426729202\n",
      "iteration 2310, dc_loss: 0.032125283032655716, tv_loss: 0.033653080463409424\n",
      "iteration 2311, dc_loss: 0.03214560076594353, tv_loss: 0.03362143784761429\n",
      "iteration 2312, dc_loss: 0.03210814669728279, tv_loss: 0.033643171191215515\n",
      "iteration 2313, dc_loss: 0.032162006944417953, tv_loss: 0.03358625993132591\n",
      "iteration 2314, dc_loss: 0.03207544609904289, tv_loss: 0.03366773948073387\n",
      "iteration 2315, dc_loss: 0.03214067965745926, tv_loss: 0.03359097242355347\n",
      "iteration 2316, dc_loss: 0.03206786885857582, tv_loss: 0.033659521490335464\n",
      "iteration 2317, dc_loss: 0.032129235565662384, tv_loss: 0.033595919609069824\n",
      "iteration 2318, dc_loss: 0.032050617039203644, tv_loss: 0.033659107983112335\n",
      "iteration 2319, dc_loss: 0.03211737051606178, tv_loss: 0.03357969596982002\n",
      "iteration 2320, dc_loss: 0.03202277421951294, tv_loss: 0.03368398919701576\n",
      "iteration 2321, dc_loss: 0.03211180865764618, tv_loss: 0.03357503190636635\n",
      "iteration 2322, dc_loss: 0.03202008828520775, tv_loss: 0.033656857907772064\n",
      "iteration 2323, dc_loss: 0.03209070488810539, tv_loss: 0.03359641879796982\n",
      "iteration 2324, dc_loss: 0.03199877589941025, tv_loss: 0.033679623156785965\n",
      "iteration 2325, dc_loss: 0.03208887204527855, tv_loss: 0.033583734184503555\n",
      "iteration 2326, dc_loss: 0.032003775238990784, tv_loss: 0.03366877883672714\n",
      "iteration 2327, dc_loss: 0.032073572278022766, tv_loss: 0.033590905368328094\n",
      "iteration 2328, dc_loss: 0.03196687623858452, tv_loss: 0.03367909789085388\n",
      "iteration 2329, dc_loss: 0.03204483166337013, tv_loss: 0.03358052298426628\n",
      "iteration 2330, dc_loss: 0.031951695680618286, tv_loss: 0.0336662158370018\n",
      "iteration 2331, dc_loss: 0.03198941424489021, tv_loss: 0.03360329940915108\n",
      "iteration 2332, dc_loss: 0.03195922449231148, tv_loss: 0.0336296521127224\n",
      "iteration 2333, dc_loss: 0.0319487564265728, tv_loss: 0.0336264967918396\n",
      "iteration 2334, dc_loss: 0.03193216770887375, tv_loss: 0.0336417555809021\n",
      "iteration 2335, dc_loss: 0.031920842826366425, tv_loss: 0.03364817053079605\n",
      "iteration 2336, dc_loss: 0.0319414921104908, tv_loss: 0.033616695553064346\n",
      "iteration 2337, dc_loss: 0.03188706934452057, tv_loss: 0.03365961089730263\n",
      "iteration 2338, dc_loss: 0.03195236995816231, tv_loss: 0.03358989953994751\n",
      "iteration 2339, dc_loss: 0.0318615697324276, tv_loss: 0.033668942749500275\n",
      "iteration 2340, dc_loss: 0.03192247822880745, tv_loss: 0.03360246494412422\n",
      "iteration 2341, dc_loss: 0.031863462179899216, tv_loss: 0.03366445004940033\n",
      "iteration 2342, dc_loss: 0.031903788447380066, tv_loss: 0.03360649198293686\n",
      "iteration 2343, dc_loss: 0.03185298666357994, tv_loss: 0.033657483756542206\n",
      "iteration 2344, dc_loss: 0.031879179179668427, tv_loss: 0.033610936254262924\n",
      "iteration 2345, dc_loss: 0.031841933727264404, tv_loss: 0.03364330530166626\n",
      "iteration 2346, dc_loss: 0.03185296803712845, tv_loss: 0.03363204374909401\n",
      "iteration 2347, dc_loss: 0.03186286985874176, tv_loss: 0.03362133353948593\n",
      "iteration 2348, dc_loss: 0.031843602657318115, tv_loss: 0.03364047408103943\n",
      "iteration 2349, dc_loss: 0.03186790272593498, tv_loss: 0.03361302614212036\n",
      "iteration 2350, dc_loss: 0.031806495040655136, tv_loss: 0.03366929292678833\n",
      "iteration 2351, dc_loss: 0.03184705600142479, tv_loss: 0.033614713698625565\n",
      "iteration 2352, dc_loss: 0.03177529573440552, tv_loss: 0.03366341441869736\n",
      "iteration 2353, dc_loss: 0.03180016577243805, tv_loss: 0.033622026443481445\n",
      "iteration 2354, dc_loss: 0.03175445646047592, tv_loss: 0.03366472199559212\n",
      "iteration 2355, dc_loss: 0.03179553896188736, tv_loss: 0.03362388163805008\n",
      "iteration 2356, dc_loss: 0.03175393491983414, tv_loss: 0.03367629274725914\n",
      "iteration 2357, dc_loss: 0.03175997734069824, tv_loss: 0.0336463525891304\n",
      "iteration 2358, dc_loss: 0.03173797205090523, tv_loss: 0.03365439176559448\n",
      "iteration 2359, dc_loss: 0.031741246581077576, tv_loss: 0.03362978622317314\n",
      "iteration 2360, dc_loss: 0.0317092165350914, tv_loss: 0.033673908561468124\n",
      "iteration 2361, dc_loss: 0.031741950660943985, tv_loss: 0.033616047352552414\n",
      "iteration 2362, dc_loss: 0.031694479286670685, tv_loss: 0.033664677292108536\n",
      "iteration 2363, dc_loss: 0.03171543404459953, tv_loss: 0.03363407775759697\n",
      "iteration 2364, dc_loss: 0.031695976853370667, tv_loss: 0.03364512696862221\n",
      "iteration 2365, dc_loss: 0.03167838975787163, tv_loss: 0.03365137428045273\n",
      "iteration 2366, dc_loss: 0.03166722506284714, tv_loss: 0.03364179655909538\n",
      "iteration 2367, dc_loss: 0.03166496753692627, tv_loss: 0.033637307584285736\n",
      "iteration 2368, dc_loss: 0.031661372631788254, tv_loss: 0.033630385994911194\n",
      "iteration 2369, dc_loss: 0.03162750229239464, tv_loss: 0.03367799520492554\n",
      "iteration 2370, dc_loss: 0.03165167570114136, tv_loss: 0.033643148839473724\n",
      "iteration 2371, dc_loss: 0.031625524163246155, tv_loss: 0.033655691891908646\n",
      "iteration 2372, dc_loss: 0.03162546083331108, tv_loss: 0.033633794635534286\n",
      "iteration 2373, dc_loss: 0.03161538764834404, tv_loss: 0.03364211320877075\n",
      "iteration 2374, dc_loss: 0.03160487115383148, tv_loss: 0.03365527465939522\n",
      "iteration 2375, dc_loss: 0.031593143939971924, tv_loss: 0.033665064722299576\n",
      "iteration 2376, dc_loss: 0.031585413962602615, tv_loss: 0.03365979716181755\n",
      "iteration 2377, dc_loss: 0.03159929811954498, tv_loss: 0.03363608196377754\n",
      "iteration 2378, dc_loss: 0.03156346455216408, tv_loss: 0.03366188704967499\n",
      "iteration 2379, dc_loss: 0.031611159443855286, tv_loss: 0.03361302986741066\n",
      "iteration 2380, dc_loss: 0.03155652433633804, tv_loss: 0.03367379680275917\n",
      "iteration 2381, dc_loss: 0.031591158360242844, tv_loss: 0.0336446538567543\n",
      "iteration 2382, dc_loss: 0.03154492378234863, tv_loss: 0.03368207812309265\n",
      "iteration 2383, dc_loss: 0.03159136325120926, tv_loss: 0.03361938148736954\n",
      "iteration 2384, dc_loss: 0.03153230994939804, tv_loss: 0.03367765620350838\n",
      "iteration 2385, dc_loss: 0.03160223737359047, tv_loss: 0.03360063582658768\n",
      "iteration 2386, dc_loss: 0.031509947031736374, tv_loss: 0.03368673101067543\n",
      "iteration 2387, dc_loss: 0.03157772123813629, tv_loss: 0.03360607475042343\n",
      "iteration 2388, dc_loss: 0.03149988502264023, tv_loss: 0.033681225031614304\n",
      "iteration 2389, dc_loss: 0.03156198933720589, tv_loss: 0.03361921384930611\n",
      "iteration 2390, dc_loss: 0.03148189187049866, tv_loss: 0.03367983177304268\n",
      "iteration 2391, dc_loss: 0.03153728321194649, tv_loss: 0.0336155891418457\n",
      "iteration 2392, dc_loss: 0.03145508095622063, tv_loss: 0.03369300439953804\n",
      "iteration 2393, dc_loss: 0.03149523586034775, tv_loss: 0.033635638654232025\n",
      "iteration 2394, dc_loss: 0.03144972026348114, tv_loss: 0.03365340083837509\n",
      "iteration 2395, dc_loss: 0.03144600987434387, tv_loss: 0.033633675426244736\n",
      "iteration 2396, dc_loss: 0.03141489997506142, tv_loss: 0.03365635871887207\n",
      "iteration 2397, dc_loss: 0.03142490237951279, tv_loss: 0.03364856541156769\n",
      "iteration 2398, dc_loss: 0.03140107914805412, tv_loss: 0.033679671585559845\n",
      "iteration 2399, dc_loss: 0.031421959400177, tv_loss: 0.03364520147442818\n",
      "iteration 2400, dc_loss: 0.03139558434486389, tv_loss: 0.03366076946258545\n",
      "iteration 2401, dc_loss: 0.03140143305063248, tv_loss: 0.03364129737019539\n",
      "iteration 2402, dc_loss: 0.03137941658496857, tv_loss: 0.03363918513059616\n",
      "iteration 2403, dc_loss: 0.03136327490210533, tv_loss: 0.03363820165395737\n",
      "iteration 2404, dc_loss: 0.03137068450450897, tv_loss: 0.033636730164289474\n",
      "iteration 2405, dc_loss: 0.031361840665340424, tv_loss: 0.03364323079586029\n",
      "iteration 2406, dc_loss: 0.0313577726483345, tv_loss: 0.03362278267741203\n",
      "iteration 2407, dc_loss: 0.031347814947366714, tv_loss: 0.033631641417741776\n",
      "iteration 2408, dc_loss: 0.03134049102663994, tv_loss: 0.03364991769194603\n",
      "iteration 2409, dc_loss: 0.03133263811469078, tv_loss: 0.03364487364888191\n",
      "iteration 2410, dc_loss: 0.03132346272468567, tv_loss: 0.03364875167608261\n",
      "iteration 2411, dc_loss: 0.031318627297878265, tv_loss: 0.03364498168230057\n",
      "iteration 2412, dc_loss: 0.0313139483332634, tv_loss: 0.033635854721069336\n",
      "iteration 2413, dc_loss: 0.031301479786634445, tv_loss: 0.033637918531894684\n",
      "iteration 2414, dc_loss: 0.03130277618765831, tv_loss: 0.03363822028040886\n",
      "iteration 2415, dc_loss: 0.031291455030441284, tv_loss: 0.03363579139113426\n",
      "iteration 2416, dc_loss: 0.031274545937776566, tv_loss: 0.03365796059370041\n",
      "iteration 2417, dc_loss: 0.031287360936403275, tv_loss: 0.033630069345235825\n",
      "iteration 2418, dc_loss: 0.0312650240957737, tv_loss: 0.03364337608218193\n",
      "iteration 2419, dc_loss: 0.0312623456120491, tv_loss: 0.03363938629627228\n",
      "iteration 2420, dc_loss: 0.03127535805106163, tv_loss: 0.03361984342336655\n",
      "iteration 2421, dc_loss: 0.031237948685884476, tv_loss: 0.03364706411957741\n",
      "iteration 2422, dc_loss: 0.031238792464137077, tv_loss: 0.03364270552992821\n",
      "iteration 2423, dc_loss: 0.03125254064798355, tv_loss: 0.033620815724134445\n",
      "iteration 2424, dc_loss: 0.031219633296132088, tv_loss: 0.033646173775196075\n",
      "iteration 2425, dc_loss: 0.031223107129335403, tv_loss: 0.03363201022148132\n",
      "iteration 2426, dc_loss: 0.031229212880134583, tv_loss: 0.03362759202718735\n",
      "iteration 2427, dc_loss: 0.03120383992791176, tv_loss: 0.0336453802883625\n",
      "iteration 2428, dc_loss: 0.031201468780636787, tv_loss: 0.033643342554569244\n",
      "iteration 2429, dc_loss: 0.031202424317598343, tv_loss: 0.03363903611898422\n",
      "iteration 2430, dc_loss: 0.031184641644358635, tv_loss: 0.03367500379681587\n",
      "iteration 2431, dc_loss: 0.031197499483823776, tv_loss: 0.03364623710513115\n",
      "iteration 2432, dc_loss: 0.031171385198831558, tv_loss: 0.03365251049399376\n",
      "iteration 2433, dc_loss: 0.03116578422486782, tv_loss: 0.03365793824195862\n",
      "iteration 2434, dc_loss: 0.031177980825304985, tv_loss: 0.03364114090800285\n",
      "iteration 2435, dc_loss: 0.031152186915278435, tv_loss: 0.03366483747959137\n",
      "iteration 2436, dc_loss: 0.031154628843069077, tv_loss: 0.033642083406448364\n",
      "iteration 2437, dc_loss: 0.031153462827205658, tv_loss: 0.03363368287682533\n",
      "iteration 2438, dc_loss: 0.031131068244576454, tv_loss: 0.03366357088088989\n",
      "iteration 2439, dc_loss: 0.03113441728055477, tv_loss: 0.03366423398256302\n",
      "iteration 2440, dc_loss: 0.031127968803048134, tv_loss: 0.033659934997558594\n",
      "iteration 2441, dc_loss: 0.031123273074626923, tv_loss: 0.033643387258052826\n",
      "iteration 2442, dc_loss: 0.031123710796236992, tv_loss: 0.0336439348757267\n",
      "iteration 2443, dc_loss: 0.031099652871489525, tv_loss: 0.03368011489510536\n",
      "iteration 2444, dc_loss: 0.03109835647046566, tv_loss: 0.03366105630993843\n",
      "iteration 2445, dc_loss: 0.031099790707230568, tv_loss: 0.03364446386694908\n",
      "iteration 2446, dc_loss: 0.031090514734387398, tv_loss: 0.03365238755941391\n",
      "iteration 2447, dc_loss: 0.03108943998813629, tv_loss: 0.0336562842130661\n",
      "iteration 2448, dc_loss: 0.031071066856384277, tv_loss: 0.0336669459939003\n",
      "iteration 2449, dc_loss: 0.03107503615319729, tv_loss: 0.03364642709493637\n",
      "iteration 2450, dc_loss: 0.03107267990708351, tv_loss: 0.03364105895161629\n",
      "iteration 2451, dc_loss: 0.031047163531184196, tv_loss: 0.03366208076477051\n",
      "iteration 2452, dc_loss: 0.03105560690164566, tv_loss: 0.03365032747387886\n",
      "iteration 2453, dc_loss: 0.031056389212608337, tv_loss: 0.033641476184129715\n",
      "iteration 2454, dc_loss: 0.03103615529835224, tv_loss: 0.03365429490804672\n",
      "iteration 2455, dc_loss: 0.03102904185652733, tv_loss: 0.033655304461717606\n",
      "iteration 2456, dc_loss: 0.03102959506213665, tv_loss: 0.03364184498786926\n",
      "iteration 2457, dc_loss: 0.031022094190120697, tv_loss: 0.033645402640104294\n",
      "iteration 2458, dc_loss: 0.03102121129631996, tv_loss: 0.03364238515496254\n",
      "iteration 2459, dc_loss: 0.031008968129754066, tv_loss: 0.033645570278167725\n",
      "iteration 2460, dc_loss: 0.031000206246972084, tv_loss: 0.03364857658743858\n",
      "iteration 2461, dc_loss: 0.03099851682782173, tv_loss: 0.03364516794681549\n",
      "iteration 2462, dc_loss: 0.03099074587225914, tv_loss: 0.03365165367722511\n",
      "iteration 2463, dc_loss: 0.03098888322710991, tv_loss: 0.03365403413772583\n",
      "iteration 2464, dc_loss: 0.030976207926869392, tv_loss: 0.03366998955607414\n",
      "iteration 2465, dc_loss: 0.030967624858021736, tv_loss: 0.03365694358944893\n",
      "iteration 2466, dc_loss: 0.030971214175224304, tv_loss: 0.03364736586809158\n",
      "iteration 2467, dc_loss: 0.03096078708767891, tv_loss: 0.03365583345293999\n",
      "iteration 2468, dc_loss: 0.03095191717147827, tv_loss: 0.033664364367723465\n",
      "iteration 2469, dc_loss: 0.030944451689720154, tv_loss: 0.03366721794009209\n",
      "iteration 2470, dc_loss: 0.03094346635043621, tv_loss: 0.033652886748313904\n",
      "iteration 2471, dc_loss: 0.030940748751163483, tv_loss: 0.03364584222435951\n",
      "iteration 2472, dc_loss: 0.03092745877802372, tv_loss: 0.03366478532552719\n",
      "iteration 2473, dc_loss: 0.030923279002308846, tv_loss: 0.03365825489163399\n",
      "iteration 2474, dc_loss: 0.030915517359972, tv_loss: 0.03366504982113838\n",
      "iteration 2475, dc_loss: 0.030909474939107895, tv_loss: 0.03365512564778328\n",
      "iteration 2476, dc_loss: 0.030911864712834358, tv_loss: 0.03364749625325203\n",
      "iteration 2477, dc_loss: 0.030901208519935608, tv_loss: 0.03365393728017807\n",
      "iteration 2478, dc_loss: 0.030887024477124214, tv_loss: 0.03366750478744507\n",
      "iteration 2479, dc_loss: 0.03089100494980812, tv_loss: 0.033656876534223557\n",
      "iteration 2480, dc_loss: 0.030880451202392578, tv_loss: 0.03365784510970116\n",
      "iteration 2481, dc_loss: 0.030870309099555016, tv_loss: 0.033660903573036194\n",
      "iteration 2482, dc_loss: 0.030869608744978905, tv_loss: 0.03364685922861099\n",
      "iteration 2483, dc_loss: 0.030865564942359924, tv_loss: 0.03365181013941765\n",
      "iteration 2484, dc_loss: 0.03085930272936821, tv_loss: 0.03365141898393631\n",
      "iteration 2485, dc_loss: 0.030845096334815025, tv_loss: 0.03365934640169144\n",
      "iteration 2486, dc_loss: 0.030841395258903503, tv_loss: 0.03366827219724655\n",
      "iteration 2487, dc_loss: 0.03084542416036129, tv_loss: 0.033659324049949646\n",
      "iteration 2488, dc_loss: 0.030832218006253242, tv_loss: 0.03365607559680939\n",
      "iteration 2489, dc_loss: 0.03082343004643917, tv_loss: 0.03366046026349068\n",
      "iteration 2490, dc_loss: 0.030820537358522415, tv_loss: 0.033658068627119064\n",
      "iteration 2491, dc_loss: 0.030809439718723297, tv_loss: 0.03366439417004585\n",
      "iteration 2492, dc_loss: 0.030809441581368446, tv_loss: 0.03366638720035553\n",
      "iteration 2493, dc_loss: 0.03080405294895172, tv_loss: 0.03365938737988472\n",
      "iteration 2494, dc_loss: 0.030795127153396606, tv_loss: 0.033658064901828766\n",
      "iteration 2495, dc_loss: 0.030786847695708275, tv_loss: 0.03365449607372284\n",
      "iteration 2496, dc_loss: 0.03078988939523697, tv_loss: 0.033651091158390045\n",
      "iteration 2497, dc_loss: 0.030779151245951653, tv_loss: 0.03365980461239815\n",
      "iteration 2498, dc_loss: 0.030762959271669388, tv_loss: 0.033677637577056885\n",
      "iteration 2499, dc_loss: 0.030764905735850334, tv_loss: 0.033661194145679474\n",
      "iteration 2500, dc_loss: 0.03076765313744545, tv_loss: 0.03364839404821396\n",
      "iteration 2501, dc_loss: 0.030751317739486694, tv_loss: 0.03365640342235565\n",
      "iteration 2502, dc_loss: 0.030745718628168106, tv_loss: 0.03365889564156532\n",
      "iteration 2503, dc_loss: 0.030742978677153587, tv_loss: 0.03366757556796074\n",
      "iteration 2504, dc_loss: 0.030736299231648445, tv_loss: 0.03366672247648239\n",
      "iteration 2505, dc_loss: 0.030730828642845154, tv_loss: 0.03366265073418617\n",
      "iteration 2506, dc_loss: 0.030721573159098625, tv_loss: 0.033655691891908646\n",
      "iteration 2507, dc_loss: 0.030713513493537903, tv_loss: 0.03366389125585556\n",
      "iteration 2508, dc_loss: 0.03070956841111183, tv_loss: 0.03366495296359062\n",
      "iteration 2509, dc_loss: 0.03070826455950737, tv_loss: 0.03365999087691307\n",
      "iteration 2510, dc_loss: 0.030705804005265236, tv_loss: 0.033660851418972015\n",
      "iteration 2511, dc_loss: 0.03069239854812622, tv_loss: 0.03366384282708168\n",
      "iteration 2512, dc_loss: 0.030684344470500946, tv_loss: 0.0336628258228302\n",
      "iteration 2513, dc_loss: 0.030679846182465553, tv_loss: 0.0336594395339489\n",
      "iteration 2514, dc_loss: 0.03067571111023426, tv_loss: 0.03366117179393768\n",
      "iteration 2515, dc_loss: 0.03067166917026043, tv_loss: 0.03366508334875107\n",
      "iteration 2516, dc_loss: 0.030666310340166092, tv_loss: 0.03366518393158913\n",
      "iteration 2517, dc_loss: 0.030655547976493835, tv_loss: 0.033669255673885345\n",
      "iteration 2518, dc_loss: 0.03065124899148941, tv_loss: 0.033664949238300323\n",
      "iteration 2519, dc_loss: 0.030648253858089447, tv_loss: 0.033657558262348175\n",
      "iteration 2520, dc_loss: 0.03063676692545414, tv_loss: 0.03365928307175636\n",
      "iteration 2521, dc_loss: 0.030633077025413513, tv_loss: 0.03366183117032051\n",
      "iteration 2522, dc_loss: 0.03063509613275528, tv_loss: 0.03365501016378403\n",
      "iteration 2523, dc_loss: 0.030619226396083832, tv_loss: 0.03366503119468689\n",
      "iteration 2524, dc_loss: 0.03061615489423275, tv_loss: 0.033670078963041306\n",
      "iteration 2525, dc_loss: 0.03061424195766449, tv_loss: 0.03366386517882347\n",
      "iteration 2526, dc_loss: 0.030596408993005753, tv_loss: 0.03367893770337105\n",
      "iteration 2527, dc_loss: 0.030599210411310196, tv_loss: 0.033661749213933945\n",
      "iteration 2528, dc_loss: 0.030595935881137848, tv_loss: 0.03365764394402504\n",
      "iteration 2529, dc_loss: 0.030582115054130554, tv_loss: 0.03366854041814804\n",
      "iteration 2530, dc_loss: 0.030584292486310005, tv_loss: 0.03366924822330475\n",
      "iteration 2531, dc_loss: 0.030579401180148125, tv_loss: 0.033667076379060745\n",
      "iteration 2532, dc_loss: 0.03056570515036583, tv_loss: 0.03367128223180771\n",
      "iteration 2533, dc_loss: 0.03055734932422638, tv_loss: 0.033674243837594986\n",
      "iteration 2534, dc_loss: 0.030556755140423775, tv_loss: 0.033658936619758606\n",
      "iteration 2535, dc_loss: 0.03055734746158123, tv_loss: 0.03366130217909813\n",
      "iteration 2536, dc_loss: 0.030545461922883987, tv_loss: 0.0336742140352726\n",
      "iteration 2537, dc_loss: 0.03054247796535492, tv_loss: 0.033669136464595795\n",
      "iteration 2538, dc_loss: 0.030529430136084557, tv_loss: 0.03367067128419876\n",
      "iteration 2539, dc_loss: 0.0305319856852293, tv_loss: 0.03365890309214592\n",
      "iteration 2540, dc_loss: 0.030521301552653313, tv_loss: 0.033663567155599594\n",
      "iteration 2541, dc_loss: 0.03051167167723179, tv_loss: 0.033669017255306244\n",
      "iteration 2542, dc_loss: 0.03051338903605938, tv_loss: 0.03366042673587799\n",
      "iteration 2543, dc_loss: 0.03050825372338295, tv_loss: 0.03365768492221832\n",
      "iteration 2544, dc_loss: 0.03049778938293457, tv_loss: 0.033663615584373474\n",
      "iteration 2545, dc_loss: 0.030492158606648445, tv_loss: 0.033663153648376465\n",
      "iteration 2546, dc_loss: 0.03049251064658165, tv_loss: 0.03366732597351074\n",
      "iteration 2547, dc_loss: 0.03048732690513134, tv_loss: 0.03368527442216873\n",
      "iteration 2548, dc_loss: 0.030477160587906837, tv_loss: 0.03368953615427017\n",
      "iteration 2549, dc_loss: 0.030477356165647507, tv_loss: 0.03367018699645996\n",
      "iteration 2550, dc_loss: 0.030468668788671494, tv_loss: 0.033670518547296524\n",
      "iteration 2551, dc_loss: 0.030475519597530365, tv_loss: 0.03366504982113838\n",
      "iteration 2552, dc_loss: 0.030456526204943657, tv_loss: 0.03367607295513153\n",
      "iteration 2553, dc_loss: 0.03045583888888359, tv_loss: 0.03366571292281151\n",
      "iteration 2554, dc_loss: 0.030451608821749687, tv_loss: 0.033664070069789886\n",
      "iteration 2555, dc_loss: 0.03043181821703911, tv_loss: 0.033671602606773376\n",
      "iteration 2556, dc_loss: 0.03043483942747116, tv_loss: 0.03366154059767723\n",
      "iteration 2557, dc_loss: 0.030421782284975052, tv_loss: 0.033667437732219696\n",
      "iteration 2558, dc_loss: 0.03042963147163391, tv_loss: 0.03366298973560333\n",
      "iteration 2559, dc_loss: 0.030411962419748306, tv_loss: 0.03367949277162552\n",
      "iteration 2560, dc_loss: 0.030423250049352646, tv_loss: 0.03367360308766365\n",
      "iteration 2561, dc_loss: 0.030406739562749863, tv_loss: 0.03368388116359711\n",
      "iteration 2562, dc_loss: 0.030407706275582314, tv_loss: 0.033680565655231476\n",
      "iteration 2563, dc_loss: 0.030390389263629913, tv_loss: 0.03368096426129341\n",
      "iteration 2564, dc_loss: 0.030401883646845818, tv_loss: 0.033664289861917496\n",
      "iteration 2565, dc_loss: 0.030381683260202408, tv_loss: 0.03367310017347336\n",
      "iteration 2566, dc_loss: 0.03038128651678562, tv_loss: 0.0336664542555809\n",
      "iteration 2567, dc_loss: 0.030365929007530212, tv_loss: 0.033685293048620224\n",
      "iteration 2568, dc_loss: 0.030372725799679756, tv_loss: 0.03367089852690697\n",
      "iteration 2569, dc_loss: 0.030357351526618004, tv_loss: 0.03368306905031204\n",
      "iteration 2570, dc_loss: 0.030352197587490082, tv_loss: 0.033676162362098694\n",
      "iteration 2571, dc_loss: 0.03035740554332733, tv_loss: 0.03366747498512268\n",
      "iteration 2572, dc_loss: 0.030341466888785362, tv_loss: 0.033678267151117325\n",
      "iteration 2573, dc_loss: 0.03033914975821972, tv_loss: 0.03368533030152321\n",
      "iteration 2574, dc_loss: 0.0303238146007061, tv_loss: 0.033678196370601654\n",
      "iteration 2575, dc_loss: 0.030323417857289314, tv_loss: 0.03367462381720543\n",
      "iteration 2576, dc_loss: 0.03031899593770504, tv_loss: 0.033671487122774124\n",
      "iteration 2577, dc_loss: 0.030299708247184753, tv_loss: 0.033687129616737366\n",
      "iteration 2578, dc_loss: 0.03030554950237274, tv_loss: 0.0336773656308651\n",
      "iteration 2579, dc_loss: 0.030309230089187622, tv_loss: 0.033678509294986725\n",
      "iteration 2580, dc_loss: 0.030296023935079575, tv_loss: 0.033675432205200195\n",
      "iteration 2581, dc_loss: 0.030280841514468193, tv_loss: 0.033682458102703094\n",
      "iteration 2582, dc_loss: 0.030292119830846786, tv_loss: 0.03367237374186516\n",
      "iteration 2583, dc_loss: 0.030283140018582344, tv_loss: 0.03367147594690323\n",
      "iteration 2584, dc_loss: 0.030264969915151596, tv_loss: 0.033681612461805344\n",
      "iteration 2585, dc_loss: 0.030264459550380707, tv_loss: 0.033678505569696426\n",
      "iteration 2586, dc_loss: 0.030254006385803223, tv_loss: 0.033686112612485886\n",
      "iteration 2587, dc_loss: 0.03026113659143448, tv_loss: 0.0336592011153698\n",
      "iteration 2588, dc_loss: 0.030247177928686142, tv_loss: 0.03367757797241211\n",
      "iteration 2589, dc_loss: 0.03024265728890896, tv_loss: 0.033682286739349365\n",
      "iteration 2590, dc_loss: 0.030238257721066475, tv_loss: 0.03367256000638008\n",
      "iteration 2591, dc_loss: 0.03023422695696354, tv_loss: 0.03367258235812187\n",
      "iteration 2592, dc_loss: 0.030220383778214455, tv_loss: 0.03368688002228737\n",
      "iteration 2593, dc_loss: 0.0302252359688282, tv_loss: 0.03367407247424126\n",
      "iteration 2594, dc_loss: 0.030216161161661148, tv_loss: 0.03367689624428749\n",
      "iteration 2595, dc_loss: 0.030217036604881287, tv_loss: 0.033683959394693375\n",
      "iteration 2596, dc_loss: 0.030201029032468796, tv_loss: 0.03368352726101875\n",
      "iteration 2597, dc_loss: 0.030202379450201988, tv_loss: 0.03367183730006218\n",
      "iteration 2598, dc_loss: 0.030179128050804138, tv_loss: 0.03369715437293053\n",
      "iteration 2599, dc_loss: 0.030198557302355766, tv_loss: 0.033665791153907776\n",
      "iteration 2600, dc_loss: 0.030186466872692108, tv_loss: 0.033671699464321136\n",
      "iteration 2601, dc_loss: 0.030178451910614967, tv_loss: 0.033676426857709885\n",
      "iteration 2602, dc_loss: 0.030170070007443428, tv_loss: 0.03369338810443878\n",
      "iteration 2603, dc_loss: 0.030186358839273453, tv_loss: 0.03367328643798828\n",
      "iteration 2604, dc_loss: 0.030174944549798965, tv_loss: 0.033683862537145615\n",
      "iteration 2605, dc_loss: 0.030167853459715843, tv_loss: 0.03369126841425896\n",
      "iteration 2606, dc_loss: 0.03016408160328865, tv_loss: 0.03368832916021347\n",
      "iteration 2607, dc_loss: 0.030161114409565926, tv_loss: 0.033677004277706146\n",
      "iteration 2608, dc_loss: 0.03014497272670269, tv_loss: 0.03368544206023216\n",
      "iteration 2609, dc_loss: 0.03014272451400757, tv_loss: 0.03366870433092117\n",
      "iteration 2610, dc_loss: 0.030129477381706238, tv_loss: 0.033680256456136703\n",
      "iteration 2611, dc_loss: 0.0301267821341753, tv_loss: 0.033676519989967346\n",
      "iteration 2612, dc_loss: 0.030115846544504166, tv_loss: 0.03368091210722923\n",
      "iteration 2613, dc_loss: 0.03012058138847351, tv_loss: 0.03367013856768608\n",
      "iteration 2614, dc_loss: 0.03010002337396145, tv_loss: 0.03369356319308281\n",
      "iteration 2615, dc_loss: 0.03009425662457943, tv_loss: 0.03370201587677002\n",
      "iteration 2616, dc_loss: 0.030100157484412193, tv_loss: 0.03367681801319122\n",
      "iteration 2617, dc_loss: 0.0300875436514616, tv_loss: 0.03368017077445984\n",
      "iteration 2618, dc_loss: 0.030072808265686035, tv_loss: 0.033686600625514984\n",
      "iteration 2619, dc_loss: 0.030080066993832588, tv_loss: 0.03367852047085762\n",
      "iteration 2620, dc_loss: 0.03007587045431137, tv_loss: 0.03367675840854645\n",
      "iteration 2621, dc_loss: 0.030070390552282333, tv_loss: 0.033684853464365005\n",
      "iteration 2622, dc_loss: 0.030065959319472313, tv_loss: 0.03369308263063431\n",
      "iteration 2623, dc_loss: 0.030054863542318344, tv_loss: 0.03370184078812599\n",
      "iteration 2624, dc_loss: 0.03005620837211609, tv_loss: 0.03368189558386803\n",
      "iteration 2625, dc_loss: 0.030044913291931152, tv_loss: 0.0336838960647583\n",
      "iteration 2626, dc_loss: 0.03004271723330021, tv_loss: 0.03367689624428749\n",
      "iteration 2627, dc_loss: 0.030016951262950897, tv_loss: 0.033705566078424454\n",
      "iteration 2628, dc_loss: 0.030037304386496544, tv_loss: 0.03369327634572983\n",
      "iteration 2629, dc_loss: 0.030018702149391174, tv_loss: 0.03370679169893265\n",
      "iteration 2630, dc_loss: 0.03001714125275612, tv_loss: 0.033686358481645584\n",
      "iteration 2631, dc_loss: 0.03001323528587818, tv_loss: 0.03368252143263817\n",
      "iteration 2632, dc_loss: 0.029997535049915314, tv_loss: 0.033695872873067856\n",
      "iteration 2633, dc_loss: 0.030010811984539032, tv_loss: 0.033685289323329926\n",
      "iteration 2634, dc_loss: 0.02999548427760601, tv_loss: 0.033692240715026855\n",
      "iteration 2635, dc_loss: 0.029989713802933693, tv_loss: 0.03368525207042694\n",
      "iteration 2636, dc_loss: 0.029986079782247543, tv_loss: 0.03368743509054184\n",
      "iteration 2637, dc_loss: 0.029989033937454224, tv_loss: 0.03367464616894722\n",
      "iteration 2638, dc_loss: 0.029966915026307106, tv_loss: 0.033688221126794815\n",
      "iteration 2639, dc_loss: 0.02996995858848095, tv_loss: 0.03369073569774628\n",
      "iteration 2640, dc_loss: 0.029975291341543198, tv_loss: 0.03369235619902611\n",
      "iteration 2641, dc_loss: 0.029961807653307915, tv_loss: 0.033704329282045364\n",
      "iteration 2642, dc_loss: 0.029958385974168777, tv_loss: 0.03369172662496567\n",
      "iteration 2643, dc_loss: 0.029959069564938545, tv_loss: 0.03368549793958664\n",
      "iteration 2644, dc_loss: 0.029950154945254326, tv_loss: 0.033688005059957504\n",
      "iteration 2645, dc_loss: 0.0299522764980793, tv_loss: 0.03369433060288429\n",
      "iteration 2646, dc_loss: 0.029947396367788315, tv_loss: 0.03369177505373955\n",
      "iteration 2647, dc_loss: 0.029940541833639145, tv_loss: 0.033696625381708145\n",
      "iteration 2648, dc_loss: 0.029934324324131012, tv_loss: 0.033694956451654434\n",
      "iteration 2649, dc_loss: 0.029927855357527733, tv_loss: 0.03368588164448738\n",
      "iteration 2650, dc_loss: 0.029912041500210762, tv_loss: 0.03369319811463356\n",
      "iteration 2651, dc_loss: 0.029911817982792854, tv_loss: 0.033682435750961304\n",
      "iteration 2652, dc_loss: 0.029894113540649414, tv_loss: 0.03370041027665138\n",
      "iteration 2653, dc_loss: 0.029903311282396317, tv_loss: 0.033682409673929214\n",
      "iteration 2654, dc_loss: 0.02988233044743538, tv_loss: 0.0337081179022789\n",
      "iteration 2655, dc_loss: 0.029888635501265526, tv_loss: 0.033687982708215714\n",
      "iteration 2656, dc_loss: 0.029867151752114296, tv_loss: 0.03369816765189171\n",
      "iteration 2657, dc_loss: 0.02987627685070038, tv_loss: 0.03367741033434868\n",
      "iteration 2658, dc_loss: 0.029853779822587967, tv_loss: 0.03369900956749916\n",
      "iteration 2659, dc_loss: 0.029866261407732964, tv_loss: 0.03367859870195389\n",
      "iteration 2660, dc_loss: 0.029837632551789284, tv_loss: 0.03370064124464989\n",
      "iteration 2661, dc_loss: 0.029859459027647972, tv_loss: 0.033677004277706146\n",
      "iteration 2662, dc_loss: 0.02984405867755413, tv_loss: 0.033689480274915695\n",
      "iteration 2663, dc_loss: 0.029840592294931412, tv_loss: 0.03368563577532768\n",
      "iteration 2664, dc_loss: 0.029825592413544655, tv_loss: 0.03370574116706848\n",
      "iteration 2665, dc_loss: 0.029830293729901314, tv_loss: 0.03369482606649399\n",
      "iteration 2666, dc_loss: 0.0298150647431612, tv_loss: 0.033707309514284134\n",
      "iteration 2667, dc_loss: 0.029810236766934395, tv_loss: 0.03369545936584473\n",
      "iteration 2668, dc_loss: 0.029812995344400406, tv_loss: 0.03369204327464104\n",
      "iteration 2669, dc_loss: 0.029799314215779305, tv_loss: 0.033692993223667145\n",
      "iteration 2670, dc_loss: 0.02979937382042408, tv_loss: 0.033691808581352234\n",
      "iteration 2671, dc_loss: 0.02979418635368347, tv_loss: 0.03369800001382828\n",
      "iteration 2672, dc_loss: 0.029809406027197838, tv_loss: 0.0336780920624733\n",
      "iteration 2673, dc_loss: 0.029776066541671753, tv_loss: 0.03371117636561394\n",
      "iteration 2674, dc_loss: 0.029795611277222633, tv_loss: 0.03369119390845299\n",
      "iteration 2675, dc_loss: 0.029795151203870773, tv_loss: 0.033687710762023926\n",
      "iteration 2676, dc_loss: 0.029801934957504272, tv_loss: 0.03368806838989258\n",
      "iteration 2677, dc_loss: 0.029774880036711693, tv_loss: 0.033709220588207245\n",
      "iteration 2678, dc_loss: 0.029819507151842117, tv_loss: 0.033668916672468185\n",
      "iteration 2679, dc_loss: 0.029772598296403885, tv_loss: 0.03370807319879532\n",
      "iteration 2680, dc_loss: 0.029790978878736496, tv_loss: 0.03369331359863281\n",
      "iteration 2681, dc_loss: 0.02975906990468502, tv_loss: 0.03371448442339897\n",
      "iteration 2682, dc_loss: 0.029774712398648262, tv_loss: 0.033678989857435226\n",
      "iteration 2683, dc_loss: 0.029723847284913063, tv_loss: 0.03370705246925354\n",
      "iteration 2684, dc_loss: 0.02974451705813408, tv_loss: 0.03367656096816063\n",
      "iteration 2685, dc_loss: 0.029711425304412842, tv_loss: 0.03369752690196037\n",
      "iteration 2686, dc_loss: 0.029722535982728004, tv_loss: 0.03368307650089264\n",
      "iteration 2687, dc_loss: 0.02969834767282009, tv_loss: 0.0337083525955677\n",
      "iteration 2688, dc_loss: 0.029711393639445305, tv_loss: 0.03370271995663643\n",
      "iteration 2689, dc_loss: 0.02971334382891655, tv_loss: 0.033708468079566956\n",
      "iteration 2690, dc_loss: 0.02970403991639614, tv_loss: 0.03370717540383339\n",
      "iteration 2691, dc_loss: 0.029695279896259308, tv_loss: 0.03369860723614693\n",
      "iteration 2692, dc_loss: 0.02969292551279068, tv_loss: 0.033694423735141754\n",
      "iteration 2693, dc_loss: 0.029687007889151573, tv_loss: 0.03369938209652901\n",
      "iteration 2694, dc_loss: 0.029680004343390465, tv_loss: 0.0337040051817894\n",
      "iteration 2695, dc_loss: 0.02967524714767933, tv_loss: 0.03370730206370354\n",
      "iteration 2696, dc_loss: 0.029677724465727806, tv_loss: 0.03369391709566116\n",
      "iteration 2697, dc_loss: 0.029658956453204155, tv_loss: 0.03370613232254982\n",
      "iteration 2698, dc_loss: 0.029662881046533585, tv_loss: 0.033689819276332855\n",
      "iteration 2699, dc_loss: 0.029651805758476257, tv_loss: 0.03369372710585594\n",
      "iteration 2700, dc_loss: 0.02964864857494831, tv_loss: 0.033681392669677734\n",
      "iteration 2701, dc_loss: 0.029627209529280663, tv_loss: 0.033699650317430496\n",
      "iteration 2702, dc_loss: 0.029632462188601494, tv_loss: 0.03369330242276192\n",
      "iteration 2703, dc_loss: 0.029625343158841133, tv_loss: 0.033699825406074524\n",
      "iteration 2704, dc_loss: 0.029614929109811783, tv_loss: 0.033715326339006424\n",
      "iteration 2705, dc_loss: 0.029613381251692772, tv_loss: 0.03370992839336395\n",
      "iteration 2706, dc_loss: 0.029617473483085632, tv_loss: 0.033689334988594055\n",
      "iteration 2707, dc_loss: 0.029597816988825798, tv_loss: 0.03370559215545654\n",
      "iteration 2708, dc_loss: 0.029601505026221275, tv_loss: 0.03370494395494461\n",
      "iteration 2709, dc_loss: 0.029596742242574692, tv_loss: 0.033697180449962616\n",
      "iteration 2710, dc_loss: 0.029589684680104256, tv_loss: 0.033700719475746155\n",
      "iteration 2711, dc_loss: 0.02956177107989788, tv_loss: 0.03372780233621597\n",
      "iteration 2712, dc_loss: 0.029594359919428825, tv_loss: 0.03367917612195015\n",
      "iteration 2713, dc_loss: 0.029568104073405266, tv_loss: 0.033705975860357285\n",
      "iteration 2714, dc_loss: 0.029585162177681923, tv_loss: 0.03369010612368584\n",
      "iteration 2715, dc_loss: 0.029547741636633873, tv_loss: 0.03372634947299957\n",
      "iteration 2716, dc_loss: 0.02960045635700226, tv_loss: 0.03366737812757492\n",
      "iteration 2717, dc_loss: 0.02953179180622101, tv_loss: 0.03373463451862335\n",
      "iteration 2718, dc_loss: 0.029589135199785233, tv_loss: 0.03367173299193382\n",
      "iteration 2719, dc_loss: 0.029543699696660042, tv_loss: 0.03372783586382866\n",
      "iteration 2720, dc_loss: 0.029591523110866547, tv_loss: 0.0336800180375576\n",
      "iteration 2721, dc_loss: 0.029535723850131035, tv_loss: 0.03372864052653313\n",
      "iteration 2722, dc_loss: 0.029608603566884995, tv_loss: 0.03365955874323845\n",
      "iteration 2723, dc_loss: 0.029518427327275276, tv_loss: 0.033742647618055344\n",
      "iteration 2724, dc_loss: 0.029571261256933212, tv_loss: 0.0336780920624733\n",
      "iteration 2725, dc_loss: 0.029513543471693993, tv_loss: 0.03371358662843704\n",
      "iteration 2726, dc_loss: 0.029534809291362762, tv_loss: 0.03368847072124481\n",
      "iteration 2727, dc_loss: 0.029500817880034447, tv_loss: 0.03371540084481239\n",
      "iteration 2728, dc_loss: 0.029506536200642586, tv_loss: 0.033695656806230545\n",
      "iteration 2729, dc_loss: 0.029493344947695732, tv_loss: 0.0337018258869648\n",
      "iteration 2730, dc_loss: 0.029495183378458023, tv_loss: 0.03369981050491333\n",
      "iteration 2731, dc_loss: 0.029499230906367302, tv_loss: 0.033699698746204376\n",
      "iteration 2732, dc_loss: 0.02949037216603756, tv_loss: 0.03370228037238121\n",
      "iteration 2733, dc_loss: 0.029498929157853127, tv_loss: 0.033695973455905914\n",
      "iteration 2734, dc_loss: 0.029490552842617035, tv_loss: 0.03370973840355873\n",
      "iteration 2735, dc_loss: 0.02949804998934269, tv_loss: 0.03371018171310425\n",
      "iteration 2736, dc_loss: 0.029493892565369606, tv_loss: 0.03369525074958801\n",
      "iteration 2737, dc_loss: 0.029454927891492844, tv_loss: 0.03373531997203827\n",
      "iteration 2738, dc_loss: 0.02947673760354519, tv_loss: 0.03369079902768135\n",
      "iteration 2739, dc_loss: 0.02943393960595131, tv_loss: 0.03371351584792137\n",
      "iteration 2740, dc_loss: 0.02946072816848755, tv_loss: 0.03368464484810829\n",
      "iteration 2741, dc_loss: 0.029432719573378563, tv_loss: 0.03372451290488243\n",
      "iteration 2742, dc_loss: 0.029459280893206596, tv_loss: 0.03370261192321777\n",
      "iteration 2743, dc_loss: 0.029427241533994675, tv_loss: 0.03374366834759712\n",
      "iteration 2744, dc_loss: 0.029470134526491165, tv_loss: 0.033672988414764404\n",
      "iteration 2745, dc_loss: 0.02941027469933033, tv_loss: 0.03371518477797508\n",
      "iteration 2746, dc_loss: 0.02942560985684395, tv_loss: 0.03370097279548645\n",
      "iteration 2747, dc_loss: 0.029398087412118912, tv_loss: 0.03373244032263756\n",
      "iteration 2748, dc_loss: 0.029436280950903893, tv_loss: 0.03368474543094635\n",
      "iteration 2749, dc_loss: 0.029374118894338608, tv_loss: 0.03374074026942253\n",
      "iteration 2750, dc_loss: 0.029408540576696396, tv_loss: 0.033688005059957504\n",
      "iteration 2751, dc_loss: 0.029380660504102707, tv_loss: 0.03370565548539162\n",
      "iteration 2752, dc_loss: 0.02937786653637886, tv_loss: 0.03369763121008873\n",
      "iteration 2753, dc_loss: 0.02936774119734764, tv_loss: 0.03371017426252365\n",
      "iteration 2754, dc_loss: 0.02937246859073639, tv_loss: 0.03370712324976921\n",
      "iteration 2755, dc_loss: 0.029362326487898827, tv_loss: 0.033705782145261765\n",
      "iteration 2756, dc_loss: 0.02935202419757843, tv_loss: 0.03371269255876541\n",
      "iteration 2757, dc_loss: 0.029365357011556625, tv_loss: 0.03368758037686348\n",
      "iteration 2758, dc_loss: 0.029329411685466766, tv_loss: 0.03371584787964821\n",
      "iteration 2759, dc_loss: 0.029346533119678497, tv_loss: 0.03369801118969917\n",
      "iteration 2760, dc_loss: 0.029330920428037643, tv_loss: 0.03370710834860802\n",
      "iteration 2761, dc_loss: 0.029351871460676193, tv_loss: 0.033676620572805405\n",
      "iteration 2762, dc_loss: 0.029299845919013023, tv_loss: 0.03373095393180847\n",
      "iteration 2763, dc_loss: 0.029352158308029175, tv_loss: 0.03366996347904205\n",
      "iteration 2764, dc_loss: 0.02929098904132843, tv_loss: 0.033729057759046555\n",
      "iteration 2765, dc_loss: 0.029349589720368385, tv_loss: 0.03368457406759262\n",
      "iteration 2766, dc_loss: 0.029277879744768143, tv_loss: 0.03376784920692444\n",
      "iteration 2767, dc_loss: 0.0293685682117939, tv_loss: 0.03367030248045921\n",
      "iteration 2768, dc_loss: 0.029266418889164925, tv_loss: 0.033755626529455185\n",
      "iteration 2769, dc_loss: 0.02935011312365532, tv_loss: 0.033675163984298706\n",
      "iteration 2770, dc_loss: 0.029273290187120438, tv_loss: 0.03375820815563202\n",
      "iteration 2771, dc_loss: 0.029341183602809906, tv_loss: 0.03368103504180908\n",
      "iteration 2772, dc_loss: 0.029249217361211777, tv_loss: 0.03375530242919922\n",
      "iteration 2773, dc_loss: 0.029319753870368004, tv_loss: 0.03367244079709053\n",
      "iteration 2774, dc_loss: 0.02925781160593033, tv_loss: 0.03373430296778679\n",
      "iteration 2775, dc_loss: 0.029279248788952827, tv_loss: 0.03371182084083557\n",
      "iteration 2776, dc_loss: 0.029270602390170097, tv_loss: 0.03372468054294586\n",
      "iteration 2777, dc_loss: 0.029255926609039307, tv_loss: 0.03372885659337044\n",
      "iteration 2778, dc_loss: 0.029259929433465004, tv_loss: 0.03370879217982292\n",
      "iteration 2779, dc_loss: 0.029242994263768196, tv_loss: 0.03372230380773544\n",
      "iteration 2780, dc_loss: 0.029288126155734062, tv_loss: 0.03367882966995239\n",
      "iteration 2781, dc_loss: 0.02922901138663292, tv_loss: 0.03375732898712158\n",
      "iteration 2782, dc_loss: 0.029294947162270546, tv_loss: 0.03368647024035454\n",
      "iteration 2783, dc_loss: 0.02924880012869835, tv_loss: 0.033742573112249374\n",
      "iteration 2784, dc_loss: 0.029295993968844414, tv_loss: 0.033675871789455414\n",
      "iteration 2785, dc_loss: 0.02920065075159073, tv_loss: 0.033763282001018524\n",
      "iteration 2786, dc_loss: 0.0292750783264637, tv_loss: 0.03367280960083008\n",
      "iteration 2787, dc_loss: 0.029213927686214447, tv_loss: 0.033731576055288315\n",
      "iteration 2788, dc_loss: 0.02923392318189144, tv_loss: 0.033702295273542404\n",
      "iteration 2789, dc_loss: 0.029195396229624748, tv_loss: 0.033731963485479355\n",
      "iteration 2790, dc_loss: 0.02921268530189991, tv_loss: 0.03370528295636177\n",
      "iteration 2791, dc_loss: 0.02917838655412197, tv_loss: 0.0337209478020668\n",
      "iteration 2792, dc_loss: 0.029162663966417313, tv_loss: 0.03372688591480255\n",
      "iteration 2793, dc_loss: 0.02919659949839115, tv_loss: 0.03369556739926338\n",
      "iteration 2794, dc_loss: 0.029166867956519127, tv_loss: 0.03372307866811752\n",
      "iteration 2795, dc_loss: 0.029190178960561752, tv_loss: 0.033702727407217026\n",
      "iteration 2796, dc_loss: 0.029151013121008873, tv_loss: 0.03373349830508232\n",
      "iteration 2797, dc_loss: 0.029187213629484177, tv_loss: 0.0336935818195343\n",
      "iteration 2798, dc_loss: 0.029134519398212433, tv_loss: 0.033744316548109055\n",
      "iteration 2799, dc_loss: 0.029169753193855286, tv_loss: 0.033704470843076706\n",
      "iteration 2800, dc_loss: 0.02914131060242653, tv_loss: 0.03372626751661301\n",
      "iteration 2801, dc_loss: 0.029158079996705055, tv_loss: 0.03369644656777382\n",
      "iteration 2802, dc_loss: 0.029115213081240654, tv_loss: 0.033718857914209366\n",
      "iteration 2803, dc_loss: 0.02911454439163208, tv_loss: 0.033713940531015396\n",
      "iteration 2804, dc_loss: 0.029135189950466156, tv_loss: 0.03369645029306412\n",
      "iteration 2805, dc_loss: 0.029101114720106125, tv_loss: 0.033723317086696625\n",
      "iteration 2806, dc_loss: 0.02910797670483589, tv_loss: 0.0337042436003685\n",
      "iteration 2807, dc_loss: 0.029122712090611458, tv_loss: 0.03368840739130974\n",
      "iteration 2808, dc_loss: 0.02909066528081894, tv_loss: 0.033717501908540726\n",
      "iteration 2809, dc_loss: 0.02909756824374199, tv_loss: 0.03370116278529167\n",
      "iteration 2810, dc_loss: 0.029101891443133354, tv_loss: 0.033693406730890274\n",
      "iteration 2811, dc_loss: 0.029082244262099266, tv_loss: 0.0337153896689415\n",
      "iteration 2812, dc_loss: 0.02908608503639698, tv_loss: 0.033701393753290176\n",
      "iteration 2813, dc_loss: 0.02908208966255188, tv_loss: 0.03370300680398941\n",
      "iteration 2814, dc_loss: 0.029069431126117706, tv_loss: 0.03371371701359749\n",
      "iteration 2815, dc_loss: 0.029075201600790024, tv_loss: 0.033697500824928284\n",
      "iteration 2816, dc_loss: 0.029071830213069916, tv_loss: 0.033697571605443954\n",
      "iteration 2817, dc_loss: 0.02905866876244545, tv_loss: 0.03370276466012001\n",
      "iteration 2818, dc_loss: 0.029067199677228928, tv_loss: 0.03368944674730301\n",
      "iteration 2819, dc_loss: 0.029051274061203003, tv_loss: 0.03370336815714836\n",
      "iteration 2820, dc_loss: 0.029048362746834755, tv_loss: 0.03370791673660278\n",
      "iteration 2821, dc_loss: 0.02905779704451561, tv_loss: 0.03371044993400574\n",
      "iteration 2822, dc_loss: 0.029040662571787834, tv_loss: 0.0337211973965168\n",
      "iteration 2823, dc_loss: 0.029035069048404694, tv_loss: 0.03371391445398331\n",
      "iteration 2824, dc_loss: 0.029041720554232597, tv_loss: 0.033697567880153656\n",
      "iteration 2825, dc_loss: 0.029029522091150284, tv_loss: 0.03370671346783638\n",
      "iteration 2826, dc_loss: 0.029028447344899178, tv_loss: 0.03370172530412674\n",
      "iteration 2827, dc_loss: 0.029021086171269417, tv_loss: 0.03370409458875656\n",
      "iteration 2828, dc_loss: 0.029018867760896683, tv_loss: 0.03370525315403938\n",
      "iteration 2829, dc_loss: 0.029028108343482018, tv_loss: 0.03369816020131111\n",
      "iteration 2830, dc_loss: 0.02900836057960987, tv_loss: 0.03372013941407204\n",
      "iteration 2831, dc_loss: 0.02899869531393051, tv_loss: 0.03372234106063843\n",
      "iteration 2832, dc_loss: 0.029011821374297142, tv_loss: 0.03370177745819092\n",
      "iteration 2833, dc_loss: 0.029003236442804337, tv_loss: 0.03369958698749542\n",
      "iteration 2834, dc_loss: 0.028990136459469795, tv_loss: 0.033714521676301956\n",
      "iteration 2835, dc_loss: 0.02899434231221676, tv_loss: 0.03370661288499832\n",
      "iteration 2836, dc_loss: 0.028994448482990265, tv_loss: 0.03370239585638046\n",
      "iteration 2837, dc_loss: 0.02898293361067772, tv_loss: 0.03371395170688629\n",
      "iteration 2838, dc_loss: 0.02897786535322666, tv_loss: 0.033711593598127365\n",
      "iteration 2839, dc_loss: 0.028974713757634163, tv_loss: 0.03371121361851692\n",
      "iteration 2840, dc_loss: 0.028984995558857918, tv_loss: 0.03369283303618431\n",
      "iteration 2841, dc_loss: 0.02896951697766781, tv_loss: 0.03370685875415802\n",
      "iteration 2842, dc_loss: 0.028955182060599327, tv_loss: 0.03371691703796387\n",
      "iteration 2843, dc_loss: 0.028963904827833176, tv_loss: 0.0337008573114872\n",
      "iteration 2844, dc_loss: 0.028964098542928696, tv_loss: 0.03370511531829834\n",
      "iteration 2845, dc_loss: 0.02895115315914154, tv_loss: 0.03370997682213783\n",
      "iteration 2846, dc_loss: 0.028946734964847565, tv_loss: 0.033711399883031845\n",
      "iteration 2847, dc_loss: 0.028949476778507233, tv_loss: 0.03370586410164833\n",
      "iteration 2848, dc_loss: 0.02895362116396427, tv_loss: 0.03370242565870285\n",
      "iteration 2849, dc_loss: 0.028934475034475327, tv_loss: 0.033709824085235596\n",
      "iteration 2850, dc_loss: 0.028925130143761635, tv_loss: 0.03371971473097801\n",
      "iteration 2851, dc_loss: 0.02893761359155178, tv_loss: 0.03369686380028725\n",
      "iteration 2852, dc_loss: 0.028931481763720512, tv_loss: 0.03370777145028114\n",
      "iteration 2853, dc_loss: 0.028916077688336372, tv_loss: 0.033724136650562286\n",
      "iteration 2854, dc_loss: 0.028920229524374008, tv_loss: 0.033711161464452744\n",
      "iteration 2855, dc_loss: 0.028925156220793724, tv_loss: 0.03370293229818344\n",
      "iteration 2856, dc_loss: 0.028911786153912544, tv_loss: 0.03371389955282211\n",
      "iteration 2857, dc_loss: 0.028899960219860077, tv_loss: 0.033716559410095215\n",
      "iteration 2858, dc_loss: 0.028908317908644676, tv_loss: 0.03370840847492218\n",
      "iteration 2859, dc_loss: 0.028908951207995415, tv_loss: 0.0337052159011364\n",
      "iteration 2860, dc_loss: 0.028895532712340355, tv_loss: 0.03370767831802368\n",
      "iteration 2861, dc_loss: 0.028890086337924004, tv_loss: 0.033716779202222824\n",
      "iteration 2862, dc_loss: 0.028891028836369514, tv_loss: 0.03370616212487221\n",
      "iteration 2863, dc_loss: 0.028886405751109123, tv_loss: 0.033711545169353485\n",
      "iteration 2864, dc_loss: 0.02887558378279209, tv_loss: 0.03370894864201546\n",
      "iteration 2865, dc_loss: 0.02887885831296444, tv_loss: 0.03371229022741318\n",
      "iteration 2866, dc_loss: 0.028882918879389763, tv_loss: 0.03370806202292442\n",
      "iteration 2867, dc_loss: 0.028870638459920883, tv_loss: 0.03371522203087807\n",
      "iteration 2868, dc_loss: 0.028861993923783302, tv_loss: 0.03372277319431305\n",
      "iteration 2869, dc_loss: 0.02886364422738552, tv_loss: 0.033710915595293045\n",
      "iteration 2870, dc_loss: 0.028865445405244827, tv_loss: 0.033704038709402084\n",
      "iteration 2871, dc_loss: 0.028850432485342026, tv_loss: 0.03372437506914139\n",
      "iteration 2872, dc_loss: 0.02884676121175289, tv_loss: 0.03371795639395714\n",
      "iteration 2873, dc_loss: 0.028852984309196472, tv_loss: 0.033708736300468445\n",
      "iteration 2874, dc_loss: 0.028846854344010353, tv_loss: 0.033714208751916885\n",
      "iteration 2875, dc_loss: 0.028840811923146248, tv_loss: 0.03371398150920868\n",
      "iteration 2876, dc_loss: 0.028830282390117645, tv_loss: 0.03372303768992424\n",
      "iteration 2877, dc_loss: 0.02882845513522625, tv_loss: 0.03372449055314064\n",
      "iteration 2878, dc_loss: 0.028838716447353363, tv_loss: 0.03370710834860802\n",
      "iteration 2879, dc_loss: 0.02882957085967064, tv_loss: 0.03370676562190056\n",
      "iteration 2880, dc_loss: 0.02881082147359848, tv_loss: 0.033729493618011475\n",
      "iteration 2881, dc_loss: 0.028819454833865166, tv_loss: 0.03371913731098175\n",
      "iteration 2882, dc_loss: 0.028817858546972275, tv_loss: 0.03370515629649162\n",
      "iteration 2883, dc_loss: 0.028804754838347435, tv_loss: 0.03372887894511223\n",
      "iteration 2884, dc_loss: 0.028803661465644836, tv_loss: 0.03371928259730339\n",
      "iteration 2885, dc_loss: 0.028808755800127983, tv_loss: 0.03370467200875282\n",
      "iteration 2886, dc_loss: 0.028800392523407936, tv_loss: 0.0337178073823452\n",
      "iteration 2887, dc_loss: 0.02879733219742775, tv_loss: 0.03371576964855194\n",
      "iteration 2888, dc_loss: 0.028788261115550995, tv_loss: 0.03370916098356247\n",
      "iteration 2889, dc_loss: 0.02878700941801071, tv_loss: 0.03370916470885277\n",
      "iteration 2890, dc_loss: 0.0287895780056715, tv_loss: 0.03371606767177582\n",
      "iteration 2891, dc_loss: 0.028775012120604515, tv_loss: 0.033714067190885544\n",
      "iteration 2892, dc_loss: 0.028773268684744835, tv_loss: 0.03371884673833847\n",
      "iteration 2893, dc_loss: 0.02877732366323471, tv_loss: 0.03371960297226906\n",
      "iteration 2894, dc_loss: 0.028766116127371788, tv_loss: 0.03371209651231766\n",
      "iteration 2895, dc_loss: 0.028758246451616287, tv_loss: 0.0337224118411541\n",
      "iteration 2896, dc_loss: 0.02876829169690609, tv_loss: 0.03371544927358627\n",
      "iteration 2897, dc_loss: 0.028761642053723335, tv_loss: 0.03370678424835205\n",
      "iteration 2898, dc_loss: 0.028750527650117874, tv_loss: 0.03372194245457649\n",
      "iteration 2899, dc_loss: 0.028749171644449234, tv_loss: 0.033720310777425766\n",
      "iteration 2900, dc_loss: 0.02874867245554924, tv_loss: 0.033708009868860245\n",
      "iteration 2901, dc_loss: 0.028739066794514656, tv_loss: 0.03372037783265114\n",
      "iteration 2902, dc_loss: 0.028739312663674355, tv_loss: 0.033719588071107864\n",
      "iteration 2903, dc_loss: 0.028734128922224045, tv_loss: 0.033712275326251984\n",
      "iteration 2904, dc_loss: 0.02873389795422554, tv_loss: 0.03370951488614082\n",
      "iteration 2905, dc_loss: 0.028728777542710304, tv_loss: 0.03371430188417435\n",
      "iteration 2906, dc_loss: 0.028722897171974182, tv_loss: 0.03371323645114899\n",
      "iteration 2907, dc_loss: 0.028719140216708183, tv_loss: 0.03371366485953331\n",
      "iteration 2908, dc_loss: 0.02871984802186489, tv_loss: 0.03372185304760933\n",
      "iteration 2909, dc_loss: 0.028708510100841522, tv_loss: 0.03372777998447418\n",
      "iteration 2910, dc_loss: 0.028704598546028137, tv_loss: 0.03373049572110176\n",
      "iteration 2911, dc_loss: 0.0287153460085392, tv_loss: 0.03370806202292442\n",
      "iteration 2912, dc_loss: 0.02870027720928192, tv_loss: 0.033712130039930344\n",
      "iteration 2913, dc_loss: 0.02868703380227089, tv_loss: 0.03373407945036888\n",
      "iteration 2914, dc_loss: 0.028701262548565865, tv_loss: 0.03372041508555412\n",
      "iteration 2915, dc_loss: 0.02869771420955658, tv_loss: 0.03372558578848839\n",
      "iteration 2916, dc_loss: 0.0286802276968956, tv_loss: 0.03373376652598381\n",
      "iteration 2917, dc_loss: 0.028681546449661255, tv_loss: 0.03371875733137131\n",
      "iteration 2918, dc_loss: 0.028688140213489532, tv_loss: 0.03371328487992287\n",
      "iteration 2919, dc_loss: 0.02866494283080101, tv_loss: 0.03373437747359276\n",
      "iteration 2920, dc_loss: 0.02866755612194538, tv_loss: 0.033721115440130234\n",
      "iteration 2921, dc_loss: 0.028684474527835846, tv_loss: 0.03370630368590355\n",
      "iteration 2922, dc_loss: 0.02866394631564617, tv_loss: 0.03371783718466759\n",
      "iteration 2923, dc_loss: 0.028652844950556755, tv_loss: 0.03372545167803764\n",
      "iteration 2924, dc_loss: 0.02865871600806713, tv_loss: 0.033720169216394424\n",
      "iteration 2925, dc_loss: 0.02865220047533512, tv_loss: 0.033720407634973526\n",
      "iteration 2926, dc_loss: 0.028652893379330635, tv_loss: 0.033719003200531006\n",
      "iteration 2927, dc_loss: 0.028646590188145638, tv_loss: 0.03372000530362129\n",
      "iteration 2928, dc_loss: 0.0286429263651371, tv_loss: 0.03372159227728844\n",
      "iteration 2929, dc_loss: 0.028636686503887177, tv_loss: 0.03372224420309067\n",
      "iteration 2930, dc_loss: 0.02863340452313423, tv_loss: 0.033716507256031036\n",
      "iteration 2931, dc_loss: 0.02863313816487789, tv_loss: 0.03372327610850334\n",
      "iteration 2932, dc_loss: 0.028628546744585037, tv_loss: 0.03371870890259743\n",
      "iteration 2933, dc_loss: 0.02862362004816532, tv_loss: 0.03372844308614731\n",
      "iteration 2934, dc_loss: 0.028614113107323647, tv_loss: 0.03372999280691147\n",
      "iteration 2935, dc_loss: 0.028617870062589645, tv_loss: 0.03371741622686386\n",
      "iteration 2936, dc_loss: 0.02861800789833069, tv_loss: 0.033711936324834824\n",
      "iteration 2937, dc_loss: 0.028609300032258034, tv_loss: 0.033718228340148926\n",
      "iteration 2938, dc_loss: 0.028600914403796196, tv_loss: 0.033714793622493744\n",
      "iteration 2939, dc_loss: 0.028603551909327507, tv_loss: 0.033710777759552\n",
      "iteration 2940, dc_loss: 0.028600094839930534, tv_loss: 0.03371185064315796\n",
      "iteration 2941, dc_loss: 0.02859416976571083, tv_loss: 0.03372202068567276\n",
      "iteration 2942, dc_loss: 0.02859421819448471, tv_loss: 0.03373538330197334\n",
      "iteration 2943, dc_loss: 0.028582673519849777, tv_loss: 0.03374023362994194\n",
      "iteration 2944, dc_loss: 0.028583500534296036, tv_loss: 0.0337214432656765\n",
      "iteration 2945, dc_loss: 0.028583014383912086, tv_loss: 0.033715248107910156\n",
      "iteration 2946, dc_loss: 0.028575854375958443, tv_loss: 0.03372877091169357\n",
      "iteration 2947, dc_loss: 0.028575580567121506, tv_loss: 0.033725012093782425\n",
      "iteration 2948, dc_loss: 0.0285621490329504, tv_loss: 0.033739227801561356\n",
      "iteration 2949, dc_loss: 0.028564225882291794, tv_loss: 0.03372516483068466\n",
      "iteration 2950, dc_loss: 0.028565803542733192, tv_loss: 0.03372209146618843\n",
      "iteration 2951, dc_loss: 0.02856280282139778, tv_loss: 0.03371875733137131\n",
      "iteration 2952, dc_loss: 0.028550218790769577, tv_loss: 0.03372695669531822\n",
      "iteration 2953, dc_loss: 0.028547147288918495, tv_loss: 0.033730048686265945\n",
      "iteration 2954, dc_loss: 0.028551267459988594, tv_loss: 0.0337207093834877\n",
      "iteration 2955, dc_loss: 0.02854391373693943, tv_loss: 0.033724214881658554\n",
      "iteration 2956, dc_loss: 0.02853807993233204, tv_loss: 0.03372574597597122\n",
      "iteration 2957, dc_loss: 0.028536701574921608, tv_loss: 0.03372359275817871\n",
      "iteration 2958, dc_loss: 0.02853192202746868, tv_loss: 0.033725570887327194\n",
      "iteration 2959, dc_loss: 0.02852637507021427, tv_loss: 0.033727142959833145\n",
      "iteration 2960, dc_loss: 0.028529338538646698, tv_loss: 0.03371788188815117\n",
      "iteration 2961, dc_loss: 0.028527431190013885, tv_loss: 0.03371870890259743\n",
      "iteration 2962, dc_loss: 0.028514038771390915, tv_loss: 0.033731162548065186\n",
      "iteration 2963, dc_loss: 0.028510745614767075, tv_loss: 0.033728402107954025\n",
      "iteration 2964, dc_loss: 0.028507616370916367, tv_loss: 0.03373075649142265\n",
      "iteration 2965, dc_loss: 0.028510356321930885, tv_loss: 0.03373052924871445\n",
      "iteration 2966, dc_loss: 0.02851138263940811, tv_loss: 0.03372286632657051\n",
      "iteration 2967, dc_loss: 0.02849966660141945, tv_loss: 0.03372536227107048\n",
      "iteration 2968, dc_loss: 0.028493721038103104, tv_loss: 0.03372780606150627\n",
      "iteration 2969, dc_loss: 0.028494959697127342, tv_loss: 0.03372413292527199\n",
      "iteration 2970, dc_loss: 0.028488636016845703, tv_loss: 0.033727411180734634\n",
      "iteration 2971, dc_loss: 0.028482487425208092, tv_loss: 0.03373919799923897\n",
      "iteration 2972, dc_loss: 0.028481438755989075, tv_loss: 0.03373110294342041\n",
      "iteration 2973, dc_loss: 0.028486158698797226, tv_loss: 0.03371911868453026\n",
      "iteration 2974, dc_loss: 0.028467640280723572, tv_loss: 0.03374010697007179\n",
      "iteration 2975, dc_loss: 0.028476012870669365, tv_loss: 0.0337216779589653\n",
      "iteration 2976, dc_loss: 0.028471166267991066, tv_loss: 0.033724330365657806\n",
      "iteration 2977, dc_loss: 0.028456352651119232, tv_loss: 0.03373531624674797\n",
      "iteration 2978, dc_loss: 0.028465893119573593, tv_loss: 0.03371701017022133\n",
      "iteration 2979, dc_loss: 0.028452368453145027, tv_loss: 0.03372513875365257\n",
      "iteration 2980, dc_loss: 0.028459737077355385, tv_loss: 0.033718183636665344\n",
      "iteration 2981, dc_loss: 0.028450481593608856, tv_loss: 0.0337214320898056\n",
      "iteration 2982, dc_loss: 0.028444522991776466, tv_loss: 0.03372548148036003\n",
      "iteration 2983, dc_loss: 0.028442436829209328, tv_loss: 0.033735331147909164\n",
      "iteration 2984, dc_loss: 0.028443796560168266, tv_loss: 0.03373367339372635\n",
      "iteration 2985, dc_loss: 0.028439577668905258, tv_loss: 0.033729348331689835\n",
      "iteration 2986, dc_loss: 0.0284188911318779, tv_loss: 0.03373827785253525\n",
      "iteration 2987, dc_loss: 0.028436312451958656, tv_loss: 0.03371819853782654\n",
      "iteration 2988, dc_loss: 0.028426222503185272, tv_loss: 0.03373061865568161\n",
      "iteration 2989, dc_loss: 0.02842508815228939, tv_loss: 0.0337357297539711\n",
      "iteration 2990, dc_loss: 0.028414269909262657, tv_loss: 0.033737942576408386\n",
      "iteration 2991, dc_loss: 0.02840639464557171, tv_loss: 0.03373550623655319\n",
      "iteration 2992, dc_loss: 0.028414897620677948, tv_loss: 0.033725522458553314\n",
      "iteration 2993, dc_loss: 0.028408648446202278, tv_loss: 0.03373194485902786\n",
      "iteration 2994, dc_loss: 0.0284110177308321, tv_loss: 0.03373033553361893\n",
      "iteration 2995, dc_loss: 0.028392774984240532, tv_loss: 0.033748067915439606\n",
      "iteration 2996, dc_loss: 0.02840585634112358, tv_loss: 0.03372601047158241\n",
      "iteration 2997, dc_loss: 0.028405869379639626, tv_loss: 0.03371795266866684\n",
      "iteration 2998, dc_loss: 0.028385933488607407, tv_loss: 0.03374454006552696\n",
      "iteration 2999, dc_loss: 0.028390871360898018, tv_loss: 0.03374636173248291\n",
      "iteration 3000, dc_loss: 0.02840282954275608, tv_loss: 0.03372824192047119\n",
      "iteration 3001, dc_loss: 0.028389889746904373, tv_loss: 0.03373672068119049\n",
      "iteration 3002, dc_loss: 0.028393952175974846, tv_loss: 0.03373175114393234\n",
      "iteration 3003, dc_loss: 0.028401926159858704, tv_loss: 0.033725276589393616\n",
      "iteration 3004, dc_loss: 0.028395814821124077, tv_loss: 0.033724792301654816\n",
      "iteration 3005, dc_loss: 0.02836606092751026, tv_loss: 0.03374600037932396\n",
      "iteration 3006, dc_loss: 0.028374772518873215, tv_loss: 0.03372307866811752\n",
      "iteration 3007, dc_loss: 0.02837139368057251, tv_loss: 0.03371576592326164\n",
      "iteration 3008, dc_loss: 0.028354918584227562, tv_loss: 0.033729929476976395\n",
      "iteration 3009, dc_loss: 0.028349384665489197, tv_loss: 0.03372436761856079\n",
      "iteration 3010, dc_loss: 0.0283500999212265, tv_loss: 0.033722907304763794\n",
      "iteration 3011, dc_loss: 0.028346164152026176, tv_loss: 0.03372349590063095\n",
      "iteration 3012, dc_loss: 0.02834002859890461, tv_loss: 0.03373491391539574\n",
      "iteration 3013, dc_loss: 0.028338471427559853, tv_loss: 0.033739980310201645\n",
      "iteration 3014, dc_loss: 0.02833425998687744, tv_loss: 0.03373786807060242\n",
      "iteration 3015, dc_loss: 0.028332889080047607, tv_loss: 0.03373026102781296\n",
      "iteration 3016, dc_loss: 0.02832837775349617, tv_loss: 0.033727627247571945\n",
      "iteration 3017, dc_loss: 0.0283297561109066, tv_loss: 0.03372202068567276\n",
      "iteration 3018, dc_loss: 0.02832525596022606, tv_loss: 0.03371943160891533\n",
      "iteration 3019, dc_loss: 0.02831808663904667, tv_loss: 0.033729035407304764\n",
      "iteration 3020, dc_loss: 0.028312252834439278, tv_loss: 0.0337333008646965\n",
      "iteration 3021, dc_loss: 0.028309615328907967, tv_loss: 0.03372889757156372\n",
      "iteration 3022, dc_loss: 0.028311820700764656, tv_loss: 0.03373377025127411\n",
      "iteration 3023, dc_loss: 0.02829737216234207, tv_loss: 0.03373569995164871\n",
      "iteration 3024, dc_loss: 0.02829653024673462, tv_loss: 0.033733952790498734\n",
      "iteration 3025, dc_loss: 0.028296733275055885, tv_loss: 0.033728692680597305\n",
      "iteration 3026, dc_loss: 0.02829359658062458, tv_loss: 0.03372399881482124\n",
      "iteration 3027, dc_loss: 0.02828754298388958, tv_loss: 0.0337265282869339\n",
      "iteration 3028, dc_loss: 0.02828294038772583, tv_loss: 0.033735621720552444\n",
      "iteration 3029, dc_loss: 0.02827543206512928, tv_loss: 0.03374774754047394\n",
      "iteration 3030, dc_loss: 0.028277283534407616, tv_loss: 0.03373205289244652\n",
      "iteration 3031, dc_loss: 0.02827819623053074, tv_loss: 0.03372707590460777\n",
      "iteration 3032, dc_loss: 0.02826530486345291, tv_loss: 0.03373801335692406\n",
      "iteration 3033, dc_loss: 0.028268776834011078, tv_loss: 0.0337299108505249\n",
      "iteration 3034, dc_loss: 0.028265954926609993, tv_loss: 0.03374631330370903\n",
      "iteration 3035, dc_loss: 0.028259310871362686, tv_loss: 0.033737663179636\n",
      "iteration 3036, dc_loss: 0.02826097048819065, tv_loss: 0.03372862562537193\n",
      "iteration 3037, dc_loss: 0.028258105739951134, tv_loss: 0.03372909128665924\n",
      "iteration 3038, dc_loss: 0.028252704069018364, tv_loss: 0.03374023735523224\n",
      "iteration 3039, dc_loss: 0.028242478147149086, tv_loss: 0.033744193613529205\n",
      "iteration 3040, dc_loss: 0.02825186401605606, tv_loss: 0.033733878284692764\n",
      "iteration 3041, dc_loss: 0.02824539691209793, tv_loss: 0.0337313637137413\n",
      "iteration 3042, dc_loss: 0.02824423462152481, tv_loss: 0.033726342022418976\n",
      "iteration 3043, dc_loss: 0.028238382190465927, tv_loss: 0.03374449908733368\n",
      "iteration 3044, dc_loss: 0.028239019215106964, tv_loss: 0.03374356031417847\n",
      "iteration 3045, dc_loss: 0.028242871165275574, tv_loss: 0.03374126926064491\n",
      "iteration 3046, dc_loss: 0.02825062908232212, tv_loss: 0.033722732216119766\n",
      "iteration 3047, dc_loss: 0.028228670358657837, tv_loss: 0.03374539688229561\n",
      "iteration 3048, dc_loss: 0.02823502942919731, tv_loss: 0.03372938930988312\n",
      "iteration 3049, dc_loss: 0.028223685920238495, tv_loss: 0.0337403304874897\n",
      "iteration 3050, dc_loss: 0.02821356989443302, tv_loss: 0.03373676538467407\n",
      "iteration 3051, dc_loss: 0.028215494006872177, tv_loss: 0.033728860318660736\n",
      "iteration 3052, dc_loss: 0.02820931188762188, tv_loss: 0.03372830152511597\n",
      "iteration 3053, dc_loss: 0.028204016387462616, tv_loss: 0.03372878581285477\n",
      "iteration 3054, dc_loss: 0.02817915566265583, tv_loss: 0.03374716639518738\n",
      "iteration 3055, dc_loss: 0.028213994577527046, tv_loss: 0.03371448069810867\n",
      "iteration 3056, dc_loss: 0.028181929141283035, tv_loss: 0.03374962881207466\n",
      "iteration 3057, dc_loss: 0.0281901266425848, tv_loss: 0.03373328223824501\n",
      "iteration 3058, dc_loss: 0.028181536123156548, tv_loss: 0.03374340012669563\n",
      "iteration 3059, dc_loss: 0.028193317353725433, tv_loss: 0.033725712448358536\n",
      "iteration 3060, dc_loss: 0.028169289231300354, tv_loss: 0.03373817354440689\n",
      "iteration 3061, dc_loss: 0.0281657874584198, tv_loss: 0.033737633377313614\n",
      "iteration 3062, dc_loss: 0.02818266861140728, tv_loss: 0.0337209478020668\n",
      "iteration 3063, dc_loss: 0.028158744797110558, tv_loss: 0.03373829647898674\n",
      "iteration 3064, dc_loss: 0.028166813775897026, tv_loss: 0.03373141586780548\n",
      "iteration 3065, dc_loss: 0.02814660593867302, tv_loss: 0.03374346345663071\n",
      "iteration 3066, dc_loss: 0.028170239180326462, tv_loss: 0.0337170735001564\n",
      "iteration 3067, dc_loss: 0.028139373287558556, tv_loss: 0.033741071820259094\n",
      "iteration 3068, dc_loss: 0.028163515031337738, tv_loss: 0.0337141714990139\n",
      "iteration 3069, dc_loss: 0.028131039813160896, tv_loss: 0.03375494107604027\n",
      "iteration 3070, dc_loss: 0.028152678161859512, tv_loss: 0.03372405096888542\n",
      "iteration 3071, dc_loss: 0.028128691017627716, tv_loss: 0.0337492898106575\n",
      "iteration 3072, dc_loss: 0.028140151873230934, tv_loss: 0.033735889941453934\n",
      "iteration 3073, dc_loss: 0.028120435774326324, tv_loss: 0.03374534100294113\n",
      "iteration 3074, dc_loss: 0.028131812810897827, tv_loss: 0.033733416348695755\n",
      "iteration 3075, dc_loss: 0.028120309114456177, tv_loss: 0.03373967111110687\n",
      "iteration 3076, dc_loss: 0.02811865136027336, tv_loss: 0.03373830392956734\n",
      "iteration 3077, dc_loss: 0.028124254196882248, tv_loss: 0.03373164311051369\n",
      "iteration 3078, dc_loss: 0.028118649497628212, tv_loss: 0.03373527154326439\n",
      "iteration 3079, dc_loss: 0.028119206428527832, tv_loss: 0.03373578563332558\n",
      "iteration 3080, dc_loss: 0.028109457343816757, tv_loss: 0.03374270349740982\n",
      "iteration 3081, dc_loss: 0.02811477892100811, tv_loss: 0.033740028738975525\n",
      "iteration 3082, dc_loss: 0.028103485703468323, tv_loss: 0.03374287113547325\n",
      "iteration 3083, dc_loss: 0.028108755126595497, tv_loss: 0.03373841941356659\n",
      "iteration 3084, dc_loss: 0.02811373956501484, tv_loss: 0.03372536972165108\n",
      "iteration 3085, dc_loss: 0.028092827647924423, tv_loss: 0.03374524787068367\n",
      "iteration 3086, dc_loss: 0.02810606174170971, tv_loss: 0.03372509032487869\n",
      "iteration 3087, dc_loss: 0.028095604851841927, tv_loss: 0.03373738378286362\n",
      "iteration 3088, dc_loss: 0.028106776997447014, tv_loss: 0.033722128719091415\n",
      "iteration 3089, dc_loss: 0.02808247320353985, tv_loss: 0.03373606130480766\n",
      "iteration 3090, dc_loss: 0.028081217780709267, tv_loss: 0.033729176968336105\n",
      "iteration 3091, dc_loss: 0.028069090098142624, tv_loss: 0.033737633377313614\n",
      "iteration 3092, dc_loss: 0.028075940907001495, tv_loss: 0.03372795879840851\n",
      "iteration 3093, dc_loss: 0.028057822957634926, tv_loss: 0.0337376669049263\n",
      "iteration 3094, dc_loss: 0.02805599570274353, tv_loss: 0.033746156841516495\n",
      "iteration 3095, dc_loss: 0.028068266808986664, tv_loss: 0.033736374229192734\n",
      "iteration 3096, dc_loss: 0.028050683438777924, tv_loss: 0.03374197706580162\n",
      "iteration 3097, dc_loss: 0.02804596722126007, tv_loss: 0.03374435007572174\n",
      "iteration 3098, dc_loss: 0.028053632006049156, tv_loss: 0.033730264753103256\n",
      "iteration 3099, dc_loss: 0.0280466265976429, tv_loss: 0.03373512998223305\n",
      "iteration 3100, dc_loss: 0.028034338727593422, tv_loss: 0.03374384343624115\n",
      "iteration 3101, dc_loss: 0.028042875230312347, tv_loss: 0.03374410793185234\n",
      "iteration 3102, dc_loss: 0.028045538812875748, tv_loss: 0.033740751445293427\n",
      "iteration 3103, dc_loss: 0.028025615960359573, tv_loss: 0.0337568074464798\n",
      "iteration 3104, dc_loss: 0.028041774407029152, tv_loss: 0.03373068571090698\n",
      "iteration 3105, dc_loss: 0.028016669675707817, tv_loss: 0.033747296780347824\n",
      "iteration 3106, dc_loss: 0.02803930640220642, tv_loss: 0.0337277390062809\n",
      "iteration 3107, dc_loss: 0.028007952496409416, tv_loss: 0.033767879009246826\n",
      "iteration 3108, dc_loss: 0.02803840860724449, tv_loss: 0.03373431786894798\n",
      "iteration 3109, dc_loss: 0.02799382247030735, tv_loss: 0.033765677362680435\n",
      "iteration 3110, dc_loss: 0.028032442554831505, tv_loss: 0.0337151363492012\n",
      "iteration 3111, dc_loss: 0.028005631640553474, tv_loss: 0.033745963126420975\n",
      "iteration 3112, dc_loss: 0.028010504320263863, tv_loss: 0.03373774141073227\n",
      "iteration 3113, dc_loss: 0.02798258140683174, tv_loss: 0.03376443684101105\n",
      "iteration 3114, dc_loss: 0.02800765633583069, tv_loss: 0.03373111039400101\n",
      "iteration 3115, dc_loss: 0.027980130165815353, tv_loss: 0.0337488055229187\n",
      "iteration 3116, dc_loss: 0.027996785938739777, tv_loss: 0.03372833877801895\n",
      "iteration 3117, dc_loss: 0.027981845661997795, tv_loss: 0.033738646656274796\n",
      "iteration 3118, dc_loss: 0.027979418635368347, tv_loss: 0.03373332694172859\n",
      "iteration 3119, dc_loss: 0.02796812355518341, tv_loss: 0.03374439477920532\n",
      "iteration 3120, dc_loss: 0.02797437272965908, tv_loss: 0.03373710438609123\n",
      "iteration 3121, dc_loss: 0.027972126379609108, tv_loss: 0.03374658152461052\n",
      "iteration 3122, dc_loss: 0.02795991860330105, tv_loss: 0.033759064972400665\n",
      "iteration 3123, dc_loss: 0.02796916477382183, tv_loss: 0.033744923770427704\n",
      "iteration 3124, dc_loss: 0.02796032279729843, tv_loss: 0.033737875521183014\n",
      "iteration 3125, dc_loss: 0.02796199917793274, tv_loss: 0.03373285382986069\n",
      "iteration 3126, dc_loss: 0.027945784851908684, tv_loss: 0.03374746814370155\n",
      "iteration 3127, dc_loss: 0.027966516092419624, tv_loss: 0.033725082874298096\n",
      "iteration 3128, dc_loss: 0.027943221852183342, tv_loss: 0.03374733775854111\n",
      "iteration 3129, dc_loss: 0.02796310931444168, tv_loss: 0.03373577073216438\n",
      "iteration 3130, dc_loss: 0.027954012155532837, tv_loss: 0.033752746880054474\n",
      "iteration 3131, dc_loss: 0.02797010727226734, tv_loss: 0.033747926354408264\n",
      "iteration 3132, dc_loss: 0.02795455977320671, tv_loss: 0.03375411033630371\n",
      "iteration 3133, dc_loss: 0.027971800416707993, tv_loss: 0.03373229131102562\n",
      "iteration 3134, dc_loss: 0.027945691719651222, tv_loss: 0.03374693915247917\n",
      "iteration 3135, dc_loss: 0.02794540300965309, tv_loss: 0.03373834863305092\n",
      "iteration 3136, dc_loss: 0.02792593091726303, tv_loss: 0.03374521806836128\n",
      "iteration 3137, dc_loss: 0.02792477235198021, tv_loss: 0.03373892605304718\n",
      "iteration 3138, dc_loss: 0.02790733613073826, tv_loss: 0.0337483286857605\n",
      "iteration 3139, dc_loss: 0.027912316843867302, tv_loss: 0.0337376743555069\n",
      "iteration 3140, dc_loss: 0.02790135331451893, tv_loss: 0.03374236449599266\n",
      "iteration 3141, dc_loss: 0.027907954528927803, tv_loss: 0.03374564275145531\n",
      "iteration 3142, dc_loss: 0.027898399159312248, tv_loss: 0.033750541508197784\n",
      "iteration 3143, dc_loss: 0.027899272739887238, tv_loss: 0.033744264394044876\n",
      "iteration 3144, dc_loss: 0.027906382456421852, tv_loss: 0.03373606503009796\n",
      "iteration 3145, dc_loss: 0.027903560549020767, tv_loss: 0.033733218908309937\n",
      "iteration 3146, dc_loss: 0.02788429521024227, tv_loss: 0.03374534472823143\n",
      "iteration 3147, dc_loss: 0.0278773196041584, tv_loss: 0.033743616193532944\n",
      "iteration 3148, dc_loss: 0.027882328256964684, tv_loss: 0.03373481333255768\n",
      "iteration 3149, dc_loss: 0.027877293527126312, tv_loss: 0.03374712914228439\n",
      "iteration 3150, dc_loss: 0.027873288840055466, tv_loss: 0.03376041725277901\n",
      "iteration 3151, dc_loss: 0.02786839008331299, tv_loss: 0.03376204892992973\n",
      "iteration 3152, dc_loss: 0.02786898799240589, tv_loss: 0.03374553471803665\n",
      "iteration 3153, dc_loss: 0.02787291258573532, tv_loss: 0.03374086692929268\n",
      "iteration 3154, dc_loss: 0.02786346711218357, tv_loss: 0.033750105649232864\n",
      "iteration 3155, dc_loss: 0.027866121381521225, tv_loss: 0.03374389186501503\n",
      "iteration 3156, dc_loss: 0.02786034718155861, tv_loss: 0.03374391794204712\n",
      "iteration 3157, dc_loss: 0.027868056669831276, tv_loss: 0.033735137432813644\n",
      "iteration 3158, dc_loss: 0.027842622250318527, tv_loss: 0.03375120460987091\n",
      "iteration 3159, dc_loss: 0.027841689065098763, tv_loss: 0.033746350556612015\n",
      "iteration 3160, dc_loss: 0.02783893048763275, tv_loss: 0.033743612468242645\n",
      "iteration 3161, dc_loss: 0.027844391763210297, tv_loss: 0.033738330006599426\n",
      "iteration 3162, dc_loss: 0.027826571837067604, tv_loss: 0.033757325261831284\n",
      "iteration 3163, dc_loss: 0.027845239266753197, tv_loss: 0.033741310238838196\n",
      "iteration 3164, dc_loss: 0.027819374576210976, tv_loss: 0.03376312181353569\n",
      "iteration 3165, dc_loss: 0.02783820405602455, tv_loss: 0.03373691812157631\n",
      "iteration 3166, dc_loss: 0.02781418338418007, tv_loss: 0.033756665885448456\n",
      "iteration 3167, dc_loss: 0.027840962633490562, tv_loss: 0.03372785449028015\n",
      "iteration 3168, dc_loss: 0.027801604941487312, tv_loss: 0.03377411514520645\n",
      "iteration 3169, dc_loss: 0.02784331515431404, tv_loss: 0.03372832015156746\n",
      "iteration 3170, dc_loss: 0.027795592322945595, tv_loss: 0.0337747223675251\n",
      "iteration 3171, dc_loss: 0.027829676866531372, tv_loss: 0.03372933715581894\n",
      "iteration 3172, dc_loss: 0.0277923084795475, tv_loss: 0.033770907670259476\n",
      "iteration 3173, dc_loss: 0.027820654213428497, tv_loss: 0.03372865170240402\n",
      "iteration 3174, dc_loss: 0.0277754794806242, tv_loss: 0.03378009423613548\n",
      "iteration 3175, dc_loss: 0.027823232114315033, tv_loss: 0.033723361790180206\n",
      "iteration 3176, dc_loss: 0.027775360271334648, tv_loss: 0.03376203030347824\n",
      "iteration 3177, dc_loss: 0.027811512351036072, tv_loss: 0.03372760862112045\n",
      "iteration 3178, dc_loss: 0.027766456827521324, tv_loss: 0.03377249464392662\n",
      "iteration 3179, dc_loss: 0.027813658118247986, tv_loss: 0.03372596949338913\n",
      "iteration 3180, dc_loss: 0.02775384671986103, tv_loss: 0.03378157317638397\n",
      "iteration 3181, dc_loss: 0.02780037187039852, tv_loss: 0.03373071551322937\n",
      "iteration 3182, dc_loss: 0.027768902480602264, tv_loss: 0.033753179013729095\n",
      "iteration 3183, dc_loss: 0.027788963168859482, tv_loss: 0.033729325979948044\n",
      "iteration 3184, dc_loss: 0.027748707681894302, tv_loss: 0.03377034515142441\n",
      "iteration 3185, dc_loss: 0.027782471850514412, tv_loss: 0.03373108059167862\n",
      "iteration 3186, dc_loss: 0.027753755450248718, tv_loss: 0.03376447781920433\n",
      "iteration 3187, dc_loss: 0.027756555005908012, tv_loss: 0.03374384716153145\n",
      "iteration 3188, dc_loss: 0.027743693441152573, tv_loss: 0.03375735506415367\n",
      "iteration 3189, dc_loss: 0.027758195996284485, tv_loss: 0.03373235836625099\n",
      "iteration 3190, dc_loss: 0.02773856744170189, tv_loss: 0.03375278040766716\n",
      "iteration 3191, dc_loss: 0.027738770470023155, tv_loss: 0.033744316548109055\n",
      "iteration 3192, dc_loss: 0.027744170278310776, tv_loss: 0.03374423086643219\n",
      "iteration 3193, dc_loss: 0.027733217924833298, tv_loss: 0.03374691680073738\n",
      "iteration 3194, dc_loss: 0.027732286602258682, tv_loss: 0.03375890105962753\n",
      "iteration 3195, dc_loss: 0.027727345004677773, tv_loss: 0.03375905007123947\n",
      "iteration 3196, dc_loss: 0.02774101309478283, tv_loss: 0.03374695032835007\n",
      "iteration 3197, dc_loss: 0.027725884690880775, tv_loss: 0.033754438161849976\n",
      "iteration 3198, dc_loss: 0.027750717476010323, tv_loss: 0.033732857555150986\n",
      "iteration 3199, dc_loss: 0.027727434411644936, tv_loss: 0.03375902399420738\n",
      "iteration 3200, dc_loss: 0.027756547555327415, tv_loss: 0.03373827040195465\n",
      "iteration 3201, dc_loss: 0.027729863300919533, tv_loss: 0.03376759588718414\n",
      "iteration 3202, dc_loss: 0.027715589851140976, tv_loss: 0.033745940774679184\n",
      "iteration 3203, dc_loss: 0.02770036831498146, tv_loss: 0.03374462574720383\n",
      "iteration 3204, dc_loss: 0.027711551636457443, tv_loss: 0.033756665885448456\n",
      "iteration 3205, dc_loss: 0.027721352875232697, tv_loss: 0.03374366834759712\n",
      "iteration 3206, dc_loss: 0.0276961512863636, tv_loss: 0.03374008461833\n",
      "iteration 3207, dc_loss: 0.027703523635864258, tv_loss: 0.033745065331459045\n",
      "iteration 3208, dc_loss: 0.027716519311070442, tv_loss: 0.033745866268873215\n",
      "iteration 3209, dc_loss: 0.027683939784765244, tv_loss: 0.03375246748328209\n",
      "iteration 3210, dc_loss: 0.027696749195456505, tv_loss: 0.033746328204870224\n",
      "iteration 3211, dc_loss: 0.027707725763320923, tv_loss: 0.03374181315302849\n",
      "iteration 3212, dc_loss: 0.02768114022910595, tv_loss: 0.03374113887548447\n",
      "iteration 3213, dc_loss: 0.027703646570444107, tv_loss: 0.03373761475086212\n",
      "iteration 3214, dc_loss: 0.02770046889781952, tv_loss: 0.033753931522369385\n",
      "iteration 3215, dc_loss: 0.027668168768286705, tv_loss: 0.03374665603041649\n",
      "iteration 3216, dc_loss: 0.02770336903631687, tv_loss: 0.03372709080576897\n",
      "iteration 3217, dc_loss: 0.027701109647750854, tv_loss: 0.03374442458152771\n",
      "iteration 3218, dc_loss: 0.027666544541716576, tv_loss: 0.03374573960900307\n",
      "iteration 3219, dc_loss: 0.027713783085346222, tv_loss: 0.033733390271663666\n",
      "iteration 3220, dc_loss: 0.02770056575536728, tv_loss: 0.03376296907663345\n",
      "iteration 3221, dc_loss: 0.02766156941652298, tv_loss: 0.0337509922683239\n",
      "iteration 3222, dc_loss: 0.02775566279888153, tv_loss: 0.03371458128094673\n",
      "iteration 3223, dc_loss: 0.02774226665496826, tv_loss: 0.033771999180316925\n",
      "iteration 3224, dc_loss: 0.02773945964872837, tv_loss: 0.03376631811261177\n",
      "iteration 3225, dc_loss: 0.027737382799386978, tv_loss: 0.03372913971543312\n",
      "iteration 3226, dc_loss: 0.02775588259100914, tv_loss: 0.03374065086245537\n",
      "iteration 3227, dc_loss: 0.027658503502607346, tv_loss: 0.03377566859126091\n",
      "iteration 3228, dc_loss: 0.027746612206101418, tv_loss: 0.03373732790350914\n",
      "iteration 3229, dc_loss: 0.02768031693994999, tv_loss: 0.03374246880412102\n",
      "iteration 3230, dc_loss: 0.02771696075797081, tv_loss: 0.0337483175098896\n",
      "iteration 3231, dc_loss: 0.027712829411029816, tv_loss: 0.033753156661987305\n",
      "iteration 3232, dc_loss: 0.02776123769581318, tv_loss: 0.03373910114169121\n",
      "iteration 3233, dc_loss: 0.027718650177121162, tv_loss: 0.033747654408216476\n",
      "iteration 3234, dc_loss: 0.02766888029873371, tv_loss: 0.03375484421849251\n",
      "iteration 3235, dc_loss: 0.027759380638599396, tv_loss: 0.03374585136771202\n",
      "iteration 3236, dc_loss: 0.0276957880705595, tv_loss: 0.033755846321582794\n",
      "iteration 3237, dc_loss: 0.027751892805099487, tv_loss: 0.033740878105163574\n",
      "iteration 3238, dc_loss: 0.027742009609937668, tv_loss: 0.033745601773262024\n",
      "iteration 3239, dc_loss: 0.02771151065826416, tv_loss: 0.03375666216015816\n",
      "iteration 3240, dc_loss: 0.027659345418214798, tv_loss: 0.0337485745549202\n",
      "iteration 3241, dc_loss: 0.0277034230530262, tv_loss: 0.03373212367296219\n",
      "iteration 3242, dc_loss: 0.027634190395474434, tv_loss: 0.03374766185879707\n",
      "iteration 3243, dc_loss: 0.027642758563160896, tv_loss: 0.03375127166509628\n",
      "iteration 3244, dc_loss: 0.02764732390642166, tv_loss: 0.03374137356877327\n",
      "iteration 3245, dc_loss: 0.02764423005282879, tv_loss: 0.033723946660757065\n",
      "iteration 3246, dc_loss: 0.027637014165520668, tv_loss: 0.03373037651181221\n",
      "iteration 3247, dc_loss: 0.027607915922999382, tv_loss: 0.03375578299164772\n",
      "iteration 3248, dc_loss: 0.02760048396885395, tv_loss: 0.033749792724847794\n",
      "iteration 3249, dc_loss: 0.027625080198049545, tv_loss: 0.033727142959833145\n",
      "iteration 3250, dc_loss: 0.027608569711446762, tv_loss: 0.0337446928024292\n",
      "iteration 3251, dc_loss: 0.027594903483986855, tv_loss: 0.03375654295086861\n",
      "iteration 3252, dc_loss: 0.027598462998867035, tv_loss: 0.03374631702899933\n",
      "iteration 3253, dc_loss: 0.027586566284298897, tv_loss: 0.03374314308166504\n",
      "iteration 3254, dc_loss: 0.02759469673037529, tv_loss: 0.03374342992901802\n",
      "iteration 3255, dc_loss: 0.02758176438510418, tv_loss: 0.03374943137168884\n",
      "iteration 3256, dc_loss: 0.02758036181330681, tv_loss: 0.033749695867300034\n",
      "iteration 3257, dc_loss: 0.027580291032791138, tv_loss: 0.033740751445293427\n",
      "iteration 3258, dc_loss: 0.02756846882402897, tv_loss: 0.033746387809515\n",
      "iteration 3259, dc_loss: 0.027562757954001427, tv_loss: 0.03375019505620003\n",
      "iteration 3260, dc_loss: 0.027575530111789703, tv_loss: 0.0337413027882576\n",
      "iteration 3261, dc_loss: 0.027562763541936874, tv_loss: 0.03374900668859482\n",
      "iteration 3262, dc_loss: 0.027557387948036194, tv_loss: 0.033749137073755264\n",
      "iteration 3263, dc_loss: 0.027551760897040367, tv_loss: 0.03374563157558441\n",
      "iteration 3264, dc_loss: 0.027552984654903412, tv_loss: 0.03374232351779938\n",
      "iteration 3265, dc_loss: 0.027556994929909706, tv_loss: 0.03373938053846359\n",
      "iteration 3266, dc_loss: 0.02754523977637291, tv_loss: 0.03375447541475296\n",
      "iteration 3267, dc_loss: 0.027536673471331596, tv_loss: 0.03375336900353432\n",
      "iteration 3268, dc_loss: 0.02754938416182995, tv_loss: 0.0337376594543457\n",
      "iteration 3269, dc_loss: 0.027545548975467682, tv_loss: 0.033739831298589706\n",
      "iteration 3270, dc_loss: 0.02752642333507538, tv_loss: 0.03375348821282387\n",
      "iteration 3271, dc_loss: 0.027525871992111206, tv_loss: 0.033756401389837265\n",
      "iteration 3272, dc_loss: 0.027533764019608498, tv_loss: 0.033746443688869476\n",
      "iteration 3273, dc_loss: 0.027529671788215637, tv_loss: 0.033742863684892654\n",
      "iteration 3274, dc_loss: 0.02751988172531128, tv_loss: 0.03374442830681801\n",
      "iteration 3275, dc_loss: 0.027519894763827324, tv_loss: 0.033747296780347824\n",
      "iteration 3276, dc_loss: 0.02751929499208927, tv_loss: 0.033750373870134354\n",
      "iteration 3277, dc_loss: 0.02752244845032692, tv_loss: 0.03374749422073364\n",
      "iteration 3278, dc_loss: 0.027508415281772614, tv_loss: 0.03375309333205223\n",
      "iteration 3279, dc_loss: 0.02750815451145172, tv_loss: 0.033745381981134415\n",
      "iteration 3280, dc_loss: 0.027508389204740524, tv_loss: 0.0337446928024292\n",
      "iteration 3281, dc_loss: 0.027503343299031258, tv_loss: 0.03374727442860603\n",
      "iteration 3282, dc_loss: 0.027502793818712234, tv_loss: 0.033747848123311996\n",
      "iteration 3283, dc_loss: 0.0275011844933033, tv_loss: 0.03374886140227318\n",
      "iteration 3284, dc_loss: 0.027497045695781708, tv_loss: 0.03374586999416351\n",
      "iteration 3285, dc_loss: 0.027495140209794044, tv_loss: 0.033746495842933655\n",
      "iteration 3286, dc_loss: 0.02748863585293293, tv_loss: 0.03374648839235306\n",
      "iteration 3287, dc_loss: 0.027489731088280678, tv_loss: 0.033740222454071045\n",
      "iteration 3288, dc_loss: 0.027491210028529167, tv_loss: 0.03373890742659569\n",
      "iteration 3289, dc_loss: 0.027483517304062843, tv_loss: 0.03374389186501503\n",
      "iteration 3290, dc_loss: 0.027478190138936043, tv_loss: 0.03374546021223068\n",
      "iteration 3291, dc_loss: 0.027479909360408783, tv_loss: 0.03374413028359413\n",
      "iteration 3292, dc_loss: 0.027476325631141663, tv_loss: 0.033743880689144135\n",
      "iteration 3293, dc_loss: 0.027473853901028633, tv_loss: 0.03374262526631355\n",
      "iteration 3294, dc_loss: 0.02747466415166855, tv_loss: 0.033742573112249374\n",
      "iteration 3295, dc_loss: 0.027469415217638016, tv_loss: 0.03373836725950241\n",
      "iteration 3296, dc_loss: 0.02746417000889778, tv_loss: 0.033746588975191116\n",
      "iteration 3297, dc_loss: 0.027465203776955605, tv_loss: 0.033740948885679245\n",
      "iteration 3298, dc_loss: 0.027460196986794472, tv_loss: 0.033741746097803116\n",
      "iteration 3299, dc_loss: 0.02745973691344261, tv_loss: 0.033744435757398605\n",
      "iteration 3300, dc_loss: 0.027454063296318054, tv_loss: 0.033747654408216476\n",
      "iteration 3301, dc_loss: 0.027452535927295685, tv_loss: 0.03375115245580673\n",
      "iteration 3302, dc_loss: 0.02744879573583603, tv_loss: 0.03375248610973358\n",
      "iteration 3303, dc_loss: 0.027453487738966942, tv_loss: 0.033744461834430695\n",
      "iteration 3304, dc_loss: 0.02745174989104271, tv_loss: 0.03373982012271881\n",
      "iteration 3305, dc_loss: 0.027439825236797333, tv_loss: 0.033750057220458984\n",
      "iteration 3306, dc_loss: 0.027437610551714897, tv_loss: 0.03374754637479782\n",
      "iteration 3307, dc_loss: 0.02744314819574356, tv_loss: 0.03374630957841873\n",
      "iteration 3308, dc_loss: 0.027431821450591087, tv_loss: 0.03374573588371277\n",
      "iteration 3309, dc_loss: 0.027432601898908615, tv_loss: 0.033750880509614944\n",
      "iteration 3310, dc_loss: 0.027436478063464165, tv_loss: 0.03374393656849861\n",
      "iteration 3311, dc_loss: 0.027430010959506035, tv_loss: 0.033744361251592636\n",
      "iteration 3312, dc_loss: 0.027425942942500114, tv_loss: 0.033746931701898575\n",
      "iteration 3313, dc_loss: 0.027422910556197166, tv_loss: 0.03374746814370155\n",
      "iteration 3314, dc_loss: 0.027417782694101334, tv_loss: 0.033748019486665726\n",
      "iteration 3315, dc_loss: 0.027422908693552017, tv_loss: 0.0337451696395874\n",
      "iteration 3316, dc_loss: 0.027418211102485657, tv_loss: 0.03374936804175377\n",
      "iteration 3317, dc_loss: 0.027413589879870415, tv_loss: 0.03375503048300743\n",
      "iteration 3318, dc_loss: 0.02741159312427044, tv_loss: 0.03375062346458435\n",
      "iteration 3319, dc_loss: 0.027406517416238785, tv_loss: 0.033753830939531326\n",
      "iteration 3320, dc_loss: 0.02740861475467682, tv_loss: 0.03374701738357544\n",
      "iteration 3321, dc_loss: 0.027401059865951538, tv_loss: 0.033749110996723175\n",
      "iteration 3322, dc_loss: 0.02740131877362728, tv_loss: 0.033748559653759\n",
      "iteration 3323, dc_loss: 0.02740326337516308, tv_loss: 0.033749498426914215\n",
      "iteration 3324, dc_loss: 0.02739385887980461, tv_loss: 0.033754654228687286\n",
      "iteration 3325, dc_loss: 0.027391860261559486, tv_loss: 0.03375375643372536\n",
      "iteration 3326, dc_loss: 0.02740170992910862, tv_loss: 0.03373897448182106\n",
      "iteration 3327, dc_loss: 0.0273862574249506, tv_loss: 0.03375321999192238\n",
      "iteration 3328, dc_loss: 0.027379343286156654, tv_loss: 0.033759936690330505\n",
      "iteration 3329, dc_loss: 0.027388691902160645, tv_loss: 0.033743783831596375\n",
      "iteration 3330, dc_loss: 0.027387801557779312, tv_loss: 0.033751003444194794\n",
      "iteration 3331, dc_loss: 0.027381762862205505, tv_loss: 0.03375587984919548\n",
      "iteration 3332, dc_loss: 0.02736830897629261, tv_loss: 0.0337616428732872\n",
      "iteration 3333, dc_loss: 0.027376070618629456, tv_loss: 0.033747632056474686\n",
      "iteration 3334, dc_loss: 0.027379197999835014, tv_loss: 0.033742379397153854\n",
      "iteration 3335, dc_loss: 0.027369273826479912, tv_loss: 0.03375060111284256\n",
      "iteration 3336, dc_loss: 0.027364859357476234, tv_loss: 0.033751316368579865\n",
      "iteration 3337, dc_loss: 0.02736247517168522, tv_loss: 0.033750079572200775\n",
      "iteration 3338, dc_loss: 0.02736450918018818, tv_loss: 0.03374379128217697\n",
      "iteration 3339, dc_loss: 0.02736278437077999, tv_loss: 0.033746976405382156\n",
      "iteration 3340, dc_loss: 0.027354665100574493, tv_loss: 0.033754922449588776\n",
      "iteration 3341, dc_loss: 0.02735571190714836, tv_loss: 0.03374822065234184\n",
      "iteration 3342, dc_loss: 0.02735738456249237, tv_loss: 0.0337463840842247\n",
      "iteration 3343, dc_loss: 0.027344675734639168, tv_loss: 0.03375725448131561\n",
      "iteration 3344, dc_loss: 0.027347881346940994, tv_loss: 0.03376074135303497\n",
      "iteration 3345, dc_loss: 0.027350936084985733, tv_loss: 0.033745814114809036\n",
      "iteration 3346, dc_loss: 0.027341851964592934, tv_loss: 0.03375202417373657\n",
      "iteration 3347, dc_loss: 0.02734079584479332, tv_loss: 0.03375132381916046\n",
      "iteration 3348, dc_loss: 0.027337227016687393, tv_loss: 0.03374810516834259\n",
      "iteration 3349, dc_loss: 0.02733517251908779, tv_loss: 0.03375079482793808\n",
      "iteration 3350, dc_loss: 0.02733408287167549, tv_loss: 0.033747151494026184\n",
      "iteration 3351, dc_loss: 0.027331925928592682, tv_loss: 0.03374573960900307\n",
      "iteration 3352, dc_loss: 0.027331097051501274, tv_loss: 0.0337434820830822\n",
      "iteration 3353, dc_loss: 0.02732480876147747, tv_loss: 0.03375141695141792\n",
      "iteration 3354, dc_loss: 0.027321601286530495, tv_loss: 0.033754609525203705\n",
      "iteration 3355, dc_loss: 0.027322182431817055, tv_loss: 0.033757951110601425\n",
      "iteration 3356, dc_loss: 0.027316251769661903, tv_loss: 0.03375827893614769\n",
      "iteration 3357, dc_loss: 0.027324549853801727, tv_loss: 0.03374449163675308\n",
      "iteration 3358, dc_loss: 0.02731272205710411, tv_loss: 0.03374995291233063\n",
      "iteration 3359, dc_loss: 0.027303842827677727, tv_loss: 0.033763542771339417\n",
      "iteration 3360, dc_loss: 0.02731470763683319, tv_loss: 0.03374851867556572\n",
      "iteration 3361, dc_loss: 0.02730974368751049, tv_loss: 0.03375433385372162\n",
      "iteration 3362, dc_loss: 0.027297984808683395, tv_loss: 0.033762410283088684\n",
      "iteration 3363, dc_loss: 0.027301767840981483, tv_loss: 0.033748164772987366\n",
      "iteration 3364, dc_loss: 0.027304289862513542, tv_loss: 0.03375348821282387\n",
      "iteration 3365, dc_loss: 0.02729269117116928, tv_loss: 0.03375938907265663\n",
      "iteration 3366, dc_loss: 0.027292096987366676, tv_loss: 0.03375962749123573\n",
      "iteration 3367, dc_loss: 0.027301177382469177, tv_loss: 0.03374544531106949\n",
      "iteration 3368, dc_loss: 0.02729177102446556, tv_loss: 0.03374727442860603\n",
      "iteration 3369, dc_loss: 0.02727770432829857, tv_loss: 0.03376394882798195\n",
      "iteration 3370, dc_loss: 0.02728380635380745, tv_loss: 0.03376297280192375\n",
      "iteration 3371, dc_loss: 0.027287041768431664, tv_loss: 0.03375645726919174\n",
      "iteration 3372, dc_loss: 0.027281736955046654, tv_loss: 0.03375408053398132\n",
      "iteration 3373, dc_loss: 0.0272754468023777, tv_loss: 0.03375519812107086\n",
      "iteration 3374, dc_loss: 0.027273600921034813, tv_loss: 0.033761344850063324\n",
      "iteration 3375, dc_loss: 0.0272735096514225, tv_loss: 0.03376258537173271\n",
      "iteration 3376, dc_loss: 0.027269553393125534, tv_loss: 0.033759139478206635\n",
      "iteration 3377, dc_loss: 0.027266327291727066, tv_loss: 0.03375471383333206\n",
      "iteration 3378, dc_loss: 0.027267973870038986, tv_loss: 0.0337480865418911\n",
      "iteration 3379, dc_loss: 0.02726750075817108, tv_loss: 0.03375460207462311\n",
      "iteration 3380, dc_loss: 0.02725915051996708, tv_loss: 0.03376433253288269\n",
      "iteration 3381, dc_loss: 0.027253415435552597, tv_loss: 0.03375961259007454\n",
      "iteration 3382, dc_loss: 0.027259573340415955, tv_loss: 0.03375033661723137\n",
      "iteration 3383, dc_loss: 0.027255715802311897, tv_loss: 0.03374933823943138\n",
      "iteration 3384, dc_loss: 0.02725064754486084, tv_loss: 0.033761121332645416\n",
      "iteration 3385, dc_loss: 0.027247726917266846, tv_loss: 0.03376220539212227\n",
      "iteration 3386, dc_loss: 0.027243150398135185, tv_loss: 0.0337589830160141\n",
      "iteration 3387, dc_loss: 0.027244295924901962, tv_loss: 0.03375707566738129\n",
      "iteration 3388, dc_loss: 0.027246011421084404, tv_loss: 0.033751752227544785\n",
      "iteration 3389, dc_loss: 0.0272419024258852, tv_loss: 0.03374900668859482\n",
      "iteration 3390, dc_loss: 0.027233073487877846, tv_loss: 0.033754076808691025\n",
      "iteration 3391, dc_loss: 0.027230678126215935, tv_loss: 0.03376179561018944\n",
      "iteration 3392, dc_loss: 0.027240542694926262, tv_loss: 0.03374888747930527\n",
      "iteration 3393, dc_loss: 0.027226276695728302, tv_loss: 0.03376052901148796\n",
      "iteration 3394, dc_loss: 0.027226194739341736, tv_loss: 0.033762697130441666\n",
      "iteration 3395, dc_loss: 0.027227504178881645, tv_loss: 0.03375474363565445\n",
      "iteration 3396, dc_loss: 0.027223868295550346, tv_loss: 0.03375614434480667\n",
      "iteration 3397, dc_loss: 0.027219701558351517, tv_loss: 0.033754613250494\n",
      "iteration 3398, dc_loss: 0.027209272608160973, tv_loss: 0.03376472741365433\n",
      "iteration 3399, dc_loss: 0.02721725031733513, tv_loss: 0.033758293837308884\n",
      "iteration 3400, dc_loss: 0.027220821008086205, tv_loss: 0.03375249356031418\n",
      "iteration 3401, dc_loss: 0.027207151055336, tv_loss: 0.03375795856118202\n",
      "iteration 3402, dc_loss: 0.02720818482339382, tv_loss: 0.03375592455267906\n",
      "iteration 3403, dc_loss: 0.027207273989915848, tv_loss: 0.033752698451280594\n",
      "iteration 3404, dc_loss: 0.02720394916832447, tv_loss: 0.03375893458724022\n",
      "iteration 3405, dc_loss: 0.027200324460864067, tv_loss: 0.033756859600543976\n",
      "iteration 3406, dc_loss: 0.027197159826755524, tv_loss: 0.03376021608710289\n",
      "iteration 3407, dc_loss: 0.027198171243071556, tv_loss: 0.03375278785824776\n",
      "iteration 3408, dc_loss: 0.02719329670071602, tv_loss: 0.033751633018255234\n",
      "iteration 3409, dc_loss: 0.027191773056983948, tv_loss: 0.03375369310379028\n",
      "iteration 3410, dc_loss: 0.027191614732146263, tv_loss: 0.033747922629117966\n",
      "iteration 3411, dc_loss: 0.027186455205082893, tv_loss: 0.03374874219298363\n",
      "iteration 3412, dc_loss: 0.0271846242249012, tv_loss: 0.03375418111681938\n",
      "iteration 3413, dc_loss: 0.027177054435014725, tv_loss: 0.03375713899731636\n",
      "iteration 3414, dc_loss: 0.027181601151823997, tv_loss: 0.033757928758859634\n",
      "iteration 3415, dc_loss: 0.0271832924336195, tv_loss: 0.03376316651701927\n",
      "iteration 3416, dc_loss: 0.027170535176992416, tv_loss: 0.03376903012394905\n",
      "iteration 3417, dc_loss: 0.027169879525899887, tv_loss: 0.03375968337059021\n",
      "iteration 3418, dc_loss: 0.027175119146704674, tv_loss: 0.03374693915247917\n",
      "iteration 3419, dc_loss: 0.02716800570487976, tv_loss: 0.03375929966568947\n",
      "iteration 3420, dc_loss: 0.0271612536162138, tv_loss: 0.0337672159075737\n",
      "iteration 3421, dc_loss: 0.02716093137860298, tv_loss: 0.0337614007294178\n",
      "iteration 3422, dc_loss: 0.02716713584959507, tv_loss: 0.033750101923942566\n",
      "iteration 3423, dc_loss: 0.027155255898833275, tv_loss: 0.03375934809446335\n",
      "iteration 3424, dc_loss: 0.027156304568052292, tv_loss: 0.033751726150512695\n",
      "iteration 3425, dc_loss: 0.027155423536896706, tv_loss: 0.03375120088458061\n",
      "iteration 3426, dc_loss: 0.027149120345711708, tv_loss: 0.03375746309757233\n",
      "iteration 3427, dc_loss: 0.027147600427269936, tv_loss: 0.033755335956811905\n",
      "iteration 3428, dc_loss: 0.027143707498908043, tv_loss: 0.033752549439668655\n",
      "iteration 3429, dc_loss: 0.027148567140102386, tv_loss: 0.033752262592315674\n",
      "iteration 3430, dc_loss: 0.027141615748405457, tv_loss: 0.03375924006104469\n",
      "iteration 3431, dc_loss: 0.027134384959936142, tv_loss: 0.03376258909702301\n",
      "iteration 3432, dc_loss: 0.027138471603393555, tv_loss: 0.03375331684947014\n",
      "iteration 3433, dc_loss: 0.027138857170939445, tv_loss: 0.0337541364133358\n",
      "iteration 3434, dc_loss: 0.02713102288544178, tv_loss: 0.03376420959830284\n",
      "iteration 3435, dc_loss: 0.027128951624035835, tv_loss: 0.033760614693164825\n",
      "iteration 3436, dc_loss: 0.027128688991069794, tv_loss: 0.03375465050339699\n",
      "iteration 3437, dc_loss: 0.027122965082526207, tv_loss: 0.03375578299164772\n",
      "iteration 3438, dc_loss: 0.02712181769311428, tv_loss: 0.03375644609332085\n",
      "iteration 3439, dc_loss: 0.02712605707347393, tv_loss: 0.033751461654901505\n",
      "iteration 3440, dc_loss: 0.027116574347019196, tv_loss: 0.03375767171382904\n",
      "iteration 3441, dc_loss: 0.02711130678653717, tv_loss: 0.033760711550712585\n",
      "iteration 3442, dc_loss: 0.027114592492580414, tv_loss: 0.03375433012843132\n",
      "iteration 3443, dc_loss: 0.02711600810289383, tv_loss: 0.033749256283044815\n",
      "iteration 3444, dc_loss: 0.027104906737804413, tv_loss: 0.03375881537795067\n",
      "iteration 3445, dc_loss: 0.027106797322630882, tv_loss: 0.033755894750356674\n",
      "iteration 3446, dc_loss: 0.02710541896522045, tv_loss: 0.033754464238882065\n",
      "iteration 3447, dc_loss: 0.027101876214146614, tv_loss: 0.033755380660295486\n",
      "iteration 3448, dc_loss: 0.027098821476101875, tv_loss: 0.03375740721821785\n",
      "iteration 3449, dc_loss: 0.02709263563156128, tv_loss: 0.03375816345214844\n",
      "iteration 3450, dc_loss: 0.027099017053842545, tv_loss: 0.03375696390867233\n",
      "iteration 3451, dc_loss: 0.027095986530184746, tv_loss: 0.03375691920518875\n",
      "iteration 3452, dc_loss: 0.027087576687335968, tv_loss: 0.033762600272893906\n",
      "iteration 3453, dc_loss: 0.027086185291409492, tv_loss: 0.03376401215791702\n",
      "iteration 3454, dc_loss: 0.027080291882157326, tv_loss: 0.03377175331115723\n",
      "iteration 3455, dc_loss: 0.02708912082016468, tv_loss: 0.033760618418455124\n",
      "iteration 3456, dc_loss: 0.027078736573457718, tv_loss: 0.03376510739326477\n",
      "iteration 3457, dc_loss: 0.02708062343299389, tv_loss: 0.03375622630119324\n",
      "iteration 3458, dc_loss: 0.02707553468644619, tv_loss: 0.03375713899731636\n",
      "iteration 3459, dc_loss: 0.027068883180618286, tv_loss: 0.03377119079232216\n",
      "iteration 3460, dc_loss: 0.02707434445619583, tv_loss: 0.033763039857149124\n",
      "iteration 3461, dc_loss: 0.027065003290772438, tv_loss: 0.03377183899283409\n",
      "iteration 3462, dc_loss: 0.027073176577687263, tv_loss: 0.0337563119828701\n",
      "iteration 3463, dc_loss: 0.027063166722655296, tv_loss: 0.03375884145498276\n",
      "iteration 3464, dc_loss: 0.027057791128754616, tv_loss: 0.03376128897070885\n",
      "iteration 3465, dc_loss: 0.02706250362098217, tv_loss: 0.03375504910945892\n",
      "iteration 3466, dc_loss: 0.027057338505983353, tv_loss: 0.03376454859972\n",
      "iteration 3467, dc_loss: 0.027058878913521767, tv_loss: 0.033767666667699814\n",
      "iteration 3468, dc_loss: 0.027046529576182365, tv_loss: 0.0337749607861042\n",
      "iteration 3469, dc_loss: 0.027053995057940483, tv_loss: 0.03375884145498276\n",
      "iteration 3470, dc_loss: 0.02705015055835247, tv_loss: 0.033756427466869354\n",
      "iteration 3471, dc_loss: 0.027038034051656723, tv_loss: 0.03376759588718414\n",
      "iteration 3472, dc_loss: 0.02704090252518654, tv_loss: 0.033763252198696136\n",
      "iteration 3473, dc_loss: 0.027043601498007774, tv_loss: 0.03375968709588051\n",
      "iteration 3474, dc_loss: 0.027044720947742462, tv_loss: 0.033754970878362656\n",
      "iteration 3475, dc_loss: 0.027034997940063477, tv_loss: 0.033763591200113297\n",
      "iteration 3476, dc_loss: 0.02703385427594185, tv_loss: 0.03376690670847893\n",
      "iteration 3477, dc_loss: 0.027029499411582947, tv_loss: 0.03377049043774605\n",
      "iteration 3478, dc_loss: 0.02702738344669342, tv_loss: 0.03376801684498787\n",
      "iteration 3479, dc_loss: 0.027026502415537834, tv_loss: 0.033764101564884186\n",
      "iteration 3480, dc_loss: 0.027026159688830376, tv_loss: 0.03376239538192749\n",
      "iteration 3481, dc_loss: 0.027033606544137, tv_loss: 0.033749185502529144\n",
      "iteration 3482, dc_loss: 0.027017096057534218, tv_loss: 0.033765655010938644\n",
      "iteration 3483, dc_loss: 0.02701331116259098, tv_loss: 0.03376656025648117\n",
      "iteration 3484, dc_loss: 0.027016475796699524, tv_loss: 0.03375724330544472\n",
      "iteration 3485, dc_loss: 0.02701127529144287, tv_loss: 0.03376227989792824\n",
      "iteration 3486, dc_loss: 0.02700740098953247, tv_loss: 0.033763229846954346\n",
      "iteration 3487, dc_loss: 0.027010232210159302, tv_loss: 0.03375442326068878\n",
      "iteration 3488, dc_loss: 0.027011925354599953, tv_loss: 0.033760178834199905\n",
      "iteration 3489, dc_loss: 0.027006618678569794, tv_loss: 0.03376171737909317\n",
      "iteration 3490, dc_loss: 0.026995941996574402, tv_loss: 0.033774230629205704\n",
      "iteration 3491, dc_loss: 0.027000365778803825, tv_loss: 0.033769335597753525\n",
      "iteration 3492, dc_loss: 0.026997970417141914, tv_loss: 0.03376346081495285\n",
      "iteration 3493, dc_loss: 0.02699331007897854, tv_loss: 0.033763330429792404\n",
      "iteration 3494, dc_loss: 0.026989882811903954, tv_loss: 0.03376942500472069\n",
      "iteration 3495, dc_loss: 0.026996774598956108, tv_loss: 0.03376348689198494\n",
      "iteration 3496, dc_loss: 0.02699345350265503, tv_loss: 0.03376093879342079\n",
      "iteration 3497, dc_loss: 0.026980804279446602, tv_loss: 0.033768150955438614\n",
      "iteration 3498, dc_loss: 0.02698570489883423, tv_loss: 0.03375878557562828\n",
      "iteration 3499, dc_loss: 0.02697882056236267, tv_loss: 0.03376418724656105\n",
      "iteration 3500, dc_loss: 0.02697858214378357, tv_loss: 0.03376543149352074\n",
      "iteration 3501, dc_loss: 0.02697485312819481, tv_loss: 0.03376253694295883\n",
      "iteration 3502, dc_loss: 0.026978015899658203, tv_loss: 0.033758316189050674\n",
      "iteration 3503, dc_loss: 0.02697109803557396, tv_loss: 0.03376931697130203\n",
      "iteration 3504, dc_loss: 0.026965592056512833, tv_loss: 0.033772218972444534\n",
      "iteration 3505, dc_loss: 0.026969727128744125, tv_loss: 0.03376086801290512\n",
      "iteration 3506, dc_loss: 0.02696496993303299, tv_loss: 0.03376585990190506\n",
      "iteration 3507, dc_loss: 0.026964865624904633, tv_loss: 0.03376088663935661\n",
      "iteration 3508, dc_loss: 0.026959018781781197, tv_loss: 0.03376278653740883\n",
      "iteration 3509, dc_loss: 0.026960456743836403, tv_loss: 0.033769264817237854\n",
      "iteration 3510, dc_loss: 0.026954442262649536, tv_loss: 0.033777233213186264\n",
      "iteration 3511, dc_loss: 0.02695288136601448, tv_loss: 0.03377114608883858\n",
      "iteration 3512, dc_loss: 0.026954561471939087, tv_loss: 0.03375828266143799\n",
      "iteration 3513, dc_loss: 0.026950063183903694, tv_loss: 0.03376816213130951\n",
      "iteration 3514, dc_loss: 0.02694351226091385, tv_loss: 0.033771075308322906\n",
      "iteration 3515, dc_loss: 0.026938576251268387, tv_loss: 0.0337708555161953\n",
      "iteration 3516, dc_loss: 0.026946255937218666, tv_loss: 0.03376311436295509\n",
      "iteration 3517, dc_loss: 0.02694014273583889, tv_loss: 0.033765800297260284\n",
      "iteration 3518, dc_loss: 0.026936952024698257, tv_loss: 0.03376523032784462\n",
      "iteration 3519, dc_loss: 0.026938656345009804, tv_loss: 0.03375891223549843\n",
      "iteration 3520, dc_loss: 0.026933597400784492, tv_loss: 0.03376070410013199\n",
      "iteration 3521, dc_loss: 0.026928212493658066, tv_loss: 0.033762361854314804\n",
      "iteration 3522, dc_loss: 0.02692406065762043, tv_loss: 0.03376496583223343\n",
      "iteration 3523, dc_loss: 0.026929140090942383, tv_loss: 0.03375701233744621\n",
      "iteration 3524, dc_loss: 0.026927400380373, tv_loss: 0.03375871106982231\n",
      "iteration 3525, dc_loss: 0.026917386800050735, tv_loss: 0.03376650810241699\n",
      "iteration 3526, dc_loss: 0.02691708132624626, tv_loss: 0.03377183899283409\n",
      "iteration 3527, dc_loss: 0.026923006400465965, tv_loss: 0.03377462923526764\n",
      "iteration 3528, dc_loss: 0.026915349066257477, tv_loss: 0.03377526253461838\n",
      "iteration 3529, dc_loss: 0.026905469596385956, tv_loss: 0.03377090394496918\n",
      "iteration 3530, dc_loss: 0.026916369795799255, tv_loss: 0.0337694026529789\n",
      "iteration 3531, dc_loss: 0.02690432034432888, tv_loss: 0.033787861466407776\n",
      "iteration 3532, dc_loss: 0.02690243162214756, tv_loss: 0.0337730310857296\n",
      "iteration 3533, dc_loss: 0.026904018595814705, tv_loss: 0.03376927599310875\n",
      "iteration 3534, dc_loss: 0.026902902871370316, tv_loss: 0.03377571329474449\n",
      "iteration 3535, dc_loss: 0.02690458670258522, tv_loss: 0.033765919506549835\n",
      "iteration 3536, dc_loss: 0.02688860520720482, tv_loss: 0.033775851130485535\n",
      "iteration 3537, dc_loss: 0.026888258755207062, tv_loss: 0.03377602994441986\n",
      "iteration 3538, dc_loss: 0.026903562247753143, tv_loss: 0.03376718610525131\n",
      "iteration 3539, dc_loss: 0.026890261098742485, tv_loss: 0.03377116098999977\n",
      "iteration 3540, dc_loss: 0.026883261278271675, tv_loss: 0.03377461060881615\n",
      "iteration 3541, dc_loss: 0.02688109129667282, tv_loss: 0.033768463879823685\n",
      "iteration 3542, dc_loss: 0.026888445019721985, tv_loss: 0.0337689183652401\n",
      "iteration 3543, dc_loss: 0.02687905542552471, tv_loss: 0.033780358731746674\n",
      "iteration 3544, dc_loss: 0.02687862515449524, tv_loss: 0.03376879170536995\n",
      "iteration 3545, dc_loss: 0.026871539652347565, tv_loss: 0.03376983478665352\n",
      "iteration 3546, dc_loss: 0.026882000267505646, tv_loss: 0.03376642242074013\n",
      "iteration 3547, dc_loss: 0.02686982788145542, tv_loss: 0.03377976641058922\n",
      "iteration 3548, dc_loss: 0.026869844645261765, tv_loss: 0.03376908600330353\n",
      "iteration 3549, dc_loss: 0.026872385293245316, tv_loss: 0.033765826374292374\n",
      "iteration 3550, dc_loss: 0.026858309283852577, tv_loss: 0.03378064185380936\n",
      "iteration 3551, dc_loss: 0.02686256170272827, tv_loss: 0.03377458453178406\n",
      "iteration 3552, dc_loss: 0.02686203084886074, tv_loss: 0.03376837074756622\n",
      "iteration 3553, dc_loss: 0.026867877691984177, tv_loss: 0.033762574195861816\n",
      "iteration 3554, dc_loss: 0.02684868313372135, tv_loss: 0.033778611570596695\n",
      "iteration 3555, dc_loss: 0.02684575505554676, tv_loss: 0.03378022462129593\n",
      "iteration 3556, dc_loss: 0.0268680602312088, tv_loss: 0.033755991607904434\n",
      "iteration 3557, dc_loss: 0.026845911517739296, tv_loss: 0.03377401456236839\n",
      "iteration 3558, dc_loss: 0.026843473315238953, tv_loss: 0.033770736306905746\n",
      "iteration 3559, dc_loss: 0.026841051876544952, tv_loss: 0.03377017378807068\n",
      "iteration 3560, dc_loss: 0.026860849931836128, tv_loss: 0.03376117721199989\n",
      "iteration 3561, dc_loss: 0.026835152879357338, tv_loss: 0.03378172218799591\n",
      "iteration 3562, dc_loss: 0.026847494766116142, tv_loss: 0.033771954476833344\n",
      "iteration 3563, dc_loss: 0.026848873123526573, tv_loss: 0.03376426547765732\n",
      "iteration 3564, dc_loss: 0.0268473569303751, tv_loss: 0.033767737448215485\n",
      "iteration 3565, dc_loss: 0.026827214285731316, tv_loss: 0.03379112482070923\n",
      "iteration 3566, dc_loss: 0.026839686557650566, tv_loss: 0.03377864137291908\n",
      "iteration 3567, dc_loss: 0.02684778906404972, tv_loss: 0.03376120701432228\n",
      "iteration 3568, dc_loss: 0.026841823011636734, tv_loss: 0.03376713767647743\n",
      "iteration 3569, dc_loss: 0.02682678960263729, tv_loss: 0.03377935662865639\n",
      "iteration 3570, dc_loss: 0.026832064613699913, tv_loss: 0.03376081585884094\n",
      "iteration 3571, dc_loss: 0.026821402832865715, tv_loss: 0.033770713955163956\n",
      "iteration 3572, dc_loss: 0.02681655064225197, tv_loss: 0.0337674543261528\n",
      "iteration 3573, dc_loss: 0.026806868612766266, tv_loss: 0.03377709910273552\n",
      "iteration 3574, dc_loss: 0.02681676298379898, tv_loss: 0.03376338258385658\n",
      "iteration 3575, dc_loss: 0.02681647054851055, tv_loss: 0.033758584409952164\n",
      "iteration 3576, dc_loss: 0.026807526126503944, tv_loss: 0.033770378679037094\n",
      "iteration 3577, dc_loss: 0.026804737746715546, tv_loss: 0.03376956284046173\n",
      "iteration 3578, dc_loss: 0.02680346742272377, tv_loss: 0.033771123737096786\n",
      "iteration 3579, dc_loss: 0.026809649541974068, tv_loss: 0.03376463055610657\n",
      "iteration 3580, dc_loss: 0.026798153296113014, tv_loss: 0.033773377537727356\n",
      "iteration 3581, dc_loss: 0.026802221313118935, tv_loss: 0.03376462310552597\n",
      "iteration 3582, dc_loss: 0.026796480640769005, tv_loss: 0.033765945583581924\n",
      "iteration 3583, dc_loss: 0.02678903564810753, tv_loss: 0.03376973792910576\n",
      "iteration 3584, dc_loss: 0.026781437918543816, tv_loss: 0.03377603739500046\n",
      "iteration 3585, dc_loss: 0.026792239397764206, tv_loss: 0.03376009687781334\n",
      "iteration 3586, dc_loss: 0.026790428906679153, tv_loss: 0.033767543733119965\n",
      "iteration 3587, dc_loss: 0.026779673993587494, tv_loss: 0.033772196620702744\n",
      "iteration 3588, dc_loss: 0.026780489832162857, tv_loss: 0.03377797082066536\n",
      "iteration 3589, dc_loss: 0.02677340991795063, tv_loss: 0.03377563878893852\n",
      "iteration 3590, dc_loss: 0.026783030480146408, tv_loss: 0.03376651927828789\n",
      "iteration 3591, dc_loss: 0.026777470484375954, tv_loss: 0.03376718610525131\n",
      "iteration 3592, dc_loss: 0.026769235730171204, tv_loss: 0.03377049043774605\n",
      "iteration 3593, dc_loss: 0.026761163026094437, tv_loss: 0.033779505640268326\n",
      "iteration 3594, dc_loss: 0.026782196015119553, tv_loss: 0.033757228404283524\n",
      "iteration 3595, dc_loss: 0.02675727568566799, tv_loss: 0.03378299996256828\n",
      "iteration 3596, dc_loss: 0.02675551176071167, tv_loss: 0.03378162160515785\n",
      "iteration 3597, dc_loss: 0.026769433170557022, tv_loss: 0.033762965351343155\n",
      "iteration 3598, dc_loss: 0.026762237772345543, tv_loss: 0.033764783293008804\n",
      "iteration 3599, dc_loss: 0.026746856048703194, tv_loss: 0.03377661854028702\n",
      "iteration 3600, dc_loss: 0.026751689612865448, tv_loss: 0.03377315402030945\n",
      "iteration 3601, dc_loss: 0.02675982564687729, tv_loss: 0.033773940056562424\n",
      "iteration 3602, dc_loss: 0.026742329820990562, tv_loss: 0.03377518430352211\n",
      "iteration 3603, dc_loss: 0.026740195229649544, tv_loss: 0.03377458080649376\n",
      "iteration 3604, dc_loss: 0.026754962280392647, tv_loss: 0.033761944621801376\n",
      "iteration 3605, dc_loss: 0.02674981951713562, tv_loss: 0.0337652824819088\n",
      "iteration 3606, dc_loss: 0.02673518843948841, tv_loss: 0.03377093747258186\n",
      "iteration 3607, dc_loss: 0.02673693746328354, tv_loss: 0.03377184644341469\n",
      "iteration 3608, dc_loss: 0.02674066461622715, tv_loss: 0.03376167640089989\n",
      "iteration 3609, dc_loss: 0.026738692075014114, tv_loss: 0.03376815840601921\n",
      "iteration 3610, dc_loss: 0.02673206850886345, tv_loss: 0.03376491367816925\n",
      "iteration 3611, dc_loss: 0.026732614263892174, tv_loss: 0.03376574441790581\n",
      "iteration 3612, dc_loss: 0.02673209272325039, tv_loss: 0.03376656770706177\n",
      "iteration 3613, dc_loss: 0.0267298873513937, tv_loss: 0.033770862966775894\n",
      "iteration 3614, dc_loss: 0.02672659419476986, tv_loss: 0.03376251086592674\n",
      "iteration 3615, dc_loss: 0.02672749198973179, tv_loss: 0.03376045078039169\n",
      "iteration 3616, dc_loss: 0.02672567218542099, tv_loss: 0.033766694366931915\n",
      "iteration 3617, dc_loss: 0.026723386719822884, tv_loss: 0.033769119530916214\n",
      "iteration 3618, dc_loss: 0.026717236265540123, tv_loss: 0.03377184271812439\n",
      "iteration 3619, dc_loss: 0.026714671403169632, tv_loss: 0.03376414626836777\n",
      "iteration 3620, dc_loss: 0.02671717293560505, tv_loss: 0.03376581892371178\n",
      "iteration 3621, dc_loss: 0.026719579473137856, tv_loss: 0.03376379609107971\n",
      "iteration 3622, dc_loss: 0.026713106781244278, tv_loss: 0.03376714885234833\n",
      "iteration 3623, dc_loss: 0.02670644409954548, tv_loss: 0.0337706096470356\n",
      "iteration 3624, dc_loss: 0.026710649952292442, tv_loss: 0.033760443329811096\n",
      "iteration 3625, dc_loss: 0.026712242513895035, tv_loss: 0.03376556932926178\n",
      "iteration 3626, dc_loss: 0.026705576106905937, tv_loss: 0.03376760333776474\n",
      "iteration 3627, dc_loss: 0.02670341730117798, tv_loss: 0.03376644477248192\n",
      "iteration 3628, dc_loss: 0.026703348383307457, tv_loss: 0.033763933926820755\n",
      "iteration 3629, dc_loss: 0.026700278744101524, tv_loss: 0.03376093506813049\n",
      "iteration 3630, dc_loss: 0.026696419343352318, tv_loss: 0.033774640411138535\n",
      "iteration 3631, dc_loss: 0.026697205379605293, tv_loss: 0.03376585245132446\n",
      "iteration 3632, dc_loss: 0.026702171191573143, tv_loss: 0.033757973462343216\n",
      "iteration 3633, dc_loss: 0.02669227309525013, tv_loss: 0.033766746520996094\n",
      "iteration 3634, dc_loss: 0.026684390380978584, tv_loss: 0.03377468138933182\n",
      "iteration 3635, dc_loss: 0.02669263817369938, tv_loss: 0.03376079350709915\n",
      "iteration 3636, dc_loss: 0.026696089655160904, tv_loss: 0.033755362033843994\n",
      "iteration 3637, dc_loss: 0.02668476291000843, tv_loss: 0.03376271575689316\n",
      "iteration 3638, dc_loss: 0.026680681854486465, tv_loss: 0.033769648522138596\n",
      "iteration 3639, dc_loss: 0.02668275125324726, tv_loss: 0.03376556187868118\n",
      "iteration 3640, dc_loss: 0.026687854900956154, tv_loss: 0.03376467525959015\n",
      "iteration 3641, dc_loss: 0.0266824159771204, tv_loss: 0.03375984728336334\n",
      "iteration 3642, dc_loss: 0.026673773303627968, tv_loss: 0.033771153539419174\n",
      "iteration 3643, dc_loss: 0.026671836152672768, tv_loss: 0.03376969322562218\n",
      "iteration 3644, dc_loss: 0.02667979709804058, tv_loss: 0.03376515209674835\n",
      "iteration 3645, dc_loss: 0.026678774505853653, tv_loss: 0.033763907849788666\n",
      "iteration 3646, dc_loss: 0.02666369266808033, tv_loss: 0.0337713323533535\n",
      "iteration 3647, dc_loss: 0.02666558139026165, tv_loss: 0.03376736119389534\n",
      "iteration 3648, dc_loss: 0.026675811037421227, tv_loss: 0.03375692665576935\n",
      "iteration 3649, dc_loss: 0.02666626311838627, tv_loss: 0.033762287348508835\n",
      "iteration 3650, dc_loss: 0.026657307520508766, tv_loss: 0.033772993832826614\n",
      "iteration 3651, dc_loss: 0.026664596050977707, tv_loss: 0.03375851735472679\n",
      "iteration 3652, dc_loss: 0.026661857962608337, tv_loss: 0.03375917673110962\n",
      "iteration 3653, dc_loss: 0.02665778063237667, tv_loss: 0.033766020089387894\n",
      "iteration 3654, dc_loss: 0.026657508686184883, tv_loss: 0.033761125057935715\n",
      "iteration 3655, dc_loss: 0.02665563113987446, tv_loss: 0.03376815468072891\n",
      "iteration 3656, dc_loss: 0.02665363997220993, tv_loss: 0.03376206383109093\n",
      "iteration 3657, dc_loss: 0.026651041582226753, tv_loss: 0.033766720443964005\n",
      "iteration 3658, dc_loss: 0.026650413870811462, tv_loss: 0.03376423940062523\n",
      "iteration 3659, dc_loss: 0.02664889581501484, tv_loss: 0.03376419097185135\n",
      "iteration 3660, dc_loss: 0.02664310112595558, tv_loss: 0.03376908227801323\n",
      "iteration 3661, dc_loss: 0.026644188910722733, tv_loss: 0.03376280516386032\n",
      "iteration 3662, dc_loss: 0.02664734609425068, tv_loss: 0.033758874982595444\n",
      "iteration 3663, dc_loss: 0.02664303407073021, tv_loss: 0.03376081958413124\n",
      "iteration 3664, dc_loss: 0.026640407741069794, tv_loss: 0.03376034274697304\n",
      "iteration 3665, dc_loss: 0.02663777954876423, tv_loss: 0.03376546502113342\n",
      "iteration 3666, dc_loss: 0.026632023975253105, tv_loss: 0.0337696447968483\n",
      "iteration 3667, dc_loss: 0.026637017726898193, tv_loss: 0.033768363296985626\n",
      "iteration 3668, dc_loss: 0.02663560025393963, tv_loss: 0.03375772759318352\n",
      "iteration 3669, dc_loss: 0.026630667969584465, tv_loss: 0.033763568848371506\n",
      "iteration 3670, dc_loss: 0.026629969477653503, tv_loss: 0.033765826374292374\n",
      "iteration 3671, dc_loss: 0.026625007390975952, tv_loss: 0.03376694768667221\n",
      "iteration 3672, dc_loss: 0.026624280959367752, tv_loss: 0.03377411887049675\n",
      "iteration 3673, dc_loss: 0.02662617526948452, tv_loss: 0.0337597094476223\n",
      "iteration 3674, dc_loss: 0.026622353121638298, tv_loss: 0.033764779567718506\n",
      "iteration 3675, dc_loss: 0.026619266718626022, tv_loss: 0.03377307578921318\n",
      "iteration 3676, dc_loss: 0.026619942858815193, tv_loss: 0.03376748785376549\n",
      "iteration 3677, dc_loss: 0.026620885357260704, tv_loss: 0.03376739099621773\n",
      "iteration 3678, dc_loss: 0.02661256492137909, tv_loss: 0.033765774220228195\n",
      "iteration 3679, dc_loss: 0.02661062404513359, tv_loss: 0.03376976400613785\n",
      "iteration 3680, dc_loss: 0.026616346091032028, tv_loss: 0.03376742824912071\n",
      "iteration 3681, dc_loss: 0.026608912274241447, tv_loss: 0.033776503056287766\n",
      "iteration 3682, dc_loss: 0.02660568803548813, tv_loss: 0.033768005669116974\n",
      "iteration 3683, dc_loss: 0.026608018204569817, tv_loss: 0.03376556932926178\n",
      "iteration 3684, dc_loss: 0.026604538783431053, tv_loss: 0.0337691493332386\n",
      "iteration 3685, dc_loss: 0.026603683829307556, tv_loss: 0.033776018768548965\n",
      "iteration 3686, dc_loss: 0.0265997052192688, tv_loss: 0.03377528116106987\n",
      "iteration 3687, dc_loss: 0.026603087782859802, tv_loss: 0.03376173600554466\n",
      "iteration 3688, dc_loss: 0.026599418371915817, tv_loss: 0.033772062510252\n",
      "iteration 3689, dc_loss: 0.02659394033253193, tv_loss: 0.03378250449895859\n",
      "iteration 3690, dc_loss: 0.02659478969871998, tv_loss: 0.033771809190511703\n",
      "iteration 3691, dc_loss: 0.026591520756483078, tv_loss: 0.033777642995119095\n",
      "iteration 3692, dc_loss: 0.026587748900055885, tv_loss: 0.03377946838736534\n",
      "iteration 3693, dc_loss: 0.026593569666147232, tv_loss: 0.03377262130379677\n",
      "iteration 3694, dc_loss: 0.02658919431269169, tv_loss: 0.03376755490899086\n",
      "iteration 3695, dc_loss: 0.026581332087516785, tv_loss: 0.03378063067793846\n",
      "iteration 3696, dc_loss: 0.02658224292099476, tv_loss: 0.03378068655729294\n",
      "iteration 3697, dc_loss: 0.026585135608911514, tv_loss: 0.033767927438020706\n",
      "iteration 3698, dc_loss: 0.026585569605231285, tv_loss: 0.033763837069272995\n",
      "iteration 3699, dc_loss: 0.026579076424241066, tv_loss: 0.03377731144428253\n",
      "iteration 3700, dc_loss: 0.026572223752737045, tv_loss: 0.03377911075949669\n",
      "iteration 3701, dc_loss: 0.026575647294521332, tv_loss: 0.033767666667699814\n",
      "iteration 3702, dc_loss: 0.026577623561024666, tv_loss: 0.033764660358428955\n",
      "iteration 3703, dc_loss: 0.026570124551653862, tv_loss: 0.033774588257074356\n",
      "iteration 3704, dc_loss: 0.026566417887806892, tv_loss: 0.033775437623262405\n",
      "iteration 3705, dc_loss: 0.026570802554488182, tv_loss: 0.03376075625419617\n",
      "iteration 3706, dc_loss: 0.026572076603770256, tv_loss: 0.03376368060708046\n",
      "iteration 3707, dc_loss: 0.02656189352273941, tv_loss: 0.033772312104701996\n",
      "iteration 3708, dc_loss: 0.02655806578695774, tv_loss: 0.03377719596028328\n",
      "iteration 3709, dc_loss: 0.026564735919237137, tv_loss: 0.03376823663711548\n",
      "iteration 3710, dc_loss: 0.026564301922917366, tv_loss: 0.033759839832782745\n",
      "iteration 3711, dc_loss: 0.02655668929219246, tv_loss: 0.03376831114292145\n",
      "iteration 3712, dc_loss: 0.02655070833861828, tv_loss: 0.03377494588494301\n",
      "iteration 3713, dc_loss: 0.026555592194199562, tv_loss: 0.033763643354177475\n",
      "iteration 3714, dc_loss: 0.026558004319667816, tv_loss: 0.03376429155468941\n",
      "iteration 3715, dc_loss: 0.0265493281185627, tv_loss: 0.03376971557736397\n",
      "iteration 3716, dc_loss: 0.026548141613602638, tv_loss: 0.03376500681042671\n",
      "iteration 3717, dc_loss: 0.026549162343144417, tv_loss: 0.0337660051882267\n",
      "iteration 3718, dc_loss: 0.02654864266514778, tv_loss: 0.03376438468694687\n",
      "iteration 3719, dc_loss: 0.026542650535702705, tv_loss: 0.03376775607466698\n",
      "iteration 3720, dc_loss: 0.026537401601672173, tv_loss: 0.0337749682366848\n",
      "iteration 3721, dc_loss: 0.02654230408370495, tv_loss: 0.03376994654536247\n",
      "iteration 3722, dc_loss: 0.02654154971241951, tv_loss: 0.03376525267958641\n",
      "iteration 3723, dc_loss: 0.026532717049121857, tv_loss: 0.03377234935760498\n",
      "iteration 3724, dc_loss: 0.026535257697105408, tv_loss: 0.033768199384212494\n",
      "iteration 3725, dc_loss: 0.026537159457802773, tv_loss: 0.033763669431209564\n",
      "iteration 3726, dc_loss: 0.026533083990216255, tv_loss: 0.0337679348886013\n",
      "iteration 3727, dc_loss: 0.026527319103479385, tv_loss: 0.03377167135477066\n",
      "iteration 3728, dc_loss: 0.026531759649515152, tv_loss: 0.03376098722219467\n",
      "iteration 3729, dc_loss: 0.026527466252446175, tv_loss: 0.03376027196645737\n",
      "iteration 3730, dc_loss: 0.026523267850279808, tv_loss: 0.03376990929245949\n",
      "iteration 3731, dc_loss: 0.026524489745497704, tv_loss: 0.033769503235816956\n",
      "iteration 3732, dc_loss: 0.02652243711054325, tv_loss: 0.0337715819478035\n",
      "iteration 3733, dc_loss: 0.026519915089011192, tv_loss: 0.033776748925447464\n",
      "iteration 3734, dc_loss: 0.026516018435359, tv_loss: 0.033772584050893784\n",
      "iteration 3735, dc_loss: 0.026519466191530228, tv_loss: 0.03376263007521629\n",
      "iteration 3736, dc_loss: 0.02651466429233551, tv_loss: 0.033768195658922195\n",
      "iteration 3737, dc_loss: 0.026507699862122536, tv_loss: 0.03377542272210121\n",
      "iteration 3738, dc_loss: 0.026513494551181793, tv_loss: 0.033771272748708725\n",
      "iteration 3739, dc_loss: 0.02651371620595455, tv_loss: 0.03376496583223343\n",
      "iteration 3740, dc_loss: 0.026508016511797905, tv_loss: 0.033767811954021454\n",
      "iteration 3741, dc_loss: 0.02650264836847782, tv_loss: 0.03377329930663109\n",
      "iteration 3742, dc_loss: 0.026505010202527046, tv_loss: 0.03377363085746765\n",
      "iteration 3743, dc_loss: 0.026505663990974426, tv_loss: 0.0337660126388073\n",
      "iteration 3744, dc_loss: 0.026499878615140915, tv_loss: 0.03377137705683708\n",
      "iteration 3745, dc_loss: 0.026500936597585678, tv_loss: 0.03376840427517891\n",
      "iteration 3746, dc_loss: 0.0264954324811697, tv_loss: 0.033770885318517685\n",
      "iteration 3747, dc_loss: 0.02649456448853016, tv_loss: 0.03377261757850647\n",
      "iteration 3748, dc_loss: 0.026498539373278618, tv_loss: 0.033777084201574326\n",
      "iteration 3749, dc_loss: 0.026491178199648857, tv_loss: 0.03377383574843407\n",
      "iteration 3750, dc_loss: 0.026487823575735092, tv_loss: 0.03377580642700195\n",
      "iteration 3751, dc_loss: 0.026490774005651474, tv_loss: 0.03377262130379677\n",
      "iteration 3752, dc_loss: 0.02648702822625637, tv_loss: 0.03378436341881752\n",
      "iteration 3753, dc_loss: 0.026485111564397812, tv_loss: 0.03377193957567215\n",
      "iteration 3754, dc_loss: 0.026485275477170944, tv_loss: 0.03377189487218857\n",
      "iteration 3755, dc_loss: 0.026481525972485542, tv_loss: 0.03377879410982132\n",
      "iteration 3756, dc_loss: 0.026480309665203094, tv_loss: 0.03377467766404152\n",
      "iteration 3757, dc_loss: 0.026476413011550903, tv_loss: 0.03377656266093254\n",
      "iteration 3758, dc_loss: 0.026478320360183716, tv_loss: 0.03377334028482437\n",
      "iteration 3759, dc_loss: 0.026479367166757584, tv_loss: 0.033771924674510956\n",
      "iteration 3760, dc_loss: 0.02647130936384201, tv_loss: 0.03378073498606682\n",
      "iteration 3761, dc_loss: 0.02647075615823269, tv_loss: 0.03377188742160797\n",
      "iteration 3762, dc_loss: 0.026473654434084892, tv_loss: 0.033768534660339355\n",
      "iteration 3763, dc_loss: 0.02647050842642784, tv_loss: 0.0337742380797863\n",
      "iteration 3764, dc_loss: 0.026464054360985756, tv_loss: 0.03377397358417511\n",
      "iteration 3765, dc_loss: 0.02646511048078537, tv_loss: 0.033771973103284836\n",
      "iteration 3766, dc_loss: 0.026464808732271194, tv_loss: 0.03376907855272293\n",
      "iteration 3767, dc_loss: 0.026460127905011177, tv_loss: 0.03377113118767738\n",
      "iteration 3768, dc_loss: 0.026460349559783936, tv_loss: 0.033775217831134796\n",
      "iteration 3769, dc_loss: 0.026461821049451828, tv_loss: 0.033765945583581924\n",
      "iteration 3770, dc_loss: 0.026454469189047813, tv_loss: 0.033770762383937836\n",
      "iteration 3771, dc_loss: 0.026455506682395935, tv_loss: 0.033773716539144516\n",
      "iteration 3772, dc_loss: 0.026456492021679878, tv_loss: 0.0337645560503006\n",
      "iteration 3773, dc_loss: 0.02644987590610981, tv_loss: 0.03376930207014084\n",
      "iteration 3774, dc_loss: 0.026448315009474754, tv_loss: 0.03377695754170418\n",
      "iteration 3775, dc_loss: 0.026447031646966934, tv_loss: 0.03377152234315872\n",
      "iteration 3776, dc_loss: 0.026449520140886307, tv_loss: 0.033768560737371445\n",
      "iteration 3777, dc_loss: 0.0264446921646595, tv_loss: 0.03377305716276169\n",
      "iteration 3778, dc_loss: 0.026440566405653954, tv_loss: 0.0337701141834259\n",
      "iteration 3779, dc_loss: 0.026445019990205765, tv_loss: 0.03376693278551102\n",
      "iteration 3780, dc_loss: 0.02643837220966816, tv_loss: 0.03377550467848778\n",
      "iteration 3781, dc_loss: 0.026436276733875275, tv_loss: 0.03376728296279907\n",
      "iteration 3782, dc_loss: 0.026441847905516624, tv_loss: 0.03376644477248192\n",
      "iteration 3783, dc_loss: 0.026437288150191307, tv_loss: 0.033771805465221405\n",
      "iteration 3784, dc_loss: 0.026429282501339912, tv_loss: 0.03377068042755127\n",
      "iteration 3785, dc_loss: 0.026429658755660057, tv_loss: 0.03377145156264305\n",
      "iteration 3786, dc_loss: 0.026428941637277603, tv_loss: 0.033771224319934845\n",
      "iteration 3787, dc_loss: 0.026427818462252617, tv_loss: 0.03377891331911087\n",
      "iteration 3788, dc_loss: 0.02643078751862049, tv_loss: 0.03376865014433861\n",
      "iteration 3789, dc_loss: 0.026422856375575066, tv_loss: 0.03377385437488556\n",
      "iteration 3790, dc_loss: 0.026418475434184074, tv_loss: 0.03377640247344971\n",
      "iteration 3791, dc_loss: 0.026422781869769096, tv_loss: 0.033769749104976654\n",
      "iteration 3792, dc_loss: 0.026424728333950043, tv_loss: 0.03377319127321243\n",
      "iteration 3793, dc_loss: 0.026416946202516556, tv_loss: 0.03377268835902214\n",
      "iteration 3794, dc_loss: 0.026413723826408386, tv_loss: 0.03377443552017212\n",
      "iteration 3795, dc_loss: 0.026417650282382965, tv_loss: 0.033768098801374435\n",
      "iteration 3796, dc_loss: 0.02641213871538639, tv_loss: 0.03378089517354965\n",
      "iteration 3797, dc_loss: 0.026408342644572258, tv_loss: 0.03378230333328247\n",
      "iteration 3798, dc_loss: 0.026410358026623726, tv_loss: 0.03377168998122215\n",
      "iteration 3799, dc_loss: 0.026409637182950974, tv_loss: 0.033771563321352005\n",
      "iteration 3800, dc_loss: 0.02640443481504917, tv_loss: 0.03377824276685715\n",
      "iteration 3801, dc_loss: 0.026406561955809593, tv_loss: 0.03378107398748398\n",
      "iteration 3802, dc_loss: 0.02640562877058983, tv_loss: 0.0337732657790184\n",
      "iteration 3803, dc_loss: 0.0263963770121336, tv_loss: 0.03378203883767128\n",
      "iteration 3804, dc_loss: 0.02639298513531685, tv_loss: 0.03378578647971153\n",
      "iteration 3805, dc_loss: 0.026400770992040634, tv_loss: 0.03377643600106239\n",
      "iteration 3806, dc_loss: 0.026401618495583534, tv_loss: 0.03376965597271919\n",
      "iteration 3807, dc_loss: 0.026392409577965736, tv_loss: 0.03377334773540497\n",
      "iteration 3808, dc_loss: 0.026386979967355728, tv_loss: 0.033781588077545166\n",
      "iteration 3809, dc_loss: 0.02639451064169407, tv_loss: 0.033771365880966187\n",
      "iteration 3810, dc_loss: 0.026396047323942184, tv_loss: 0.03377125412225723\n",
      "iteration 3811, dc_loss: 0.02638668566942215, tv_loss: 0.03377106040716171\n",
      "iteration 3812, dc_loss: 0.02637750469148159, tv_loss: 0.03377741202712059\n",
      "iteration 3813, dc_loss: 0.026384077966213226, tv_loss: 0.033770766109228134\n",
      "iteration 3814, dc_loss: 0.02638549730181694, tv_loss: 0.03376724570989609\n",
      "iteration 3815, dc_loss: 0.026378275826573372, tv_loss: 0.03377695381641388\n",
      "iteration 3816, dc_loss: 0.026381414383649826, tv_loss: 0.03377191349864006\n",
      "iteration 3817, dc_loss: 0.026377808302640915, tv_loss: 0.03377380222082138\n",
      "iteration 3818, dc_loss: 0.02637815847992897, tv_loss: 0.03377002477645874\n",
      "iteration 3819, dc_loss: 0.026371710002422333, tv_loss: 0.033773090690374374\n",
      "iteration 3820, dc_loss: 0.026370834559202194, tv_loss: 0.033772047609090805\n",
      "iteration 3821, dc_loss: 0.026374222710728645, tv_loss: 0.03376619145274162\n",
      "iteration 3822, dc_loss: 0.026372091844677925, tv_loss: 0.03376772999763489\n",
      "iteration 3823, dc_loss: 0.026361564174294472, tv_loss: 0.033777039498090744\n",
      "iteration 3824, dc_loss: 0.02636200189590454, tv_loss: 0.03377223759889603\n",
      "iteration 3825, dc_loss: 0.026364939287304878, tv_loss: 0.03376852348446846\n",
      "iteration 3826, dc_loss: 0.02636944130063057, tv_loss: 0.03376192972064018\n",
      "iteration 3827, dc_loss: 0.026357734575867653, tv_loss: 0.033774808049201965\n",
      "iteration 3828, dc_loss: 0.026355665177106857, tv_loss: 0.03377498686313629\n",
      "iteration 3829, dc_loss: 0.02636202983558178, tv_loss: 0.03376300260424614\n",
      "iteration 3830, dc_loss: 0.026355892419815063, tv_loss: 0.03376689925789833\n",
      "iteration 3831, dc_loss: 0.026350872591137886, tv_loss: 0.03377654030919075\n",
      "iteration 3832, dc_loss: 0.026348644867539406, tv_loss: 0.03377449885010719\n",
      "iteration 3833, dc_loss: 0.026353605091571808, tv_loss: 0.03377004712820053\n",
      "iteration 3834, dc_loss: 0.026352351531386375, tv_loss: 0.03376665338873863\n",
      "iteration 3835, dc_loss: 0.026344893500208855, tv_loss: 0.03377757593989372\n",
      "iteration 3836, dc_loss: 0.026342591270804405, tv_loss: 0.033779021352529526\n",
      "iteration 3837, dc_loss: 0.02634395845234394, tv_loss: 0.033771343529224396\n",
      "iteration 3838, dc_loss: 0.02634301409125328, tv_loss: 0.03377140685915947\n",
      "iteration 3839, dc_loss: 0.02634373866021633, tv_loss: 0.03376997634768486\n",
      "iteration 3840, dc_loss: 0.02633804827928543, tv_loss: 0.033773571252822876\n",
      "iteration 3841, dc_loss: 0.026334216818213463, tv_loss: 0.03377286717295647\n",
      "iteration 3842, dc_loss: 0.02633501961827278, tv_loss: 0.03377772122621536\n",
      "iteration 3843, dc_loss: 0.026333920657634735, tv_loss: 0.033777572214603424\n",
      "iteration 3844, dc_loss: 0.026336779817938805, tv_loss: 0.03377091512084007\n",
      "iteration 3845, dc_loss: 0.026323705911636353, tv_loss: 0.0337810218334198\n",
      "iteration 3846, dc_loss: 0.026332233101129532, tv_loss: 0.03376953303813934\n",
      "iteration 3847, dc_loss: 0.026331055909395218, tv_loss: 0.033769574016332626\n",
      "iteration 3848, dc_loss: 0.02631979249417782, tv_loss: 0.033786699175834656\n",
      "iteration 3849, dc_loss: 0.026320680975914, tv_loss: 0.03378383815288544\n",
      "iteration 3850, dc_loss: 0.026325881481170654, tv_loss: 0.03376820310950279\n",
      "iteration 3851, dc_loss: 0.026320770382881165, tv_loss: 0.033772993832826614\n",
      "iteration 3852, dc_loss: 0.026319414377212524, tv_loss: 0.03378168120980263\n",
      "iteration 3853, dc_loss: 0.026314040645956993, tv_loss: 0.0337836891412735\n",
      "iteration 3854, dc_loss: 0.026316655799746513, tv_loss: 0.03377210348844528\n",
      "iteration 3855, dc_loss: 0.02631491981446743, tv_loss: 0.03377364203333855\n",
      "iteration 3856, dc_loss: 0.02630949392914772, tv_loss: 0.03377909958362579\n",
      "iteration 3857, dc_loss: 0.02631068415939808, tv_loss: 0.03377141058444977\n",
      "iteration 3858, dc_loss: 0.026309261098504066, tv_loss: 0.03377343714237213\n",
      "iteration 3859, dc_loss: 0.026305969804525375, tv_loss: 0.03377337008714676\n",
      "iteration 3860, dc_loss: 0.02630433812737465, tv_loss: 0.03377312421798706\n",
      "iteration 3861, dc_loss: 0.026307832449674606, tv_loss: 0.033769454807043076\n",
      "iteration 3862, dc_loss: 0.02630133181810379, tv_loss: 0.033770132809877396\n",
      "iteration 3863, dc_loss: 0.026297107338905334, tv_loss: 0.03377450630068779\n",
      "iteration 3864, dc_loss: 0.02630295790731907, tv_loss: 0.03376537561416626\n",
      "iteration 3865, dc_loss: 0.0262969508767128, tv_loss: 0.03376814350485802\n",
      "iteration 3866, dc_loss: 0.02629459649324417, tv_loss: 0.03377065435051918\n",
      "iteration 3867, dc_loss: 0.026293179020285606, tv_loss: 0.0337713360786438\n",
      "iteration 3868, dc_loss: 0.026291238144040108, tv_loss: 0.0337698869407177\n",
      "iteration 3869, dc_loss: 0.026295268908143044, tv_loss: 0.03376489505171776\n",
      "iteration 3870, dc_loss: 0.026287177577614784, tv_loss: 0.03377365693449974\n",
      "iteration 3871, dc_loss: 0.026283638551831245, tv_loss: 0.03378294035792351\n",
      "iteration 3872, dc_loss: 0.026285216212272644, tv_loss: 0.03378131240606308\n",
      "iteration 3873, dc_loss: 0.026287920773029327, tv_loss: 0.03377560153603554\n",
      "iteration 3874, dc_loss: 0.026282140985131264, tv_loss: 0.03377413749694824\n",
      "iteration 3875, dc_loss: 0.026278305798768997, tv_loss: 0.03377428278326988\n",
      "iteration 3876, dc_loss: 0.026280609890818596, tv_loss: 0.03377504274249077\n",
      "iteration 3877, dc_loss: 0.02627730369567871, tv_loss: 0.033782780170440674\n",
      "iteration 3878, dc_loss: 0.026271913200616837, tv_loss: 0.03378310427069664\n",
      "iteration 3879, dc_loss: 0.0262764785438776, tv_loss: 0.03377538546919823\n",
      "iteration 3880, dc_loss: 0.02627405896782875, tv_loss: 0.03377785533666611\n",
      "iteration 3881, dc_loss: 0.02626713365316391, tv_loss: 0.033782150596380234\n",
      "iteration 3882, dc_loss: 0.026270847767591476, tv_loss: 0.033777255564928055\n",
      "iteration 3883, dc_loss: 0.026271646842360497, tv_loss: 0.033774059265851974\n",
      "iteration 3884, dc_loss: 0.026262085884809494, tv_loss: 0.03378422558307648\n",
      "iteration 3885, dc_loss: 0.026257866993546486, tv_loss: 0.03378630802035332\n",
      "iteration 3886, dc_loss: 0.02626684121787548, tv_loss: 0.03377343714237213\n",
      "iteration 3887, dc_loss: 0.02626267820596695, tv_loss: 0.033781748265028\n",
      "iteration 3888, dc_loss: 0.026254119351506233, tv_loss: 0.03378009423613548\n",
      "iteration 3889, dc_loss: 0.026262162253260612, tv_loss: 0.033774688839912415\n",
      "iteration 3890, dc_loss: 0.026257110759615898, tv_loss: 0.033783722668886185\n",
      "iteration 3891, dc_loss: 0.02624882198870182, tv_loss: 0.0337807834148407\n",
      "iteration 3892, dc_loss: 0.026255620643496513, tv_loss: 0.03377693518996239\n",
      "iteration 3893, dc_loss: 0.02625374123454094, tv_loss: 0.033777590841054916\n",
      "iteration 3894, dc_loss: 0.02624817192554474, tv_loss: 0.03377581387758255\n",
      "iteration 3895, dc_loss: 0.02624676190316677, tv_loss: 0.0337812677025795\n",
      "iteration 3896, dc_loss: 0.026243846863508224, tv_loss: 0.03378070890903473\n",
      "iteration 3897, dc_loss: 0.026248516514897346, tv_loss: 0.03377380594611168\n",
      "iteration 3898, dc_loss: 0.026239857077598572, tv_loss: 0.033780913800001144\n",
      "iteration 3899, dc_loss: 0.026235200464725494, tv_loss: 0.0337795689702034\n",
      "iteration 3900, dc_loss: 0.026247352361679077, tv_loss: 0.0337715782225132\n",
      "iteration 3901, dc_loss: 0.026237519457936287, tv_loss: 0.0337788350880146\n",
      "iteration 3902, dc_loss: 0.026230329647660255, tv_loss: 0.03377701714634895\n",
      "iteration 3903, dc_loss: 0.026236699894070625, tv_loss: 0.03377235308289528\n",
      "iteration 3904, dc_loss: 0.026238825172185898, tv_loss: 0.03377126529812813\n",
      "iteration 3905, dc_loss: 0.026232212781906128, tv_loss: 0.03377682715654373\n",
      "iteration 3906, dc_loss: 0.026222169399261475, tv_loss: 0.033790893852710724\n",
      "iteration 3907, dc_loss: 0.026227563619613647, tv_loss: 0.03377868980169296\n",
      "iteration 3908, dc_loss: 0.026230521500110626, tv_loss: 0.033771906048059464\n",
      "iteration 3909, dc_loss: 0.02622448280453682, tv_loss: 0.033773548901081085\n",
      "iteration 3910, dc_loss: 0.026222804561257362, tv_loss: 0.033778171986341476\n",
      "iteration 3911, dc_loss: 0.026219941675662994, tv_loss: 0.03378395736217499\n",
      "iteration 3912, dc_loss: 0.026219964027404785, tv_loss: 0.03378044813871384\n",
      "iteration 3913, dc_loss: 0.026220830157399178, tv_loss: 0.03376886621117592\n",
      "iteration 3914, dc_loss: 0.026212330907583237, tv_loss: 0.03378620743751526\n",
      "iteration 3915, dc_loss: 0.026212602853775024, tv_loss: 0.03378161042928696\n",
      "iteration 3916, dc_loss: 0.026215611025691032, tv_loss: 0.03377684950828552\n",
      "iteration 3917, dc_loss: 0.026215625926852226, tv_loss: 0.03377428650856018\n",
      "iteration 3918, dc_loss: 0.026204735040664673, tv_loss: 0.033780839294195175\n",
      "iteration 3919, dc_loss: 0.02620766870677471, tv_loss: 0.03378484398126602\n",
      "iteration 3920, dc_loss: 0.0262125376611948, tv_loss: 0.03377958759665489\n",
      "iteration 3921, dc_loss: 0.026199042797088623, tv_loss: 0.033784057945013046\n",
      "iteration 3922, dc_loss: 0.026204435154795647, tv_loss: 0.033776961266994476\n",
      "iteration 3923, dc_loss: 0.02620496042072773, tv_loss: 0.0337776243686676\n",
      "iteration 3924, dc_loss: 0.026200255379080772, tv_loss: 0.033786285668611526\n",
      "iteration 3925, dc_loss: 0.026196938008069992, tv_loss: 0.03378034383058548\n",
      "iteration 3926, dc_loss: 0.026203572750091553, tv_loss: 0.03377244994044304\n",
      "iteration 3927, dc_loss: 0.026190534234046936, tv_loss: 0.03379261493682861\n",
      "iteration 3928, dc_loss: 0.026190854609012604, tv_loss: 0.033780504018068314\n",
      "iteration 3929, dc_loss: 0.026194218546152115, tv_loss: 0.03377869352698326\n",
      "iteration 3930, dc_loss: 0.026192156597971916, tv_loss: 0.03378244861960411\n",
      "iteration 3931, dc_loss: 0.026185182854533195, tv_loss: 0.033783987164497375\n",
      "iteration 3932, dc_loss: 0.026188910007476807, tv_loss: 0.03377921134233475\n",
      "iteration 3933, dc_loss: 0.02618728205561638, tv_loss: 0.03377586603164673\n",
      "iteration 3934, dc_loss: 0.026184218004345894, tv_loss: 0.03377755731344223\n",
      "iteration 3935, dc_loss: 0.026178516447544098, tv_loss: 0.03378289192914963\n",
      "iteration 3936, dc_loss: 0.02618379332125187, tv_loss: 0.033770669251680374\n",
      "iteration 3937, dc_loss: 0.02617751434445381, tv_loss: 0.03378354385495186\n",
      "iteration 3938, dc_loss: 0.026172811165452003, tv_loss: 0.03378399461507797\n",
      "iteration 3939, dc_loss: 0.026178857311606407, tv_loss: 0.0337790809571743\n",
      "iteration 3940, dc_loss: 0.02617599070072174, tv_loss: 0.03378216177225113\n",
      "iteration 3941, dc_loss: 0.026171086356043816, tv_loss: 0.033778708428144455\n",
      "iteration 3942, dc_loss: 0.0261723343282938, tv_loss: 0.03378072753548622\n",
      "iteration 3943, dc_loss: 0.026164522394537926, tv_loss: 0.03379480168223381\n",
      "iteration 3944, dc_loss: 0.026169560849666595, tv_loss: 0.03378356248140335\n",
      "iteration 3945, dc_loss: 0.026169724762439728, tv_loss: 0.03377324342727661\n",
      "iteration 3946, dc_loss: 0.02616492472589016, tv_loss: 0.03378253057599068\n",
      "iteration 3947, dc_loss: 0.026155222207307816, tv_loss: 0.03379274159669876\n",
      "iteration 3948, dc_loss: 0.02616141363978386, tv_loss: 0.03378725051879883\n",
      "iteration 3949, dc_loss: 0.026166649535298347, tv_loss: 0.033774297684431076\n",
      "iteration 3950, dc_loss: 0.026156602427363396, tv_loss: 0.03378532454371452\n",
      "iteration 3951, dc_loss: 0.026150109246373177, tv_loss: 0.03379669412970543\n",
      "iteration 3952, dc_loss: 0.026162702590227127, tv_loss: 0.033774930983781815\n",
      "iteration 3953, dc_loss: 0.026158243417739868, tv_loss: 0.033772505819797516\n",
      "iteration 3954, dc_loss: 0.026138516142964363, tv_loss: 0.033790476620197296\n",
      "iteration 3955, dc_loss: 0.026152851060032845, tv_loss: 0.03377743065357208\n",
      "iteration 3956, dc_loss: 0.02615487016737461, tv_loss: 0.033777251839637756\n",
      "iteration 3957, dc_loss: 0.02614094875752926, tv_loss: 0.03378336504101753\n",
      "iteration 3958, dc_loss: 0.026143355295062065, tv_loss: 0.03378170356154442\n",
      "iteration 3959, dc_loss: 0.026150383055210114, tv_loss: 0.033770907670259476\n",
      "iteration 3960, dc_loss: 0.026148932054638863, tv_loss: 0.033769745379686356\n",
      "iteration 3961, dc_loss: 0.02613016404211521, tv_loss: 0.033790431916713715\n",
      "iteration 3962, dc_loss: 0.02614816091954708, tv_loss: 0.033771105110645294\n",
      "iteration 3963, dc_loss: 0.026143506169319153, tv_loss: 0.03378012031316757\n",
      "iteration 3964, dc_loss: 0.02613689750432968, tv_loss: 0.033783312886953354\n",
      "iteration 3965, dc_loss: 0.026135053485631943, tv_loss: 0.03378161042928696\n",
      "iteration 3966, dc_loss: 0.02614631876349449, tv_loss: 0.03376735374331474\n",
      "iteration 3967, dc_loss: 0.026135481894016266, tv_loss: 0.03377984091639519\n",
      "iteration 3968, dc_loss: 0.02614407241344452, tv_loss: 0.03376910462975502\n",
      "iteration 3969, dc_loss: 0.026132868602871895, tv_loss: 0.03378157690167427\n",
      "iteration 3970, dc_loss: 0.026139618828892708, tv_loss: 0.03377422317862511\n",
      "iteration 3971, dc_loss: 0.026122931391000748, tv_loss: 0.03378559648990631\n",
      "iteration 3972, dc_loss: 0.026124827563762665, tv_loss: 0.03377986699342728\n",
      "iteration 3973, dc_loss: 0.026122063398361206, tv_loss: 0.03377674147486687\n",
      "iteration 3974, dc_loss: 0.026124335825443268, tv_loss: 0.03377170115709305\n",
      "iteration 3975, dc_loss: 0.026118582114577293, tv_loss: 0.03377294912934303\n",
      "iteration 3976, dc_loss: 0.026111522689461708, tv_loss: 0.033784378319978714\n",
      "iteration 3977, dc_loss: 0.02611653506755829, tv_loss: 0.03377602621912956\n",
      "iteration 3978, dc_loss: 0.026115018874406815, tv_loss: 0.03377637639641762\n",
      "iteration 3979, dc_loss: 0.026116754859685898, tv_loss: 0.033777885138988495\n",
      "iteration 3980, dc_loss: 0.02610909566283226, tv_loss: 0.03378070145845413\n",
      "iteration 3981, dc_loss: 0.026113737374544144, tv_loss: 0.03377484157681465\n",
      "iteration 3982, dc_loss: 0.026109294965863228, tv_loss: 0.03377436473965645\n",
      "iteration 3983, dc_loss: 0.026101665571331978, tv_loss: 0.033777739852666855\n",
      "iteration 3984, dc_loss: 0.0261024571955204, tv_loss: 0.03377474844455719\n",
      "iteration 3985, dc_loss: 0.02610321156680584, tv_loss: 0.0337747298181057\n",
      "iteration 3986, dc_loss: 0.02609788440167904, tv_loss: 0.033784713596105576\n",
      "iteration 3987, dc_loss: 0.026097921654582024, tv_loss: 0.03378479927778244\n",
      "iteration 3988, dc_loss: 0.02610332891345024, tv_loss: 0.03377171978354454\n",
      "iteration 3989, dc_loss: 0.026093890890479088, tv_loss: 0.03377857059240341\n",
      "iteration 3990, dc_loss: 0.026091318577528, tv_loss: 0.033778078854084015\n",
      "iteration 3991, dc_loss: 0.02609226293861866, tv_loss: 0.03377709537744522\n",
      "iteration 3992, dc_loss: 0.02609960548579693, tv_loss: 0.033770885318517685\n",
      "iteration 3993, dc_loss: 0.026082970201969147, tv_loss: 0.03378771245479584\n",
      "iteration 3994, dc_loss: 0.026082023978233337, tv_loss: 0.03378727659583092\n",
      "iteration 3995, dc_loss: 0.02608773671090603, tv_loss: 0.03377868980169296\n",
      "iteration 3996, dc_loss: 0.026083867996931076, tv_loss: 0.03377420827746391\n",
      "iteration 3997, dc_loss: 0.026081692427396774, tv_loss: 0.03377935662865639\n",
      "iteration 3998, dc_loss: 0.026079310104250908, tv_loss: 0.03378056362271309\n",
      "iteration 3999, dc_loss: 0.026084188371896744, tv_loss: 0.03377297893166542\n",
      "iteration 4000, dc_loss: 0.02607300505042076, tv_loss: 0.033779531717300415\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params = dict(fi: dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.935042142868042, tv_loss: 0.0008581512374803424\n",
      "iteration 2, dc_loss: 2.813906192779541, tv_loss: 0.004346647299826145\n",
      "iteration 3, dc_loss: 2.748802423477173, tv_loss: 0.0064378357492387295\n",
      "iteration 4, dc_loss: 2.7061820030212402, tv_loss: 0.007817002013325691\n",
      "iteration 5, dc_loss: 2.6745200157165527, tv_loss: 0.008727215230464935\n",
      "iteration 6, dc_loss: 2.6500349044799805, tv_loss: 0.009370515123009682\n",
      "iteration 7, dc_loss: 2.6297852993011475, tv_loss: 0.009868411347270012\n",
      "iteration 8, dc_loss: 2.611405849456787, tv_loss: 0.010255226865410805\n",
      "iteration 9, dc_loss: 2.594622850418091, tv_loss: 0.010551880113780499\n",
      "iteration 10, dc_loss: 2.5781402587890625, tv_loss: 0.010800882242619991\n",
      "iteration 11, dc_loss: 2.5617458820343018, tv_loss: 0.010989588685333729\n",
      "iteration 12, dc_loss: 2.5462396144866943, tv_loss: 0.011076439172029495\n",
      "iteration 13, dc_loss: 2.531658172607422, tv_loss: 0.01109855342656374\n",
      "iteration 14, dc_loss: 2.517054796218872, tv_loss: 0.01116863265633583\n",
      "iteration 15, dc_loss: 2.502668857574463, tv_loss: 0.011266229674220085\n",
      "iteration 16, dc_loss: 2.4888088703155518, tv_loss: 0.011269018054008484\n",
      "iteration 17, dc_loss: 2.4751460552215576, tv_loss: 0.011155589483678341\n",
      "iteration 18, dc_loss: 2.4615657329559326, tv_loss: 0.011036506853997707\n",
      "iteration 19, dc_loss: 2.4481449127197266, tv_loss: 0.010928362607955933\n",
      "iteration 20, dc_loss: 2.4349217414855957, tv_loss: 0.010742252692580223\n",
      "iteration 21, dc_loss: 2.4220285415649414, tv_loss: 0.010493257082998753\n",
      "iteration 22, dc_loss: 2.4092087745666504, tv_loss: 0.010173100046813488\n",
      "iteration 23, dc_loss: 2.396359443664551, tv_loss: 0.009708648547530174\n",
      "iteration 24, dc_loss: 2.383430004119873, tv_loss: 0.009269512258470058\n",
      "iteration 25, dc_loss: 2.37048602104187, tv_loss: 0.008882624097168446\n",
      "iteration 26, dc_loss: 2.357898473739624, tv_loss: 0.008371677249670029\n",
      "iteration 27, dc_loss: 2.345548152923584, tv_loss: 0.007998956367373466\n",
      "iteration 28, dc_loss: 2.3332881927490234, tv_loss: 0.0077134487219154835\n",
      "iteration 29, dc_loss: 2.3210158348083496, tv_loss: 0.007435501087456942\n",
      "iteration 30, dc_loss: 2.308638095855713, tv_loss: 0.007380526512861252\n",
      "iteration 31, dc_loss: 2.2965729236602783, tv_loss: 0.0073352050967514515\n",
      "iteration 32, dc_loss: 2.2846381664276123, tv_loss: 0.007513796910643578\n",
      "iteration 33, dc_loss: 2.273057699203491, tv_loss: 0.007582914549857378\n",
      "iteration 34, dc_loss: 2.261791467666626, tv_loss: 0.007820215076208115\n",
      "iteration 35, dc_loss: 2.2501800060272217, tv_loss: 0.007840747945010662\n",
      "iteration 36, dc_loss: 2.237851142883301, tv_loss: 0.008013046346604824\n",
      "iteration 37, dc_loss: 2.225429058074951, tv_loss: 0.008038497529923916\n",
      "iteration 38, dc_loss: 2.2146713733673096, tv_loss: 0.00800212100148201\n",
      "iteration 39, dc_loss: 2.203653573989868, tv_loss: 0.008070992305874825\n",
      "iteration 40, dc_loss: 2.1913740634918213, tv_loss: 0.008010054007172585\n",
      "iteration 41, dc_loss: 2.1805877685546875, tv_loss: 0.008050150237977505\n",
      "iteration 42, dc_loss: 2.1699681282043457, tv_loss: 0.008266543969511986\n",
      "iteration 43, dc_loss: 2.1580233573913574, tv_loss: 0.008329886943101883\n",
      "iteration 44, dc_loss: 2.147406816482544, tv_loss: 0.008443048223853111\n",
      "iteration 45, dc_loss: 2.1368508338928223, tv_loss: 0.008667045272886753\n",
      "iteration 46, dc_loss: 2.125201940536499, tv_loss: 0.008733219467103481\n",
      "iteration 47, dc_loss: 2.1149799823760986, tv_loss: 0.00881839357316494\n",
      "iteration 48, dc_loss: 2.104484796524048, tv_loss: 0.009042047895491123\n",
      "iteration 49, dc_loss: 2.093158006668091, tv_loss: 0.009121677838265896\n",
      "iteration 50, dc_loss: 2.083294153213501, tv_loss: 0.009175378829240799\n",
      "iteration 51, dc_loss: 2.0728585720062256, tv_loss: 0.009385653771460056\n",
      "iteration 52, dc_loss: 2.061903476715088, tv_loss: 0.009465817362070084\n",
      "iteration 53, dc_loss: 2.0522215366363525, tv_loss: 0.009514004923403263\n",
      "iteration 54, dc_loss: 2.0418570041656494, tv_loss: 0.009734655730426311\n",
      "iteration 55, dc_loss: 2.0312790870666504, tv_loss: 0.009822849184274673\n",
      "iteration 56, dc_loss: 2.021744728088379, tv_loss: 0.009858423843979836\n",
      "iteration 57, dc_loss: 2.0115468502044678, tv_loss: 0.010075741447508335\n",
      "iteration 58, dc_loss: 2.001242160797119, tv_loss: 0.010170924477279186\n",
      "iteration 59, dc_loss: 1.9917986392974854, tv_loss: 0.010206427425146103\n",
      "iteration 60, dc_loss: 1.9818650484085083, tv_loss: 0.010405845008790493\n",
      "iteration 61, dc_loss: 1.9718133211135864, tv_loss: 0.010500254109501839\n",
      "iteration 62, dc_loss: 1.96242094039917, tv_loss: 0.010549466125667095\n",
      "iteration 63, dc_loss: 1.9527220726013184, tv_loss: 0.010745364241302013\n",
      "iteration 64, dc_loss: 1.9429221153259277, tv_loss: 0.010842064395546913\n",
      "iteration 65, dc_loss: 1.9335927963256836, tv_loss: 0.010901452042162418\n",
      "iteration 66, dc_loss: 1.9241076707839966, tv_loss: 0.011077705770730972\n",
      "iteration 67, dc_loss: 1.9145539999008179, tv_loss: 0.01115917693823576\n",
      "iteration 68, dc_loss: 1.9053200483322144, tv_loss: 0.011229629628360271\n",
      "iteration 69, dc_loss: 1.8960086107254028, tv_loss: 0.011408320628106594\n",
      "iteration 70, dc_loss: 1.88668692111969, tv_loss: 0.01148979738354683\n",
      "iteration 71, dc_loss: 1.8775511980056763, tv_loss: 0.011581465601921082\n",
      "iteration 72, dc_loss: 1.8683875799179077, tv_loss: 0.011747310869395733\n",
      "iteration 73, dc_loss: 1.8592829704284668, tv_loss: 0.011819526553153992\n",
      "iteration 74, dc_loss: 1.8502824306488037, tv_loss: 0.011937695555388927\n",
      "iteration 75, dc_loss: 1.841280221939087, tv_loss: 0.012077311053872108\n",
      "iteration 76, dc_loss: 1.8323451280593872, tv_loss: 0.012137374840676785\n",
      "iteration 77, dc_loss: 1.8234741687774658, tv_loss: 0.012270993553102016\n",
      "iteration 78, dc_loss: 1.8147058486938477, tv_loss: 0.012391733936965466\n",
      "iteration 79, dc_loss: 1.805937647819519, tv_loss: 0.012492126785218716\n",
      "iteration 80, dc_loss: 1.7971508502960205, tv_loss: 0.01261052954941988\n",
      "iteration 81, dc_loss: 1.788522720336914, tv_loss: 0.01268870197236538\n",
      "iteration 82, dc_loss: 1.779945731163025, tv_loss: 0.01278824545443058\n",
      "iteration 83, dc_loss: 1.771353840827942, tv_loss: 0.012898554094135761\n",
      "iteration 84, dc_loss: 1.7628326416015625, tv_loss: 0.012999948114156723\n",
      "iteration 85, dc_loss: 1.7544023990631104, tv_loss: 0.013152052648365498\n",
      "iteration 86, dc_loss: 1.7460546493530273, tv_loss: 0.013277552090585232\n",
      "iteration 87, dc_loss: 1.7377192974090576, tv_loss: 0.013339167460799217\n",
      "iteration 88, dc_loss: 1.7293906211853027, tv_loss: 0.013442798517644405\n",
      "iteration 89, dc_loss: 1.7212001085281372, tv_loss: 0.013539237901568413\n",
      "iteration 90, dc_loss: 1.7131397724151611, tv_loss: 0.01367997657507658\n",
      "iteration 91, dc_loss: 1.7053278684616089, tv_loss: 0.01371720153838396\n",
      "iteration 92, dc_loss: 1.6977877616882324, tv_loss: 0.013902447186410427\n",
      "iteration 93, dc_loss: 1.6901236772537231, tv_loss: 0.013873429968953133\n",
      "iteration 94, dc_loss: 1.682025671005249, tv_loss: 0.01409157644957304\n",
      "iteration 95, dc_loss: 1.673101544380188, tv_loss: 0.014044278301298618\n",
      "iteration 96, dc_loss: 1.6651244163513184, tv_loss: 0.01408371515572071\n",
      "iteration 97, dc_loss: 1.6577262878417969, tv_loss: 0.014247618615627289\n",
      "iteration 98, dc_loss: 1.6496878862380981, tv_loss: 0.01429404690861702\n",
      "iteration 99, dc_loss: 1.6414331197738647, tv_loss: 0.014537873677909374\n",
      "iteration 100, dc_loss: 1.6337954998016357, tv_loss: 0.01455637812614441\n",
      "iteration 101, dc_loss: 1.6264523267745972, tv_loss: 0.014512206427752972\n",
      "iteration 102, dc_loss: 1.618512749671936, tv_loss: 0.014731224626302719\n",
      "iteration 103, dc_loss: 1.6106505393981934, tv_loss: 0.01484970934689045\n",
      "iteration 104, dc_loss: 1.6032452583312988, tv_loss: 0.01486106589436531\n",
      "iteration 105, dc_loss: 1.595840334892273, tv_loss: 0.015000221319496632\n",
      "iteration 106, dc_loss: 1.58823823928833, tv_loss: 0.015023910440504551\n",
      "iteration 107, dc_loss: 1.5806437730789185, tv_loss: 0.015156558714807034\n",
      "iteration 108, dc_loss: 1.5732914209365845, tv_loss: 0.015283815562725067\n",
      "iteration 109, dc_loss: 1.565980076789856, tv_loss: 0.015257217921316624\n",
      "iteration 110, dc_loss: 1.5584163665771484, tv_loss: 0.015396158210933208\n",
      "iteration 111, dc_loss: 1.5511186122894287, tv_loss: 0.01543427538126707\n",
      "iteration 112, dc_loss: 1.5439248085021973, tv_loss: 0.015511082485318184\n",
      "iteration 113, dc_loss: 1.53672456741333, tv_loss: 0.01566700078547001\n",
      "iteration 114, dc_loss: 1.5296350717544556, tv_loss: 0.01573939621448517\n",
      "iteration 115, dc_loss: 1.5224854946136475, tv_loss: 0.01580565981566906\n",
      "iteration 116, dc_loss: 1.5152491331100464, tv_loss: 0.015880340710282326\n",
      "iteration 117, dc_loss: 1.508165717124939, tv_loss: 0.015927685424685478\n",
      "iteration 118, dc_loss: 1.5010851621627808, tv_loss: 0.01605679653584957\n",
      "iteration 119, dc_loss: 1.494096279144287, tv_loss: 0.016050733625888824\n",
      "iteration 120, dc_loss: 1.4870364665985107, tv_loss: 0.016142576932907104\n",
      "iteration 121, dc_loss: 1.4801450967788696, tv_loss: 0.016204211860895157\n",
      "iteration 122, dc_loss: 1.4733185768127441, tv_loss: 0.01632785052061081\n",
      "iteration 123, dc_loss: 1.4665697813034058, tv_loss: 0.016470719128847122\n",
      "iteration 124, dc_loss: 1.4598608016967773, tv_loss: 0.016464941203594208\n",
      "iteration 125, dc_loss: 1.4530431032180786, tv_loss: 0.01657894253730774\n",
      "iteration 126, dc_loss: 1.4462482929229736, tv_loss: 0.016616908833384514\n",
      "iteration 127, dc_loss: 1.4394505023956299, tv_loss: 0.016693202778697014\n",
      "iteration 128, dc_loss: 1.4327235221862793, tv_loss: 0.01674642413854599\n",
      "iteration 129, dc_loss: 1.426100492477417, tv_loss: 0.016788864508271217\n",
      "iteration 130, dc_loss: 1.4195185899734497, tv_loss: 0.016921954229474068\n",
      "iteration 131, dc_loss: 1.4131759405136108, tv_loss: 0.01693829707801342\n",
      "iteration 132, dc_loss: 1.4068350791931152, tv_loss: 0.017099393531680107\n",
      "iteration 133, dc_loss: 1.4006630182266235, tv_loss: 0.0170203298330307\n",
      "iteration 134, dc_loss: 1.3944023847579956, tv_loss: 0.01718701422214508\n",
      "iteration 135, dc_loss: 1.3879505395889282, tv_loss: 0.017107119783759117\n",
      "iteration 136, dc_loss: 1.3811593055725098, tv_loss: 0.017194826155900955\n",
      "iteration 137, dc_loss: 1.3742636442184448, tv_loss: 0.017430009320378304\n",
      "iteration 138, dc_loss: 1.3679616451263428, tv_loss: 0.017532259225845337\n",
      "iteration 139, dc_loss: 1.3617323637008667, tv_loss: 0.017445189878344536\n",
      "iteration 140, dc_loss: 1.355499505996704, tv_loss: 0.017445102334022522\n",
      "iteration 141, dc_loss: 1.3491487503051758, tv_loss: 0.017566794529557228\n",
      "iteration 142, dc_loss: 1.3428162336349487, tv_loss: 0.01790025644004345\n",
      "iteration 143, dc_loss: 1.3367021083831787, tv_loss: 0.01768689788877964\n",
      "iteration 144, dc_loss: 1.3304885625839233, tv_loss: 0.01778229884803295\n",
      "iteration 145, dc_loss: 1.3243144750595093, tv_loss: 0.017889264971017838\n",
      "iteration 146, dc_loss: 1.3182438611984253, tv_loss: 0.018027514219284058\n",
      "iteration 147, dc_loss: 1.3121623992919922, tv_loss: 0.017991259694099426\n",
      "iteration 148, dc_loss: 1.3062152862548828, tv_loss: 0.017983894795179367\n",
      "iteration 149, dc_loss: 1.3002897500991821, tv_loss: 0.018210120499134064\n",
      "iteration 150, dc_loss: 1.2944227457046509, tv_loss: 0.018295634537935257\n",
      "iteration 151, dc_loss: 1.2885396480560303, tv_loss: 0.018234161660075188\n",
      "iteration 152, dc_loss: 1.2824136018753052, tv_loss: 0.01831180602312088\n",
      "iteration 153, dc_loss: 1.2763029336929321, tv_loss: 0.01847633719444275\n",
      "iteration 154, dc_loss: 1.270389199256897, tv_loss: 0.018409043550491333\n",
      "iteration 155, dc_loss: 1.2645126581192017, tv_loss: 0.01847761869430542\n",
      "iteration 156, dc_loss: 1.2587335109710693, tv_loss: 0.018525755032896996\n",
      "iteration 157, dc_loss: 1.2529857158660889, tv_loss: 0.018651172518730164\n",
      "iteration 158, dc_loss: 1.2472214698791504, tv_loss: 0.018756629899144173\n",
      "iteration 159, dc_loss: 1.2415839433670044, tv_loss: 0.018710391595959663\n",
      "iteration 160, dc_loss: 1.2357579469680786, tv_loss: 0.0188994649797678\n",
      "iteration 161, dc_loss: 1.2302074432373047, tv_loss: 0.01880117692053318\n",
      "iteration 162, dc_loss: 1.224461317062378, tv_loss: 0.018955809995532036\n",
      "iteration 163, dc_loss: 1.2189228534698486, tv_loss: 0.018947042524814606\n",
      "iteration 164, dc_loss: 1.2133615016937256, tv_loss: 0.019036708399653435\n",
      "iteration 165, dc_loss: 1.2078505754470825, tv_loss: 0.01912054233253002\n",
      "iteration 166, dc_loss: 1.2023983001708984, tv_loss: 0.01915598288178444\n",
      "iteration 167, dc_loss: 1.1968108415603638, tv_loss: 0.0192511435598135\n",
      "iteration 168, dc_loss: 1.1912692785263062, tv_loss: 0.019255520775914192\n",
      "iteration 169, dc_loss: 1.1856749057769775, tv_loss: 0.019349854439496994\n",
      "iteration 170, dc_loss: 1.1802842617034912, tv_loss: 0.01933758705854416\n",
      "iteration 171, dc_loss: 1.1749439239501953, tv_loss: 0.01943429931998253\n",
      "iteration 172, dc_loss: 1.169875144958496, tv_loss: 0.0194168109446764\n",
      "iteration 173, dc_loss: 1.1649627685546875, tv_loss: 0.019585130736231804\n",
      "iteration 174, dc_loss: 1.1600719690322876, tv_loss: 0.019489675760269165\n",
      "iteration 175, dc_loss: 1.154651403427124, tv_loss: 0.01969234272837639\n",
      "iteration 176, dc_loss: 1.1487481594085693, tv_loss: 0.019594918936491013\n",
      "iteration 177, dc_loss: 1.1431468725204468, tv_loss: 0.019672613590955734\n",
      "iteration 178, dc_loss: 1.13817298412323, tv_loss: 0.019892636686563492\n",
      "iteration 179, dc_loss: 1.1333776712417603, tv_loss: 0.019749341532588005\n",
      "iteration 180, dc_loss: 1.127756953239441, tv_loss: 0.01998668909072876\n",
      "iteration 181, dc_loss: 1.122347116470337, tv_loss: 0.019859962165355682\n",
      "iteration 182, dc_loss: 1.1171208620071411, tv_loss: 0.019999206066131592\n",
      "iteration 183, dc_loss: 1.1121389865875244, tv_loss: 0.020109813660383224\n",
      "iteration 184, dc_loss: 1.1070865392684937, tv_loss: 0.019994430243968964\n",
      "iteration 185, dc_loss: 1.1016813516616821, tv_loss: 0.020133372396230698\n",
      "iteration 186, dc_loss: 1.0965958833694458, tv_loss: 0.020083488896489143\n",
      "iteration 187, dc_loss: 1.0916467905044556, tv_loss: 0.02022836171090603\n",
      "iteration 188, dc_loss: 1.0868128538131714, tv_loss: 0.020355258136987686\n",
      "iteration 189, dc_loss: 1.0817880630493164, tv_loss: 0.02030343748629093\n",
      "iteration 190, dc_loss: 1.076560139656067, tv_loss: 0.020429544150829315\n",
      "iteration 191, dc_loss: 1.0716643333435059, tv_loss: 0.020364388823509216\n",
      "iteration 192, dc_loss: 1.0666568279266357, tv_loss: 0.02056148275732994\n",
      "iteration 193, dc_loss: 1.0619789361953735, tv_loss: 0.02038857713341713\n",
      "iteration 194, dc_loss: 1.0570541620254517, tv_loss: 0.020668040961027145\n",
      "iteration 195, dc_loss: 1.0528262853622437, tv_loss: 0.02044059894979\n",
      "iteration 196, dc_loss: 1.0482004880905151, tv_loss: 0.020939666777849197\n",
      "iteration 197, dc_loss: 1.04339599609375, tv_loss: 0.02044672891497612\n",
      "iteration 198, dc_loss: 1.0380618572235107, tv_loss: 0.020793749019503593\n",
      "iteration 199, dc_loss: 1.0330607891082764, tv_loss: 0.020730094984173775\n",
      "iteration 200, dc_loss: 1.0291105508804321, tv_loss: 0.020629102364182472\n",
      "iteration 201, dc_loss: 1.0243475437164307, tv_loss: 0.02113056182861328\n",
      "iteration 202, dc_loss: 1.019270658493042, tv_loss: 0.02091730758547783\n",
      "iteration 203, dc_loss: 1.015006184577942, tv_loss: 0.020889732986688614\n",
      "iteration 204, dc_loss: 1.0098921060562134, tv_loss: 0.021143082529306412\n",
      "iteration 205, dc_loss: 1.0058997869491577, tv_loss: 0.02075701579451561\n",
      "iteration 206, dc_loss: 1.000871181488037, tv_loss: 0.021249400451779366\n",
      "iteration 207, dc_loss: 0.9959497451782227, tv_loss: 0.02111741341650486\n",
      "iteration 208, dc_loss: 0.9920324683189392, tv_loss: 0.02099403366446495\n",
      "iteration 209, dc_loss: 0.9869171977043152, tv_loss: 0.02140714041888714\n",
      "iteration 210, dc_loss: 0.9824144244194031, tv_loss: 0.02132805995643139\n",
      "iteration 211, dc_loss: 0.9777811765670776, tv_loss: 0.021179456263780594\n",
      "iteration 212, dc_loss: 0.9729010462760925, tv_loss: 0.021282481029629707\n",
      "iteration 213, dc_loss: 0.9684903621673584, tv_loss: 0.021320993080735207\n",
      "iteration 214, dc_loss: 0.9638854265213013, tv_loss: 0.021439094096422195\n",
      "iteration 215, dc_loss: 0.9595968127250671, tv_loss: 0.021496035158634186\n",
      "iteration 216, dc_loss: 0.9548926949501038, tv_loss: 0.021508174017071724\n",
      "iteration 217, dc_loss: 0.9507360458374023, tv_loss: 0.02139509841799736\n",
      "iteration 218, dc_loss: 0.9458784461021423, tv_loss: 0.02173648029565811\n",
      "iteration 219, dc_loss: 0.9418804049491882, tv_loss: 0.021474013105034828\n",
      "iteration 220, dc_loss: 0.9378833174705505, tv_loss: 0.021751446649432182\n",
      "iteration 221, dc_loss: 0.933602511882782, tv_loss: 0.021637214347720146\n",
      "iteration 222, dc_loss: 0.9295021891593933, tv_loss: 0.021687528118491173\n",
      "iteration 223, dc_loss: 0.9250213503837585, tv_loss: 0.02188577875494957\n",
      "iteration 224, dc_loss: 0.9208976030349731, tv_loss: 0.0215549748390913\n",
      "iteration 225, dc_loss: 0.9163246154785156, tv_loss: 0.021993739530444145\n",
      "iteration 226, dc_loss: 0.9121382832527161, tv_loss: 0.021836616098880768\n",
      "iteration 227, dc_loss: 0.9078700542449951, tv_loss: 0.021810809150338173\n",
      "iteration 228, dc_loss: 0.9033103585243225, tv_loss: 0.02214706875383854\n",
      "iteration 229, dc_loss: 0.8988894820213318, tv_loss: 0.021877773106098175\n",
      "iteration 230, dc_loss: 0.8949419856071472, tv_loss: 0.02195640839636326\n",
      "iteration 231, dc_loss: 0.8904241323471069, tv_loss: 0.022153569385409355\n",
      "iteration 232, dc_loss: 0.8864822387695312, tv_loss: 0.02216400019824505\n",
      "iteration 233, dc_loss: 0.8825796842575073, tv_loss: 0.022101514041423798\n",
      "iteration 234, dc_loss: 0.8786178827285767, tv_loss: 0.02229716256260872\n",
      "iteration 235, dc_loss: 0.8743868470191956, tv_loss: 0.022053781896829605\n",
      "iteration 236, dc_loss: 0.8702950477600098, tv_loss: 0.02231941558420658\n",
      "iteration 237, dc_loss: 0.866208016872406, tv_loss: 0.02209528721868992\n",
      "iteration 238, dc_loss: 0.8616970777511597, tv_loss: 0.022486723959445953\n",
      "iteration 239, dc_loss: 0.8576297163963318, tv_loss: 0.02235044352710247\n",
      "iteration 240, dc_loss: 0.8535963892936707, tv_loss: 0.022390464320778847\n",
      "iteration 241, dc_loss: 0.8496198654174805, tv_loss: 0.022464986890554428\n",
      "iteration 242, dc_loss: 0.8456031084060669, tv_loss: 0.022383952513337135\n",
      "iteration 243, dc_loss: 0.8419910073280334, tv_loss: 0.02277442440390587\n",
      "iteration 244, dc_loss: 0.8377484679222107, tv_loss: 0.02243926003575325\n",
      "iteration 245, dc_loss: 0.8336479067802429, tv_loss: 0.022599944844841957\n",
      "iteration 246, dc_loss: 0.829777717590332, tv_loss: 0.02272048406302929\n",
      "iteration 247, dc_loss: 0.8263360261917114, tv_loss: 0.022565359249711037\n",
      "iteration 248, dc_loss: 0.8224180936813354, tv_loss: 0.022930707782506943\n",
      "iteration 249, dc_loss: 0.8188925385475159, tv_loss: 0.022546866908669472\n",
      "iteration 250, dc_loss: 0.8154228329658508, tv_loss: 0.022932568565011024\n",
      "iteration 251, dc_loss: 0.8111844062805176, tv_loss: 0.022760096937417984\n",
      "iteration 252, dc_loss: 0.8070796132087708, tv_loss: 0.022778881713747978\n",
      "iteration 253, dc_loss: 0.8036841750144958, tv_loss: 0.023137573152780533\n",
      "iteration 254, dc_loss: 0.8007376194000244, tv_loss: 0.022725483402609825\n",
      "iteration 255, dc_loss: 0.7965889573097229, tv_loss: 0.023239651694893837\n",
      "iteration 256, dc_loss: 0.7922319769859314, tv_loss: 0.02294202335178852\n",
      "iteration 257, dc_loss: 0.7887230515480042, tv_loss: 0.02289765328168869\n",
      "iteration 258, dc_loss: 0.7853130102157593, tv_loss: 0.02332933060824871\n",
      "iteration 259, dc_loss: 0.7815579175949097, tv_loss: 0.02287515066564083\n",
      "iteration 260, dc_loss: 0.777833878993988, tv_loss: 0.023213645443320274\n",
      "iteration 261, dc_loss: 0.773635983467102, tv_loss: 0.02330293320119381\n",
      "iteration 262, dc_loss: 0.7708386182785034, tv_loss: 0.0230390764772892\n",
      "iteration 263, dc_loss: 0.7670855522155762, tv_loss: 0.023515619337558746\n",
      "iteration 264, dc_loss: 0.7631739974021912, tv_loss: 0.023115728050470352\n",
      "iteration 265, dc_loss: 0.7596444487571716, tv_loss: 0.023308780044317245\n",
      "iteration 266, dc_loss: 0.7557445764541626, tv_loss: 0.023587385192513466\n",
      "iteration 267, dc_loss: 0.7529644966125488, tv_loss: 0.023114480078220367\n",
      "iteration 268, dc_loss: 0.74880051612854, tv_loss: 0.023509124293923378\n",
      "iteration 269, dc_loss: 0.7451977729797363, tv_loss: 0.023494571447372437\n",
      "iteration 270, dc_loss: 0.742107629776001, tv_loss: 0.023405499756336212\n",
      "iteration 271, dc_loss: 0.7380845546722412, tv_loss: 0.02365492470562458\n",
      "iteration 272, dc_loss: 0.735008180141449, tv_loss: 0.023443147540092468\n",
      "iteration 273, dc_loss: 0.7313302159309387, tv_loss: 0.0235883891582489\n",
      "iteration 274, dc_loss: 0.7277599573135376, tv_loss: 0.0236782506108284\n",
      "iteration 275, dc_loss: 0.7243296504020691, tv_loss: 0.02365845814347267\n",
      "iteration 276, dc_loss: 0.7207774519920349, tv_loss: 0.0236455537378788\n",
      "iteration 277, dc_loss: 0.7176376581192017, tv_loss: 0.023562844842672348\n",
      "iteration 278, dc_loss: 0.7140089273452759, tv_loss: 0.02388572134077549\n",
      "iteration 279, dc_loss: 0.7108177542686462, tv_loss: 0.023812653496861458\n",
      "iteration 280, dc_loss: 0.7074459195137024, tv_loss: 0.02380640245974064\n",
      "iteration 281, dc_loss: 0.7040117383003235, tv_loss: 0.023877128958702087\n",
      "iteration 282, dc_loss: 0.7009527087211609, tv_loss: 0.023846229538321495\n",
      "iteration 283, dc_loss: 0.6975639462471008, tv_loss: 0.024061139672994614\n",
      "iteration 284, dc_loss: 0.6943122744560242, tv_loss: 0.023909995332360268\n",
      "iteration 285, dc_loss: 0.6910018920898438, tv_loss: 0.023991333320736885\n",
      "iteration 286, dc_loss: 0.6877984404563904, tv_loss: 0.024128755554556847\n",
      "iteration 287, dc_loss: 0.6848485469818115, tv_loss: 0.023934602737426758\n",
      "iteration 288, dc_loss: 0.681577205657959, tv_loss: 0.02426229976117611\n",
      "iteration 289, dc_loss: 0.6786374449729919, tv_loss: 0.023997778072953224\n",
      "iteration 290, dc_loss: 0.6754151582717896, tv_loss: 0.02425171062350273\n",
      "iteration 291, dc_loss: 0.67194002866745, tv_loss: 0.024111321195960045\n",
      "iteration 292, dc_loss: 0.6685925722122192, tv_loss: 0.02422078512609005\n",
      "iteration 293, dc_loss: 0.6653910875320435, tv_loss: 0.02433888427913189\n",
      "iteration 294, dc_loss: 0.6626280546188354, tv_loss: 0.02418055199086666\n",
      "iteration 295, dc_loss: 0.6595061421394348, tv_loss: 0.024578802287578583\n",
      "iteration 296, dc_loss: 0.6568442583084106, tv_loss: 0.024167411029338837\n",
      "iteration 297, dc_loss: 0.653594970703125, tv_loss: 0.02455269731581211\n",
      "iteration 298, dc_loss: 0.6503866910934448, tv_loss: 0.02432229183614254\n",
      "iteration 299, dc_loss: 0.6470314860343933, tv_loss: 0.02446158044040203\n",
      "iteration 300, dc_loss: 0.6437257528305054, tv_loss: 0.024566326290369034\n",
      "iteration 301, dc_loss: 0.6408844590187073, tv_loss: 0.02436874620616436\n",
      "iteration 302, dc_loss: 0.6376586556434631, tv_loss: 0.024680685251951218\n",
      "iteration 303, dc_loss: 0.634972095489502, tv_loss: 0.02443067729473114\n",
      "iteration 304, dc_loss: 0.6318610310554504, tv_loss: 0.024726351723074913\n",
      "iteration 305, dc_loss: 0.6290431022644043, tv_loss: 0.02457687072455883\n",
      "iteration 306, dc_loss: 0.6261909008026123, tv_loss: 0.024643167853355408\n",
      "iteration 307, dc_loss: 0.623172402381897, tv_loss: 0.02469860017299652\n",
      "iteration 308, dc_loss: 0.6202839016914368, tv_loss: 0.024647844955325127\n",
      "iteration 309, dc_loss: 0.6170459389686584, tv_loss: 0.02487283945083618\n",
      "iteration 310, dc_loss: 0.6141642928123474, tv_loss: 0.024669280275702477\n",
      "iteration 311, dc_loss: 0.6107833385467529, tv_loss: 0.02492128312587738\n",
      "iteration 312, dc_loss: 0.6080245971679688, tv_loss: 0.024692445993423462\n",
      "iteration 313, dc_loss: 0.6051616072654724, tv_loss: 0.025043215602636337\n",
      "iteration 314, dc_loss: 0.603415310382843, tv_loss: 0.024690035730600357\n",
      "iteration 315, dc_loss: 0.6029053330421448, tv_loss: 0.025304371491074562\n",
      "iteration 316, dc_loss: 0.5987134575843811, tv_loss: 0.024618634954094887\n",
      "iteration 317, dc_loss: 0.5941416025161743, tv_loss: 0.025041023269295692\n",
      "iteration 318, dc_loss: 0.5912278294563293, tv_loss: 0.024928437545895576\n",
      "iteration 319, dc_loss: 0.5894008874893188, tv_loss: 0.024826206266880035\n",
      "iteration 320, dc_loss: 0.5868229866027832, tv_loss: 0.025212198495864868\n",
      "iteration 321, dc_loss: 0.5831171274185181, tv_loss: 0.024766061455011368\n",
      "iteration 322, dc_loss: 0.5800870060920715, tv_loss: 0.02513939142227173\n",
      "iteration 323, dc_loss: 0.577662467956543, tv_loss: 0.02516220510005951\n",
      "iteration 324, dc_loss: 0.5743680000305176, tv_loss: 0.02525685913860798\n",
      "iteration 325, dc_loss: 0.5719180107116699, tv_loss: 0.025065775960683823\n",
      "iteration 326, dc_loss: 0.5685783624649048, tv_loss: 0.025256404653191566\n",
      "iteration 327, dc_loss: 0.5660232901573181, tv_loss: 0.025255434215068817\n",
      "iteration 328, dc_loss: 0.5632567405700684, tv_loss: 0.025357553735375404\n",
      "iteration 329, dc_loss: 0.560562252998352, tv_loss: 0.025272062048316002\n",
      "iteration 330, dc_loss: 0.5577758550643921, tv_loss: 0.025313152000308037\n",
      "iteration 331, dc_loss: 0.5548850893974304, tv_loss: 0.025200417265295982\n",
      "iteration 332, dc_loss: 0.552211344242096, tv_loss: 0.025609206408262253\n",
      "iteration 333, dc_loss: 0.5500575304031372, tv_loss: 0.025306330993771553\n",
      "iteration 334, dc_loss: 0.5468906760215759, tv_loss: 0.025616994127631187\n",
      "iteration 335, dc_loss: 0.5445259809494019, tv_loss: 0.02535516396164894\n",
      "iteration 336, dc_loss: 0.5413734316825867, tv_loss: 0.02566247433423996\n",
      "iteration 337, dc_loss: 0.5391618013381958, tv_loss: 0.025423331186175346\n",
      "iteration 338, dc_loss: 0.5367874503135681, tv_loss: 0.02569049596786499\n",
      "iteration 339, dc_loss: 0.5341144800186157, tv_loss: 0.02551122196018696\n",
      "iteration 340, dc_loss: 0.5313422679901123, tv_loss: 0.025779809802770615\n",
      "iteration 341, dc_loss: 0.5289884805679321, tv_loss: 0.02555190771818161\n",
      "iteration 342, dc_loss: 0.5260231494903564, tv_loss: 0.025824205949902534\n",
      "iteration 343, dc_loss: 0.5238606333732605, tv_loss: 0.0255160853266716\n",
      "iteration 344, dc_loss: 0.5210358500480652, tv_loss: 0.025951765477657318\n",
      "iteration 345, dc_loss: 0.5186848044395447, tv_loss: 0.02572222799062729\n",
      "iteration 346, dc_loss: 0.5159453749656677, tv_loss: 0.025844566524028778\n",
      "iteration 347, dc_loss: 0.5134092569351196, tv_loss: 0.025851568207144737\n",
      "iteration 348, dc_loss: 0.511067807674408, tv_loss: 0.025866568088531494\n",
      "iteration 349, dc_loss: 0.5086192488670349, tv_loss: 0.02600819058716297\n",
      "iteration 350, dc_loss: 0.5064300298690796, tv_loss: 0.02580082044005394\n",
      "iteration 351, dc_loss: 0.5038843750953674, tv_loss: 0.026167087256908417\n",
      "iteration 352, dc_loss: 0.5024433135986328, tv_loss: 0.025718633085489273\n",
      "iteration 353, dc_loss: 0.5014222264289856, tv_loss: 0.026478784158825874\n",
      "iteration 354, dc_loss: 0.5002858638763428, tv_loss: 0.025586720556020737\n",
      "iteration 355, dc_loss: 0.4991951584815979, tv_loss: 0.026515765115618706\n",
      "iteration 356, dc_loss: 0.4941466152667999, tv_loss: 0.02587296813726425\n",
      "iteration 357, dc_loss: 0.491768479347229, tv_loss: 0.025876779109239578\n",
      "iteration 358, dc_loss: 0.4897741675376892, tv_loss: 0.02650783769786358\n",
      "iteration 359, dc_loss: 0.48724469542503357, tv_loss: 0.025884751230478287\n",
      "iteration 360, dc_loss: 0.48443129658699036, tv_loss: 0.02621382847428322\n",
      "iteration 361, dc_loss: 0.4814881682395935, tv_loss: 0.02623654156923294\n",
      "iteration 362, dc_loss: 0.47991788387298584, tv_loss: 0.025976095348596573\n",
      "iteration 363, dc_loss: 0.4772808253765106, tv_loss: 0.026392290368676186\n",
      "iteration 364, dc_loss: 0.4750344455242157, tv_loss: 0.026153361424803734\n",
      "iteration 365, dc_loss: 0.4721287786960602, tv_loss: 0.02635045535862446\n",
      "iteration 366, dc_loss: 0.4701409339904785, tv_loss: 0.026552636176347733\n",
      "iteration 367, dc_loss: 0.4681127071380615, tv_loss: 0.026165936142206192\n",
      "iteration 368, dc_loss: 0.4653831124305725, tv_loss: 0.026357999071478844\n",
      "iteration 369, dc_loss: 0.4632754623889923, tv_loss: 0.0263900775462389\n",
      "iteration 370, dc_loss: 0.46102049946784973, tv_loss: 0.026356007903814316\n",
      "iteration 371, dc_loss: 0.4587186574935913, tv_loss: 0.02652263641357422\n",
      "iteration 372, dc_loss: 0.4566301703453064, tv_loss: 0.026589414104819298\n",
      "iteration 373, dc_loss: 0.45432212948799133, tv_loss: 0.02654661424458027\n",
      "iteration 374, dc_loss: 0.45223191380500793, tv_loss: 0.02648123726248741\n",
      "iteration 375, dc_loss: 0.44993841648101807, tv_loss: 0.02650163322687149\n",
      "iteration 376, dc_loss: 0.447900652885437, tv_loss: 0.026502937078475952\n",
      "iteration 377, dc_loss: 0.44563764333724976, tv_loss: 0.02665133960545063\n",
      "iteration 378, dc_loss: 0.4435710906982422, tv_loss: 0.026789376512169838\n",
      "iteration 379, dc_loss: 0.44145023822784424, tv_loss: 0.026682430878281593\n",
      "iteration 380, dc_loss: 0.43939366936683655, tv_loss: 0.026588723063468933\n",
      "iteration 381, dc_loss: 0.4371996223926544, tv_loss: 0.026677237823605537\n",
      "iteration 382, dc_loss: 0.43520063161849976, tv_loss: 0.026622368022799492\n",
      "iteration 383, dc_loss: 0.4330201745033264, tv_loss: 0.026883680373430252\n",
      "iteration 384, dc_loss: 0.4311799705028534, tv_loss: 0.026836756616830826\n",
      "iteration 385, dc_loss: 0.4290812313556671, tv_loss: 0.026895374059677124\n",
      "iteration 386, dc_loss: 0.42766812443733215, tv_loss: 0.026668058708310127\n",
      "iteration 387, dc_loss: 0.42702531814575195, tv_loss: 0.02713610976934433\n",
      "iteration 388, dc_loss: 0.4255448281764984, tv_loss: 0.026570379734039307\n",
      "iteration 389, dc_loss: 0.4230387806892395, tv_loss: 0.027201242744922638\n",
      "iteration 390, dc_loss: 0.4207304120063782, tv_loss: 0.026730036363005638\n",
      "iteration 391, dc_loss: 0.41798967123031616, tv_loss: 0.027274353429675102\n",
      "iteration 392, dc_loss: 0.4164462089538574, tv_loss: 0.027029071003198624\n",
      "iteration 393, dc_loss: 0.41456782817840576, tv_loss: 0.026967724785208702\n",
      "iteration 394, dc_loss: 0.41273146867752075, tv_loss: 0.02730889804661274\n",
      "iteration 395, dc_loss: 0.41066667437553406, tv_loss: 0.027049042284488678\n",
      "iteration 396, dc_loss: 0.408380925655365, tv_loss: 0.027265749871730804\n",
      "iteration 397, dc_loss: 0.4066452383995056, tv_loss: 0.027126261964440346\n",
      "iteration 398, dc_loss: 0.4045916795730591, tv_loss: 0.027423633262515068\n",
      "iteration 399, dc_loss: 0.40269941091537476, tv_loss: 0.02712981402873993\n",
      "iteration 400, dc_loss: 0.4007347524166107, tv_loss: 0.02729102596640587\n",
      "iteration 401, dc_loss: 0.39879724383354187, tv_loss: 0.027272123843431473\n",
      "iteration 402, dc_loss: 0.3969990611076355, tv_loss: 0.027198990806937218\n",
      "iteration 403, dc_loss: 0.39537984132766724, tv_loss: 0.02746276557445526\n",
      "iteration 404, dc_loss: 0.3939387798309326, tv_loss: 0.027239181101322174\n",
      "iteration 405, dc_loss: 0.3924204707145691, tv_loss: 0.027163490653038025\n",
      "iteration 406, dc_loss: 0.3908466100692749, tv_loss: 0.027449624612927437\n",
      "iteration 407, dc_loss: 0.38930222392082214, tv_loss: 0.027322061359882355\n",
      "iteration 408, dc_loss: 0.387977659702301, tv_loss: 0.027229709550738335\n",
      "iteration 409, dc_loss: 0.38629186153411865, tv_loss: 0.027527043595910072\n",
      "iteration 410, dc_loss: 0.3848350942134857, tv_loss: 0.027379021048545837\n",
      "iteration 411, dc_loss: 0.383537620306015, tv_loss: 0.027333557605743408\n",
      "iteration 412, dc_loss: 0.38188862800598145, tv_loss: 0.027614101767539978\n",
      "iteration 413, dc_loss: 0.3805030584335327, tv_loss: 0.02740195393562317\n",
      "iteration 414, dc_loss: 0.3791000247001648, tv_loss: 0.027486125007271767\n",
      "iteration 415, dc_loss: 0.37755563855171204, tv_loss: 0.02759524993598461\n",
      "iteration 416, dc_loss: 0.37620851397514343, tv_loss: 0.027428247034549713\n",
      "iteration 417, dc_loss: 0.37479284405708313, tv_loss: 0.027535703033208847\n",
      "iteration 418, dc_loss: 0.37331023812294006, tv_loss: 0.027618946507573128\n",
      "iteration 419, dc_loss: 0.37191304564476013, tv_loss: 0.027540260925889015\n",
      "iteration 420, dc_loss: 0.3705388605594635, tv_loss: 0.027552004903554916\n",
      "iteration 421, dc_loss: 0.36915427446365356, tv_loss: 0.027706915512681007\n",
      "iteration 422, dc_loss: 0.3677242398262024, tv_loss: 0.027646921575069427\n",
      "iteration 423, dc_loss: 0.3664253056049347, tv_loss: 0.027686510235071182\n",
      "iteration 424, dc_loss: 0.3649924695491791, tv_loss: 0.027687864378094673\n",
      "iteration 425, dc_loss: 0.3635922372341156, tv_loss: 0.027689959853887558\n",
      "iteration 426, dc_loss: 0.36228448152542114, tv_loss: 0.02771044708788395\n",
      "iteration 427, dc_loss: 0.36091500520706177, tv_loss: 0.027758382260799408\n",
      "iteration 428, dc_loss: 0.3595725893974304, tv_loss: 0.027712415903806686\n",
      "iteration 429, dc_loss: 0.3582397699356079, tv_loss: 0.02775273099541664\n",
      "iteration 430, dc_loss: 0.3568866550922394, tv_loss: 0.027802782133221626\n",
      "iteration 431, dc_loss: 0.3555074632167816, tv_loss: 0.027805790305137634\n",
      "iteration 432, dc_loss: 0.3542691469192505, tv_loss: 0.02772599086165428\n",
      "iteration 433, dc_loss: 0.35288962721824646, tv_loss: 0.02785753458738327\n",
      "iteration 434, dc_loss: 0.3516444265842438, tv_loss: 0.027773462235927582\n",
      "iteration 435, dc_loss: 0.35026925802230835, tv_loss: 0.02787439338862896\n",
      "iteration 436, dc_loss: 0.349059522151947, tv_loss: 0.027828948572278023\n",
      "iteration 437, dc_loss: 0.34767329692840576, tv_loss: 0.027952095493674278\n",
      "iteration 438, dc_loss: 0.346658855676651, tv_loss: 0.02778339758515358\n",
      "iteration 439, dc_loss: 0.34531867504119873, tv_loss: 0.02806832455098629\n",
      "iteration 440, dc_loss: 0.3443891704082489, tv_loss: 0.02776077389717102\n",
      "iteration 441, dc_loss: 0.34300607442855835, tv_loss: 0.028207892552018166\n",
      "iteration 442, dc_loss: 0.3420819044113159, tv_loss: 0.027704685926437378\n",
      "iteration 443, dc_loss: 0.3405490517616272, tv_loss: 0.02817360870540142\n",
      "iteration 444, dc_loss: 0.33923768997192383, tv_loss: 0.0278268251568079\n",
      "iteration 445, dc_loss: 0.33752092719078064, tv_loss: 0.0281857680529356\n",
      "iteration 446, dc_loss: 0.33622851967811584, tv_loss: 0.028093060478568077\n",
      "iteration 447, dc_loss: 0.33520838618278503, tv_loss: 0.027931207790970802\n",
      "iteration 448, dc_loss: 0.3339252769947052, tv_loss: 0.02814938686788082\n",
      "iteration 449, dc_loss: 0.33294814825057983, tv_loss: 0.027928855270147324\n",
      "iteration 450, dc_loss: 0.33143773674964905, tv_loss: 0.028377246111631393\n",
      "iteration 451, dc_loss: 0.33032068610191345, tv_loss: 0.028042856603860855\n",
      "iteration 452, dc_loss: 0.3287809193134308, tv_loss: 0.02822604402899742\n",
      "iteration 453, dc_loss: 0.3276815116405487, tv_loss: 0.028165774419903755\n",
      "iteration 454, dc_loss: 0.32643234729766846, tv_loss: 0.02825489267706871\n",
      "iteration 455, dc_loss: 0.32519063353538513, tv_loss: 0.028289297595620155\n",
      "iteration 456, dc_loss: 0.32439544796943665, tv_loss: 0.0280902162194252\n",
      "iteration 457, dc_loss: 0.32292571663856506, tv_loss: 0.028496582061052322\n",
      "iteration 458, dc_loss: 0.3219242990016937, tv_loss: 0.02807566337287426\n",
      "iteration 459, dc_loss: 0.3204649090766907, tv_loss: 0.028406785801053047\n",
      "iteration 460, dc_loss: 0.31932324171066284, tv_loss: 0.0284134354442358\n",
      "iteration 461, dc_loss: 0.31807348132133484, tv_loss: 0.028282692655920982\n",
      "iteration 462, dc_loss: 0.3168502449989319, tv_loss: 0.028490180149674416\n",
      "iteration 463, dc_loss: 0.3156064748764038, tv_loss: 0.028414864093065262\n",
      "iteration 464, dc_loss: 0.31446099281311035, tv_loss: 0.02855444885790348\n",
      "iteration 465, dc_loss: 0.3133915960788727, tv_loss: 0.028389232233166695\n",
      "iteration 466, dc_loss: 0.3122110366821289, tv_loss: 0.028401030227541924\n",
      "iteration 467, dc_loss: 0.3111184239387512, tv_loss: 0.028400901705026627\n",
      "iteration 468, dc_loss: 0.30991294980049133, tv_loss: 0.028654402121901512\n",
      "iteration 469, dc_loss: 0.3090173006057739, tv_loss: 0.028331760317087173\n",
      "iteration 470, dc_loss: 0.30781498551368713, tv_loss: 0.028751341626048088\n",
      "iteration 471, dc_loss: 0.3071542978286743, tv_loss: 0.028317783027887344\n",
      "iteration 472, dc_loss: 0.30574825406074524, tv_loss: 0.028772976249456406\n",
      "iteration 473, dc_loss: 0.30497780442237854, tv_loss: 0.028320981189608574\n",
      "iteration 474, dc_loss: 0.3036469519138336, tv_loss: 0.02892947755753994\n",
      "iteration 475, dc_loss: 0.30246543884277344, tv_loss: 0.02849014662206173\n",
      "iteration 476, dc_loss: 0.3009684383869171, tv_loss: 0.028869768604636192\n",
      "iteration 477, dc_loss: 0.2996898293495178, tv_loss: 0.028595516458153725\n",
      "iteration 478, dc_loss: 0.29867830872535706, tv_loss: 0.028746310621500015\n",
      "iteration 479, dc_loss: 0.29753148555755615, tv_loss: 0.028652559965848923\n",
      "iteration 480, dc_loss: 0.29667773842811584, tv_loss: 0.028761766850948334\n",
      "iteration 481, dc_loss: 0.2955876588821411, tv_loss: 0.028791552409529686\n",
      "iteration 482, dc_loss: 0.2946043610572815, tv_loss: 0.028635691851377487\n",
      "iteration 483, dc_loss: 0.29318559169769287, tv_loss: 0.02893662080168724\n",
      "iteration 484, dc_loss: 0.2921414077281952, tv_loss: 0.028641352429986\n",
      "iteration 485, dc_loss: 0.29089245200157166, tv_loss: 0.0288790762424469\n",
      "iteration 486, dc_loss: 0.2898767292499542, tv_loss: 0.028709840029478073\n",
      "iteration 487, dc_loss: 0.2887681722640991, tv_loss: 0.028769109398126602\n",
      "iteration 488, dc_loss: 0.2877262830734253, tv_loss: 0.028833508491516113\n",
      "iteration 489, dc_loss: 0.2867477834224701, tv_loss: 0.02873784862458706\n",
      "iteration 490, dc_loss: 0.2856113910675049, tv_loss: 0.028876161202788353\n",
      "iteration 491, dc_loss: 0.28486907482147217, tv_loss: 0.028626350685954094\n",
      "iteration 492, dc_loss: 0.28361740708351135, tv_loss: 0.02911105751991272\n",
      "iteration 493, dc_loss: 0.2828975021839142, tv_loss: 0.028599578887224197\n",
      "iteration 494, dc_loss: 0.2814377546310425, tv_loss: 0.029018549248576164\n",
      "iteration 495, dc_loss: 0.28064340353012085, tv_loss: 0.028705209493637085\n",
      "iteration 496, dc_loss: 0.27929019927978516, tv_loss: 0.029085665941238403\n",
      "iteration 497, dc_loss: 0.2783435881137848, tv_loss: 0.028860190883278847\n",
      "iteration 498, dc_loss: 0.2771790027618408, tv_loss: 0.028930528089404106\n",
      "iteration 499, dc_loss: 0.2761365473270416, tv_loss: 0.02903180569410324\n",
      "iteration 500, dc_loss: 0.2751164734363556, tv_loss: 0.028973035514354706\n",
      "iteration 501, dc_loss: 0.27405625581741333, tv_loss: 0.028961583971977234\n",
      "iteration 502, dc_loss: 0.2730605900287628, tv_loss: 0.029015839099884033\n",
      "iteration 503, dc_loss: 0.2719501554965973, tv_loss: 0.02906801924109459\n",
      "iteration 504, dc_loss: 0.2710111439228058, tv_loss: 0.0289500392973423\n",
      "iteration 505, dc_loss: 0.2699468731880188, tv_loss: 0.02907564304769039\n",
      "iteration 506, dc_loss: 0.26898112893104553, tv_loss: 0.029087094590067863\n",
      "iteration 507, dc_loss: 0.2680249512195587, tv_loss: 0.029044542461633682\n",
      "iteration 508, dc_loss: 0.26715418696403503, tv_loss: 0.029008567333221436\n",
      "iteration 509, dc_loss: 0.26616498827934265, tv_loss: 0.029254881665110588\n",
      "iteration 510, dc_loss: 0.2657129168510437, tv_loss: 0.028990089893341064\n",
      "iteration 511, dc_loss: 0.2650442123413086, tv_loss: 0.02935614250600338\n",
      "iteration 512, dc_loss: 0.2651473879814148, tv_loss: 0.028768662363290787\n",
      "iteration 513, dc_loss: 0.2648038864135742, tv_loss: 0.029659630730748177\n",
      "iteration 514, dc_loss: 0.2627306580543518, tv_loss: 0.02891283668577671\n",
      "iteration 515, dc_loss: 0.26021748781204224, tv_loss: 0.02926315739750862\n",
      "iteration 516, dc_loss: 0.25949397683143616, tv_loss: 0.029287947341799736\n",
      "iteration 517, dc_loss: 0.2596152722835541, tv_loss: 0.02902342937886715\n",
      "iteration 518, dc_loss: 0.2583412528038025, tv_loss: 0.02948753349483013\n",
      "iteration 519, dc_loss: 0.25674712657928467, tv_loss: 0.029033256694674492\n",
      "iteration 520, dc_loss: 0.2557486593723297, tv_loss: 0.029283670708537102\n",
      "iteration 521, dc_loss: 0.25520509481430054, tv_loss: 0.029530316591262817\n",
      "iteration 522, dc_loss: 0.25439396500587463, tv_loss: 0.029046522453427315\n",
      "iteration 523, dc_loss: 0.2527596652507782, tv_loss: 0.029514048248529434\n",
      "iteration 524, dc_loss: 0.25174009799957275, tv_loss: 0.029354676604270935\n",
      "iteration 525, dc_loss: 0.2514922618865967, tv_loss: 0.029128629714250565\n",
      "iteration 526, dc_loss: 0.2503586709499359, tv_loss: 0.02960820682346821\n",
      "iteration 527, dc_loss: 0.2493274062871933, tv_loss: 0.029153738170862198\n",
      "iteration 528, dc_loss: 0.24826006591320038, tv_loss: 0.029318664222955704\n",
      "iteration 529, dc_loss: 0.24727047979831696, tv_loss: 0.02957308478653431\n",
      "iteration 530, dc_loss: 0.2469077706336975, tv_loss: 0.02908623218536377\n",
      "iteration 531, dc_loss: 0.24544011056423187, tv_loss: 0.029570352286100388\n",
      "iteration 532, dc_loss: 0.24432618916034698, tv_loss: 0.02944043092429638\n",
      "iteration 533, dc_loss: 0.24380819499492645, tv_loss: 0.029247891157865524\n",
      "iteration 534, dc_loss: 0.24274109303951263, tv_loss: 0.029632428660988808\n",
      "iteration 535, dc_loss: 0.24214377999305725, tv_loss: 0.02923237718641758\n",
      "iteration 536, dc_loss: 0.2407430112361908, tv_loss: 0.029573801904916763\n",
      "iteration 537, dc_loss: 0.23993226885795593, tv_loss: 0.029608260840177536\n",
      "iteration 538, dc_loss: 0.23927944898605347, tv_loss: 0.02940857969224453\n",
      "iteration 539, dc_loss: 0.2381858080625534, tv_loss: 0.02972118742763996\n",
      "iteration 540, dc_loss: 0.23742812871932983, tv_loss: 0.0295240581035614\n",
      "iteration 541, dc_loss: 0.23622120916843414, tv_loss: 0.02959781512618065\n",
      "iteration 542, dc_loss: 0.23545019328594208, tv_loss: 0.029682451859116554\n",
      "iteration 543, dc_loss: 0.2348605841398239, tv_loss: 0.02950240485370159\n",
      "iteration 544, dc_loss: 0.2337774783372879, tv_loss: 0.029658427461981773\n",
      "iteration 545, dc_loss: 0.232847660779953, tv_loss: 0.029514603316783905\n",
      "iteration 546, dc_loss: 0.2319084256887436, tv_loss: 0.029701782390475273\n",
      "iteration 547, dc_loss: 0.23114292323589325, tv_loss: 0.029645778238773346\n",
      "iteration 548, dc_loss: 0.23036430776119232, tv_loss: 0.029568737372756004\n",
      "iteration 549, dc_loss: 0.22943921387195587, tv_loss: 0.029829582199454308\n",
      "iteration 550, dc_loss: 0.22865816950798035, tv_loss: 0.029591605067253113\n",
      "iteration 551, dc_loss: 0.22763977944850922, tv_loss: 0.029717838391661644\n",
      "iteration 552, dc_loss: 0.2267325222492218, tv_loss: 0.029741624370217323\n",
      "iteration 553, dc_loss: 0.22626523673534393, tv_loss: 0.02963113784790039\n",
      "iteration 554, dc_loss: 0.2252192199230194, tv_loss: 0.029927276074886322\n",
      "iteration 555, dc_loss: 0.22442743182182312, tv_loss: 0.02964869700372219\n",
      "iteration 556, dc_loss: 0.22358962893486023, tv_loss: 0.02970081754028797\n",
      "iteration 557, dc_loss: 0.22254705429077148, tv_loss: 0.029896868392825127\n",
      "iteration 558, dc_loss: 0.22193382680416107, tv_loss: 0.029722213745117188\n",
      "iteration 559, dc_loss: 0.22103510797023773, tv_loss: 0.029848730191588402\n",
      "iteration 560, dc_loss: 0.2203962802886963, tv_loss: 0.029801761731505394\n",
      "iteration 561, dc_loss: 0.21959203481674194, tv_loss: 0.02995821088552475\n",
      "iteration 562, dc_loss: 0.21863320469856262, tv_loss: 0.029765920713543892\n",
      "iteration 563, dc_loss: 0.21779409050941467, tv_loss: 0.02984810061752796\n",
      "iteration 564, dc_loss: 0.21690873801708221, tv_loss: 0.029986971989274025\n",
      "iteration 565, dc_loss: 0.2162555605173111, tv_loss: 0.02982817403972149\n",
      "iteration 566, dc_loss: 0.21536143124103546, tv_loss: 0.029988739639520645\n",
      "iteration 567, dc_loss: 0.2147430032491684, tv_loss: 0.029811017215251923\n",
      "iteration 568, dc_loss: 0.2137714922428131, tv_loss: 0.030067743733525276\n",
      "iteration 569, dc_loss: 0.21308305859565735, tv_loss: 0.029824642464518547\n",
      "iteration 570, dc_loss: 0.21218514442443848, tv_loss: 0.029936164617538452\n",
      "iteration 571, dc_loss: 0.21140256524085999, tv_loss: 0.029968494549393654\n",
      "iteration 572, dc_loss: 0.21074733138084412, tv_loss: 0.02985425852239132\n",
      "iteration 573, dc_loss: 0.20982269942760468, tv_loss: 0.030050022527575493\n",
      "iteration 574, dc_loss: 0.20918291807174683, tv_loss: 0.029859181493520737\n",
      "iteration 575, dc_loss: 0.2082453966140747, tv_loss: 0.030028555542230606\n",
      "iteration 576, dc_loss: 0.20757099986076355, tv_loss: 0.029924284666776657\n",
      "iteration 577, dc_loss: 0.20667795836925507, tv_loss: 0.03008299693465233\n",
      "iteration 578, dc_loss: 0.20608939230442047, tv_loss: 0.030051955953240395\n",
      "iteration 579, dc_loss: 0.2051975280046463, tv_loss: 0.030118653550744057\n",
      "iteration 580, dc_loss: 0.20462599396705627, tv_loss: 0.02993449568748474\n",
      "iteration 581, dc_loss: 0.20374268293380737, tv_loss: 0.030157050117850304\n",
      "iteration 582, dc_loss: 0.20330774784088135, tv_loss: 0.029887797310948372\n",
      "iteration 583, dc_loss: 0.20228378474712372, tv_loss: 0.030280176550149918\n",
      "iteration 584, dc_loss: 0.2019132822751999, tv_loss: 0.029954180121421814\n",
      "iteration 585, dc_loss: 0.20083196461200714, tv_loss: 0.030301310122013092\n",
      "iteration 586, dc_loss: 0.2002718299627304, tv_loss: 0.029959598556160927\n",
      "iteration 587, dc_loss: 0.19922871887683868, tv_loss: 0.030213935300707817\n",
      "iteration 588, dc_loss: 0.19876247644424438, tv_loss: 0.03001369908452034\n",
      "iteration 589, dc_loss: 0.19815683364868164, tv_loss: 0.030244505032896996\n",
      "iteration 590, dc_loss: 0.19804178178310394, tv_loss: 0.03026161529123783\n",
      "iteration 591, dc_loss: 0.19846133887767792, tv_loss: 0.03026994690299034\n",
      "iteration 592, dc_loss: 0.1986878663301468, tv_loss: 0.03010823018848896\n",
      "iteration 593, dc_loss: 0.1986345797777176, tv_loss: 0.030380627140402794\n",
      "iteration 594, dc_loss: 0.19596664607524872, tv_loss: 0.030123889446258545\n",
      "iteration 595, dc_loss: 0.19378456473350525, tv_loss: 0.030445588752627373\n",
      "iteration 596, dc_loss: 0.19408352673053741, tv_loss: 0.03021148405969143\n",
      "iteration 597, dc_loss: 0.19392754137516022, tv_loss: 0.03032779134809971\n",
      "iteration 598, dc_loss: 0.1923043131828308, tv_loss: 0.030448688194155693\n",
      "iteration 599, dc_loss: 0.19122137129306793, tv_loss: 0.03016599826514721\n",
      "iteration 600, dc_loss: 0.19119110703468323, tv_loss: 0.03045961819589138\n",
      "iteration 601, dc_loss: 0.1905553787946701, tv_loss: 0.030380327254533768\n",
      "iteration 602, dc_loss: 0.18906815350055695, tv_loss: 0.03028799220919609\n",
      "iteration 603, dc_loss: 0.18852251768112183, tv_loss: 0.030467890202999115\n",
      "iteration 604, dc_loss: 0.1885187178850174, tv_loss: 0.03029032237827778\n",
      "iteration 605, dc_loss: 0.18703266978263855, tv_loss: 0.03041882999241352\n",
      "iteration 606, dc_loss: 0.18630002439022064, tv_loss: 0.03034363128244877\n",
      "iteration 607, dc_loss: 0.18615075945854187, tv_loss: 0.030398037284612656\n",
      "iteration 608, dc_loss: 0.18497806787490845, tv_loss: 0.030493328347802162\n",
      "iteration 609, dc_loss: 0.18425872921943665, tv_loss: 0.030336951836943626\n",
      "iteration 610, dc_loss: 0.183831587433815, tv_loss: 0.03041759505867958\n",
      "iteration 611, dc_loss: 0.18308976292610168, tv_loss: 0.0304563008248806\n",
      "iteration 612, dc_loss: 0.182235985994339, tv_loss: 0.03052108734846115\n",
      "iteration 613, dc_loss: 0.18169119954109192, tv_loss: 0.03038186766207218\n",
      "iteration 614, dc_loss: 0.18093827366828918, tv_loss: 0.03055875562131405\n",
      "iteration 615, dc_loss: 0.18031956255435944, tv_loss: 0.03052329272031784\n",
      "iteration 616, dc_loss: 0.1796492487192154, tv_loss: 0.030431648716330528\n",
      "iteration 617, dc_loss: 0.1789228767156601, tv_loss: 0.030626017600297928\n",
      "iteration 618, dc_loss: 0.1784520298242569, tv_loss: 0.030441032722592354\n",
      "iteration 619, dc_loss: 0.17757929861545563, tv_loss: 0.030674997717142105\n",
      "iteration 620, dc_loss: 0.17704196274280548, tv_loss: 0.030578002333641052\n",
      "iteration 621, dc_loss: 0.17641891539096832, tv_loss: 0.030505232512950897\n",
      "iteration 622, dc_loss: 0.17569763958454132, tv_loss: 0.030715737491846085\n",
      "iteration 623, dc_loss: 0.1750643402338028, tv_loss: 0.030623717233538628\n",
      "iteration 624, dc_loss: 0.17451371252536774, tv_loss: 0.030488645657896996\n",
      "iteration 625, dc_loss: 0.17376984655857086, tv_loss: 0.03068295121192932\n",
      "iteration 626, dc_loss: 0.17322075366973877, tv_loss: 0.030622150748968124\n",
      "iteration 627, dc_loss: 0.1726427972316742, tv_loss: 0.030562711879611015\n",
      "iteration 628, dc_loss: 0.1718687117099762, tv_loss: 0.030650729313492775\n",
      "iteration 629, dc_loss: 0.17137162387371063, tv_loss: 0.030570324510335922\n",
      "iteration 630, dc_loss: 0.17073485255241394, tv_loss: 0.03071446903049946\n",
      "iteration 631, dc_loss: 0.17018985748291016, tv_loss: 0.030622221529483795\n",
      "iteration 632, dc_loss: 0.16946208477020264, tv_loss: 0.030707087367773056\n",
      "iteration 633, dc_loss: 0.16907048225402832, tv_loss: 0.030542759224772453\n",
      "iteration 634, dc_loss: 0.16834509372711182, tv_loss: 0.030797414481639862\n",
      "iteration 635, dc_loss: 0.1681397706270218, tv_loss: 0.03061184287071228\n",
      "iteration 636, dc_loss: 0.1675288826227188, tv_loss: 0.031011654064059258\n",
      "iteration 637, dc_loss: 0.16786478459835052, tv_loss: 0.030285438522696495\n",
      "iteration 638, dc_loss: 0.16717776656150818, tv_loss: 0.031220652163028717\n",
      "iteration 639, dc_loss: 0.16680772602558136, tv_loss: 0.030320551246404648\n",
      "iteration 640, dc_loss: 0.16511137783527374, tv_loss: 0.030931569635868073\n",
      "iteration 641, dc_loss: 0.1643524020910263, tv_loss: 0.030733155086636543\n",
      "iteration 642, dc_loss: 0.16405636072158813, tv_loss: 0.03069852851331234\n",
      "iteration 643, dc_loss: 0.16331924498081207, tv_loss: 0.031010916456580162\n",
      "iteration 644, dc_loss: 0.16309413313865662, tv_loss: 0.03066410683095455\n",
      "iteration 645, dc_loss: 0.16226917505264282, tv_loss: 0.030990295112133026\n",
      "iteration 646, dc_loss: 0.16144908964633942, tv_loss: 0.03082374297082424\n",
      "iteration 647, dc_loss: 0.1610720157623291, tv_loss: 0.030836062505841255\n",
      "iteration 648, dc_loss: 0.16042065620422363, tv_loss: 0.030994266271591187\n",
      "iteration 649, dc_loss: 0.16008366644382477, tv_loss: 0.030709994956851006\n",
      "iteration 650, dc_loss: 0.15934225916862488, tv_loss: 0.030942825600504875\n",
      "iteration 651, dc_loss: 0.15863406658172607, tv_loss: 0.030933057889342308\n",
      "iteration 652, dc_loss: 0.15812629461288452, tv_loss: 0.030814750120043755\n",
      "iteration 653, dc_loss: 0.15762917697429657, tv_loss: 0.030976073816418648\n",
      "iteration 654, dc_loss: 0.15730945765972137, tv_loss: 0.030820097774267197\n",
      "iteration 655, dc_loss: 0.15644364058971405, tv_loss: 0.031001338735222816\n",
      "iteration 656, dc_loss: 0.15587671101093292, tv_loss: 0.030879346653819084\n",
      "iteration 657, dc_loss: 0.15533792972564697, tv_loss: 0.030945727601647377\n",
      "iteration 658, dc_loss: 0.15476416051387787, tv_loss: 0.031002864241600037\n",
      "iteration 659, dc_loss: 0.15441104769706726, tv_loss: 0.030843382701277733\n",
      "iteration 660, dc_loss: 0.1537691205739975, tv_loss: 0.030962642282247543\n",
      "iteration 661, dc_loss: 0.1532927304506302, tv_loss: 0.030876033008098602\n",
      "iteration 662, dc_loss: 0.1525721549987793, tv_loss: 0.031111760064959526\n",
      "iteration 663, dc_loss: 0.15217390656471252, tv_loss: 0.030927633866667747\n",
      "iteration 664, dc_loss: 0.15155287086963654, tv_loss: 0.030994242057204247\n",
      "iteration 665, dc_loss: 0.15106289088726044, tv_loss: 0.031003383919596672\n",
      "iteration 666, dc_loss: 0.15060989558696747, tv_loss: 0.031035827472805977\n",
      "iteration 667, dc_loss: 0.14997214078903198, tv_loss: 0.03113316185772419\n",
      "iteration 668, dc_loss: 0.14975036680698395, tv_loss: 0.030840223655104637\n",
      "iteration 669, dc_loss: 0.14897692203521729, tv_loss: 0.031199119985103607\n",
      "iteration 670, dc_loss: 0.1487082988023758, tv_loss: 0.030962761491537094\n",
      "iteration 671, dc_loss: 0.1479130983352661, tv_loss: 0.031144144013524055\n",
      "iteration 672, dc_loss: 0.14748342335224152, tv_loss: 0.03102875128388405\n",
      "iteration 673, dc_loss: 0.14703978598117828, tv_loss: 0.031115524470806122\n",
      "iteration 674, dc_loss: 0.1464652121067047, tv_loss: 0.031166071072220802\n",
      "iteration 675, dc_loss: 0.1459442526102066, tv_loss: 0.031102556735277176\n",
      "iteration 676, dc_loss: 0.14546436071395874, tv_loss: 0.03109654411673546\n",
      "iteration 677, dc_loss: 0.1449323445558548, tv_loss: 0.0311561431735754\n",
      "iteration 678, dc_loss: 0.14448121190071106, tv_loss: 0.031126227229833603\n",
      "iteration 679, dc_loss: 0.14395134150981903, tv_loss: 0.031135061755776405\n",
      "iteration 680, dc_loss: 0.14348863065242767, tv_loss: 0.031104888767004013\n",
      "iteration 681, dc_loss: 0.14303140342235565, tv_loss: 0.031108178198337555\n",
      "iteration 682, dc_loss: 0.14247651398181915, tv_loss: 0.031255610287189484\n",
      "iteration 683, dc_loss: 0.142129048705101, tv_loss: 0.03112269565463066\n",
      "iteration 684, dc_loss: 0.14152748882770538, tv_loss: 0.031247446313500404\n",
      "iteration 685, dc_loss: 0.14134614169597626, tv_loss: 0.03111935220658779\n",
      "iteration 686, dc_loss: 0.14077357947826385, tv_loss: 0.03147399052977562\n",
      "iteration 687, dc_loss: 0.1410606950521469, tv_loss: 0.030953427776694298\n",
      "iteration 688, dc_loss: 0.14072199165821075, tv_loss: 0.03161657601594925\n",
      "iteration 689, dc_loss: 0.14113686978816986, tv_loss: 0.030807485803961754\n",
      "iteration 690, dc_loss: 0.14020128548145294, tv_loss: 0.03166018798947334\n",
      "iteration 691, dc_loss: 0.13943490386009216, tv_loss: 0.030978741124272346\n",
      "iteration 692, dc_loss: 0.13806487619876862, tv_loss: 0.03128890320658684\n",
      "iteration 693, dc_loss: 0.13742145895957947, tv_loss: 0.03143341466784477\n",
      "iteration 694, dc_loss: 0.1377529799938202, tv_loss: 0.030963024124503136\n",
      "iteration 695, dc_loss: 0.13704895973205566, tv_loss: 0.031495802104473114\n",
      "iteration 696, dc_loss: 0.13654226064682007, tv_loss: 0.031139910221099854\n",
      "iteration 697, dc_loss: 0.13585850596427917, tv_loss: 0.031147867441177368\n",
      "iteration 698, dc_loss: 0.13519811630249023, tv_loss: 0.03146705776453018\n",
      "iteration 699, dc_loss: 0.1352291852235794, tv_loss: 0.031048348173499107\n",
      "iteration 700, dc_loss: 0.13443580269813538, tv_loss: 0.03147608041763306\n",
      "iteration 701, dc_loss: 0.1339392215013504, tv_loss: 0.03139492869377136\n",
      "iteration 702, dc_loss: 0.13352741301059723, tv_loss: 0.031210487708449364\n",
      "iteration 703, dc_loss: 0.1328948587179184, tv_loss: 0.03143376111984253\n",
      "iteration 704, dc_loss: 0.1327444463968277, tv_loss: 0.031287916004657745\n",
      "iteration 705, dc_loss: 0.13215753436088562, tv_loss: 0.03152138367295265\n",
      "iteration 706, dc_loss: 0.13169196248054504, tv_loss: 0.03134169802069664\n",
      "iteration 707, dc_loss: 0.131204292178154, tv_loss: 0.03135973960161209\n",
      "iteration 708, dc_loss: 0.13066674768924713, tv_loss: 0.03153619170188904\n",
      "iteration 709, dc_loss: 0.1304706186056137, tv_loss: 0.03134014084935188\n",
      "iteration 710, dc_loss: 0.12991003692150116, tv_loss: 0.03148498758673668\n",
      "iteration 711, dc_loss: 0.12954504787921906, tv_loss: 0.03137552738189697\n",
      "iteration 712, dc_loss: 0.12905538082122803, tv_loss: 0.03149070963263512\n",
      "iteration 713, dc_loss: 0.1285756528377533, tv_loss: 0.03150556981563568\n",
      "iteration 714, dc_loss: 0.12827394902706146, tv_loss: 0.03137373551726341\n",
      "iteration 715, dc_loss: 0.1277577430009842, tv_loss: 0.03157338872551918\n",
      "iteration 716, dc_loss: 0.12750107049942017, tv_loss: 0.03141697868704796\n",
      "iteration 717, dc_loss: 0.12693333625793457, tv_loss: 0.03152036666870117\n",
      "iteration 718, dc_loss: 0.12651538848876953, tv_loss: 0.031472980976104736\n",
      "iteration 719, dc_loss: 0.12614966928958893, tv_loss: 0.03144620731472969\n",
      "iteration 720, dc_loss: 0.12562747299671173, tv_loss: 0.03157820925116539\n",
      "iteration 721, dc_loss: 0.1253480762243271, tv_loss: 0.031442344188690186\n",
      "iteration 722, dc_loss: 0.12488548457622528, tv_loss: 0.03152065724134445\n",
      "iteration 723, dc_loss: 0.124514140188694, tv_loss: 0.031495604664087296\n",
      "iteration 724, dc_loss: 0.12412126362323761, tv_loss: 0.03150181844830513\n",
      "iteration 725, dc_loss: 0.12368835508823395, tv_loss: 0.0315321646630764\n",
      "iteration 726, dc_loss: 0.12323510646820068, tv_loss: 0.0315437875688076\n",
      "iteration 727, dc_loss: 0.1228729784488678, tv_loss: 0.03149312734603882\n",
      "iteration 728, dc_loss: 0.12241005152463913, tv_loss: 0.031571757048368454\n",
      "iteration 729, dc_loss: 0.12214777618646622, tv_loss: 0.03146896883845329\n",
      "iteration 730, dc_loss: 0.12164221704006195, tv_loss: 0.03158475086092949\n",
      "iteration 731, dc_loss: 0.1213487833738327, tv_loss: 0.03147825598716736\n",
      "iteration 732, dc_loss: 0.12088900804519653, tv_loss: 0.03154783323407173\n",
      "iteration 733, dc_loss: 0.12050127983093262, tv_loss: 0.03157784789800644\n",
      "iteration 734, dc_loss: 0.12014862149953842, tv_loss: 0.03167722746729851\n",
      "iteration 735, dc_loss: 0.11972752958536148, tv_loss: 0.03167462348937988\n",
      "iteration 736, dc_loss: 0.1194051206111908, tv_loss: 0.031562454998493195\n",
      "iteration 737, dc_loss: 0.11898038536310196, tv_loss: 0.03160935640335083\n",
      "iteration 738, dc_loss: 0.1186646893620491, tv_loss: 0.031738247722387314\n",
      "iteration 739, dc_loss: 0.11821384727954865, tv_loss: 0.0317765511572361\n",
      "iteration 740, dc_loss: 0.11817005276679993, tv_loss: 0.03150397911667824\n",
      "iteration 741, dc_loss: 0.11762730032205582, tv_loss: 0.0319327749311924\n",
      "iteration 742, dc_loss: 0.11776602268218994, tv_loss: 0.03145702928304672\n",
      "iteration 743, dc_loss: 0.11727529764175415, tv_loss: 0.03193308413028717\n",
      "iteration 744, dc_loss: 0.11762924492359161, tv_loss: 0.03142658248543739\n",
      "iteration 745, dc_loss: 0.11698669195175171, tv_loss: 0.032076988369226456\n",
      "iteration 746, dc_loss: 0.11692649126052856, tv_loss: 0.0314139723777771\n",
      "iteration 747, dc_loss: 0.11579691618680954, tv_loss: 0.03183508291840553\n",
      "iteration 748, dc_loss: 0.11513317376375198, tv_loss: 0.03164256736636162\n",
      "iteration 749, dc_loss: 0.1148330494761467, tv_loss: 0.03157447651028633\n",
      "iteration 750, dc_loss: 0.1144445389509201, tv_loss: 0.03199242427945137\n",
      "iteration 751, dc_loss: 0.1145937368273735, tv_loss: 0.03154793381690979\n",
      "iteration 752, dc_loss: 0.11372731626033783, tv_loss: 0.031926751136779785\n",
      "iteration 753, dc_loss: 0.1134175956249237, tv_loss: 0.03162028267979622\n",
      "iteration 754, dc_loss: 0.11295340955257416, tv_loss: 0.03182607889175415\n",
      "iteration 755, dc_loss: 0.11248879879713058, tv_loss: 0.03195304423570633\n",
      "iteration 756, dc_loss: 0.11257896572351456, tv_loss: 0.03153659403324127\n",
      "iteration 757, dc_loss: 0.11183790862560272, tv_loss: 0.03201865777373314\n",
      "iteration 758, dc_loss: 0.11165270954370499, tv_loss: 0.03172827512025833\n",
      "iteration 759, dc_loss: 0.11121043562889099, tv_loss: 0.03173332288861275\n",
      "iteration 760, dc_loss: 0.11072832345962524, tv_loss: 0.031884048134088516\n",
      "iteration 761, dc_loss: 0.11075281351804733, tv_loss: 0.031598303467035294\n",
      "iteration 762, dc_loss: 0.11005785316228867, tv_loss: 0.032021742314100266\n",
      "iteration 763, dc_loss: 0.1098993718624115, tv_loss: 0.031717658042907715\n",
      "iteration 764, dc_loss: 0.109463170170784, tv_loss: 0.03177782893180847\n",
      "iteration 765, dc_loss: 0.1090887114405632, tv_loss: 0.03189771994948387\n",
      "iteration 766, dc_loss: 0.10910748690366745, tv_loss: 0.031607527285814285\n",
      "iteration 767, dc_loss: 0.10840419679880142, tv_loss: 0.03208034485578537\n",
      "iteration 768, dc_loss: 0.10828465968370438, tv_loss: 0.03184361010789871\n",
      "iteration 769, dc_loss: 0.10772660374641418, tv_loss: 0.03188794106245041\n",
      "iteration 770, dc_loss: 0.10735797137022018, tv_loss: 0.031894486397504807\n",
      "iteration 771, dc_loss: 0.10719472169876099, tv_loss: 0.03190929442644119\n",
      "iteration 772, dc_loss: 0.10674022138118744, tv_loss: 0.032024577260017395\n",
      "iteration 773, dc_loss: 0.10670439153909683, tv_loss: 0.03173123300075531\n",
      "iteration 774, dc_loss: 0.10608955472707748, tv_loss: 0.0320294052362442\n",
      "iteration 775, dc_loss: 0.10585296899080276, tv_loss: 0.03189896419644356\n",
      "iteration 776, dc_loss: 0.10551279783248901, tv_loss: 0.031883761286735535\n",
      "iteration 777, dc_loss: 0.10510117560625076, tv_loss: 0.031988270580768585\n",
      "iteration 778, dc_loss: 0.10509960353374481, tv_loss: 0.031740594655275345\n",
      "iteration 779, dc_loss: 0.10448584705591202, tv_loss: 0.032111264765262604\n",
      "iteration 780, dc_loss: 0.1044037938117981, tv_loss: 0.03182273358106613\n",
      "iteration 781, dc_loss: 0.10385406762361526, tv_loss: 0.03196040168404579\n",
      "iteration 782, dc_loss: 0.10356467217206955, tv_loss: 0.03194945305585861\n",
      "iteration 783, dc_loss: 0.10344383120536804, tv_loss: 0.031860604882240295\n",
      "iteration 784, dc_loss: 0.10305429995059967, tv_loss: 0.03207114711403847\n",
      "iteration 785, dc_loss: 0.10299240052700043, tv_loss: 0.031961191445589066\n",
      "iteration 786, dc_loss: 0.10251441597938538, tv_loss: 0.03210453316569328\n",
      "iteration 787, dc_loss: 0.10237392783164978, tv_loss: 0.03192427009344101\n",
      "iteration 788, dc_loss: 0.10211331397294998, tv_loss: 0.03191376104950905\n",
      "iteration 789, dc_loss: 0.10174641758203506, tv_loss: 0.03219227492809296\n",
      "iteration 790, dc_loss: 0.10180513560771942, tv_loss: 0.031856145709753036\n",
      "iteration 791, dc_loss: 0.10118399560451508, tv_loss: 0.03227218613028526\n",
      "iteration 792, dc_loss: 0.10130723565816879, tv_loss: 0.031697873026132584\n",
      "iteration 793, dc_loss: 0.10034432262182236, tv_loss: 0.032316919416189194\n",
      "iteration 794, dc_loss: 0.10018501430749893, tv_loss: 0.03193182498216629\n",
      "iteration 795, dc_loss: 0.0997510626912117, tv_loss: 0.03198917582631111\n",
      "iteration 796, dc_loss: 0.09936907142400742, tv_loss: 0.0321732759475708\n",
      "iteration 797, dc_loss: 0.09956622123718262, tv_loss: 0.03180825337767601\n",
      "iteration 798, dc_loss: 0.09884078800678253, tv_loss: 0.032306235283613205\n",
      "iteration 799, dc_loss: 0.09878213703632355, tv_loss: 0.0319388248026371\n",
      "iteration 800, dc_loss: 0.09832732379436493, tv_loss: 0.032040346413850784\n",
      "iteration 801, dc_loss: 0.09804341197013855, tv_loss: 0.03212182596325874\n",
      "iteration 802, dc_loss: 0.09793845564126968, tv_loss: 0.03190672770142555\n",
      "iteration 803, dc_loss: 0.09747020155191422, tv_loss: 0.03207552433013916\n",
      "iteration 804, dc_loss: 0.09727517515420914, tv_loss: 0.032181527465581894\n",
      "iteration 805, dc_loss: 0.09713730216026306, tv_loss: 0.03198137879371643\n",
      "iteration 806, dc_loss: 0.09683367609977722, tv_loss: 0.03200841695070267\n",
      "iteration 807, dc_loss: 0.09657885134220123, tv_loss: 0.03215157613158226\n",
      "iteration 808, dc_loss: 0.09639991074800491, tv_loss: 0.03208639845252037\n",
      "iteration 809, dc_loss: 0.0961873009800911, tv_loss: 0.03203457593917847\n",
      "iteration 810, dc_loss: 0.09591009467840195, tv_loss: 0.03214877471327782\n",
      "iteration 811, dc_loss: 0.09575290977954865, tv_loss: 0.032022688537836075\n",
      "iteration 812, dc_loss: 0.09550545364618301, tv_loss: 0.03204701468348503\n",
      "iteration 813, dc_loss: 0.0952015295624733, tv_loss: 0.032202836126089096\n",
      "iteration 814, dc_loss: 0.09506479650735855, tv_loss: 0.03211860731244087\n",
      "iteration 815, dc_loss: 0.09488873183727264, tv_loss: 0.03205075114965439\n",
      "iteration 816, dc_loss: 0.09453809261322021, tv_loss: 0.03218157961964607\n",
      "iteration 817, dc_loss: 0.0944080576300621, tv_loss: 0.03206689655780792\n",
      "iteration 818, dc_loss: 0.09421846270561218, tv_loss: 0.03210262209177017\n",
      "iteration 819, dc_loss: 0.09390345960855484, tv_loss: 0.03227473795413971\n",
      "iteration 820, dc_loss: 0.09374547749757767, tv_loss: 0.0321163684129715\n",
      "iteration 821, dc_loss: 0.09357486665248871, tv_loss: 0.03214864432811737\n",
      "iteration 822, dc_loss: 0.09325911104679108, tv_loss: 0.03228134289383888\n",
      "iteration 823, dc_loss: 0.09309054911136627, tv_loss: 0.03214050084352493\n",
      "iteration 824, dc_loss: 0.09293302893638611, tv_loss: 0.03216991573572159\n",
      "iteration 825, dc_loss: 0.09264575690031052, tv_loss: 0.03224693611264229\n",
      "iteration 826, dc_loss: 0.09247726947069168, tv_loss: 0.03214891254901886\n",
      "iteration 827, dc_loss: 0.09227888286113739, tv_loss: 0.03222489356994629\n",
      "iteration 828, dc_loss: 0.09202786535024643, tv_loss: 0.032241735607385635\n",
      "iteration 829, dc_loss: 0.09186744689941406, tv_loss: 0.03215137869119644\n",
      "iteration 830, dc_loss: 0.09167739748954773, tv_loss: 0.0322139598429203\n",
      "iteration 831, dc_loss: 0.09141132235527039, tv_loss: 0.03227444738149643\n",
      "iteration 832, dc_loss: 0.0912090465426445, tv_loss: 0.03221869468688965\n",
      "iteration 833, dc_loss: 0.09107229858636856, tv_loss: 0.03218057379126549\n",
      "iteration 834, dc_loss: 0.09082384407520294, tv_loss: 0.03229063004255295\n",
      "iteration 835, dc_loss: 0.09059973806142807, tv_loss: 0.03225192427635193\n",
      "iteration 836, dc_loss: 0.09048034995794296, tv_loss: 0.03219189867377281\n",
      "iteration 837, dc_loss: 0.09021002054214478, tv_loss: 0.032328467816114426\n",
      "iteration 838, dc_loss: 0.0899830013513565, tv_loss: 0.03228132799267769\n",
      "iteration 839, dc_loss: 0.08990651369094849, tv_loss: 0.0321960523724556\n",
      "iteration 840, dc_loss: 0.08961267024278641, tv_loss: 0.03231358155608177\n",
      "iteration 841, dc_loss: 0.08939962834119797, tv_loss: 0.032272618263959885\n",
      "iteration 842, dc_loss: 0.08928756415843964, tv_loss: 0.03224845230579376\n",
      "iteration 843, dc_loss: 0.08901304751634598, tv_loss: 0.03232339769601822\n",
      "iteration 844, dc_loss: 0.08882458508014679, tv_loss: 0.03227265179157257\n",
      "iteration 845, dc_loss: 0.08869414776563644, tv_loss: 0.03230361267924309\n",
      "iteration 846, dc_loss: 0.08843083679676056, tv_loss: 0.032326266169548035\n",
      "iteration 847, dc_loss: 0.08824802935123444, tv_loss: 0.03232506662607193\n",
      "iteration 848, dc_loss: 0.08808550983667374, tv_loss: 0.03236433118581772\n",
      "iteration 849, dc_loss: 0.08786418288946152, tv_loss: 0.03230965510010719\n",
      "iteration 850, dc_loss: 0.08765871077775955, tv_loss: 0.032374948263168335\n",
      "iteration 851, dc_loss: 0.0874786227941513, tv_loss: 0.03235394507646561\n",
      "iteration 852, dc_loss: 0.08730919659137726, tv_loss: 0.0322788767516613\n",
      "iteration 853, dc_loss: 0.08710391074419022, tv_loss: 0.03235854208469391\n",
      "iteration 854, dc_loss: 0.08690992742776871, tv_loss: 0.03235375136137009\n",
      "iteration 855, dc_loss: 0.0867357924580574, tv_loss: 0.032300375401973724\n",
      "iteration 856, dc_loss: 0.08654314279556274, tv_loss: 0.03236064687371254\n",
      "iteration 857, dc_loss: 0.0863441675901413, tv_loss: 0.03237884119153023\n",
      "iteration 858, dc_loss: 0.0861690565943718, tv_loss: 0.03231527656316757\n",
      "iteration 859, dc_loss: 0.08594958484172821, tv_loss: 0.03235763683915138\n",
      "iteration 860, dc_loss: 0.08580280095338821, tv_loss: 0.03235229104757309\n",
      "iteration 861, dc_loss: 0.08563520759344101, tv_loss: 0.03231394290924072\n",
      "iteration 862, dc_loss: 0.08539331704378128, tv_loss: 0.032369211316108704\n",
      "iteration 863, dc_loss: 0.08528592437505722, tv_loss: 0.03229337930679321\n",
      "iteration 864, dc_loss: 0.08503753691911697, tv_loss: 0.032329924404621124\n",
      "iteration 865, dc_loss: 0.08485573530197144, tv_loss: 0.032360177487134933\n",
      "iteration 866, dc_loss: 0.0847250372171402, tv_loss: 0.03233290836215019\n",
      "iteration 867, dc_loss: 0.08449675142765045, tv_loss: 0.03239478915929794\n",
      "iteration 868, dc_loss: 0.08429770171642303, tv_loss: 0.03240150213241577\n",
      "iteration 869, dc_loss: 0.08418243378400803, tv_loss: 0.0322997123003006\n",
      "iteration 870, dc_loss: 0.08395490050315857, tv_loss: 0.03236909210681915\n",
      "iteration 871, dc_loss: 0.0837894007563591, tv_loss: 0.0323546938598156\n",
      "iteration 872, dc_loss: 0.08360577374696732, tv_loss: 0.032412540167570114\n",
      "iteration 873, dc_loss: 0.08341645449399948, tv_loss: 0.032450754195451736\n",
      "iteration 874, dc_loss: 0.08332318067550659, tv_loss: 0.03230958804488182\n",
      "iteration 875, dc_loss: 0.08303304016590118, tv_loss: 0.03241322189569473\n",
      "iteration 876, dc_loss: 0.08288723975419998, tv_loss: 0.032374776899814606\n",
      "iteration 877, dc_loss: 0.08276136219501495, tv_loss: 0.03234776109457016\n",
      "iteration 878, dc_loss: 0.08253099769353867, tv_loss: 0.03243676945567131\n",
      "iteration 879, dc_loss: 0.082427479326725, tv_loss: 0.03243233636021614\n",
      "iteration 880, dc_loss: 0.08217219263315201, tv_loss: 0.032490842044353485\n",
      "iteration 881, dc_loss: 0.08203617483377457, tv_loss: 0.03238464891910553\n",
      "iteration 882, dc_loss: 0.08188486099243164, tv_loss: 0.03244968503713608\n",
      "iteration 883, dc_loss: 0.08167052268981934, tv_loss: 0.03252352774143219\n",
      "iteration 884, dc_loss: 0.08157019317150116, tv_loss: 0.03240136057138443\n",
      "iteration 885, dc_loss: 0.0813145786523819, tv_loss: 0.03249206393957138\n",
      "iteration 886, dc_loss: 0.08116070926189423, tv_loss: 0.032494429498910904\n",
      "iteration 887, dc_loss: 0.08107352256774902, tv_loss: 0.03239154443144798\n",
      "iteration 888, dc_loss: 0.0808306336402893, tv_loss: 0.032478976994752884\n",
      "iteration 889, dc_loss: 0.08070559799671173, tv_loss: 0.03242442384362221\n",
      "iteration 890, dc_loss: 0.08051615208387375, tv_loss: 0.032427456229925156\n",
      "iteration 891, dc_loss: 0.0803130641579628, tv_loss: 0.03248083218932152\n",
      "iteration 892, dc_loss: 0.08025716990232468, tv_loss: 0.03238821029663086\n",
      "iteration 893, dc_loss: 0.07997474074363708, tv_loss: 0.032553836703300476\n",
      "iteration 894, dc_loss: 0.07993265986442566, tv_loss: 0.032442606985569\n",
      "iteration 895, dc_loss: 0.0796569436788559, tv_loss: 0.03253462538123131\n",
      "iteration 896, dc_loss: 0.07959996163845062, tv_loss: 0.03241981565952301\n",
      "iteration 897, dc_loss: 0.07943447679281235, tv_loss: 0.03247600421309471\n",
      "iteration 898, dc_loss: 0.07936687022447586, tv_loss: 0.03244224563241005\n",
      "iteration 899, dc_loss: 0.07917789369821548, tv_loss: 0.03255553916096687\n",
      "iteration 900, dc_loss: 0.07923436909914017, tv_loss: 0.03239220380783081\n",
      "iteration 901, dc_loss: 0.07889524102210999, tv_loss: 0.03263336047530174\n",
      "iteration 902, dc_loss: 0.07888218760490417, tv_loss: 0.03243472799658775\n",
      "iteration 903, dc_loss: 0.07847405225038528, tv_loss: 0.03256748244166374\n",
      "iteration 904, dc_loss: 0.07833150774240494, tv_loss: 0.032441895455121994\n",
      "iteration 905, dc_loss: 0.07806161046028137, tv_loss: 0.032501187175512314\n",
      "iteration 906, dc_loss: 0.07787685096263885, tv_loss: 0.03253167122602463\n",
      "iteration 907, dc_loss: 0.07787974923849106, tv_loss: 0.032466672360897064\n",
      "iteration 908, dc_loss: 0.0776568278670311, tv_loss: 0.03262903541326523\n",
      "iteration 909, dc_loss: 0.07759872823953629, tv_loss: 0.03249898552894592\n",
      "iteration 910, dc_loss: 0.07736079394817352, tv_loss: 0.03251585736870766\n",
      "iteration 911, dc_loss: 0.07715669274330139, tv_loss: 0.032496970146894455\n",
      "iteration 912, dc_loss: 0.07693309336900711, tv_loss: 0.03259473666548729\n",
      "iteration 913, dc_loss: 0.07686915993690491, tv_loss: 0.03257245942950249\n",
      "iteration 914, dc_loss: 0.07668427377939224, tv_loss: 0.03258734941482544\n",
      "iteration 915, dc_loss: 0.07658357918262482, tv_loss: 0.03250870108604431\n",
      "iteration 916, dc_loss: 0.07642408460378647, tv_loss: 0.03252191096544266\n",
      "iteration 917, dc_loss: 0.07619874179363251, tv_loss: 0.032638829201459885\n",
      "iteration 918, dc_loss: 0.07617628574371338, tv_loss: 0.0325227826833725\n",
      "iteration 919, dc_loss: 0.07583726942539215, tv_loss: 0.032672781497240067\n",
      "iteration 920, dc_loss: 0.07585413753986359, tv_loss: 0.03247919678688049\n",
      "iteration 921, dc_loss: 0.07556555420160294, tv_loss: 0.03265110030770302\n",
      "iteration 922, dc_loss: 0.0754394382238388, tv_loss: 0.0326303206384182\n",
      "iteration 923, dc_loss: 0.07539936155080795, tv_loss: 0.03249436989426613\n",
      "iteration 924, dc_loss: 0.07510484755039215, tv_loss: 0.03267011418938637\n",
      "iteration 925, dc_loss: 0.07513546943664551, tv_loss: 0.032497987151145935\n",
      "iteration 926, dc_loss: 0.07481968402862549, tv_loss: 0.0326591394841671\n",
      "iteration 927, dc_loss: 0.07473708689212799, tv_loss: 0.032609906047582626\n",
      "iteration 928, dc_loss: 0.07464336603879929, tv_loss: 0.0325629897415638\n",
      "iteration 929, dc_loss: 0.07440687716007233, tv_loss: 0.03263797610998154\n",
      "iteration 930, dc_loss: 0.07433117181062698, tv_loss: 0.032548412680625916\n",
      "iteration 931, dc_loss: 0.07409002631902695, tv_loss: 0.03264651820063591\n",
      "iteration 932, dc_loss: 0.07396697252988815, tv_loss: 0.03265603631734848\n",
      "iteration 933, dc_loss: 0.07384752482175827, tv_loss: 0.032617442309856415\n",
      "iteration 934, dc_loss: 0.07369916886091232, tv_loss: 0.03258338198065758\n",
      "iteration 935, dc_loss: 0.07353615015745163, tv_loss: 0.03260735049843788\n",
      "iteration 936, dc_loss: 0.07343927770853043, tv_loss: 0.032574329525232315\n",
      "iteration 937, dc_loss: 0.07320315390825272, tv_loss: 0.032726772129535675\n",
      "iteration 938, dc_loss: 0.07317241281270981, tv_loss: 0.03261284530162811\n",
      "iteration 939, dc_loss: 0.07295620441436768, tv_loss: 0.032649021595716476\n",
      "iteration 940, dc_loss: 0.07284705340862274, tv_loss: 0.03259849175810814\n",
      "iteration 941, dc_loss: 0.0727289468050003, tv_loss: 0.032607678323984146\n",
      "iteration 942, dc_loss: 0.07260044664144516, tv_loss: 0.03263266384601593\n",
      "iteration 943, dc_loss: 0.07249664515256882, tv_loss: 0.03262235224246979\n",
      "iteration 944, dc_loss: 0.07236205041408539, tv_loss: 0.032651763409376144\n",
      "iteration 945, dc_loss: 0.07223149389028549, tv_loss: 0.0327301025390625\n",
      "iteration 946, dc_loss: 0.07221151888370514, tv_loss: 0.03259647265076637\n",
      "iteration 947, dc_loss: 0.07189241051673889, tv_loss: 0.03277742490172386\n",
      "iteration 948, dc_loss: 0.07201559841632843, tv_loss: 0.032501962035894394\n",
      "iteration 949, dc_loss: 0.07166461646556854, tv_loss: 0.032797396183013916\n",
      "iteration 950, dc_loss: 0.07187467068433762, tv_loss: 0.032498154789209366\n",
      "iteration 951, dc_loss: 0.07148760557174683, tv_loss: 0.03282763808965683\n",
      "iteration 952, dc_loss: 0.07156746089458466, tv_loss: 0.03254534304141998\n",
      "iteration 953, dc_loss: 0.07107140868902206, tv_loss: 0.032810747623443604\n",
      "iteration 954, dc_loss: 0.07097324728965759, tv_loss: 0.032655637711286545\n",
      "iteration 955, dc_loss: 0.07080861926078796, tv_loss: 0.032640255987644196\n",
      "iteration 956, dc_loss: 0.07063441723585129, tv_loss: 0.0327472947537899\n",
      "iteration 957, dc_loss: 0.07069098204374313, tv_loss: 0.03258063271641731\n",
      "iteration 958, dc_loss: 0.07033057510852814, tv_loss: 0.03280433639883995\n",
      "iteration 959, dc_loss: 0.07040884345769882, tv_loss: 0.03258302062749863\n",
      "iteration 960, dc_loss: 0.0700824186205864, tv_loss: 0.03281944990158081\n",
      "iteration 961, dc_loss: 0.07007329910993576, tv_loss: 0.03272460028529167\n",
      "iteration 962, dc_loss: 0.06990102678537369, tv_loss: 0.03270350769162178\n",
      "iteration 963, dc_loss: 0.06975050270557404, tv_loss: 0.032751813530921936\n",
      "iteration 964, dc_loss: 0.06967297196388245, tv_loss: 0.032678090035915375\n",
      "iteration 965, dc_loss: 0.0694451704621315, tv_loss: 0.032784998416900635\n",
      "iteration 966, dc_loss: 0.06933002173900604, tv_loss: 0.03270365670323372\n",
      "iteration 967, dc_loss: 0.069188691675663, tv_loss: 0.03272005170583725\n",
      "iteration 968, dc_loss: 0.0690140575170517, tv_loss: 0.03273482620716095\n",
      "iteration 969, dc_loss: 0.06896232068538666, tv_loss: 0.03273291140794754\n",
      "iteration 970, dc_loss: 0.06874722242355347, tv_loss: 0.03284475952386856\n",
      "iteration 971, dc_loss: 0.06875475496053696, tv_loss: 0.03266673535108566\n",
      "iteration 972, dc_loss: 0.06852762401103973, tv_loss: 0.032769281417131424\n",
      "iteration 973, dc_loss: 0.06843966990709305, tv_loss: 0.032808270305395126\n",
      "iteration 974, dc_loss: 0.0683455839753151, tv_loss: 0.03273490071296692\n",
      "iteration 975, dc_loss: 0.0681113600730896, tv_loss: 0.03285422548651695\n",
      "iteration 976, dc_loss: 0.06823962181806564, tv_loss: 0.03263816982507706\n",
      "iteration 977, dc_loss: 0.0679149478673935, tv_loss: 0.03290495648980141\n",
      "iteration 978, dc_loss: 0.06801006197929382, tv_loss: 0.03267017379403114\n",
      "iteration 979, dc_loss: 0.06773404777050018, tv_loss: 0.032838981598615646\n",
      "iteration 980, dc_loss: 0.06777925789356232, tv_loss: 0.03268156573176384\n",
      "iteration 981, dc_loss: 0.06759355962276459, tv_loss: 0.0328056700527668\n",
      "iteration 982, dc_loss: 0.06760383397340775, tv_loss: 0.03269629552960396\n",
      "iteration 983, dc_loss: 0.06741180270910263, tv_loss: 0.032850492745637894\n",
      "iteration 984, dc_loss: 0.06739117205142975, tv_loss: 0.03273772820830345\n",
      "iteration 985, dc_loss: 0.06702900677919388, tv_loss: 0.03290500119328499\n",
      "iteration 986, dc_loss: 0.06702331453561783, tv_loss: 0.032649651169776917\n",
      "iteration 987, dc_loss: 0.06664430350065231, tv_loss: 0.03284701332449913\n",
      "iteration 988, dc_loss: 0.06664496660232544, tv_loss: 0.03269581124186516\n",
      "iteration 989, dc_loss: 0.06642239540815353, tv_loss: 0.03281119093298912\n",
      "iteration 990, dc_loss: 0.06641405820846558, tv_loss: 0.03272289037704468\n",
      "iteration 991, dc_loss: 0.06624172627925873, tv_loss: 0.0327807292342186\n",
      "iteration 992, dc_loss: 0.06608021259307861, tv_loss: 0.03290024399757385\n",
      "iteration 993, dc_loss: 0.06607749313116074, tv_loss: 0.03278370574116707\n",
      "iteration 994, dc_loss: 0.06580797582864761, tv_loss: 0.03290267288684845\n",
      "iteration 995, dc_loss: 0.06596731394529343, tv_loss: 0.032723307609558105\n",
      "iteration 996, dc_loss: 0.065595842897892, tv_loss: 0.03300033137202263\n",
      "iteration 997, dc_loss: 0.06564082205295563, tv_loss: 0.032724328339099884\n",
      "iteration 998, dc_loss: 0.06531336158514023, tv_loss: 0.032919496297836304\n",
      "iteration 999, dc_loss: 0.06528797745704651, tv_loss: 0.03283701464533806\n",
      "iteration 1000, dc_loss: 0.0650951936841011, tv_loss: 0.03286159411072731\n",
      "iteration 1001, dc_loss: 0.0649743303656578, tv_loss: 0.03287293389439583\n",
      "iteration 1002, dc_loss: 0.06498201191425323, tv_loss: 0.032775577157735825\n",
      "iteration 1003, dc_loss: 0.06471478939056396, tv_loss: 0.03295348212122917\n",
      "iteration 1004, dc_loss: 0.0647684782743454, tv_loss: 0.03277437016367912\n",
      "iteration 1005, dc_loss: 0.0645155981183052, tv_loss: 0.03291313722729683\n",
      "iteration 1006, dc_loss: 0.0645236149430275, tv_loss: 0.0327659510076046\n",
      "iteration 1007, dc_loss: 0.0642724484205246, tv_loss: 0.032884154468774796\n",
      "iteration 1008, dc_loss: 0.06424970179796219, tv_loss: 0.03279561549425125\n",
      "iteration 1009, dc_loss: 0.06410367786884308, tv_loss: 0.03287035971879959\n",
      "iteration 1010, dc_loss: 0.06404800713062286, tv_loss: 0.032853711396455765\n",
      "iteration 1011, dc_loss: 0.06399104744195938, tv_loss: 0.032875511795282364\n",
      "iteration 1012, dc_loss: 0.06385224312543869, tv_loss: 0.0329110324382782\n",
      "iteration 1013, dc_loss: 0.06380818039178848, tv_loss: 0.03286406770348549\n",
      "iteration 1014, dc_loss: 0.06375154852867126, tv_loss: 0.032823190093040466\n",
      "iteration 1015, dc_loss: 0.06361038982868195, tv_loss: 0.032915856689214706\n",
      "iteration 1016, dc_loss: 0.06365541368722916, tv_loss: 0.03278373181819916\n",
      "iteration 1017, dc_loss: 0.06338822096586227, tv_loss: 0.03301561623811722\n",
      "iteration 1018, dc_loss: 0.06355517357587814, tv_loss: 0.03274533525109291\n",
      "iteration 1019, dc_loss: 0.0631740391254425, tv_loss: 0.033055663108825684\n",
      "iteration 1020, dc_loss: 0.06333722174167633, tv_loss: 0.03270293027162552\n",
      "iteration 1021, dc_loss: 0.06284722685813904, tv_loss: 0.03305163234472275\n",
      "iteration 1022, dc_loss: 0.06295623630285263, tv_loss: 0.03273366764187813\n",
      "iteration 1023, dc_loss: 0.062498804181814194, tv_loss: 0.033060744404792786\n",
      "iteration 1024, dc_loss: 0.06254105269908905, tv_loss: 0.03284848481416702\n",
      "iteration 1025, dc_loss: 0.0623517706990242, tv_loss: 0.03289297968149185\n",
      "iteration 1026, dc_loss: 0.06222778558731079, tv_loss: 0.03293873742222786\n",
      "iteration 1027, dc_loss: 0.06231638789176941, tv_loss: 0.032793078571558\n",
      "iteration 1028, dc_loss: 0.062048424035310745, tv_loss: 0.03305867686867714\n",
      "iteration 1029, dc_loss: 0.062209602445364, tv_loss: 0.03278299421072006\n",
      "iteration 1030, dc_loss: 0.0617690347135067, tv_loss: 0.033087752759456635\n",
      "iteration 1031, dc_loss: 0.06182484328746796, tv_loss: 0.03282564878463745\n",
      "iteration 1032, dc_loss: 0.06156815215945244, tv_loss: 0.032911427319049835\n",
      "iteration 1033, dc_loss: 0.06144716590642929, tv_loss: 0.03294152766466141\n",
      "iteration 1034, dc_loss: 0.0614507757127285, tv_loss: 0.032832179218530655\n",
      "iteration 1035, dc_loss: 0.06124347820878029, tv_loss: 0.03296361863613129\n",
      "iteration 1036, dc_loss: 0.061311621218919754, tv_loss: 0.03284354507923126\n",
      "iteration 1037, dc_loss: 0.0610683336853981, tv_loss: 0.033053942024707794\n",
      "iteration 1038, dc_loss: 0.06105919927358627, tv_loss: 0.032926660031080246\n",
      "iteration 1039, dc_loss: 0.06089584156870842, tv_loss: 0.03292522579431534\n",
      "iteration 1040, dc_loss: 0.060804583132267, tv_loss: 0.03291795030236244\n",
      "iteration 1041, dc_loss: 0.06070544570684433, tv_loss: 0.03296280652284622\n",
      "iteration 1042, dc_loss: 0.060561422258615494, tv_loss: 0.03303306922316551\n",
      "iteration 1043, dc_loss: 0.060447756201028824, tv_loss: 0.0329548642039299\n",
      "iteration 1044, dc_loss: 0.06036372110247612, tv_loss: 0.03293471410870552\n",
      "iteration 1045, dc_loss: 0.060199227184057236, tv_loss: 0.03310168534517288\n",
      "iteration 1046, dc_loss: 0.06015755981206894, tv_loss: 0.032958563417196274\n",
      "iteration 1047, dc_loss: 0.0600433275103569, tv_loss: 0.033023953437805176\n",
      "iteration 1048, dc_loss: 0.059936828911304474, tv_loss: 0.033020444214344025\n",
      "iteration 1049, dc_loss: 0.059834904968738556, tv_loss: 0.0329643152654171\n",
      "iteration 1050, dc_loss: 0.05975307151675224, tv_loss: 0.0329517163336277\n",
      "iteration 1051, dc_loss: 0.05970326066017151, tv_loss: 0.032935746014118195\n",
      "iteration 1052, dc_loss: 0.059526897966861725, tv_loss: 0.03306368365883827\n",
      "iteration 1053, dc_loss: 0.059574294835329056, tv_loss: 0.03291799873113632\n",
      "iteration 1054, dc_loss: 0.05934639647603035, tv_loss: 0.03306332603096962\n",
      "iteration 1055, dc_loss: 0.05955807492136955, tv_loss: 0.032802727073431015\n",
      "iteration 1056, dc_loss: 0.05924020707607269, tv_loss: 0.033162280917167664\n",
      "iteration 1057, dc_loss: 0.05954645201563835, tv_loss: 0.03281480818986893\n",
      "iteration 1058, dc_loss: 0.05915720760822296, tv_loss: 0.033213648945093155\n",
      "iteration 1059, dc_loss: 0.05949041619896889, tv_loss: 0.03276529163122177\n",
      "iteration 1060, dc_loss: 0.05909045413136482, tv_loss: 0.03315190598368645\n",
      "iteration 1061, dc_loss: 0.059180568903684616, tv_loss: 0.03293429687619209\n",
      "iteration 1062, dc_loss: 0.05886134132742882, tv_loss: 0.033099256455898285\n",
      "iteration 1063, dc_loss: 0.05869586020708084, tv_loss: 0.03294425085186958\n",
      "iteration 1064, dc_loss: 0.0584583654999733, tv_loss: 0.03298773989081383\n",
      "iteration 1065, dc_loss: 0.05822987109422684, tv_loss: 0.03315511718392372\n",
      "iteration 1066, dc_loss: 0.0583818145096302, tv_loss: 0.032928142696619034\n",
      "iteration 1067, dc_loss: 0.05819542706012726, tv_loss: 0.0331515409052372\n",
      "iteration 1068, dc_loss: 0.05832380801439285, tv_loss: 0.03296501934528351\n",
      "iteration 1069, dc_loss: 0.05801776424050331, tv_loss: 0.03306937590241432\n",
      "iteration 1070, dc_loss: 0.05794435366988182, tv_loss: 0.03301184996962547\n",
      "iteration 1071, dc_loss: 0.05785086378455162, tv_loss: 0.0330234058201313\n",
      "iteration 1072, dc_loss: 0.057646509259939194, tv_loss: 0.03308738023042679\n",
      "iteration 1073, dc_loss: 0.05765033885836601, tv_loss: 0.03293831646442413\n",
      "iteration 1074, dc_loss: 0.05754449963569641, tv_loss: 0.03307799622416496\n",
      "iteration 1075, dc_loss: 0.057638563215732574, tv_loss: 0.033007677644491196\n",
      "iteration 1076, dc_loss: 0.05736945569515228, tv_loss: 0.03308584541082382\n",
      "iteration 1077, dc_loss: 0.05729059875011444, tv_loss: 0.03297637775540352\n",
      "iteration 1078, dc_loss: 0.057224683463573456, tv_loss: 0.03304420784115791\n",
      "iteration 1079, dc_loss: 0.0571100190281868, tv_loss: 0.033003635704517365\n",
      "iteration 1080, dc_loss: 0.05696583539247513, tv_loss: 0.03306173160672188\n",
      "iteration 1081, dc_loss: 0.05689554661512375, tv_loss: 0.033059261739254\n",
      "iteration 1082, dc_loss: 0.056858256459236145, tv_loss: 0.03300051763653755\n",
      "iteration 1083, dc_loss: 0.05663943663239479, tv_loss: 0.03310500457882881\n",
      "iteration 1084, dc_loss: 0.056685205549001694, tv_loss: 0.03294651210308075\n",
      "iteration 1085, dc_loss: 0.05643254518508911, tv_loss: 0.03311756253242493\n",
      "iteration 1086, dc_loss: 0.05648453161120415, tv_loss: 0.03296010568737984\n",
      "iteration 1087, dc_loss: 0.0562954917550087, tv_loss: 0.0330611951649189\n",
      "iteration 1088, dc_loss: 0.05625046789646149, tv_loss: 0.033053744584321976\n",
      "iteration 1089, dc_loss: 0.056121088564395905, tv_loss: 0.03314577415585518\n",
      "iteration 1090, dc_loss: 0.056063197553157806, tv_loss: 0.03305013105273247\n",
      "iteration 1091, dc_loss: 0.05596292391419411, tv_loss: 0.033043257892131805\n",
      "iteration 1092, dc_loss: 0.05586061254143715, tv_loss: 0.03307311609387398\n",
      "iteration 1093, dc_loss: 0.05578957870602608, tv_loss: 0.03310127183794975\n",
      "iteration 1094, dc_loss: 0.05565078184008598, tv_loss: 0.03312530741095543\n",
      "iteration 1095, dc_loss: 0.05570222809910774, tv_loss: 0.03298352286219597\n",
      "iteration 1096, dc_loss: 0.0554632842540741, tv_loss: 0.033141497522592545\n",
      "iteration 1097, dc_loss: 0.055542200803756714, tv_loss: 0.03300684317946434\n",
      "iteration 1098, dc_loss: 0.05529084429144859, tv_loss: 0.03322162106633186\n",
      "iteration 1099, dc_loss: 0.05539793521165848, tv_loss: 0.032993804663419724\n",
      "iteration 1100, dc_loss: 0.05513114854693413, tv_loss: 0.03313037008047104\n",
      "iteration 1101, dc_loss: 0.05514099821448326, tv_loss: 0.03303959593176842\n",
      "iteration 1102, dc_loss: 0.054952848702669144, tv_loss: 0.033221956342458725\n",
      "iteration 1103, dc_loss: 0.055023133754730225, tv_loss: 0.033038023859262466\n",
      "iteration 1104, dc_loss: 0.05481233075261116, tv_loss: 0.03315145894885063\n",
      "iteration 1105, dc_loss: 0.05487776920199394, tv_loss: 0.033016834408044815\n",
      "iteration 1106, dc_loss: 0.05462581664323807, tv_loss: 0.033224619925022125\n",
      "iteration 1107, dc_loss: 0.054768647998571396, tv_loss: 0.03301924467086792\n",
      "iteration 1108, dc_loss: 0.054576147347688675, tv_loss: 0.033157266676425934\n",
      "iteration 1109, dc_loss: 0.05467969551682472, tv_loss: 0.03300277888774872\n",
      "iteration 1110, dc_loss: 0.0544402115046978, tv_loss: 0.03320952132344246\n",
      "iteration 1111, dc_loss: 0.05468039587140083, tv_loss: 0.03294761851429939\n",
      "iteration 1112, dc_loss: 0.05442342162132263, tv_loss: 0.03327818587422371\n",
      "iteration 1113, dc_loss: 0.054674264043569565, tv_loss: 0.03294763341546059\n",
      "iteration 1114, dc_loss: 0.05421113595366478, tv_loss: 0.03333607316017151\n",
      "iteration 1115, dc_loss: 0.054401155561208725, tv_loss: 0.0329132080078125\n",
      "iteration 1116, dc_loss: 0.05385845899581909, tv_loss: 0.03329680487513542\n",
      "iteration 1117, dc_loss: 0.053965114057064056, tv_loss: 0.03299284726381302\n",
      "iteration 1118, dc_loss: 0.053647927939891815, tv_loss: 0.033186085522174835\n",
      "iteration 1119, dc_loss: 0.05368681997060776, tv_loss: 0.03308846428990364\n",
      "iteration 1120, dc_loss: 0.053658198565244675, tv_loss: 0.03311767801642418\n",
      "iteration 1121, dc_loss: 0.05357012897729874, tv_loss: 0.03321203961968422\n",
      "iteration 1122, dc_loss: 0.053597863763570786, tv_loss: 0.033064186573028564\n",
      "iteration 1123, dc_loss: 0.05334247648715973, tv_loss: 0.033197030425071716\n",
      "iteration 1124, dc_loss: 0.05335238575935364, tv_loss: 0.033042505383491516\n",
      "iteration 1125, dc_loss: 0.05310143157839775, tv_loss: 0.033176448196172714\n",
      "iteration 1126, dc_loss: 0.05310596898198128, tv_loss: 0.03306720778346062\n",
      "iteration 1127, dc_loss: 0.05299048870801926, tv_loss: 0.03315024450421333\n",
      "iteration 1128, dc_loss: 0.052941471338272095, tv_loss: 0.03318844735622406\n",
      "iteration 1129, dc_loss: 0.05294763296842575, tv_loss: 0.03313165903091431\n",
      "iteration 1130, dc_loss: 0.0528196282684803, tv_loss: 0.03315481171011925\n",
      "iteration 1131, dc_loss: 0.05277206376194954, tv_loss: 0.033089715987443924\n",
      "iteration 1132, dc_loss: 0.05261244252324104, tv_loss: 0.033132873475551605\n",
      "iteration 1133, dc_loss: 0.05253998935222626, tv_loss: 0.033102620393037796\n",
      "iteration 1134, dc_loss: 0.05241158604621887, tv_loss: 0.0331314317882061\n",
      "iteration 1135, dc_loss: 0.05236257240176201, tv_loss: 0.033172860741615295\n",
      "iteration 1136, dc_loss: 0.0522836409509182, tv_loss: 0.03322548791766167\n",
      "iteration 1137, dc_loss: 0.052195094525814056, tv_loss: 0.033169012516736984\n",
      "iteration 1138, dc_loss: 0.05217951536178589, tv_loss: 0.03310331702232361\n",
      "iteration 1139, dc_loss: 0.05203435197472572, tv_loss: 0.0332607701420784\n",
      "iteration 1140, dc_loss: 0.05203502997756004, tv_loss: 0.033162739127874374\n",
      "iteration 1141, dc_loss: 0.05186982452869415, tv_loss: 0.03321373090147972\n",
      "iteration 1142, dc_loss: 0.05189187452197075, tv_loss: 0.03314751386642456\n",
      "iteration 1143, dc_loss: 0.051696404814720154, tv_loss: 0.033324480056762695\n",
      "iteration 1144, dc_loss: 0.051814425736665726, tv_loss: 0.03308861330151558\n",
      "iteration 1145, dc_loss: 0.05154537409543991, tv_loss: 0.03329507261514664\n",
      "iteration 1146, dc_loss: 0.05171307176351547, tv_loss: 0.033086761832237244\n",
      "iteration 1147, dc_loss: 0.05142553150653839, tv_loss: 0.03333152458071709\n",
      "iteration 1148, dc_loss: 0.05157895013689995, tv_loss: 0.03306443244218826\n",
      "iteration 1149, dc_loss: 0.051281362771987915, tv_loss: 0.03332442790269852\n",
      "iteration 1150, dc_loss: 0.05146472156047821, tv_loss: 0.03309072554111481\n",
      "iteration 1151, dc_loss: 0.051130518317222595, tv_loss: 0.03336593881249428\n",
      "iteration 1152, dc_loss: 0.05132804438471794, tv_loss: 0.033038925379514694\n",
      "iteration 1153, dc_loss: 0.05098723620176315, tv_loss: 0.03331459313631058\n",
      "iteration 1154, dc_loss: 0.05111795663833618, tv_loss: 0.03313151374459267\n",
      "iteration 1155, dc_loss: 0.05082475394010544, tv_loss: 0.03330657631158829\n",
      "iteration 1156, dc_loss: 0.050932448357343674, tv_loss: 0.03311408683657646\n",
      "iteration 1157, dc_loss: 0.050780389457941055, tv_loss: 0.03323552384972572\n",
      "iteration 1158, dc_loss: 0.05080033838748932, tv_loss: 0.03320416063070297\n",
      "iteration 1159, dc_loss: 0.05072556063532829, tv_loss: 0.033235158771276474\n",
      "iteration 1160, dc_loss: 0.050704121589660645, tv_loss: 0.03317762538790703\n",
      "iteration 1161, dc_loss: 0.050670791417360306, tv_loss: 0.03322131186723709\n",
      "iteration 1162, dc_loss: 0.05066441372036934, tv_loss: 0.033193230628967285\n",
      "iteration 1163, dc_loss: 0.05055608972907066, tv_loss: 0.03328844532370567\n",
      "iteration 1164, dc_loss: 0.050605013966560364, tv_loss: 0.03313956782221794\n",
      "iteration 1165, dc_loss: 0.05041885003447533, tv_loss: 0.03326483070850372\n",
      "iteration 1166, dc_loss: 0.05041709169745445, tv_loss: 0.03311047703027725\n",
      "iteration 1167, dc_loss: 0.05012628436088562, tv_loss: 0.03328133746981621\n",
      "iteration 1168, dc_loss: 0.05021314695477486, tv_loss: 0.03307335078716278\n",
      "iteration 1169, dc_loss: 0.04986374080181122, tv_loss: 0.033364053815603256\n",
      "iteration 1170, dc_loss: 0.05015959218144417, tv_loss: 0.03300682455301285\n",
      "iteration 1171, dc_loss: 0.04976791888475418, tv_loss: 0.03340618684887886\n",
      "iteration 1172, dc_loss: 0.05005117878317833, tv_loss: 0.03312111645936966\n",
      "iteration 1173, dc_loss: 0.04963552951812744, tv_loss: 0.03340538591146469\n",
      "iteration 1174, dc_loss: 0.04974909871816635, tv_loss: 0.033111944794654846\n",
      "iteration 1175, dc_loss: 0.04947230964899063, tv_loss: 0.03336884826421738\n",
      "iteration 1176, dc_loss: 0.049424104392528534, tv_loss: 0.0332733616232872\n",
      "iteration 1177, dc_loss: 0.04947569593787193, tv_loss: 0.033140189945697784\n",
      "iteration 1178, dc_loss: 0.049267031252384186, tv_loss: 0.03337669372558594\n",
      "iteration 1179, dc_loss: 0.04944188892841339, tv_loss: 0.033182669430971146\n",
      "iteration 1180, dc_loss: 0.049141738563776016, tv_loss: 0.03334442153573036\n",
      "iteration 1181, dc_loss: 0.04924502968788147, tv_loss: 0.033179860562086105\n",
      "iteration 1182, dc_loss: 0.049015022814273834, tv_loss: 0.033343371003866196\n",
      "iteration 1183, dc_loss: 0.04901403933763504, tv_loss: 0.03321828693151474\n",
      "iteration 1184, dc_loss: 0.048921044915914536, tv_loss: 0.03322761133313179\n",
      "iteration 1185, dc_loss: 0.0488322488963604, tv_loss: 0.03328453376889229\n",
      "iteration 1186, dc_loss: 0.04881209507584572, tv_loss: 0.03326481580734253\n",
      "iteration 1187, dc_loss: 0.048713233321905136, tv_loss: 0.03326791897416115\n",
      "iteration 1188, dc_loss: 0.048691075295209885, tv_loss: 0.033220984041690826\n",
      "iteration 1189, dc_loss: 0.04860306531190872, tv_loss: 0.03323369100689888\n",
      "iteration 1190, dc_loss: 0.04853927344083786, tv_loss: 0.033227160573005676\n",
      "iteration 1191, dc_loss: 0.04846222326159477, tv_loss: 0.0332372710108757\n",
      "iteration 1192, dc_loss: 0.048406511545181274, tv_loss: 0.033248718827962875\n",
      "iteration 1193, dc_loss: 0.04835585132241249, tv_loss: 0.0332520455121994\n",
      "iteration 1194, dc_loss: 0.048236116766929626, tv_loss: 0.033330053091049194\n",
      "iteration 1195, dc_loss: 0.0482734851539135, tv_loss: 0.033194445073604584\n",
      "iteration 1196, dc_loss: 0.048075005412101746, tv_loss: 0.033332519233226776\n",
      "iteration 1197, dc_loss: 0.0481748953461647, tv_loss: 0.033160217106342316\n",
      "iteration 1198, dc_loss: 0.04796949028968811, tv_loss: 0.03332459181547165\n",
      "iteration 1199, dc_loss: 0.04811463505029678, tv_loss: 0.03315240889787674\n",
      "iteration 1200, dc_loss: 0.04787224903702736, tv_loss: 0.033405330032110214\n",
      "iteration 1201, dc_loss: 0.04809148237109184, tv_loss: 0.033135440200567245\n",
      "iteration 1202, dc_loss: 0.047764722257852554, tv_loss: 0.03334818407893181\n",
      "iteration 1203, dc_loss: 0.0477171428501606, tv_loss: 0.03323788195848465\n",
      "iteration 1204, dc_loss: 0.04781411960721016, tv_loss: 0.033178746700286865\n",
      "iteration 1205, dc_loss: 0.04761641472578049, tv_loss: 0.03335030376911163\n",
      "iteration 1206, dc_loss: 0.04764346405863762, tv_loss: 0.03320308029651642\n",
      "iteration 1207, dc_loss: 0.04759787395596504, tv_loss: 0.033218979835510254\n",
      "iteration 1208, dc_loss: 0.04744580015540123, tv_loss: 0.03333407640457153\n",
      "iteration 1209, dc_loss: 0.047537725418806076, tv_loss: 0.033169329166412354\n",
      "iteration 1210, dc_loss: 0.047414399683475494, tv_loss: 0.03325467184185982\n",
      "iteration 1211, dc_loss: 0.04730387032032013, tv_loss: 0.03331505134701729\n",
      "iteration 1212, dc_loss: 0.04739508777856827, tv_loss: 0.033209528774023056\n",
      "iteration 1213, dc_loss: 0.04726765304803848, tv_loss: 0.0332871749997139\n",
      "iteration 1214, dc_loss: 0.04716663807630539, tv_loss: 0.033304400742053986\n",
      "iteration 1215, dc_loss: 0.047251779586076736, tv_loss: 0.03320099413394928\n",
      "iteration 1216, dc_loss: 0.047125931829214096, tv_loss: 0.033347003161907196\n",
      "iteration 1217, dc_loss: 0.04706985503435135, tv_loss: 0.03327513858675957\n",
      "iteration 1218, dc_loss: 0.047125332057476044, tv_loss: 0.03321259096264839\n",
      "iteration 1219, dc_loss: 0.04695410653948784, tv_loss: 0.03332702815532684\n",
      "iteration 1220, dc_loss: 0.0469646081328392, tv_loss: 0.03322884067893028\n",
      "iteration 1221, dc_loss: 0.046974342316389084, tv_loss: 0.03323002904653549\n",
      "iteration 1222, dc_loss: 0.046772003173828125, tv_loss: 0.033353157341480255\n",
      "iteration 1223, dc_loss: 0.0468515045940876, tv_loss: 0.033240724354982376\n",
      "iteration 1224, dc_loss: 0.04676143825054169, tv_loss: 0.033259227871894836\n",
      "iteration 1225, dc_loss: 0.046627238392829895, tv_loss: 0.03331970050930977\n",
      "iteration 1226, dc_loss: 0.04670584946870804, tv_loss: 0.03323245048522949\n",
      "iteration 1227, dc_loss: 0.04657316580414772, tv_loss: 0.0332830548286438\n",
      "iteration 1228, dc_loss: 0.046510715037584305, tv_loss: 0.03331659734249115\n",
      "iteration 1229, dc_loss: 0.04651989787817001, tv_loss: 0.033238817006349564\n",
      "iteration 1230, dc_loss: 0.04641653597354889, tv_loss: 0.033298954367637634\n",
      "iteration 1231, dc_loss: 0.04642174392938614, tv_loss: 0.03331291303038597\n",
      "iteration 1232, dc_loss: 0.0463433563709259, tv_loss: 0.03330826386809349\n",
      "iteration 1233, dc_loss: 0.0462968684732914, tv_loss: 0.033290937542915344\n",
      "iteration 1234, dc_loss: 0.04627075791358948, tv_loss: 0.033307384699583054\n",
      "iteration 1235, dc_loss: 0.04618643596768379, tv_loss: 0.03335481137037277\n",
      "iteration 1236, dc_loss: 0.046186696738004684, tv_loss: 0.03325515612959862\n",
      "iteration 1237, dc_loss: 0.046115756034851074, tv_loss: 0.03331649675965309\n",
      "iteration 1238, dc_loss: 0.04604000970721245, tv_loss: 0.03331783786416054\n",
      "iteration 1239, dc_loss: 0.046051979064941406, tv_loss: 0.03329591825604439\n",
      "iteration 1240, dc_loss: 0.045943424105644226, tv_loss: 0.03331573307514191\n",
      "iteration 1241, dc_loss: 0.04593276605010033, tv_loss: 0.03332975134253502\n",
      "iteration 1242, dc_loss: 0.045903805643320084, tv_loss: 0.03326354920864105\n",
      "iteration 1243, dc_loss: 0.04579683020710945, tv_loss: 0.033375151455402374\n",
      "iteration 1244, dc_loss: 0.045819900929927826, tv_loss: 0.03325536847114563\n",
      "iteration 1245, dc_loss: 0.04574670270085335, tv_loss: 0.03333815559744835\n",
      "iteration 1246, dc_loss: 0.04565194621682167, tv_loss: 0.03333015739917755\n",
      "iteration 1247, dc_loss: 0.04568704217672348, tv_loss: 0.03331330046057701\n",
      "iteration 1248, dc_loss: 0.04558074474334717, tv_loss: 0.03334176912903786\n",
      "iteration 1249, dc_loss: 0.0455472394824028, tv_loss: 0.03332719951868057\n",
      "iteration 1250, dc_loss: 0.045546650886535645, tv_loss: 0.033294256776571274\n",
      "iteration 1251, dc_loss: 0.045446764677762985, tv_loss: 0.0333370566368103\n",
      "iteration 1252, dc_loss: 0.04543366655707359, tv_loss: 0.033298641443252563\n",
      "iteration 1253, dc_loss: 0.04537886753678322, tv_loss: 0.033305034041404724\n",
      "iteration 1254, dc_loss: 0.04531651735305786, tv_loss: 0.0333760529756546\n",
      "iteration 1255, dc_loss: 0.04529218748211861, tv_loss: 0.033319104462862015\n",
      "iteration 1256, dc_loss: 0.04525632783770561, tv_loss: 0.03333134204149246\n",
      "iteration 1257, dc_loss: 0.04518549144268036, tv_loss: 0.033365171402692795\n",
      "iteration 1258, dc_loss: 0.045156080275774, tv_loss: 0.033325277268886566\n",
      "iteration 1259, dc_loss: 0.045124392956495285, tv_loss: 0.03329053893685341\n",
      "iteration 1260, dc_loss: 0.04506727680563927, tv_loss: 0.033329155296087265\n",
      "iteration 1261, dc_loss: 0.045015688985586166, tv_loss: 0.03333122655749321\n",
      "iteration 1262, dc_loss: 0.0449824295938015, tv_loss: 0.03333291783928871\n",
      "iteration 1263, dc_loss: 0.04493187740445137, tv_loss: 0.03332841768860817\n",
      "iteration 1264, dc_loss: 0.04489518702030182, tv_loss: 0.033336494117975235\n",
      "iteration 1265, dc_loss: 0.04484681040048599, tv_loss: 0.033348605036735535\n",
      "iteration 1266, dc_loss: 0.04480213299393654, tv_loss: 0.03332103416323662\n",
      "iteration 1267, dc_loss: 0.04478250816464424, tv_loss: 0.03332669287919998\n",
      "iteration 1268, dc_loss: 0.04471328482031822, tv_loss: 0.03334883600473404\n",
      "iteration 1269, dc_loss: 0.04465871304273605, tv_loss: 0.03336401283740997\n",
      "iteration 1270, dc_loss: 0.044671181589365005, tv_loss: 0.03328089043498039\n",
      "iteration 1271, dc_loss: 0.04458768665790558, tv_loss: 0.03335054591298103\n",
      "iteration 1272, dc_loss: 0.044530805200338364, tv_loss: 0.03334544226527214\n",
      "iteration 1273, dc_loss: 0.04451342672109604, tv_loss: 0.033331647515296936\n",
      "iteration 1274, dc_loss: 0.044462502002716064, tv_loss: 0.03332291916012764\n",
      "iteration 1275, dc_loss: 0.04444250464439392, tv_loss: 0.03333067521452904\n",
      "iteration 1276, dc_loss: 0.0443655364215374, tv_loss: 0.033372215926647186\n",
      "iteration 1277, dc_loss: 0.04432590305805206, tv_loss: 0.033353447914123535\n",
      "iteration 1278, dc_loss: 0.0443330854177475, tv_loss: 0.033293213695287704\n",
      "iteration 1279, dc_loss: 0.044234465807676315, tv_loss: 0.03334847092628479\n",
      "iteration 1280, dc_loss: 0.044215817004442215, tv_loss: 0.03336716815829277\n",
      "iteration 1281, dc_loss: 0.04417264834046364, tv_loss: 0.0333394818007946\n",
      "iteration 1282, dc_loss: 0.0441158190369606, tv_loss: 0.03334910422563553\n",
      "iteration 1283, dc_loss: 0.04411425068974495, tv_loss: 0.03330279886722565\n",
      "iteration 1284, dc_loss: 0.04403191804885864, tv_loss: 0.033389244228601456\n",
      "iteration 1285, dc_loss: 0.043991997838020325, tv_loss: 0.033375486731529236\n",
      "iteration 1286, dc_loss: 0.043981894850730896, tv_loss: 0.03331008180975914\n",
      "iteration 1287, dc_loss: 0.04389560967683792, tv_loss: 0.03337431326508522\n",
      "iteration 1288, dc_loss: 0.04390177130699158, tv_loss: 0.033360570669174194\n",
      "iteration 1289, dc_loss: 0.043852947652339935, tv_loss: 0.03334379568696022\n",
      "iteration 1290, dc_loss: 0.043770696967840195, tv_loss: 0.033365894109010696\n",
      "iteration 1291, dc_loss: 0.04377666860818863, tv_loss: 0.0333065390586853\n",
      "iteration 1292, dc_loss: 0.04371317848563194, tv_loss: 0.03333461284637451\n",
      "iteration 1293, dc_loss: 0.043663837015628815, tv_loss: 0.033367808908224106\n",
      "iteration 1294, dc_loss: 0.04364686459302902, tv_loss: 0.03338060900568962\n",
      "iteration 1295, dc_loss: 0.043573424220085144, tv_loss: 0.033381298184394836\n",
      "iteration 1296, dc_loss: 0.04358783736824989, tv_loss: 0.03330449387431145\n",
      "iteration 1297, dc_loss: 0.0434986837208271, tv_loss: 0.033342719078063965\n",
      "iteration 1298, dc_loss: 0.04346944019198418, tv_loss: 0.03334934636950493\n",
      "iteration 1299, dc_loss: 0.043444495648145676, tv_loss: 0.03337359428405762\n",
      "iteration 1300, dc_loss: 0.04338498041033745, tv_loss: 0.03338884934782982\n",
      "iteration 1301, dc_loss: 0.0433686263859272, tv_loss: 0.03332863375544548\n",
      "iteration 1302, dc_loss: 0.043289463967084885, tv_loss: 0.03335491940379143\n",
      "iteration 1303, dc_loss: 0.043290071189403534, tv_loss: 0.03331213444471359\n",
      "iteration 1304, dc_loss: 0.043221380561590195, tv_loss: 0.03333760052919388\n",
      "iteration 1305, dc_loss: 0.04320153966546059, tv_loss: 0.03331974893808365\n",
      "iteration 1306, dc_loss: 0.04314667731523514, tv_loss: 0.03334250673651695\n",
      "iteration 1307, dc_loss: 0.043114736676216125, tv_loss: 0.03336804732680321\n",
      "iteration 1308, dc_loss: 0.04306741803884506, tv_loss: 0.03340402618050575\n",
      "iteration 1309, dc_loss: 0.0430455282330513, tv_loss: 0.03335539624094963\n",
      "iteration 1310, dc_loss: 0.04295865818858147, tv_loss: 0.033378731459379196\n",
      "iteration 1311, dc_loss: 0.04297107458114624, tv_loss: 0.03331576660275459\n",
      "iteration 1312, dc_loss: 0.042898111045360565, tv_loss: 0.033354751765728\n",
      "iteration 1313, dc_loss: 0.042890798300504684, tv_loss: 0.03334534913301468\n",
      "iteration 1314, dc_loss: 0.042838335037231445, tv_loss: 0.033413343131542206\n",
      "iteration 1315, dc_loss: 0.04278542101383209, tv_loss: 0.03339952230453491\n",
      "iteration 1316, dc_loss: 0.04277471825480461, tv_loss: 0.03333587944507599\n",
      "iteration 1317, dc_loss: 0.04270090535283089, tv_loss: 0.03337051719427109\n",
      "iteration 1318, dc_loss: 0.04267379641532898, tv_loss: 0.033440180122852325\n",
      "iteration 1319, dc_loss: 0.0426531657576561, tv_loss: 0.03337956964969635\n",
      "iteration 1320, dc_loss: 0.04258687049150467, tv_loss: 0.03338043391704559\n",
      "iteration 1321, dc_loss: 0.04258761182427406, tv_loss: 0.03342575579881668\n",
      "iteration 1322, dc_loss: 0.042523354291915894, tv_loss: 0.03340282663702965\n",
      "iteration 1323, dc_loss: 0.042459581047296524, tv_loss: 0.033419545739889145\n",
      "iteration 1324, dc_loss: 0.042477428913116455, tv_loss: 0.033429160714149475\n",
      "iteration 1325, dc_loss: 0.04240098223090172, tv_loss: 0.03339211270213127\n",
      "iteration 1326, dc_loss: 0.04237717017531395, tv_loss: 0.03341984748840332\n",
      "iteration 1327, dc_loss: 0.04233282431960106, tv_loss: 0.03345372900366783\n",
      "iteration 1328, dc_loss: 0.04230250045657158, tv_loss: 0.03338317200541496\n",
      "iteration 1329, dc_loss: 0.0422559529542923, tv_loss: 0.03344309329986572\n",
      "iteration 1330, dc_loss: 0.042195264250040054, tv_loss: 0.03344743326306343\n",
      "iteration 1331, dc_loss: 0.04220544546842575, tv_loss: 0.033378906548023224\n",
      "iteration 1332, dc_loss: 0.04215385764837265, tv_loss: 0.03346268832683563\n",
      "iteration 1333, dc_loss: 0.04208648204803467, tv_loss: 0.033429261296987534\n",
      "iteration 1334, dc_loss: 0.042092565447092056, tv_loss: 0.033440716564655304\n",
      "iteration 1335, dc_loss: 0.04202481731772423, tv_loss: 0.033438991755247116\n",
      "iteration 1336, dc_loss: 0.04198106378316879, tv_loss: 0.033440232276916504\n",
      "iteration 1337, dc_loss: 0.041969213634729385, tv_loss: 0.03345330432057381\n",
      "iteration 1338, dc_loss: 0.041917022317647934, tv_loss: 0.0334191657602787\n",
      "iteration 1339, dc_loss: 0.04187662526965141, tv_loss: 0.03346488997340202\n",
      "iteration 1340, dc_loss: 0.04183686524629593, tv_loss: 0.03342569246888161\n",
      "iteration 1341, dc_loss: 0.041849203407764435, tv_loss: 0.033370014280080795\n",
      "iteration 1342, dc_loss: 0.041742559522390366, tv_loss: 0.03349791467189789\n",
      "iteration 1343, dc_loss: 0.04175698757171631, tv_loss: 0.03339265286922455\n",
      "iteration 1344, dc_loss: 0.0417100228369236, tv_loss: 0.033440664410591125\n",
      "iteration 1345, dc_loss: 0.04175283759832382, tv_loss: 0.033395156264305115\n",
      "iteration 1346, dc_loss: 0.04162914305925369, tv_loss: 0.033488981425762177\n",
      "iteration 1347, dc_loss: 0.04188651591539383, tv_loss: 0.033298999071121216\n",
      "iteration 1348, dc_loss: 0.04175253584980965, tv_loss: 0.03360480070114136\n",
      "iteration 1349, dc_loss: 0.042177312076091766, tv_loss: 0.0331961028277874\n",
      "iteration 1350, dc_loss: 0.04194742068648338, tv_loss: 0.033644042909145355\n",
      "iteration 1351, dc_loss: 0.04221729934215546, tv_loss: 0.033193979412317276\n",
      "iteration 1352, dc_loss: 0.04164190590381622, tv_loss: 0.03357213735580444\n",
      "iteration 1353, dc_loss: 0.04161687195301056, tv_loss: 0.033299852162599564\n",
      "iteration 1354, dc_loss: 0.04145180433988571, tv_loss: 0.03338921070098877\n",
      "iteration 1355, dc_loss: 0.04134254902601242, tv_loss: 0.03350578993558884\n",
      "iteration 1356, dc_loss: 0.04155740141868591, tv_loss: 0.03328341618180275\n",
      "iteration 1357, dc_loss: 0.041326556354761124, tv_loss: 0.033528897911310196\n",
      "iteration 1358, dc_loss: 0.04148438200354576, tv_loss: 0.03331189230084419\n",
      "iteration 1359, dc_loss: 0.041308291256427765, tv_loss: 0.03342069685459137\n",
      "iteration 1360, dc_loss: 0.04123419150710106, tv_loss: 0.03343137353658676\n",
      "iteration 1361, dc_loss: 0.04123024642467499, tv_loss: 0.03333186358213425\n",
      "iteration 1362, dc_loss: 0.0411573089659214, tv_loss: 0.03346308693289757\n",
      "iteration 1363, dc_loss: 0.04135223478078842, tv_loss: 0.03331702575087547\n",
      "iteration 1364, dc_loss: 0.04102591797709465, tv_loss: 0.03345226123929024\n",
      "iteration 1365, dc_loss: 0.04105377569794655, tv_loss: 0.03338531777262688\n",
      "iteration 1366, dc_loss: 0.041078321635723114, tv_loss: 0.03336469456553459\n",
      "iteration 1367, dc_loss: 0.04092356562614441, tv_loss: 0.03344139829277992\n",
      "iteration 1368, dc_loss: 0.041064921766519547, tv_loss: 0.033362723886966705\n",
      "iteration 1369, dc_loss: 0.04084716737270355, tv_loss: 0.03345499560236931\n",
      "iteration 1370, dc_loss: 0.04089396819472313, tv_loss: 0.03335009142756462\n",
      "iteration 1371, dc_loss: 0.04085936397314072, tv_loss: 0.03341372311115265\n",
      "iteration 1372, dc_loss: 0.04070102795958519, tv_loss: 0.033423323184251785\n",
      "iteration 1373, dc_loss: 0.040815576910972595, tv_loss: 0.03337453305721283\n",
      "iteration 1374, dc_loss: 0.040662623941898346, tv_loss: 0.0334668830037117\n",
      "iteration 1375, dc_loss: 0.04067542031407356, tv_loss: 0.033374935388565063\n",
      "iteration 1376, dc_loss: 0.04064277186989784, tv_loss: 0.03340071067214012\n",
      "iteration 1377, dc_loss: 0.04052552580833435, tv_loss: 0.03344586119055748\n",
      "iteration 1378, dc_loss: 0.040570735931396484, tv_loss: 0.03340214490890503\n",
      "iteration 1379, dc_loss: 0.040509216487407684, tv_loss: 0.03342566266655922\n",
      "iteration 1380, dc_loss: 0.04050545394420624, tv_loss: 0.03336511179804802\n",
      "iteration 1381, dc_loss: 0.040430549532175064, tv_loss: 0.033446330577135086\n",
      "iteration 1382, dc_loss: 0.040367722511291504, tv_loss: 0.033433154225349426\n",
      "iteration 1383, dc_loss: 0.04039817303419113, tv_loss: 0.03338795155286789\n",
      "iteration 1384, dc_loss: 0.040335770696401596, tv_loss: 0.03344671055674553\n",
      "iteration 1385, dc_loss: 0.040292445570230484, tv_loss: 0.033415842801332474\n",
      "iteration 1386, dc_loss: 0.04025783762335777, tv_loss: 0.03340170532464981\n",
      "iteration 1387, dc_loss: 0.04021785408258438, tv_loss: 0.03340904042124748\n",
      "iteration 1388, dc_loss: 0.040158290416002274, tv_loss: 0.03341684862971306\n",
      "iteration 1389, dc_loss: 0.0401604026556015, tv_loss: 0.033389996737241745\n",
      "iteration 1390, dc_loss: 0.04011838883161545, tv_loss: 0.033418770879507065\n",
      "iteration 1391, dc_loss: 0.04005345702171326, tv_loss: 0.03344571217894554\n",
      "iteration 1392, dc_loss: 0.040048375725746155, tv_loss: 0.03341936692595482\n",
      "iteration 1393, dc_loss: 0.03998655453324318, tv_loss: 0.033419396728277206\n",
      "iteration 1394, dc_loss: 0.039951957762241364, tv_loss: 0.033400796353816986\n",
      "iteration 1395, dc_loss: 0.039924781769514084, tv_loss: 0.03342307358980179\n",
      "iteration 1396, dc_loss: 0.03989534080028534, tv_loss: 0.03343704342842102\n",
      "iteration 1397, dc_loss: 0.039868392050266266, tv_loss: 0.03345304727554321\n",
      "iteration 1398, dc_loss: 0.039812978357076645, tv_loss: 0.03343648836016655\n",
      "iteration 1399, dc_loss: 0.03979415073990822, tv_loss: 0.03340836986899376\n",
      "iteration 1400, dc_loss: 0.0397629477083683, tv_loss: 0.033413153141736984\n",
      "iteration 1401, dc_loss: 0.03971124812960625, tv_loss: 0.033458590507507324\n",
      "iteration 1402, dc_loss: 0.039669763296842575, tv_loss: 0.03347548097372055\n",
      "iteration 1403, dc_loss: 0.03968507796525955, tv_loss: 0.0333985760807991\n",
      "iteration 1404, dc_loss: 0.03961583971977234, tv_loss: 0.033447664231061935\n",
      "iteration 1405, dc_loss: 0.0396001823246479, tv_loss: 0.03342583402991295\n",
      "iteration 1406, dc_loss: 0.039548471570014954, tv_loss: 0.03345293551683426\n",
      "iteration 1407, dc_loss: 0.03951118513941765, tv_loss: 0.033450160175561905\n",
      "iteration 1408, dc_loss: 0.03952173516154289, tv_loss: 0.03339175507426262\n",
      "iteration 1409, dc_loss: 0.03942100331187248, tv_loss: 0.033470917493104935\n",
      "iteration 1410, dc_loss: 0.039481498301029205, tv_loss: 0.033366426825523376\n",
      "iteration 1411, dc_loss: 0.039362356066703796, tv_loss: 0.03346895053982735\n",
      "iteration 1412, dc_loss: 0.03936704620718956, tv_loss: 0.03342001512646675\n",
      "iteration 1413, dc_loss: 0.03932943940162659, tv_loss: 0.03343454375863075\n",
      "iteration 1414, dc_loss: 0.03928570821881294, tv_loss: 0.03344103321433067\n",
      "iteration 1415, dc_loss: 0.039299093186855316, tv_loss: 0.03340506926178932\n",
      "iteration 1416, dc_loss: 0.03920910507440567, tv_loss: 0.03346980735659599\n",
      "iteration 1417, dc_loss: 0.03923089802265167, tv_loss: 0.03341227397322655\n",
      "iteration 1418, dc_loss: 0.03914763033390045, tv_loss: 0.03346681967377663\n",
      "iteration 1419, dc_loss: 0.039149992167949677, tv_loss: 0.03342519327998161\n",
      "iteration 1420, dc_loss: 0.039083853363990784, tv_loss: 0.03346184268593788\n",
      "iteration 1421, dc_loss: 0.03907504305243492, tv_loss: 0.03343457356095314\n",
      "iteration 1422, dc_loss: 0.0390433631837368, tv_loss: 0.033416036516427994\n",
      "iteration 1423, dc_loss: 0.03901359438896179, tv_loss: 0.03343707323074341\n",
      "iteration 1424, dc_loss: 0.0389779657125473, tv_loss: 0.03342803940176964\n",
      "iteration 1425, dc_loss: 0.03893740102648735, tv_loss: 0.03344588354229927\n",
      "iteration 1426, dc_loss: 0.03892046585679054, tv_loss: 0.033430855721235275\n",
      "iteration 1427, dc_loss: 0.0388985201716423, tv_loss: 0.03343365341424942\n",
      "iteration 1428, dc_loss: 0.03885100781917572, tv_loss: 0.033475447446107864\n",
      "iteration 1429, dc_loss: 0.038832325488328934, tv_loss: 0.033457811921834946\n",
      "iteration 1430, dc_loss: 0.038789212703704834, tv_loss: 0.03346198797225952\n",
      "iteration 1431, dc_loss: 0.03881487250328064, tv_loss: 0.033386874943971634\n",
      "iteration 1432, dc_loss: 0.03870148956775665, tv_loss: 0.033505238592624664\n",
      "iteration 1433, dc_loss: 0.03882588446140289, tv_loss: 0.033380597829818726\n",
      "iteration 1434, dc_loss: 0.038667336106300354, tv_loss: 0.03358866646885872\n",
      "iteration 1435, dc_loss: 0.03898270055651665, tv_loss: 0.033274948596954346\n",
      "iteration 1436, dc_loss: 0.03870958089828491, tv_loss: 0.033680181950330734\n",
      "iteration 1437, dc_loss: 0.03921079263091087, tv_loss: 0.03320131078362465\n",
      "iteration 1438, dc_loss: 0.03882177919149399, tv_loss: 0.03370491787791252\n",
      "iteration 1439, dc_loss: 0.039161015301942825, tv_loss: 0.03324900567531586\n",
      "iteration 1440, dc_loss: 0.03868266940116882, tv_loss: 0.033626437187194824\n",
      "iteration 1441, dc_loss: 0.03876866400241852, tv_loss: 0.03335314244031906\n",
      "iteration 1442, dc_loss: 0.03855004906654358, tv_loss: 0.03343600779771805\n",
      "iteration 1443, dc_loss: 0.03837083280086517, tv_loss: 0.03353124111890793\n",
      "iteration 1444, dc_loss: 0.03855776786804199, tv_loss: 0.03334542736411095\n",
      "iteration 1445, dc_loss: 0.03833969682455063, tv_loss: 0.033604931086301804\n",
      "iteration 1446, dc_loss: 0.038585927337408066, tv_loss: 0.03332770988345146\n",
      "iteration 1447, dc_loss: 0.03836199268698692, tv_loss: 0.0335281603038311\n",
      "iteration 1448, dc_loss: 0.038384415209293365, tv_loss: 0.03341003879904747\n",
      "iteration 1449, dc_loss: 0.038310207426548004, tv_loss: 0.033419784158468246\n",
      "iteration 1450, dc_loss: 0.038152627646923065, tv_loss: 0.03354542702436447\n",
      "iteration 1451, dc_loss: 0.03829555585980415, tv_loss: 0.03338753432035446\n",
      "iteration 1452, dc_loss: 0.038148801773786545, tv_loss: 0.033547304570674896\n",
      "iteration 1453, dc_loss: 0.03826281428337097, tv_loss: 0.033384062349796295\n",
      "iteration 1454, dc_loss: 0.03808962553739548, tv_loss: 0.033508289605379105\n",
      "iteration 1455, dc_loss: 0.03809601813554764, tv_loss: 0.03344803303480148\n",
      "iteration 1456, dc_loss: 0.03803270310163498, tv_loss: 0.03347576782107353\n",
      "iteration 1457, dc_loss: 0.03796372562646866, tv_loss: 0.0335082970559597\n",
      "iteration 1458, dc_loss: 0.03805080056190491, tv_loss: 0.03338250145316124\n",
      "iteration 1459, dc_loss: 0.037911418825387955, tv_loss: 0.03351568803191185\n",
      "iteration 1460, dc_loss: 0.037988077849149704, tv_loss: 0.03341027721762657\n",
      "iteration 1461, dc_loss: 0.03785361722111702, tv_loss: 0.03351026028394699\n",
      "iteration 1462, dc_loss: 0.03788337484002113, tv_loss: 0.033435363322496414\n",
      "iteration 1463, dc_loss: 0.03781381621956825, tv_loss: 0.033471446484327316\n",
      "iteration 1464, dc_loss: 0.03775882348418236, tv_loss: 0.03348791226744652\n",
      "iteration 1465, dc_loss: 0.03779402747750282, tv_loss: 0.033419568091630936\n",
      "iteration 1466, dc_loss: 0.03769111633300781, tv_loss: 0.033489424735307693\n",
      "iteration 1467, dc_loss: 0.037767037749290466, tv_loss: 0.033405669033527374\n",
      "iteration 1468, dc_loss: 0.03762038052082062, tv_loss: 0.03351952135562897\n",
      "iteration 1469, dc_loss: 0.0377236045897007, tv_loss: 0.03339764475822449\n",
      "iteration 1470, dc_loss: 0.037581343203783035, tv_loss: 0.03351974114775658\n",
      "iteration 1471, dc_loss: 0.03764137998223305, tv_loss: 0.03343711048364639\n",
      "iteration 1472, dc_loss: 0.03751387819647789, tv_loss: 0.033528123050928116\n",
      "iteration 1473, dc_loss: 0.03757501766085625, tv_loss: 0.03342418372631073\n",
      "iteration 1474, dc_loss: 0.037458617240190506, tv_loss: 0.033489201217889786\n",
      "iteration 1475, dc_loss: 0.03747370094060898, tv_loss: 0.033456701785326004\n",
      "iteration 1476, dc_loss: 0.037424392998218536, tv_loss: 0.033487387001514435\n",
      "iteration 1477, dc_loss: 0.03744269907474518, tv_loss: 0.03344925493001938\n",
      "iteration 1478, dc_loss: 0.037379294633865356, tv_loss: 0.03349020704627037\n",
      "iteration 1479, dc_loss: 0.03733697161078453, tv_loss: 0.0334959551692009\n",
      "iteration 1480, dc_loss: 0.03732997924089432, tv_loss: 0.033460699021816254\n",
      "iteration 1481, dc_loss: 0.03727749362587929, tv_loss: 0.0334855355322361\n",
      "iteration 1482, dc_loss: 0.037320949137210846, tv_loss: 0.033416975289583206\n",
      "iteration 1483, dc_loss: 0.03720853850245476, tv_loss: 0.03350432217121124\n",
      "iteration 1484, dc_loss: 0.037304528057575226, tv_loss: 0.03338775411248207\n",
      "iteration 1485, dc_loss: 0.03713317587971687, tv_loss: 0.0335627943277359\n",
      "iteration 1486, dc_loss: 0.03730710968375206, tv_loss: 0.03338761627674103\n",
      "iteration 1487, dc_loss: 0.03708355873823166, tv_loss: 0.03364885225892067\n",
      "iteration 1488, dc_loss: 0.03732864931225777, tv_loss: 0.03334661200642586\n",
      "iteration 1489, dc_loss: 0.03704507276415825, tv_loss: 0.03360149264335632\n",
      "iteration 1490, dc_loss: 0.03728507459163666, tv_loss: 0.03334914147853851\n",
      "iteration 1491, dc_loss: 0.037019938230514526, tv_loss: 0.03362453728914261\n",
      "iteration 1492, dc_loss: 0.03725077211856842, tv_loss: 0.033372458070516586\n",
      "iteration 1493, dc_loss: 0.03704381734132767, tv_loss: 0.03356426581740379\n",
      "iteration 1494, dc_loss: 0.037154920399188995, tv_loss: 0.033413469791412354\n",
      "iteration 1495, dc_loss: 0.03702317178249359, tv_loss: 0.033523090183734894\n",
      "iteration 1496, dc_loss: 0.03704926371574402, tv_loss: 0.03345201164484024\n",
      "iteration 1497, dc_loss: 0.03696123883128166, tv_loss: 0.033504944294691086\n",
      "iteration 1498, dc_loss: 0.03689337894320488, tv_loss: 0.03349664434790611\n",
      "iteration 1499, dc_loss: 0.03680998086929321, tv_loss: 0.03349214047193527\n",
      "iteration 1500, dc_loss: 0.036797840148210526, tv_loss: 0.03345465287566185\n",
      "iteration 1501, dc_loss: 0.036736927926540375, tv_loss: 0.033479586243629456\n",
      "iteration 1502, dc_loss: 0.03675777092576027, tv_loss: 0.03345029056072235\n",
      "iteration 1503, dc_loss: 0.03670284524559975, tv_loss: 0.03348672762513161\n",
      "iteration 1504, dc_loss: 0.03672608733177185, tv_loss: 0.033434852957725525\n",
      "iteration 1505, dc_loss: 0.03667447715997696, tv_loss: 0.033457133919000626\n",
      "iteration 1506, dc_loss: 0.03661603480577469, tv_loss: 0.03348121419548988\n",
      "iteration 1507, dc_loss: 0.03662487491965294, tv_loss: 0.03344380855560303\n",
      "iteration 1508, dc_loss: 0.03653937950730324, tv_loss: 0.03352881968021393\n",
      "iteration 1509, dc_loss: 0.03665298596024513, tv_loss: 0.03341333195567131\n",
      "iteration 1510, dc_loss: 0.036452967673540115, tv_loss: 0.03360695019364357\n",
      "iteration 1511, dc_loss: 0.03664021193981171, tv_loss: 0.033390577882528305\n",
      "iteration 1512, dc_loss: 0.03639953210949898, tv_loss: 0.03358563035726547\n",
      "iteration 1513, dc_loss: 0.03655507415533066, tv_loss: 0.03338538110256195\n",
      "iteration 1514, dc_loss: 0.0363614596426487, tv_loss: 0.033557333052158356\n",
      "iteration 1515, dc_loss: 0.036497216671705246, tv_loss: 0.033385373651981354\n",
      "iteration 1516, dc_loss: 0.03632371872663498, tv_loss: 0.03354860469698906\n",
      "iteration 1517, dc_loss: 0.03646484762430191, tv_loss: 0.03338763490319252\n",
      "iteration 1518, dc_loss: 0.036309827119112015, tv_loss: 0.0335562564432621\n",
      "iteration 1519, dc_loss: 0.03642752394080162, tv_loss: 0.03347324952483177\n",
      "iteration 1520, dc_loss: 0.036265578120946884, tv_loss: 0.03360632061958313\n",
      "iteration 1521, dc_loss: 0.03638036921620369, tv_loss: 0.03340750187635422\n",
      "iteration 1522, dc_loss: 0.03622167184948921, tv_loss: 0.033557843416929245\n",
      "iteration 1523, dc_loss: 0.03633112460374832, tv_loss: 0.033462755382061005\n",
      "iteration 1524, dc_loss: 0.03613129258155823, tv_loss: 0.03361600264906883\n",
      "iteration 1525, dc_loss: 0.03627565875649452, tv_loss: 0.03339440003037453\n",
      "iteration 1526, dc_loss: 0.036050714552402496, tv_loss: 0.033635303378105164\n",
      "iteration 1527, dc_loss: 0.03622838482260704, tv_loss: 0.033432915806770325\n",
      "iteration 1528, dc_loss: 0.035989001393318176, tv_loss: 0.03359127789735794\n",
      "iteration 1529, dc_loss: 0.036118123680353165, tv_loss: 0.033431027084589005\n",
      "iteration 1530, dc_loss: 0.03594544902443886, tv_loss: 0.0335906557738781\n",
      "iteration 1531, dc_loss: 0.036020807921886444, tv_loss: 0.03347742184996605\n",
      "iteration 1532, dc_loss: 0.035924628376960754, tv_loss: 0.03353201225399971\n",
      "iteration 1533, dc_loss: 0.035940833389759064, tv_loss: 0.03346746042370796\n",
      "iteration 1534, dc_loss: 0.03587455302476883, tv_loss: 0.033522117882966995\n",
      "iteration 1535, dc_loss: 0.03589276969432831, tv_loss: 0.033469174057245255\n",
      "iteration 1536, dc_loss: 0.03582387417554855, tv_loss: 0.03350260481238365\n",
      "iteration 1537, dc_loss: 0.03583190590143204, tv_loss: 0.033463578671216965\n",
      "iteration 1538, dc_loss: 0.03577783703804016, tv_loss: 0.03349728137254715\n",
      "iteration 1539, dc_loss: 0.03576795384287834, tv_loss: 0.03349126875400543\n",
      "iteration 1540, dc_loss: 0.03573647513985634, tv_loss: 0.033542245626449585\n",
      "iteration 1541, dc_loss: 0.035729676485061646, tv_loss: 0.033506978303194046\n",
      "iteration 1542, dc_loss: 0.035667531192302704, tv_loss: 0.03351730853319168\n",
      "iteration 1543, dc_loss: 0.035707321017980576, tv_loss: 0.033456794917583466\n",
      "iteration 1544, dc_loss: 0.035604629665613174, tv_loss: 0.03359369561076164\n",
      "iteration 1545, dc_loss: 0.03568914532661438, tv_loss: 0.0334748774766922\n",
      "iteration 1546, dc_loss: 0.035580676048994064, tv_loss: 0.03353758156299591\n",
      "iteration 1547, dc_loss: 0.03569534048438072, tv_loss: 0.0334404818713665\n",
      "iteration 1548, dc_loss: 0.03555018827319145, tv_loss: 0.03366722911596298\n",
      "iteration 1549, dc_loss: 0.03584974259138107, tv_loss: 0.03338165208697319\n",
      "iteration 1550, dc_loss: 0.035686127841472626, tv_loss: 0.03368594869971275\n",
      "iteration 1551, dc_loss: 0.03614573925733566, tv_loss: 0.03330602869391441\n",
      "iteration 1552, dc_loss: 0.03592076152563095, tv_loss: 0.033762868493795395\n",
      "iteration 1553, dc_loss: 0.036313753575086594, tv_loss: 0.03330173343420029\n",
      "iteration 1554, dc_loss: 0.03583564609289169, tv_loss: 0.03369346633553505\n",
      "iteration 1555, dc_loss: 0.0358063243329525, tv_loss: 0.033395517617464066\n",
      "iteration 1556, dc_loss: 0.03542738035321236, tv_loss: 0.033546846359968185\n",
      "iteration 1557, dc_loss: 0.03534874692559242, tv_loss: 0.0335334911942482\n",
      "iteration 1558, dc_loss: 0.035530973225831985, tv_loss: 0.03340104967355728\n",
      "iteration 1559, dc_loss: 0.03541604429483414, tv_loss: 0.03360747918486595\n",
      "iteration 1560, dc_loss: 0.03558969870209694, tv_loss: 0.0333995558321476\n",
      "iteration 1561, dc_loss: 0.035340238362550735, tv_loss: 0.03358856216073036\n",
      "iteration 1562, dc_loss: 0.035352371633052826, tv_loss: 0.03344971686601639\n",
      "iteration 1563, dc_loss: 0.03523855283856392, tv_loss: 0.03355023264884949\n",
      "iteration 1564, dc_loss: 0.03528212383389473, tv_loss: 0.033506568521261215\n",
      "iteration 1565, dc_loss: 0.03528019040822983, tv_loss: 0.03349527716636658\n",
      "iteration 1566, dc_loss: 0.03523889183998108, tv_loss: 0.03353621065616608\n",
      "iteration 1567, dc_loss: 0.03520321100950241, tv_loss: 0.033483706414699554\n",
      "iteration 1568, dc_loss: 0.035068877041339874, tv_loss: 0.03355851769447327\n",
      "iteration 1569, dc_loss: 0.035132408142089844, tv_loss: 0.03347131237387657\n",
      "iteration 1570, dc_loss: 0.03508609160780907, tv_loss: 0.033511560410261154\n",
      "iteration 1571, dc_loss: 0.03506944328546524, tv_loss: 0.033488865941762924\n",
      "iteration 1572, dc_loss: 0.035044264048337936, tv_loss: 0.03345778211951256\n",
      "iteration 1573, dc_loss: 0.03493919596076012, tv_loss: 0.03352491557598114\n",
      "iteration 1574, dc_loss: 0.035016871988773346, tv_loss: 0.033442579209804535\n",
      "iteration 1575, dc_loss: 0.03489457443356514, tv_loss: 0.03356456384062767\n",
      "iteration 1576, dc_loss: 0.034978292882442474, tv_loss: 0.03349566087126732\n",
      "iteration 1577, dc_loss: 0.03488663211464882, tv_loss: 0.03354071453213692\n",
      "iteration 1578, dc_loss: 0.034814439713954926, tv_loss: 0.03352554142475128\n",
      "iteration 1579, dc_loss: 0.034890685230493546, tv_loss: 0.03344006836414337\n",
      "iteration 1580, dc_loss: 0.03475979343056679, tv_loss: 0.03359609842300415\n",
      "iteration 1581, dc_loss: 0.03488466888666153, tv_loss: 0.03348185867071152\n",
      "iteration 1582, dc_loss: 0.034720126539468765, tv_loss: 0.033574189990758896\n",
      "iteration 1583, dc_loss: 0.034727826714515686, tv_loss: 0.0334957018494606\n",
      "iteration 1584, dc_loss: 0.03473449870944023, tv_loss: 0.033469829708337784\n",
      "iteration 1585, dc_loss: 0.034672223031520844, tv_loss: 0.033574655652046204\n",
      "iteration 1586, dc_loss: 0.03471776470541954, tv_loss: 0.03351525589823723\n",
      "iteration 1587, dc_loss: 0.034606512635946274, tv_loss: 0.033538129180669785\n",
      "iteration 1588, dc_loss: 0.034611161798238754, tv_loss: 0.0335090309381485\n",
      "iteration 1589, dc_loss: 0.03460052236914635, tv_loss: 0.033542878925800323\n",
      "iteration 1590, dc_loss: 0.03455132991075516, tv_loss: 0.03358535096049309\n",
      "iteration 1591, dc_loss: 0.034563951194286346, tv_loss: 0.03349321708083153\n",
      "iteration 1592, dc_loss: 0.03453376144170761, tv_loss: 0.0335313081741333\n",
      "iteration 1593, dc_loss: 0.03448767960071564, tv_loss: 0.03356320783495903\n",
      "iteration 1594, dc_loss: 0.03450435772538185, tv_loss: 0.03349534049630165\n",
      "iteration 1595, dc_loss: 0.034424182027578354, tv_loss: 0.0335400253534317\n",
      "iteration 1596, dc_loss: 0.034473732113838196, tv_loss: 0.033482082188129425\n",
      "iteration 1597, dc_loss: 0.03437238186597824, tv_loss: 0.033567819744348526\n",
      "iteration 1598, dc_loss: 0.03443801403045654, tv_loss: 0.033492643386125565\n",
      "iteration 1599, dc_loss: 0.03433363884687424, tv_loss: 0.033582575619220734\n",
      "iteration 1600, dc_loss: 0.034453585743904114, tv_loss: 0.0334344357252121\n",
      "iteration 1601, dc_loss: 0.034270573407411575, tv_loss: 0.03363358974456787\n",
      "iteration 1602, dc_loss: 0.03440804407000542, tv_loss: 0.033424898982048035\n",
      "iteration 1603, dc_loss: 0.03430139273405075, tv_loss: 0.03348603472113609\n",
      "iteration 1604, dc_loss: 0.03420756757259369, tv_loss: 0.03358706459403038\n",
      "iteration 1605, dc_loss: 0.03433986380696297, tv_loss: 0.033428262919187546\n",
      "iteration 1606, dc_loss: 0.03421685844659805, tv_loss: 0.0335078202188015\n",
      "iteration 1607, dc_loss: 0.03417490795254707, tv_loss: 0.033544715493917465\n",
      "iteration 1608, dc_loss: 0.034286074340343475, tv_loss: 0.03343188390135765\n",
      "iteration 1609, dc_loss: 0.03414159268140793, tv_loss: 0.033550456166267395\n",
      "iteration 1610, dc_loss: 0.03415708616375923, tv_loss: 0.03351019322872162\n",
      "iteration 1611, dc_loss: 0.03419508785009384, tv_loss: 0.03346269205212593\n",
      "iteration 1612, dc_loss: 0.03408786654472351, tv_loss: 0.03355932608246803\n",
      "iteration 1613, dc_loss: 0.03411911427974701, tv_loss: 0.03348779305815697\n",
      "iteration 1614, dc_loss: 0.03411395102739334, tv_loss: 0.03346700221300125\n",
      "iteration 1615, dc_loss: 0.03404488041996956, tv_loss: 0.03352820873260498\n",
      "iteration 1616, dc_loss: 0.034086450934410095, tv_loss: 0.03346369415521622\n",
      "iteration 1617, dc_loss: 0.034035179764032364, tv_loss: 0.033502403646707535\n",
      "iteration 1618, dc_loss: 0.033998843282461166, tv_loss: 0.03353526443243027\n",
      "iteration 1619, dc_loss: 0.034042224287986755, tv_loss: 0.033490728586912155\n",
      "iteration 1620, dc_loss: 0.033959295600652695, tv_loss: 0.03353886678814888\n",
      "iteration 1621, dc_loss: 0.03397151455283165, tv_loss: 0.033492691814899445\n",
      "iteration 1622, dc_loss: 0.03398102521896362, tv_loss: 0.033468883484601974\n",
      "iteration 1623, dc_loss: 0.033912744373083115, tv_loss: 0.0335177406668663\n",
      "iteration 1624, dc_loss: 0.033929578959941864, tv_loss: 0.03348233178257942\n",
      "iteration 1625, dc_loss: 0.03390321135520935, tv_loss: 0.03348696976900101\n",
      "iteration 1626, dc_loss: 0.03386509418487549, tv_loss: 0.03351613134145737\n",
      "iteration 1627, dc_loss: 0.03389626741409302, tv_loss: 0.03349334001541138\n",
      "iteration 1628, dc_loss: 0.03383670374751091, tv_loss: 0.03355216979980469\n",
      "iteration 1629, dc_loss: 0.03383263573050499, tv_loss: 0.03350651636719704\n",
      "iteration 1630, dc_loss: 0.03382284566760063, tv_loss: 0.033490777015686035\n",
      "iteration 1631, dc_loss: 0.03377585858106613, tv_loss: 0.03351876884698868\n",
      "iteration 1632, dc_loss: 0.03380382061004639, tv_loss: 0.03348010778427124\n",
      "iteration 1633, dc_loss: 0.033767636865377426, tv_loss: 0.03350343555212021\n",
      "iteration 1634, dc_loss: 0.03374211862683296, tv_loss: 0.03353384509682655\n",
      "iteration 1635, dc_loss: 0.033738307654857635, tv_loss: 0.033518075942993164\n",
      "iteration 1636, dc_loss: 0.033710092306137085, tv_loss: 0.033511631190776825\n",
      "iteration 1637, dc_loss: 0.03368956223130226, tv_loss: 0.03350738808512688\n",
      "iteration 1638, dc_loss: 0.033688776195049286, tv_loss: 0.03349069133400917\n",
      "iteration 1639, dc_loss: 0.03365730121731758, tv_loss: 0.033511288464069366\n",
      "iteration 1640, dc_loss: 0.03364973142743111, tv_loss: 0.03351225703954697\n",
      "iteration 1641, dc_loss: 0.03364258259534836, tv_loss: 0.03352190554141998\n",
      "iteration 1642, dc_loss: 0.03359747305512428, tv_loss: 0.033529605716466904\n",
      "iteration 1643, dc_loss: 0.033607084304094315, tv_loss: 0.033493395894765854\n",
      "iteration 1644, dc_loss: 0.03358529880642891, tv_loss: 0.0334944985806942\n",
      "iteration 1645, dc_loss: 0.03355320170521736, tv_loss: 0.03351027891039848\n",
      "iteration 1646, dc_loss: 0.033551886677742004, tv_loss: 0.03350304439663887\n",
      "iteration 1647, dc_loss: 0.03352677449584007, tv_loss: 0.033530570566654205\n",
      "iteration 1648, dc_loss: 0.03352503478527069, tv_loss: 0.03352715075016022\n",
      "iteration 1649, dc_loss: 0.03348971903324127, tv_loss: 0.033524394035339355\n",
      "iteration 1650, dc_loss: 0.03348517045378685, tv_loss: 0.03350105509161949\n",
      "iteration 1651, dc_loss: 0.03346661105751991, tv_loss: 0.033496614545583725\n",
      "iteration 1652, dc_loss: 0.033443257212638855, tv_loss: 0.03350849449634552\n",
      "iteration 1653, dc_loss: 0.03344191610813141, tv_loss: 0.033500585705041885\n",
      "iteration 1654, dc_loss: 0.03340228274464607, tv_loss: 0.03355540335178375\n",
      "iteration 1655, dc_loss: 0.03340571001172066, tv_loss: 0.033512942492961884\n",
      "iteration 1656, dc_loss: 0.033379968255758286, tv_loss: 0.03351840004324913\n",
      "iteration 1657, dc_loss: 0.03337937965989113, tv_loss: 0.033488474786281586\n",
      "iteration 1658, dc_loss: 0.0333591066300869, tv_loss: 0.03348805010318756\n",
      "iteration 1659, dc_loss: 0.033320918679237366, tv_loss: 0.033517833799123764\n",
      "iteration 1660, dc_loss: 0.033326178789138794, tv_loss: 0.033498313277959824\n",
      "iteration 1661, dc_loss: 0.033296138048172, tv_loss: 0.033537138253450394\n",
      "iteration 1662, dc_loss: 0.03328842669725418, tv_loss: 0.03354937583208084\n",
      "iteration 1663, dc_loss: 0.033261511474847794, tv_loss: 0.03352813795208931\n",
      "iteration 1664, dc_loss: 0.03325694799423218, tv_loss: 0.03349699825048447\n",
      "iteration 1665, dc_loss: 0.03324663266539574, tv_loss: 0.03350793197751045\n",
      "iteration 1666, dc_loss: 0.03322237357497215, tv_loss: 0.03354952856898308\n",
      "iteration 1667, dc_loss: 0.033211834728717804, tv_loss: 0.03352656960487366\n",
      "iteration 1668, dc_loss: 0.033176593482494354, tv_loss: 0.033526308834552765\n",
      "iteration 1669, dc_loss: 0.03318425267934799, tv_loss: 0.03349635377526283\n",
      "iteration 1670, dc_loss: 0.033165473490953445, tv_loss: 0.03350549936294556\n",
      "iteration 1671, dc_loss: 0.03313048556447029, tv_loss: 0.03353229537606239\n",
      "iteration 1672, dc_loss: 0.03313891589641571, tv_loss: 0.03352135419845581\n",
      "iteration 1673, dc_loss: 0.03310662880539894, tv_loss: 0.0335225947201252\n",
      "iteration 1674, dc_loss: 0.03309947997331619, tv_loss: 0.03350232541561127\n",
      "iteration 1675, dc_loss: 0.03307100385427475, tv_loss: 0.033515699207782745\n",
      "iteration 1676, dc_loss: 0.03305928036570549, tv_loss: 0.03350841999053955\n",
      "iteration 1677, dc_loss: 0.03305717557668686, tv_loss: 0.03349333256483078\n",
      "iteration 1678, dc_loss: 0.033028654754161835, tv_loss: 0.03350900113582611\n",
      "iteration 1679, dc_loss: 0.03302018716931343, tv_loss: 0.033516738563776016\n",
      "iteration 1680, dc_loss: 0.03299623355269432, tv_loss: 0.03355831652879715\n",
      "iteration 1681, dc_loss: 0.03299514949321747, tv_loss: 0.03352411836385727\n",
      "iteration 1682, dc_loss: 0.03295483812689781, tv_loss: 0.03352038189768791\n",
      "iteration 1683, dc_loss: 0.032963503152132034, tv_loss: 0.03350294008851051\n",
      "iteration 1684, dc_loss: 0.032935917377471924, tv_loss: 0.03353066369891167\n",
      "iteration 1685, dc_loss: 0.03291526436805725, tv_loss: 0.03355361148715019\n",
      "iteration 1686, dc_loss: 0.03291856870055199, tv_loss: 0.03351322561502457\n",
      "iteration 1687, dc_loss: 0.03287428617477417, tv_loss: 0.0335255041718483\n",
      "iteration 1688, dc_loss: 0.03288906067609787, tv_loss: 0.033502597361803055\n",
      "iteration 1689, dc_loss: 0.032860975712537766, tv_loss: 0.03351714462041855\n",
      "iteration 1690, dc_loss: 0.03284180164337158, tv_loss: 0.033536043018102646\n",
      "iteration 1691, dc_loss: 0.0328296422958374, tv_loss: 0.03352653980255127\n",
      "iteration 1692, dc_loss: 0.032813239842653275, tv_loss: 0.033522725105285645\n",
      "iteration 1693, dc_loss: 0.03280538693070412, tv_loss: 0.03350707143545151\n",
      "iteration 1694, dc_loss: 0.03277666121721268, tv_loss: 0.03352319821715355\n",
      "iteration 1695, dc_loss: 0.032763510942459106, tv_loss: 0.03350912407040596\n",
      "iteration 1696, dc_loss: 0.032754454761743546, tv_loss: 0.033518269658088684\n",
      "iteration 1697, dc_loss: 0.03273941949009895, tv_loss: 0.03350410982966423\n",
      "iteration 1698, dc_loss: 0.032713476568460464, tv_loss: 0.03352053090929985\n",
      "iteration 1699, dc_loss: 0.0327148512005806, tv_loss: 0.03350022807717323\n",
      "iteration 1700, dc_loss: 0.03267965465784073, tv_loss: 0.0335262194275856\n",
      "iteration 1701, dc_loss: 0.03269611671566963, tv_loss: 0.03351178020238876\n",
      "iteration 1702, dc_loss: 0.03264796361327171, tv_loss: 0.03356662765145302\n",
      "iteration 1703, dc_loss: 0.0326516330242157, tv_loss: 0.03351258113980293\n",
      "iteration 1704, dc_loss: 0.03261827304959297, tv_loss: 0.03352302312850952\n",
      "iteration 1705, dc_loss: 0.03261800855398178, tv_loss: 0.033525560051202774\n",
      "iteration 1706, dc_loss: 0.0326090082526207, tv_loss: 0.033537548035383224\n",
      "iteration 1707, dc_loss: 0.03256623446941376, tv_loss: 0.03356461226940155\n",
      "iteration 1708, dc_loss: 0.0325855128467083, tv_loss: 0.033510804176330566\n",
      "iteration 1709, dc_loss: 0.03254341334104538, tv_loss: 0.033521123230457306\n",
      "iteration 1710, dc_loss: 0.03253776207566261, tv_loss: 0.03351619094610214\n",
      "iteration 1711, dc_loss: 0.03252962604165077, tv_loss: 0.03351292014122009\n",
      "iteration 1712, dc_loss: 0.032508108764886856, tv_loss: 0.03352835401892662\n",
      "iteration 1713, dc_loss: 0.03250492736697197, tv_loss: 0.03352709114551544\n",
      "iteration 1714, dc_loss: 0.03247113898396492, tv_loss: 0.03353797271847725\n",
      "iteration 1715, dc_loss: 0.03245750069618225, tv_loss: 0.033525194972753525\n",
      "iteration 1716, dc_loss: 0.03245362639427185, tv_loss: 0.03350915387272835\n",
      "iteration 1717, dc_loss: 0.03243263438344002, tv_loss: 0.033526644110679626\n",
      "iteration 1718, dc_loss: 0.03242909535765648, tv_loss: 0.03352796286344528\n",
      "iteration 1719, dc_loss: 0.032378602772951126, tv_loss: 0.03357227146625519\n",
      "iteration 1720, dc_loss: 0.03240225836634636, tv_loss: 0.03352353721857071\n",
      "iteration 1721, dc_loss: 0.032374151051044464, tv_loss: 0.03352529555559158\n",
      "iteration 1722, dc_loss: 0.03237206116318703, tv_loss: 0.03350913152098656\n",
      "iteration 1723, dc_loss: 0.03233732283115387, tv_loss: 0.033541325479745865\n",
      "iteration 1724, dc_loss: 0.032321829348802567, tv_loss: 0.033535122871398926\n",
      "iteration 1725, dc_loss: 0.03232696279883385, tv_loss: 0.03351849317550659\n",
      "iteration 1726, dc_loss: 0.032292407006025314, tv_loss: 0.03353169932961464\n",
      "iteration 1727, dc_loss: 0.03228329122066498, tv_loss: 0.033520910888910294\n",
      "iteration 1728, dc_loss: 0.032297033816576004, tv_loss: 0.03349318355321884\n",
      "iteration 1729, dc_loss: 0.03223496302962303, tv_loss: 0.0335366390645504\n",
      "iteration 1730, dc_loss: 0.03226570412516594, tv_loss: 0.033496033400297165\n",
      "iteration 1731, dc_loss: 0.03221248835325241, tv_loss: 0.03353102132678032\n",
      "iteration 1732, dc_loss: 0.03224731981754303, tv_loss: 0.03348703682422638\n",
      "iteration 1733, dc_loss: 0.03218385577201843, tv_loss: 0.033544957637786865\n",
      "iteration 1734, dc_loss: 0.03221878781914711, tv_loss: 0.033500127494335175\n",
      "iteration 1735, dc_loss: 0.032168518751859665, tv_loss: 0.03357117995619774\n",
      "iteration 1736, dc_loss: 0.03220939263701439, tv_loss: 0.03352028876543045\n",
      "iteration 1737, dc_loss: 0.032128237187862396, tv_loss: 0.03356248512864113\n",
      "iteration 1738, dc_loss: 0.03216233104467392, tv_loss: 0.033495139330625534\n",
      "iteration 1739, dc_loss: 0.0321076437830925, tv_loss: 0.03357527777552605\n",
      "iteration 1740, dc_loss: 0.0321599543094635, tv_loss: 0.03352285176515579\n",
      "iteration 1741, dc_loss: 0.032080113887786865, tv_loss: 0.03356687352061272\n",
      "iteration 1742, dc_loss: 0.03215300664305687, tv_loss: 0.033500492572784424\n",
      "iteration 1743, dc_loss: 0.0320885069668293, tv_loss: 0.03357686847448349\n",
      "iteration 1744, dc_loss: 0.032123763114213943, tv_loss: 0.033530570566654205\n",
      "iteration 1745, dc_loss: 0.0320623405277729, tv_loss: 0.033537667244672775\n",
      "iteration 1746, dc_loss: 0.03205886110663414, tv_loss: 0.03350602835416794\n",
      "iteration 1747, dc_loss: 0.03199175000190735, tv_loss: 0.03353689983487129\n",
      "iteration 1748, dc_loss: 0.03201581910252571, tv_loss: 0.03351207450032234\n",
      "iteration 1749, dc_loss: 0.03196059539914131, tv_loss: 0.03356501832604408\n",
      "iteration 1750, dc_loss: 0.03199734911322594, tv_loss: 0.033509913831949234\n",
      "iteration 1751, dc_loss: 0.03193476051092148, tv_loss: 0.033539436757564545\n",
      "iteration 1752, dc_loss: 0.03194255754351616, tv_loss: 0.03350824862718582\n",
      "iteration 1753, dc_loss: 0.03191681578755379, tv_loss: 0.03351660072803497\n",
      "iteration 1754, dc_loss: 0.031890831887722015, tv_loss: 0.03354349732398987\n",
      "iteration 1755, dc_loss: 0.031909022480249405, tv_loss: 0.03352117910981178\n",
      "iteration 1756, dc_loss: 0.03187508508563042, tv_loss: 0.03354552015662193\n",
      "iteration 1757, dc_loss: 0.03185659274458885, tv_loss: 0.03354250267148018\n",
      "iteration 1758, dc_loss: 0.031833257526159286, tv_loss: 0.03353700414299965\n",
      "iteration 1759, dc_loss: 0.03183848410844803, tv_loss: 0.033519964665174484\n",
      "iteration 1760, dc_loss: 0.031809836626052856, tv_loss: 0.03353557735681534\n",
      "iteration 1761, dc_loss: 0.031817108392715454, tv_loss: 0.03351924568414688\n",
      "iteration 1762, dc_loss: 0.0317641906440258, tv_loss: 0.03357245400547981\n",
      "iteration 1763, dc_loss: 0.03179251775145531, tv_loss: 0.033519916236400604\n",
      "iteration 1764, dc_loss: 0.03175395727157593, tv_loss: 0.0335296094417572\n",
      "iteration 1765, dc_loss: 0.031757794320583344, tv_loss: 0.03351464495062828\n",
      "iteration 1766, dc_loss: 0.031723253428936005, tv_loss: 0.033531758934259415\n",
      "iteration 1767, dc_loss: 0.03172506392002106, tv_loss: 0.0335070863366127\n",
      "iteration 1768, dc_loss: 0.03170323371887207, tv_loss: 0.033519353717565536\n",
      "iteration 1769, dc_loss: 0.03168140724301338, tv_loss: 0.033527594059705734\n",
      "iteration 1770, dc_loss: 0.03166632726788521, tv_loss: 0.033524930477142334\n",
      "iteration 1771, dc_loss: 0.03168439492583275, tv_loss: 0.033502645790576935\n",
      "iteration 1772, dc_loss: 0.03161988407373428, tv_loss: 0.0335657000541687\n",
      "iteration 1773, dc_loss: 0.031646665185689926, tv_loss: 0.03352772817015648\n",
      "iteration 1774, dc_loss: 0.03160883113741875, tv_loss: 0.033554594963788986\n",
      "iteration 1775, dc_loss: 0.03162263706326485, tv_loss: 0.033513229340314865\n",
      "iteration 1776, dc_loss: 0.03156978636980057, tv_loss: 0.03354473412036896\n",
      "iteration 1777, dc_loss: 0.03159783035516739, tv_loss: 0.03352158144116402\n",
      "iteration 1778, dc_loss: 0.0315437950193882, tv_loss: 0.0335700660943985\n",
      "iteration 1779, dc_loss: 0.031583189964294434, tv_loss: 0.03351694718003273\n",
      "iteration 1780, dc_loss: 0.03150869905948639, tv_loss: 0.03356083109974861\n",
      "iteration 1781, dc_loss: 0.031570229679346085, tv_loss: 0.033487718552351\n",
      "iteration 1782, dc_loss: 0.031488075852394104, tv_loss: 0.03356240317225456\n",
      "iteration 1783, dc_loss: 0.03155577927827835, tv_loss: 0.03347961604595184\n",
      "iteration 1784, dc_loss: 0.031453315168619156, tv_loss: 0.03358946740627289\n",
      "iteration 1785, dc_loss: 0.03153461590409279, tv_loss: 0.03351178020238876\n",
      "iteration 1786, dc_loss: 0.03145527467131615, tv_loss: 0.03357841446995735\n",
      "iteration 1787, dc_loss: 0.03151347488164902, tv_loss: 0.03349239379167557\n",
      "iteration 1788, dc_loss: 0.03141843155026436, tv_loss: 0.03357798978686333\n",
      "iteration 1789, dc_loss: 0.031499359756708145, tv_loss: 0.03347545117139816\n",
      "iteration 1790, dc_loss: 0.03139762580394745, tv_loss: 0.03357020393013954\n",
      "iteration 1791, dc_loss: 0.03146732971072197, tv_loss: 0.03348973020911217\n",
      "iteration 1792, dc_loss: 0.031362175941467285, tv_loss: 0.0335945300757885\n",
      "iteration 1793, dc_loss: 0.031468942761421204, tv_loss: 0.03347758948802948\n",
      "iteration 1794, dc_loss: 0.031321048736572266, tv_loss: 0.033626068383455276\n",
      "iteration 1795, dc_loss: 0.03142567723989487, tv_loss: 0.0334755964577198\n",
      "iteration 1796, dc_loss: 0.03128752484917641, tv_loss: 0.033589448779821396\n",
      "iteration 1797, dc_loss: 0.03138361871242523, tv_loss: 0.03346904367208481\n",
      "iteration 1798, dc_loss: 0.03127105534076691, tv_loss: 0.03355993703007698\n",
      "iteration 1799, dc_loss: 0.03129485994577408, tv_loss: 0.033537495881319046\n",
      "iteration 1800, dc_loss: 0.031263526529073715, tv_loss: 0.03356364741921425\n",
      "iteration 1801, dc_loss: 0.0312366746366024, tv_loss: 0.033553384244441986\n",
      "iteration 1802, dc_loss: 0.03127481788396835, tv_loss: 0.03348875045776367\n",
      "iteration 1803, dc_loss: 0.031200988218188286, tv_loss: 0.03357204422354698\n",
      "iteration 1804, dc_loss: 0.03125395625829697, tv_loss: 0.03351835161447525\n",
      "iteration 1805, dc_loss: 0.031191866844892502, tv_loss: 0.033573031425476074\n",
      "iteration 1806, dc_loss: 0.031219560652971268, tv_loss: 0.03350888937711716\n",
      "iteration 1807, dc_loss: 0.031158149242401123, tv_loss: 0.033564627170562744\n",
      "iteration 1808, dc_loss: 0.03119543194770813, tv_loss: 0.03351388871669769\n",
      "iteration 1809, dc_loss: 0.031147561967372894, tv_loss: 0.033544234931468964\n",
      "iteration 1810, dc_loss: 0.031178440898656845, tv_loss: 0.033490944653749466\n",
      "iteration 1811, dc_loss: 0.031097711995244026, tv_loss: 0.03356976434588432\n",
      "iteration 1812, dc_loss: 0.031165195629000664, tv_loss: 0.03347953036427498\n",
      "iteration 1813, dc_loss: 0.031069394201040268, tv_loss: 0.03355815261602402\n",
      "iteration 1814, dc_loss: 0.03112407959997654, tv_loss: 0.033480532467365265\n",
      "iteration 1815, dc_loss: 0.031044738367199898, tv_loss: 0.033548370003700256\n",
      "iteration 1816, dc_loss: 0.03108983300626278, tv_loss: 0.03351348266005516\n",
      "iteration 1817, dc_loss: 0.031016331166028976, tv_loss: 0.03361140191555023\n",
      "iteration 1818, dc_loss: 0.031079281121492386, tv_loss: 0.03351564705371857\n",
      "iteration 1819, dc_loss: 0.030969351530075073, tv_loss: 0.03357715904712677\n",
      "iteration 1820, dc_loss: 0.031051525846123695, tv_loss: 0.03349296748638153\n",
      "iteration 1821, dc_loss: 0.030979277566075325, tv_loss: 0.033568523824214935\n",
      "iteration 1822, dc_loss: 0.031003212556242943, tv_loss: 0.03355196490883827\n",
      "iteration 1823, dc_loss: 0.030958279967308044, tv_loss: 0.0335688479244709\n",
      "iteration 1824, dc_loss: 0.03099236451089382, tv_loss: 0.03350392356514931\n",
      "iteration 1825, dc_loss: 0.03095940127968788, tv_loss: 0.03354138135910034\n",
      "iteration 1826, dc_loss: 0.03097129799425602, tv_loss: 0.033526014536619186\n",
      "iteration 1827, dc_loss: 0.030917299911379814, tv_loss: 0.03356393426656723\n",
      "iteration 1828, dc_loss: 0.031013712286949158, tv_loss: 0.03347676619887352\n",
      "iteration 1829, dc_loss: 0.030862553045153618, tv_loss: 0.0336245633661747\n",
      "iteration 1830, dc_loss: 0.031021704897284508, tv_loss: 0.033467721194028854\n",
      "iteration 1831, dc_loss: 0.030841482803225517, tv_loss: 0.03364013880491257\n",
      "iteration 1832, dc_loss: 0.03100426495075226, tv_loss: 0.03345483914017677\n",
      "iteration 1833, dc_loss: 0.03082950972020626, tv_loss: 0.03363235294818878\n",
      "iteration 1834, dc_loss: 0.030953319743275642, tv_loss: 0.03345168009400368\n",
      "iteration 1835, dc_loss: 0.03079332783818245, tv_loss: 0.03360440954566002\n",
      "iteration 1836, dc_loss: 0.030859054997563362, tv_loss: 0.03350609913468361\n",
      "iteration 1837, dc_loss: 0.030792459845542908, tv_loss: 0.03355507552623749\n",
      "iteration 1838, dc_loss: 0.03077816776931286, tv_loss: 0.03355729579925537\n",
      "iteration 1839, dc_loss: 0.030793163925409317, tv_loss: 0.03352459892630577\n",
      "iteration 1840, dc_loss: 0.030729511752724648, tv_loss: 0.03357487544417381\n",
      "iteration 1841, dc_loss: 0.030782785266637802, tv_loss: 0.033497102558612823\n",
      "iteration 1842, dc_loss: 0.030683469027280807, tv_loss: 0.033602744340896606\n",
      "iteration 1843, dc_loss: 0.03076321817934513, tv_loss: 0.03350469842553139\n",
      "iteration 1844, dc_loss: 0.0306894239038229, tv_loss: 0.033572424203157425\n",
      "iteration 1845, dc_loss: 0.03073795698583126, tv_loss: 0.03350813686847687\n",
      "iteration 1846, dc_loss: 0.030661601573228836, tv_loss: 0.03357064351439476\n",
      "iteration 1847, dc_loss: 0.03070208430290222, tv_loss: 0.03351030871272087\n",
      "iteration 1848, dc_loss: 0.030642099678516388, tv_loss: 0.03356613218784332\n",
      "iteration 1849, dc_loss: 0.030687406659126282, tv_loss: 0.033496905118227005\n",
      "iteration 1850, dc_loss: 0.030601464211940765, tv_loss: 0.033585112541913986\n",
      "iteration 1851, dc_loss: 0.030643999576568604, tv_loss: 0.03352649509906769\n",
      "iteration 1852, dc_loss: 0.030582066625356674, tv_loss: 0.0335674062371254\n",
      "iteration 1853, dc_loss: 0.030596772208809853, tv_loss: 0.033522579818964005\n",
      "iteration 1854, dc_loss: 0.03056313470005989, tv_loss: 0.03354211524128914\n",
      "iteration 1855, dc_loss: 0.03056417778134346, tv_loss: 0.03353067860007286\n",
      "iteration 1856, dc_loss: 0.030545957386493683, tv_loss: 0.03355429694056511\n",
      "iteration 1857, dc_loss: 0.03054448962211609, tv_loss: 0.03355614095926285\n",
      "iteration 1858, dc_loss: 0.030506236478686333, tv_loss: 0.033565230667591095\n",
      "iteration 1859, dc_loss: 0.0305193904787302, tv_loss: 0.03352028876543045\n",
      "iteration 1860, dc_loss: 0.030493304133415222, tv_loss: 0.03355512395501137\n",
      "iteration 1861, dc_loss: 0.030479973182082176, tv_loss: 0.03355995938181877\n",
      "iteration 1862, dc_loss: 0.030480066314339638, tv_loss: 0.03354337066411972\n",
      "iteration 1863, dc_loss: 0.03044792450964451, tv_loss: 0.0335482656955719\n",
      "iteration 1864, dc_loss: 0.030477015301585197, tv_loss: 0.03350701555609703\n",
      "iteration 1865, dc_loss: 0.030426032841205597, tv_loss: 0.03355327993631363\n",
      "iteration 1866, dc_loss: 0.030428284779191017, tv_loss: 0.03354625776410103\n",
      "iteration 1867, dc_loss: 0.030405886471271515, tv_loss: 0.033567119389772415\n",
      "iteration 1868, dc_loss: 0.030402719974517822, tv_loss: 0.03355879336595535\n",
      "iteration 1869, dc_loss: 0.030415957793593407, tv_loss: 0.03351474180817604\n",
      "iteration 1870, dc_loss: 0.03037094697356224, tv_loss: 0.03356332331895828\n",
      "iteration 1871, dc_loss: 0.030407950282096863, tv_loss: 0.03351187705993652\n",
      "iteration 1872, dc_loss: 0.030368169769644737, tv_loss: 0.03356615826487541\n",
      "iteration 1873, dc_loss: 0.030466055497527122, tv_loss: 0.033481910824775696\n",
      "iteration 1874, dc_loss: 0.03038802742958069, tv_loss: 0.033604696393013\n",
      "iteration 1875, dc_loss: 0.030558917671442032, tv_loss: 0.033478032797575\n",
      "iteration 1876, dc_loss: 0.03041917085647583, tv_loss: 0.03368773311376572\n",
      "iteration 1877, dc_loss: 0.030621526762843132, tv_loss: 0.033426299691200256\n",
      "iteration 1878, dc_loss: 0.030346475541591644, tv_loss: 0.033691778779029846\n",
      "iteration 1879, dc_loss: 0.03058576211333275, tv_loss: 0.03340357914566994\n",
      "iteration 1880, dc_loss: 0.030336758121848106, tv_loss: 0.03370624780654907\n",
      "iteration 1881, dc_loss: 0.03048548847436905, tv_loss: 0.03347492963075638\n",
      "iteration 1882, dc_loss: 0.03032197616994381, tv_loss: 0.033607736229896545\n",
      "iteration 1883, dc_loss: 0.030328217893838882, tv_loss: 0.0335460901260376\n",
      "iteration 1884, dc_loss: 0.03028927370905876, tv_loss: 0.03354456275701523\n",
      "iteration 1885, dc_loss: 0.030185941606760025, tv_loss: 0.03360280767083168\n",
      "iteration 1886, dc_loss: 0.030275991186499596, tv_loss: 0.03349619358778\n",
      "iteration 1887, dc_loss: 0.03019425831735134, tv_loss: 0.033621128648519516\n",
      "iteration 1888, dc_loss: 0.03026508167386055, tv_loss: 0.03353169932961464\n",
      "iteration 1889, dc_loss: 0.03018663078546524, tv_loss: 0.03357669338583946\n",
      "iteration 1890, dc_loss: 0.030187703669071198, tv_loss: 0.03353523463010788\n",
      "iteration 1891, dc_loss: 0.030145032331347466, tv_loss: 0.03357313200831413\n",
      "iteration 1892, dc_loss: 0.030143139883875847, tv_loss: 0.033564887940883636\n",
      "iteration 1893, dc_loss: 0.030123760923743248, tv_loss: 0.0335472971200943\n",
      "iteration 1894, dc_loss: 0.030107969418168068, tv_loss: 0.03354763612151146\n",
      "iteration 1895, dc_loss: 0.03013298474252224, tv_loss: 0.033541273325681686\n",
      "iteration 1896, dc_loss: 0.030060354620218277, tv_loss: 0.03360765799880028\n",
      "iteration 1897, dc_loss: 0.03010687232017517, tv_loss: 0.033517297357320786\n",
      "iteration 1898, dc_loss: 0.03003658726811409, tv_loss: 0.03356194496154785\n",
      "iteration 1899, dc_loss: 0.03005896881222725, tv_loss: 0.03353717550635338\n",
      "iteration 1900, dc_loss: 0.030013877898454666, tv_loss: 0.033585600554943085\n",
      "iteration 1901, dc_loss: 0.030020644888281822, tv_loss: 0.033559806644916534\n",
      "iteration 1902, dc_loss: 0.03002890571951866, tv_loss: 0.033524591475725174\n",
      "iteration 1903, dc_loss: 0.02996564283967018, tv_loss: 0.03356945887207985\n",
      "iteration 1904, dc_loss: 0.030011622235178947, tv_loss: 0.03351987153291702\n",
      "iteration 1905, dc_loss: 0.02994828298687935, tv_loss: 0.0335811972618103\n",
      "iteration 1906, dc_loss: 0.0299812201410532, tv_loss: 0.03353578597307205\n",
      "iteration 1907, dc_loss: 0.02992120198905468, tv_loss: 0.03357933461666107\n",
      "iteration 1908, dc_loss: 0.02994587831199169, tv_loss: 0.033528804779052734\n",
      "iteration 1909, dc_loss: 0.029918856918811798, tv_loss: 0.03354211896657944\n",
      "iteration 1910, dc_loss: 0.029911762103438377, tv_loss: 0.033535122871398926\n",
      "iteration 1911, dc_loss: 0.02989468164741993, tv_loss: 0.03353529050946236\n",
      "iteration 1912, dc_loss: 0.02987806499004364, tv_loss: 0.033561766147613525\n",
      "iteration 1913, dc_loss: 0.0298873670399189, tv_loss: 0.03354291617870331\n",
      "iteration 1914, dc_loss: 0.02984902448952198, tv_loss: 0.03356488049030304\n",
      "iteration 1915, dc_loss: 0.02985009178519249, tv_loss: 0.033546701073646545\n",
      "iteration 1916, dc_loss: 0.02983819879591465, tv_loss: 0.03354030102491379\n",
      "iteration 1917, dc_loss: 0.029828237369656563, tv_loss: 0.03355449438095093\n",
      "iteration 1918, dc_loss: 0.029820449650287628, tv_loss: 0.03354768455028534\n",
      "iteration 1919, dc_loss: 0.029786746948957443, tv_loss: 0.03356638550758362\n",
      "iteration 1920, dc_loss: 0.02981141395866871, tv_loss: 0.033532727509737015\n",
      "iteration 1921, dc_loss: 0.029760250821709633, tv_loss: 0.03357881307601929\n",
      "iteration 1922, dc_loss: 0.029785549268126488, tv_loss: 0.03354358673095703\n",
      "iteration 1923, dc_loss: 0.029755355790257454, tv_loss: 0.03356371819972992\n",
      "iteration 1924, dc_loss: 0.029767414554953575, tv_loss: 0.03352265805006027\n",
      "iteration 1925, dc_loss: 0.029717881232500076, tv_loss: 0.03357400372624397\n",
      "iteration 1926, dc_loss: 0.029731011018157005, tv_loss: 0.03355201333761215\n",
      "iteration 1927, dc_loss: 0.029713477939367294, tv_loss: 0.033553265035152435\n",
      "iteration 1928, dc_loss: 0.029716065153479576, tv_loss: 0.03353697061538696\n",
      "iteration 1929, dc_loss: 0.029679851606488228, tv_loss: 0.03356463834643364\n",
      "iteration 1930, dc_loss: 0.02969438023865223, tv_loss: 0.03352866694331169\n",
      "iteration 1931, dc_loss: 0.02966243401169777, tv_loss: 0.03355874866247177\n",
      "iteration 1932, dc_loss: 0.029710307717323303, tv_loss: 0.03352050110697746\n",
      "iteration 1933, dc_loss: 0.029620354995131493, tv_loss: 0.03361855447292328\n",
      "iteration 1934, dc_loss: 0.029705436900258064, tv_loss: 0.033515289425849915\n",
      "iteration 1935, dc_loss: 0.029593247920274734, tv_loss: 0.03361711651086807\n",
      "iteration 1936, dc_loss: 0.02973352186381817, tv_loss: 0.0334773063659668\n",
      "iteration 1937, dc_loss: 0.029606366530060768, tv_loss: 0.03362366184592247\n",
      "iteration 1938, dc_loss: 0.029790125787258148, tv_loss: 0.03345254436135292\n",
      "iteration 1939, dc_loss: 0.029614565894007683, tv_loss: 0.03367139771580696\n",
      "iteration 1940, dc_loss: 0.02984420210123062, tv_loss: 0.033445805311203\n",
      "iteration 1941, dc_loss: 0.02967853844165802, tv_loss: 0.03365873172879219\n",
      "iteration 1942, dc_loss: 0.029879117384552956, tv_loss: 0.033433571457862854\n",
      "iteration 1943, dc_loss: 0.029643790796399117, tv_loss: 0.03366144746541977\n",
      "iteration 1944, dc_loss: 0.02977045625448227, tv_loss: 0.03346629440784454\n",
      "iteration 1945, dc_loss: 0.02955956757068634, tv_loss: 0.03362298011779785\n",
      "iteration 1946, dc_loss: 0.029623284935951233, tv_loss: 0.03348151221871376\n",
      "iteration 1947, dc_loss: 0.029484525322914124, tv_loss: 0.03358500823378563\n",
      "iteration 1948, dc_loss: 0.02950163744390011, tv_loss: 0.033560942858457565\n",
      "iteration 1949, dc_loss: 0.029535971581935883, tv_loss: 0.033536314964294434\n",
      "iteration 1950, dc_loss: 0.029451929032802582, tv_loss: 0.03362457454204559\n",
      "iteration 1951, dc_loss: 0.02959039993584156, tv_loss: 0.033480092883110046\n",
      "iteration 1952, dc_loss: 0.029430905357003212, tv_loss: 0.033631645143032074\n",
      "iteration 1953, dc_loss: 0.029578013345599174, tv_loss: 0.03344578668475151\n",
      "iteration 1954, dc_loss: 0.029379000887274742, tv_loss: 0.03361492231488228\n",
      "iteration 1955, dc_loss: 0.029444625601172447, tv_loss: 0.03354116901755333\n",
      "iteration 1956, dc_loss: 0.029416630044579506, tv_loss: 0.033571381121873856\n",
      "iteration 1957, dc_loss: 0.029371201992034912, tv_loss: 0.03359502553939819\n",
      "iteration 1958, dc_loss: 0.02948031947016716, tv_loss: 0.03347845375537872\n",
      "iteration 1959, dc_loss: 0.02931906096637249, tv_loss: 0.033638473600149155\n",
      "iteration 1960, dc_loss: 0.029474778100848198, tv_loss: 0.03348001092672348\n",
      "iteration 1961, dc_loss: 0.029317276552319527, tv_loss: 0.03361845761537552\n",
      "iteration 1962, dc_loss: 0.029374985024333, tv_loss: 0.03353148698806763\n",
      "iteration 1963, dc_loss: 0.029339902102947235, tv_loss: 0.03353935107588768\n",
      "iteration 1964, dc_loss: 0.029290396720170975, tv_loss: 0.033577967435121536\n",
      "iteration 1965, dc_loss: 0.029369553551077843, tv_loss: 0.03348453342914581\n",
      "iteration 1966, dc_loss: 0.029252205044031143, tv_loss: 0.03360159695148468\n",
      "iteration 1967, dc_loss: 0.02935512736439705, tv_loss: 0.033487718552351\n",
      "iteration 1968, dc_loss: 0.029262175783514977, tv_loss: 0.03356717526912689\n",
      "iteration 1969, dc_loss: 0.029298905283212662, tv_loss: 0.03352005407214165\n",
      "iteration 1970, dc_loss: 0.02925023064017296, tv_loss: 0.033560652285814285\n",
      "iteration 1971, dc_loss: 0.029262421652674675, tv_loss: 0.03354522958397865\n",
      "iteration 1972, dc_loss: 0.02923605591058731, tv_loss: 0.03357595205307007\n",
      "iteration 1973, dc_loss: 0.029215611517429352, tv_loss: 0.03356922045350075\n",
      "iteration 1974, dc_loss: 0.0292078647762537, tv_loss: 0.03355088084936142\n",
      "iteration 1975, dc_loss: 0.029223332181572914, tv_loss: 0.033528003841638565\n",
      "iteration 1976, dc_loss: 0.02918120287358761, tv_loss: 0.03356354311108589\n",
      "iteration 1977, dc_loss: 0.02917701005935669, tv_loss: 0.03358279541134834\n",
      "iteration 1978, dc_loss: 0.029179822653532028, tv_loss: 0.03356820344924927\n",
      "iteration 1979, dc_loss: 0.029151270166039467, tv_loss: 0.03356113284826279\n",
      "iteration 1980, dc_loss: 0.02917957492172718, tv_loss: 0.03352061286568642\n",
      "iteration 1981, dc_loss: 0.029117483645677567, tv_loss: 0.033578500151634216\n",
      "iteration 1982, dc_loss: 0.029168128967285156, tv_loss: 0.03351868689060211\n",
      "iteration 1983, dc_loss: 0.029101679101586342, tv_loss: 0.0335879884660244\n",
      "iteration 1984, dc_loss: 0.0291331447660923, tv_loss: 0.033559370785951614\n",
      "iteration 1985, dc_loss: 0.029088549315929413, tv_loss: 0.03358133137226105\n",
      "iteration 1986, dc_loss: 0.02910037338733673, tv_loss: 0.03354674577713013\n",
      "iteration 1987, dc_loss: 0.029092680662870407, tv_loss: 0.03354707360267639\n",
      "iteration 1988, dc_loss: 0.029072169214487076, tv_loss: 0.033596694469451904\n",
      "iteration 1989, dc_loss: 0.02904495596885681, tv_loss: 0.03359068185091019\n",
      "iteration 1990, dc_loss: 0.02907097525894642, tv_loss: 0.033535249531269073\n",
      "iteration 1991, dc_loss: 0.02900751121342182, tv_loss: 0.03360559046268463\n",
      "iteration 1992, dc_loss: 0.02906065806746483, tv_loss: 0.0335637666285038\n",
      "iteration 1993, dc_loss: 0.02900226041674614, tv_loss: 0.03359612822532654\n",
      "iteration 1994, dc_loss: 0.029042845591902733, tv_loss: 0.03351942077279091\n",
      "iteration 1995, dc_loss: 0.028969688341021538, tv_loss: 0.03361900895833969\n",
      "iteration 1996, dc_loss: 0.029027989134192467, tv_loss: 0.03355788812041283\n",
      "iteration 1997, dc_loss: 0.028954029083251953, tv_loss: 0.03359755501151085\n",
      "iteration 1998, dc_loss: 0.029003256931900978, tv_loss: 0.03353226184844971\n",
      "iteration 1999, dc_loss: 0.028951091691851616, tv_loss: 0.03361213952302933\n",
      "iteration 2000, dc_loss: 0.02899373322725296, tv_loss: 0.03356409817934036\n",
      "iteration 2001, dc_loss: 0.02894720435142517, tv_loss: 0.03359171748161316\n",
      "iteration 2002, dc_loss: 0.028966808691620827, tv_loss: 0.0335380882024765\n",
      "iteration 2003, dc_loss: 0.028921645134687424, tv_loss: 0.033551111817359924\n",
      "iteration 2004, dc_loss: 0.028914455324411392, tv_loss: 0.03356722369790077\n",
      "iteration 2005, dc_loss: 0.028943048790097237, tv_loss: 0.03352813422679901\n",
      "iteration 2006, dc_loss: 0.02888704650104046, tv_loss: 0.03355535492300987\n",
      "iteration 2007, dc_loss: 0.02888103760778904, tv_loss: 0.03354785963892937\n",
      "iteration 2008, dc_loss: 0.028903581202030182, tv_loss: 0.033531554043293\n",
      "iteration 2009, dc_loss: 0.02887478470802307, tv_loss: 0.03354133665561676\n",
      "iteration 2010, dc_loss: 0.028877541422843933, tv_loss: 0.033517029136419296\n",
      "iteration 2011, dc_loss: 0.028869029134511948, tv_loss: 0.03353119641542435\n",
      "iteration 2012, dc_loss: 0.028840938583016396, tv_loss: 0.03355154022574425\n",
      "iteration 2013, dc_loss: 0.02885427512228489, tv_loss: 0.03352515771985054\n",
      "iteration 2014, dc_loss: 0.028823575004935265, tv_loss: 0.033563509583473206\n",
      "iteration 2015, dc_loss: 0.028815982863307, tv_loss: 0.03356611356139183\n",
      "iteration 2016, dc_loss: 0.028825420886278152, tv_loss: 0.033525269478559494\n",
      "iteration 2017, dc_loss: 0.028798991814255714, tv_loss: 0.033542804419994354\n",
      "iteration 2018, dc_loss: 0.02880280651152134, tv_loss: 0.033536218106746674\n",
      "iteration 2019, dc_loss: 0.028808610513806343, tv_loss: 0.033534660935401917\n",
      "iteration 2020, dc_loss: 0.02877608872950077, tv_loss: 0.03357359394431114\n",
      "iteration 2021, dc_loss: 0.0287740807980299, tv_loss: 0.03354533016681671\n",
      "iteration 2022, dc_loss: 0.028780587017536163, tv_loss: 0.03352687880396843\n",
      "iteration 2023, dc_loss: 0.028744643554091454, tv_loss: 0.03355836123228073\n",
      "iteration 2024, dc_loss: 0.02875983715057373, tv_loss: 0.033530499786138535\n",
      "iteration 2025, dc_loss: 0.028752412647008896, tv_loss: 0.03352949768304825\n",
      "iteration 2026, dc_loss: 0.028718702495098114, tv_loss: 0.03356142342090607\n",
      "iteration 2027, dc_loss: 0.02874171920120716, tv_loss: 0.03352340683341026\n",
      "iteration 2028, dc_loss: 0.02871048077940941, tv_loss: 0.03353916481137276\n",
      "iteration 2029, dc_loss: 0.028696522116661072, tv_loss: 0.033557429909706116\n",
      "iteration 2030, dc_loss: 0.02873491682112217, tv_loss: 0.03350553661584854\n",
      "iteration 2031, dc_loss: 0.028691833838820457, tv_loss: 0.0335327610373497\n",
      "iteration 2032, dc_loss: 0.028675995767116547, tv_loss: 0.03354613855481148\n",
      "iteration 2033, dc_loss: 0.028698623180389404, tv_loss: 0.03352295979857445\n",
      "iteration 2034, dc_loss: 0.028661904856562614, tv_loss: 0.03354467824101448\n",
      "iteration 2035, dc_loss: 0.028667224571108818, tv_loss: 0.03354533761739731\n",
      "iteration 2036, dc_loss: 0.028662674129009247, tv_loss: 0.03355841711163521\n",
      "iteration 2037, dc_loss: 0.02864295057952404, tv_loss: 0.033552248030900955\n",
      "iteration 2038, dc_loss: 0.02864082157611847, tv_loss: 0.033550623804330826\n",
      "iteration 2039, dc_loss: 0.028639964759349823, tv_loss: 0.03353433683514595\n",
      "iteration 2040, dc_loss: 0.028622249141335487, tv_loss: 0.03355896472930908\n",
      "iteration 2041, dc_loss: 0.028614766895771027, tv_loss: 0.03355875983834267\n",
      "iteration 2042, dc_loss: 0.028620008379220963, tv_loss: 0.0335434190928936\n",
      "iteration 2043, dc_loss: 0.028607778251171112, tv_loss: 0.033541809767484665\n",
      "iteration 2044, dc_loss: 0.028591344133019447, tv_loss: 0.03353947028517723\n",
      "iteration 2045, dc_loss: 0.02858497016131878, tv_loss: 0.033560700714588165\n",
      "iteration 2046, dc_loss: 0.028579305857419968, tv_loss: 0.0335618332028389\n",
      "iteration 2047, dc_loss: 0.028580883517861366, tv_loss: 0.033542852848768234\n",
      "iteration 2048, dc_loss: 0.02855767123401165, tv_loss: 0.03354987874627113\n",
      "iteration 2049, dc_loss: 0.02855994924902916, tv_loss: 0.03354332596063614\n",
      "iteration 2050, dc_loss: 0.028558267280459404, tv_loss: 0.033543337136507034\n",
      "iteration 2051, dc_loss: 0.028537794947624207, tv_loss: 0.03356027975678444\n",
      "iteration 2052, dc_loss: 0.028540104627609253, tv_loss: 0.033539727330207825\n",
      "iteration 2053, dc_loss: 0.028523830696940422, tv_loss: 0.03354737535119057\n",
      "iteration 2054, dc_loss: 0.028523266315460205, tv_loss: 0.03353634849190712\n",
      "iteration 2055, dc_loss: 0.02851850353181362, tv_loss: 0.03353060036897659\n",
      "iteration 2056, dc_loss: 0.028498798608779907, tv_loss: 0.03354587405920029\n",
      "iteration 2057, dc_loss: 0.02849205583333969, tv_loss: 0.03354095667600632\n",
      "iteration 2058, dc_loss: 0.028486870229244232, tv_loss: 0.033545151352882385\n",
      "iteration 2059, dc_loss: 0.02849169634282589, tv_loss: 0.03353126347064972\n",
      "iteration 2060, dc_loss: 0.028478000313043594, tv_loss: 0.033551957458257675\n",
      "iteration 2061, dc_loss: 0.028470637276768684, tv_loss: 0.03354958817362785\n",
      "iteration 2062, dc_loss: 0.028449013829231262, tv_loss: 0.03356024622917175\n",
      "iteration 2063, dc_loss: 0.02845098450779915, tv_loss: 0.03354210406541824\n",
      "iteration 2064, dc_loss: 0.028449878096580505, tv_loss: 0.03353262320160866\n",
      "iteration 2065, dc_loss: 0.028436724096536636, tv_loss: 0.033538952469825745\n",
      "iteration 2066, dc_loss: 0.02842571586370468, tv_loss: 0.033540092408657074\n",
      "iteration 2067, dc_loss: 0.02842722460627556, tv_loss: 0.033527620136737823\n",
      "iteration 2068, dc_loss: 0.028413191437721252, tv_loss: 0.03353271260857582\n",
      "iteration 2069, dc_loss: 0.028405286371707916, tv_loss: 0.03353988006711006\n",
      "iteration 2070, dc_loss: 0.028411313891410828, tv_loss: 0.03352661803364754\n",
      "iteration 2071, dc_loss: 0.028387688100337982, tv_loss: 0.033568039536476135\n",
      "iteration 2072, dc_loss: 0.02838119864463806, tv_loss: 0.033570535480976105\n",
      "iteration 2073, dc_loss: 0.02837308496236801, tv_loss: 0.03355002403259277\n",
      "iteration 2074, dc_loss: 0.028379948809742928, tv_loss: 0.0335373692214489\n",
      "iteration 2075, dc_loss: 0.0283597931265831, tv_loss: 0.03355218842625618\n",
      "iteration 2076, dc_loss: 0.02834155783057213, tv_loss: 0.03357886150479317\n",
      "iteration 2077, dc_loss: 0.028366388753056526, tv_loss: 0.03353967145085335\n",
      "iteration 2078, dc_loss: 0.02834295481443405, tv_loss: 0.033542681485414505\n",
      "iteration 2079, dc_loss: 0.028332103043794632, tv_loss: 0.03354879841208458\n",
      "iteration 2080, dc_loss: 0.028329618275165558, tv_loss: 0.03353916481137276\n",
      "iteration 2081, dc_loss: 0.02830316312611103, tv_loss: 0.03357526287436485\n",
      "iteration 2082, dc_loss: 0.0283131692558527, tv_loss: 0.03356156498193741\n",
      "iteration 2083, dc_loss: 0.028307493776082993, tv_loss: 0.033552736043930054\n",
      "iteration 2084, dc_loss: 0.028306981548666954, tv_loss: 0.03353200480341911\n",
      "iteration 2085, dc_loss: 0.02829708717763424, tv_loss: 0.033537574112415314\n",
      "iteration 2086, dc_loss: 0.02827523835003376, tv_loss: 0.03355461731553078\n",
      "iteration 2087, dc_loss: 0.028278527781367302, tv_loss: 0.03354901075363159\n",
      "iteration 2088, dc_loss: 0.02825630083680153, tv_loss: 0.03356274589896202\n",
      "iteration 2089, dc_loss: 0.028259925544261932, tv_loss: 0.03354624658823013\n",
      "iteration 2090, dc_loss: 0.028255879878997803, tv_loss: 0.0335426926612854\n",
      "iteration 2091, dc_loss: 0.02824942208826542, tv_loss: 0.03353699669241905\n",
      "iteration 2092, dc_loss: 0.028239386156201363, tv_loss: 0.033547401428222656\n",
      "iteration 2093, dc_loss: 0.02823932282626629, tv_loss: 0.033540379256010056\n",
      "iteration 2094, dc_loss: 0.028219588100910187, tv_loss: 0.03355986624956131\n",
      "iteration 2095, dc_loss: 0.028209125623106956, tv_loss: 0.033554576337337494\n",
      "iteration 2096, dc_loss: 0.028201894834637642, tv_loss: 0.03355375677347183\n",
      "iteration 2097, dc_loss: 0.028209466487169266, tv_loss: 0.03353789076209068\n",
      "iteration 2098, dc_loss: 0.028199922293424606, tv_loss: 0.033538103103637695\n",
      "iteration 2099, dc_loss: 0.028183437883853912, tv_loss: 0.033545106649398804\n",
      "iteration 2100, dc_loss: 0.028181863948702812, tv_loss: 0.03353561460971832\n",
      "iteration 2101, dc_loss: 0.02817589044570923, tv_loss: 0.03353176638484001\n",
      "iteration 2102, dc_loss: 0.028165241703391075, tv_loss: 0.033541321754455566\n",
      "iteration 2103, dc_loss: 0.02816077321767807, tv_loss: 0.033544089645147324\n",
      "iteration 2104, dc_loss: 0.028146017342805862, tv_loss: 0.033578354865312576\n",
      "iteration 2105, dc_loss: 0.02815023623406887, tv_loss: 0.03356463462114334\n",
      "iteration 2106, dc_loss: 0.028137603774666786, tv_loss: 0.03354014456272125\n",
      "iteration 2107, dc_loss: 0.02812589704990387, tv_loss: 0.033570386469364166\n",
      "iteration 2108, dc_loss: 0.028126949444413185, tv_loss: 0.03358536958694458\n",
      "iteration 2109, dc_loss: 0.028103653341531754, tv_loss: 0.03357158973813057\n",
      "iteration 2110, dc_loss: 0.02811572328209877, tv_loss: 0.03354409709572792\n",
      "iteration 2111, dc_loss: 0.028108691796660423, tv_loss: 0.03357529267668724\n",
      "iteration 2112, dc_loss: 0.028086429461836815, tv_loss: 0.03358042240142822\n",
      "iteration 2113, dc_loss: 0.028094109147787094, tv_loss: 0.033547427505254745\n",
      "iteration 2114, dc_loss: 0.02807621844112873, tv_loss: 0.033585671335458755\n",
      "iteration 2115, dc_loss: 0.028061553835868835, tv_loss: 0.033577755093574524\n",
      "iteration 2116, dc_loss: 0.028075283393263817, tv_loss: 0.03353629261255264\n",
      "iteration 2117, dc_loss: 0.02805919200181961, tv_loss: 0.03357410430908203\n",
      "iteration 2118, dc_loss: 0.028052957728505135, tv_loss: 0.03357609361410141\n",
      "iteration 2119, dc_loss: 0.028049452230334282, tv_loss: 0.03354574367403984\n",
      "iteration 2120, dc_loss: 0.028032246977090836, tv_loss: 0.03357221186161041\n",
      "iteration 2121, dc_loss: 0.028032535687088966, tv_loss: 0.03357527405023575\n",
      "iteration 2122, dc_loss: 0.028023671358823776, tv_loss: 0.03355683386325836\n",
      "iteration 2123, dc_loss: 0.028007660061120987, tv_loss: 0.033559348434209824\n",
      "iteration 2124, dc_loss: 0.028012340888381004, tv_loss: 0.03356168046593666\n",
      "iteration 2125, dc_loss: 0.028009139001369476, tv_loss: 0.03354771062731743\n",
      "iteration 2126, dc_loss: 0.027990445494651794, tv_loss: 0.03354820981621742\n",
      "iteration 2127, dc_loss: 0.027985677123069763, tv_loss: 0.033549029380083084\n",
      "iteration 2128, dc_loss: 0.027982760220766068, tv_loss: 0.03354634717106819\n",
      "iteration 2129, dc_loss: 0.02797372080385685, tv_loss: 0.03354964777827263\n",
      "iteration 2130, dc_loss: 0.027974430471658707, tv_loss: 0.033541206270456314\n",
      "iteration 2131, dc_loss: 0.027950366958975792, tv_loss: 0.033551283180713654\n",
      "iteration 2132, dc_loss: 0.027948936447501183, tv_loss: 0.0335548110306263\n",
      "iteration 2133, dc_loss: 0.027952568605542183, tv_loss: 0.03353842347860336\n",
      "iteration 2134, dc_loss: 0.027940664440393448, tv_loss: 0.03354579210281372\n",
      "iteration 2135, dc_loss: 0.027918867766857147, tv_loss: 0.03355904668569565\n",
      "iteration 2136, dc_loss: 0.027933627367019653, tv_loss: 0.033529482781887054\n",
      "iteration 2137, dc_loss: 0.02792731486260891, tv_loss: 0.03352970629930496\n",
      "iteration 2138, dc_loss: 0.027901552617549896, tv_loss: 0.03354920819401741\n",
      "iteration 2139, dc_loss: 0.027901673689484596, tv_loss: 0.03354382887482643\n",
      "iteration 2140, dc_loss: 0.02789895236492157, tv_loss: 0.03353472799062729\n",
      "iteration 2141, dc_loss: 0.027888134121894836, tv_loss: 0.03353971242904663\n",
      "iteration 2142, dc_loss: 0.02788996510207653, tv_loss: 0.033536218106746674\n",
      "iteration 2143, dc_loss: 0.027865519747138023, tv_loss: 0.03355332836508751\n",
      "iteration 2144, dc_loss: 0.027876807376742363, tv_loss: 0.03354518488049507\n",
      "iteration 2145, dc_loss: 0.027859853580594063, tv_loss: 0.03357892110943794\n",
      "iteration 2146, dc_loss: 0.027853842824697495, tv_loss: 0.033561766147613525\n",
      "iteration 2147, dc_loss: 0.02784290537238121, tv_loss: 0.033554308116436005\n",
      "iteration 2148, dc_loss: 0.027850331738591194, tv_loss: 0.0335492268204689\n",
      "iteration 2149, dc_loss: 0.027836250141263008, tv_loss: 0.0335603803396225\n",
      "iteration 2150, dc_loss: 0.027825145050883293, tv_loss: 0.03356355056166649\n",
      "iteration 2151, dc_loss: 0.027827788144350052, tv_loss: 0.03353951871395111\n",
      "iteration 2152, dc_loss: 0.027800170704722404, tv_loss: 0.03355927765369415\n",
      "iteration 2153, dc_loss: 0.02780074253678322, tv_loss: 0.033565886318683624\n",
      "iteration 2154, dc_loss: 0.02782152220606804, tv_loss: 0.03354543447494507\n",
      "iteration 2155, dc_loss: 0.02778276428580284, tv_loss: 0.03357003256678581\n",
      "iteration 2156, dc_loss: 0.027795273810625076, tv_loss: 0.03354437276721001\n",
      "iteration 2157, dc_loss: 0.02778034470975399, tv_loss: 0.033542435616254807\n",
      "iteration 2158, dc_loss: 0.02777118980884552, tv_loss: 0.033555563539266586\n",
      "iteration 2159, dc_loss: 0.0277716051787138, tv_loss: 0.03354432061314583\n",
      "iteration 2160, dc_loss: 0.02775789424777031, tv_loss: 0.033548787236213684\n",
      "iteration 2161, dc_loss: 0.02775433659553528, tv_loss: 0.03354295715689659\n",
      "iteration 2162, dc_loss: 0.027744244784116745, tv_loss: 0.03354937583208084\n",
      "iteration 2163, dc_loss: 0.02774054743349552, tv_loss: 0.03354264423251152\n",
      "iteration 2164, dc_loss: 0.027743324637413025, tv_loss: 0.03354526311159134\n",
      "iteration 2165, dc_loss: 0.027731209993362427, tv_loss: 0.03356148675084114\n",
      "iteration 2166, dc_loss: 0.02770349569618702, tv_loss: 0.033575303852558136\n",
      "iteration 2167, dc_loss: 0.027718165889382362, tv_loss: 0.03354661166667938\n",
      "iteration 2168, dc_loss: 0.027708007022738457, tv_loss: 0.03354547545313835\n",
      "iteration 2169, dc_loss: 0.02769893780350685, tv_loss: 0.03356126323342323\n",
      "iteration 2170, dc_loss: 0.02769658900797367, tv_loss: 0.03355047479271889\n",
      "iteration 2171, dc_loss: 0.02768680639564991, tv_loss: 0.03355443477630615\n",
      "iteration 2172, dc_loss: 0.027681443840265274, tv_loss: 0.03354344889521599\n",
      "iteration 2173, dc_loss: 0.02767915092408657, tv_loss: 0.03354112058877945\n",
      "iteration 2174, dc_loss: 0.027664517983794212, tv_loss: 0.033548641949892044\n",
      "iteration 2175, dc_loss: 0.02766764722764492, tv_loss: 0.03354286774992943\n",
      "iteration 2176, dc_loss: 0.02765100821852684, tv_loss: 0.0335678905248642\n",
      "iteration 2177, dc_loss: 0.027681197971105576, tv_loss: 0.03354380279779434\n",
      "iteration 2178, dc_loss: 0.02765273116528988, tv_loss: 0.03357328101992607\n",
      "iteration 2179, dc_loss: 0.027694376185536385, tv_loss: 0.03352303430438042\n",
      "iteration 2180, dc_loss: 0.027650197967886925, tv_loss: 0.03357330337166786\n",
      "iteration 2181, dc_loss: 0.027695555239915848, tv_loss: 0.03351984918117523\n",
      "iteration 2182, dc_loss: 0.027638040482997894, tv_loss: 0.03356105834245682\n",
      "iteration 2183, dc_loss: 0.027648407965898514, tv_loss: 0.033524952828884125\n",
      "iteration 2184, dc_loss: 0.027588067576289177, tv_loss: 0.033562373369932175\n",
      "iteration 2185, dc_loss: 0.02760344371199608, tv_loss: 0.03356284275650978\n",
      "iteration 2186, dc_loss: 0.027593979611992836, tv_loss: 0.03358101844787598\n",
      "iteration 2187, dc_loss: 0.02759874239563942, tv_loss: 0.03355276957154274\n",
      "iteration 2188, dc_loss: 0.027590662240982056, tv_loss: 0.03354113921523094\n",
      "iteration 2189, dc_loss: 0.027546362951397896, tv_loss: 0.0335916168987751\n",
      "iteration 2190, dc_loss: 0.027587831020355225, tv_loss: 0.03353572636842728\n",
      "iteration 2191, dc_loss: 0.027549905702471733, tv_loss: 0.03356176242232323\n",
      "iteration 2192, dc_loss: 0.027555568143725395, tv_loss: 0.033545903861522675\n",
      "iteration 2193, dc_loss: 0.027550803497433662, tv_loss: 0.03353622928261757\n",
      "iteration 2194, dc_loss: 0.027519606053829193, tv_loss: 0.03356815502047539\n",
      "iteration 2195, dc_loss: 0.027550790458917618, tv_loss: 0.03354330360889435\n",
      "iteration 2196, dc_loss: 0.027503253892064095, tv_loss: 0.03358086198568344\n",
      "iteration 2197, dc_loss: 0.027523359283804893, tv_loss: 0.033543121069669724\n",
      "iteration 2198, dc_loss: 0.0275090504437685, tv_loss: 0.03354766219854355\n",
      "iteration 2199, dc_loss: 0.02749500796198845, tv_loss: 0.03355219215154648\n",
      "iteration 2200, dc_loss: 0.027510978281497955, tv_loss: 0.03353611379861832\n",
      "iteration 2201, dc_loss: 0.027474859729409218, tv_loss: 0.033565014600753784\n",
      "iteration 2202, dc_loss: 0.027491068467497826, tv_loss: 0.03355182707309723\n",
      "iteration 2203, dc_loss: 0.02746845968067646, tv_loss: 0.033574026077985764\n",
      "iteration 2204, dc_loss: 0.027466651052236557, tv_loss: 0.03356019780039787\n",
      "iteration 2205, dc_loss: 0.02746317908167839, tv_loss: 0.03355100005865097\n",
      "iteration 2206, dc_loss: 0.027458932250738144, tv_loss: 0.033542435616254807\n",
      "iteration 2207, dc_loss: 0.027445416897535324, tv_loss: 0.033554598689079285\n",
      "iteration 2208, dc_loss: 0.027437640354037285, tv_loss: 0.033559199422597885\n",
      "iteration 2209, dc_loss: 0.027446720749139786, tv_loss: 0.0335502065718174\n",
      "iteration 2210, dc_loss: 0.02741897664964199, tv_loss: 0.033570148050785065\n",
      "iteration 2211, dc_loss: 0.027426136657595634, tv_loss: 0.03355049714446068\n",
      "iteration 2212, dc_loss: 0.027410876005887985, tv_loss: 0.033555563539266586\n",
      "iteration 2213, dc_loss: 0.027419086545705795, tv_loss: 0.03354161977767944\n",
      "iteration 2214, dc_loss: 0.027398768812417984, tv_loss: 0.033561382442712784\n",
      "iteration 2215, dc_loss: 0.027395229786634445, tv_loss: 0.03356339409947395\n",
      "iteration 2216, dc_loss: 0.027394937351346016, tv_loss: 0.033554110676050186\n",
      "iteration 2217, dc_loss: 0.02738768607378006, tv_loss: 0.03355187922716141\n",
      "iteration 2218, dc_loss: 0.027376821264624596, tv_loss: 0.03354731202125549\n",
      "iteration 2219, dc_loss: 0.02736218273639679, tv_loss: 0.033557165414094925\n",
      "iteration 2220, dc_loss: 0.027371427044272423, tv_loss: 0.0335397869348526\n",
      "iteration 2221, dc_loss: 0.02735736407339573, tv_loss: 0.03354243189096451\n",
      "iteration 2222, dc_loss: 0.02735494263470173, tv_loss: 0.033544983714818954\n",
      "iteration 2223, dc_loss: 0.02733929082751274, tv_loss: 0.033569157123565674\n",
      "iteration 2224, dc_loss: 0.027344075962901115, tv_loss: 0.03356130048632622\n",
      "iteration 2225, dc_loss: 0.02733185514807701, tv_loss: 0.03355751559138298\n",
      "iteration 2226, dc_loss: 0.027327144518494606, tv_loss: 0.03354707360267639\n",
      "iteration 2227, dc_loss: 0.02731313370168209, tv_loss: 0.03354991227388382\n",
      "iteration 2228, dc_loss: 0.027317434549331665, tv_loss: 0.03353721648454666\n",
      "iteration 2229, dc_loss: 0.02730851247906685, tv_loss: 0.03353719785809517\n",
      "iteration 2230, dc_loss: 0.027295265346765518, tv_loss: 0.033541254699230194\n",
      "iteration 2231, dc_loss: 0.027298688888549805, tv_loss: 0.03353207930922508\n",
      "iteration 2232, dc_loss: 0.027286922559142113, tv_loss: 0.033535994589328766\n",
      "iteration 2233, dc_loss: 0.027278538793325424, tv_loss: 0.03353746235370636\n",
      "iteration 2234, dc_loss: 0.0272762980312109, tv_loss: 0.03353150933980942\n",
      "iteration 2235, dc_loss: 0.027265870943665504, tv_loss: 0.03354182094335556\n",
      "iteration 2236, dc_loss: 0.027262086048722267, tv_loss: 0.0335431843996048\n",
      "iteration 2237, dc_loss: 0.027256831526756287, tv_loss: 0.03354692459106445\n",
      "iteration 2238, dc_loss: 0.027258047834038734, tv_loss: 0.03356486186385155\n",
      "iteration 2239, dc_loss: 0.027229895815253258, tv_loss: 0.03358769416809082\n",
      "iteration 2240, dc_loss: 0.027248084545135498, tv_loss: 0.033548757433891296\n",
      "iteration 2241, dc_loss: 0.027227558195590973, tv_loss: 0.033553291112184525\n",
      "iteration 2242, dc_loss: 0.02721981890499592, tv_loss: 0.03355502709746361\n",
      "iteration 2243, dc_loss: 0.027218565344810486, tv_loss: 0.03355851396918297\n",
      "iteration 2244, dc_loss: 0.02721378207206726, tv_loss: 0.033549144864082336\n",
      "iteration 2245, dc_loss: 0.02721773460507393, tv_loss: 0.03353704512119293\n",
      "iteration 2246, dc_loss: 0.027186406776309013, tv_loss: 0.03356725722551346\n",
      "iteration 2247, dc_loss: 0.027201594784855843, tv_loss: 0.033546071499586105\n",
      "iteration 2248, dc_loss: 0.027190441265702248, tv_loss: 0.0335594080388546\n",
      "iteration 2249, dc_loss: 0.027175024151802063, tv_loss: 0.03356745094060898\n",
      "iteration 2250, dc_loss: 0.027180293574929237, tv_loss: 0.03355369344353676\n",
      "iteration 2251, dc_loss: 0.027177082374691963, tv_loss: 0.033547293394804\n",
      "iteration 2252, dc_loss: 0.027163658291101456, tv_loss: 0.0335543192923069\n",
      "iteration 2253, dc_loss: 0.027154620736837387, tv_loss: 0.03355058655142784\n",
      "iteration 2254, dc_loss: 0.027174148708581924, tv_loss: 0.033532362431287766\n",
      "iteration 2255, dc_loss: 0.027126185595989227, tv_loss: 0.03357428312301636\n",
      "iteration 2256, dc_loss: 0.02716219425201416, tv_loss: 0.03354451060295105\n",
      "iteration 2257, dc_loss: 0.027122553437948227, tv_loss: 0.033594369888305664\n",
      "iteration 2258, dc_loss: 0.027157241478562355, tv_loss: 0.0335371159017086\n",
      "iteration 2259, dc_loss: 0.02711908146739006, tv_loss: 0.03357892856001854\n",
      "iteration 2260, dc_loss: 0.027171041816473007, tv_loss: 0.03352496400475502\n",
      "iteration 2261, dc_loss: 0.027131764218211174, tv_loss: 0.03358086571097374\n",
      "iteration 2262, dc_loss: 0.02719303034245968, tv_loss: 0.0335368737578392\n",
      "iteration 2263, dc_loss: 0.02715182490646839, tv_loss: 0.03358076140284538\n",
      "iteration 2264, dc_loss: 0.02721693366765976, tv_loss: 0.03351851925253868\n",
      "iteration 2265, dc_loss: 0.027108022943139076, tv_loss: 0.03359273821115494\n",
      "iteration 2266, dc_loss: 0.027154065668582916, tv_loss: 0.03351244702935219\n",
      "iteration 2267, dc_loss: 0.027063854038715363, tv_loss: 0.03359110653400421\n",
      "iteration 2268, dc_loss: 0.02713109366595745, tv_loss: 0.033521831035614014\n",
      "iteration 2269, dc_loss: 0.027086671441793442, tv_loss: 0.03356315940618515\n",
      "iteration 2270, dc_loss: 0.027089735493063927, tv_loss: 0.03353464975953102\n",
      "iteration 2271, dc_loss: 0.027067583054304123, tv_loss: 0.03354484587907791\n",
      "iteration 2272, dc_loss: 0.02705475315451622, tv_loss: 0.033554017543792725\n",
      "iteration 2273, dc_loss: 0.0270626749843359, tv_loss: 0.03354572877287865\n",
      "iteration 2274, dc_loss: 0.027018336579203606, tv_loss: 0.03359308838844299\n",
      "iteration 2275, dc_loss: 0.02704511396586895, tv_loss: 0.033566251397132874\n",
      "iteration 2276, dc_loss: 0.0270218662917614, tv_loss: 0.03356754034757614\n",
      "iteration 2277, dc_loss: 0.027016203850507736, tv_loss: 0.033550821244716644\n",
      "iteration 2278, dc_loss: 0.027018699795007706, tv_loss: 0.033563919365406036\n",
      "iteration 2279, dc_loss: 0.027020394802093506, tv_loss: 0.03356077894568443\n",
      "iteration 2280, dc_loss: 0.02699338085949421, tv_loss: 0.03357839584350586\n",
      "iteration 2281, dc_loss: 0.026979660615324974, tv_loss: 0.03356862813234329\n",
      "iteration 2282, dc_loss: 0.026989825069904327, tv_loss: 0.03356894105672836\n",
      "iteration 2283, dc_loss: 0.02698909491300583, tv_loss: 0.0335676409304142\n",
      "iteration 2284, dc_loss: 0.026960719376802444, tv_loss: 0.03358248248696327\n",
      "iteration 2285, dc_loss: 0.026964476332068443, tv_loss: 0.03356232866644859\n",
      "iteration 2286, dc_loss: 0.02696220763027668, tv_loss: 0.033560629934072495\n",
      "iteration 2287, dc_loss: 0.026962779462337494, tv_loss: 0.03356296196579933\n",
      "iteration 2288, dc_loss: 0.026947643607854843, tv_loss: 0.033565256744623184\n",
      "iteration 2289, dc_loss: 0.026943610981106758, tv_loss: 0.03355848789215088\n",
      "iteration 2290, dc_loss: 0.02694598026573658, tv_loss: 0.03354739770293236\n",
      "iteration 2291, dc_loss: 0.0269102081656456, tv_loss: 0.03357089310884476\n",
      "iteration 2292, dc_loss: 0.026943648234009743, tv_loss: 0.03352933004498482\n",
      "iteration 2293, dc_loss: 0.026901423931121826, tv_loss: 0.03356832265853882\n",
      "iteration 2294, dc_loss: 0.02691604010760784, tv_loss: 0.03354077413678169\n",
      "iteration 2295, dc_loss: 0.026906166225671768, tv_loss: 0.03354625403881073\n",
      "iteration 2296, dc_loss: 0.026904158294200897, tv_loss: 0.03354842960834503\n",
      "iteration 2297, dc_loss: 0.02689269743859768, tv_loss: 0.03356309235095978\n",
      "iteration 2298, dc_loss: 0.026883786544203758, tv_loss: 0.03357689455151558\n",
      "iteration 2299, dc_loss: 0.026883039623498917, tv_loss: 0.03355860337615013\n",
      "iteration 2300, dc_loss: 0.026868917047977448, tv_loss: 0.03355453535914421\n",
      "iteration 2301, dc_loss: 0.026876993477344513, tv_loss: 0.033550482243299484\n",
      "iteration 2302, dc_loss: 0.02687816135585308, tv_loss: 0.03355400636792183\n",
      "iteration 2303, dc_loss: 0.026846997439861298, tv_loss: 0.03357699140906334\n",
      "iteration 2304, dc_loss: 0.02684292010962963, tv_loss: 0.03356851264834404\n",
      "iteration 2305, dc_loss: 0.026842569932341576, tv_loss: 0.033554647117853165\n",
      "iteration 2306, dc_loss: 0.02684759348630905, tv_loss: 0.033543720841407776\n",
      "iteration 2307, dc_loss: 0.02682357095181942, tv_loss: 0.03356846794486046\n",
      "iteration 2308, dc_loss: 0.02684127911925316, tv_loss: 0.03354900702834129\n",
      "iteration 2309, dc_loss: 0.026826536282896996, tv_loss: 0.03355881944298744\n",
      "iteration 2310, dc_loss: 0.02681891992688179, tv_loss: 0.03355627879500389\n",
      "iteration 2311, dc_loss: 0.026805438101291656, tv_loss: 0.03355765342712402\n",
      "iteration 2312, dc_loss: 0.026808667927980423, tv_loss: 0.03355175256729126\n",
      "iteration 2313, dc_loss: 0.0267917700111866, tv_loss: 0.03356752544641495\n",
      "iteration 2314, dc_loss: 0.026796821504831314, tv_loss: 0.03355495631694794\n",
      "iteration 2315, dc_loss: 0.026789244264364243, tv_loss: 0.033559370785951614\n",
      "iteration 2316, dc_loss: 0.02678551897406578, tv_loss: 0.033561162650585175\n",
      "iteration 2317, dc_loss: 0.026776278391480446, tv_loss: 0.033562835305929184\n",
      "iteration 2318, dc_loss: 0.02678353153169155, tv_loss: 0.033550918102264404\n",
      "iteration 2319, dc_loss: 0.0267709381878376, tv_loss: 0.03355656936764717\n",
      "iteration 2320, dc_loss: 0.026770323514938354, tv_loss: 0.03355264663696289\n",
      "iteration 2321, dc_loss: 0.026764750480651855, tv_loss: 0.033550627529621124\n",
      "iteration 2322, dc_loss: 0.026787448674440384, tv_loss: 0.0335344523191452\n",
      "iteration 2323, dc_loss: 0.026743164286017418, tv_loss: 0.03357565402984619\n",
      "iteration 2324, dc_loss: 0.026812661439180374, tv_loss: 0.03353044390678406\n",
      "iteration 2325, dc_loss: 0.026751650497317314, tv_loss: 0.033610448241233826\n",
      "iteration 2326, dc_loss: 0.026830369606614113, tv_loss: 0.03353257477283478\n",
      "iteration 2327, dc_loss: 0.02676503360271454, tv_loss: 0.033616140484809875\n",
      "iteration 2328, dc_loss: 0.026915375143289566, tv_loss: 0.0334932878613472\n",
      "iteration 2329, dc_loss: 0.026788154616951942, tv_loss: 0.03365008533000946\n",
      "iteration 2330, dc_loss: 0.026951968669891357, tv_loss: 0.03346666321158409\n",
      "iteration 2331, dc_loss: 0.02677331119775772, tv_loss: 0.03364650532603264\n",
      "iteration 2332, dc_loss: 0.026898005977272987, tv_loss: 0.03347517549991608\n",
      "iteration 2333, dc_loss: 0.026694804430007935, tv_loss: 0.03364010155200958\n",
      "iteration 2334, dc_loss: 0.026730911806225777, tv_loss: 0.033533673733472824\n",
      "iteration 2335, dc_loss: 0.026670053601264954, tv_loss: 0.03356904909014702\n",
      "iteration 2336, dc_loss: 0.02665592171251774, tv_loss: 0.033576302230358124\n",
      "iteration 2337, dc_loss: 0.02675398625433445, tv_loss: 0.03350657597184181\n",
      "iteration 2338, dc_loss: 0.02666468173265457, tv_loss: 0.03360222652554512\n",
      "iteration 2339, dc_loss: 0.026751438155770302, tv_loss: 0.03350720927119255\n",
      "iteration 2340, dc_loss: 0.026638206094503403, tv_loss: 0.03360220789909363\n",
      "iteration 2341, dc_loss: 0.026705723255872726, tv_loss: 0.03350333496928215\n",
      "iteration 2342, dc_loss: 0.02662797085940838, tv_loss: 0.03356809914112091\n",
      "iteration 2343, dc_loss: 0.02662576362490654, tv_loss: 0.033558692783117294\n",
      "iteration 2344, dc_loss: 0.026662196964025497, tv_loss: 0.03353346884250641\n",
      "iteration 2345, dc_loss: 0.026597879827022552, tv_loss: 0.03359242528676987\n",
      "iteration 2346, dc_loss: 0.026656340807676315, tv_loss: 0.03353406488895416\n",
      "iteration 2347, dc_loss: 0.02661045640707016, tv_loss: 0.03356579318642616\n",
      "iteration 2348, dc_loss: 0.02663405053317547, tv_loss: 0.033532269299030304\n",
      "iteration 2349, dc_loss: 0.026605000719428062, tv_loss: 0.033548496663570404\n",
      "iteration 2350, dc_loss: 0.026585659012198448, tv_loss: 0.033556018024683\n",
      "iteration 2351, dc_loss: 0.026597466319799423, tv_loss: 0.03355201706290245\n",
      "iteration 2352, dc_loss: 0.026564152911305428, tv_loss: 0.03360305353999138\n",
      "iteration 2353, dc_loss: 0.026611048728227615, tv_loss: 0.033538222312927246\n",
      "iteration 2354, dc_loss: 0.026548022404313087, tv_loss: 0.033581651747226715\n",
      "iteration 2355, dc_loss: 0.026582082733511925, tv_loss: 0.03353676199913025\n",
      "iteration 2356, dc_loss: 0.026557156816124916, tv_loss: 0.0335640124976635\n",
      "iteration 2357, dc_loss: 0.026562636718153954, tv_loss: 0.03355855494737625\n",
      "iteration 2358, dc_loss: 0.02654479444026947, tv_loss: 0.033564090728759766\n",
      "iteration 2359, dc_loss: 0.02652701921761036, tv_loss: 0.03356822580099106\n",
      "iteration 2360, dc_loss: 0.026549795642495155, tv_loss: 0.03353048115968704\n",
      "iteration 2361, dc_loss: 0.026521459221839905, tv_loss: 0.03355816751718521\n",
      "iteration 2362, dc_loss: 0.026544969528913498, tv_loss: 0.03353527560830116\n",
      "iteration 2363, dc_loss: 0.026500215753912926, tv_loss: 0.033577729016542435\n",
      "iteration 2364, dc_loss: 0.026538526639342308, tv_loss: 0.03354375809431076\n",
      "iteration 2365, dc_loss: 0.02649451047182083, tv_loss: 0.0335836224257946\n",
      "iteration 2366, dc_loss: 0.026524199172854424, tv_loss: 0.033544838428497314\n",
      "iteration 2367, dc_loss: 0.026466762647032738, tv_loss: 0.03357860445976257\n",
      "iteration 2368, dc_loss: 0.026499826461076736, tv_loss: 0.03353739529848099\n",
      "iteration 2369, dc_loss: 0.026485580950975418, tv_loss: 0.03354474529623985\n",
      "iteration 2370, dc_loss: 0.02646910399198532, tv_loss: 0.03355514630675316\n",
      "iteration 2371, dc_loss: 0.026493147015571594, tv_loss: 0.03352196887135506\n",
      "iteration 2372, dc_loss: 0.026447506621479988, tv_loss: 0.03357076272368431\n",
      "iteration 2373, dc_loss: 0.02648022584617138, tv_loss: 0.033550843596458435\n",
      "iteration 2374, dc_loss: 0.02643568068742752, tv_loss: 0.03359782695770264\n",
      "iteration 2375, dc_loss: 0.02647951804101467, tv_loss: 0.03353556618094444\n",
      "iteration 2376, dc_loss: 0.026425037533044815, tv_loss: 0.033571161329746246\n",
      "iteration 2377, dc_loss: 0.02645958960056305, tv_loss: 0.03353137895464897\n",
      "iteration 2378, dc_loss: 0.02642187662422657, tv_loss: 0.033556342124938965\n",
      "iteration 2379, dc_loss: 0.026435354724526405, tv_loss: 0.033537548035383224\n",
      "iteration 2380, dc_loss: 0.026423489674925804, tv_loss: 0.03353709727525711\n",
      "iteration 2381, dc_loss: 0.026414213702082634, tv_loss: 0.03355250507593155\n",
      "iteration 2382, dc_loss: 0.026408562436699867, tv_loss: 0.03356926143169403\n",
      "iteration 2383, dc_loss: 0.0263962484896183, tv_loss: 0.03358330950140953\n",
      "iteration 2384, dc_loss: 0.026417210698127747, tv_loss: 0.033547960221767426\n",
      "iteration 2385, dc_loss: 0.026380419731140137, tv_loss: 0.03356770798563957\n",
      "iteration 2386, dc_loss: 0.026402950286865234, tv_loss: 0.033537667244672775\n",
      "iteration 2387, dc_loss: 0.026379289105534554, tv_loss: 0.03355893865227699\n",
      "iteration 2388, dc_loss: 0.026376893743872643, tv_loss: 0.03356650844216347\n",
      "iteration 2389, dc_loss: 0.02636924758553505, tv_loss: 0.03356868773698807\n",
      "iteration 2390, dc_loss: 0.02636273019015789, tv_loss: 0.03356414660811424\n",
      "iteration 2391, dc_loss: 0.026371896266937256, tv_loss: 0.03354267776012421\n",
      "iteration 2392, dc_loss: 0.02634674310684204, tv_loss: 0.033563707023859024\n",
      "iteration 2393, dc_loss: 0.026362024247646332, tv_loss: 0.03353724256157875\n",
      "iteration 2394, dc_loss: 0.026341894641518593, tv_loss: 0.0335543230175972\n",
      "iteration 2395, dc_loss: 0.026344509795308113, tv_loss: 0.033549170941114426\n",
      "iteration 2396, dc_loss: 0.026335295289754868, tv_loss: 0.03355289623141289\n",
      "iteration 2397, dc_loss: 0.02633696049451828, tv_loss: 0.03355349600315094\n",
      "iteration 2398, dc_loss: 0.026306863874197006, tv_loss: 0.033584654331207275\n",
      "iteration 2399, dc_loss: 0.02633942849934101, tv_loss: 0.033539362251758575\n",
      "iteration 2400, dc_loss: 0.026316111907362938, tv_loss: 0.033553920686244965\n",
      "iteration 2401, dc_loss: 0.026325548067688942, tv_loss: 0.03353872522711754\n",
      "iteration 2402, dc_loss: 0.026292309165000916, tv_loss: 0.03356120362877846\n",
      "iteration 2403, dc_loss: 0.02629193477332592, tv_loss: 0.03355366736650467\n",
      "iteration 2404, dc_loss: 0.026312272995710373, tv_loss: 0.033528659492731094\n",
      "iteration 2405, dc_loss: 0.02628338150680065, tv_loss: 0.03354332223534584\n",
      "iteration 2406, dc_loss: 0.026276659220457077, tv_loss: 0.03354234620928764\n",
      "iteration 2407, dc_loss: 0.026294592767953873, tv_loss: 0.03352069482207298\n",
      "iteration 2408, dc_loss: 0.026278384029865265, tv_loss: 0.03353484719991684\n",
      "iteration 2409, dc_loss: 0.026269875466823578, tv_loss: 0.033531248569488525\n",
      "iteration 2410, dc_loss: 0.02626989223062992, tv_loss: 0.03353374823927879\n",
      "iteration 2411, dc_loss: 0.026257917284965515, tv_loss: 0.033548083156347275\n",
      "iteration 2412, dc_loss: 0.02626483328640461, tv_loss: 0.03355021774768829\n",
      "iteration 2413, dc_loss: 0.026256315410137177, tv_loss: 0.03355855122208595\n",
      "iteration 2414, dc_loss: 0.026248760521411896, tv_loss: 0.03354766592383385\n",
      "iteration 2415, dc_loss: 0.02624414674937725, tv_loss: 0.03353719413280487\n",
      "iteration 2416, dc_loss: 0.02623438648879528, tv_loss: 0.03354603797197342\n",
      "iteration 2417, dc_loss: 0.02623807080090046, tv_loss: 0.033548757433891296\n",
      "iteration 2418, dc_loss: 0.026239216327667236, tv_loss: 0.033544812351465225\n",
      "iteration 2419, dc_loss: 0.0262349434196949, tv_loss: 0.03354291245341301\n",
      "iteration 2420, dc_loss: 0.026222992688417435, tv_loss: 0.033543724566698074\n",
      "iteration 2421, dc_loss: 0.026220479980111122, tv_loss: 0.03353726118803024\n",
      "iteration 2422, dc_loss: 0.026214372366666794, tv_loss: 0.03354174643754959\n",
      "iteration 2423, dc_loss: 0.026208920404314995, tv_loss: 0.03354405239224434\n",
      "iteration 2424, dc_loss: 0.026208698749542236, tv_loss: 0.03353366255760193\n",
      "iteration 2425, dc_loss: 0.026204532012343407, tv_loss: 0.033541131764650345\n",
      "iteration 2426, dc_loss: 0.026194903999567032, tv_loss: 0.03354164958000183\n",
      "iteration 2427, dc_loss: 0.02619781345129013, tv_loss: 0.033533044159412384\n",
      "iteration 2428, dc_loss: 0.026192767545580864, tv_loss: 0.033537667244672775\n",
      "iteration 2429, dc_loss: 0.026187101379036903, tv_loss: 0.03353624418377876\n",
      "iteration 2430, dc_loss: 0.02618248388171196, tv_loss: 0.0335451140999794\n",
      "iteration 2431, dc_loss: 0.026178037747740746, tv_loss: 0.03355489671230316\n",
      "iteration 2432, dc_loss: 0.026168320327997208, tv_loss: 0.03355442360043526\n",
      "iteration 2433, dc_loss: 0.026170426979660988, tv_loss: 0.03354260325431824\n",
      "iteration 2434, dc_loss: 0.026169970631599426, tv_loss: 0.03353654593229294\n",
      "iteration 2435, dc_loss: 0.026156265288591385, tv_loss: 0.03354572504758835\n",
      "iteration 2436, dc_loss: 0.026156457141041756, tv_loss: 0.03355081379413605\n",
      "iteration 2437, dc_loss: 0.026155980303883553, tv_loss: 0.03354498744010925\n",
      "iteration 2438, dc_loss: 0.026149949058890343, tv_loss: 0.03354200720787048\n",
      "iteration 2439, dc_loss: 0.026142384856939316, tv_loss: 0.03354593738913536\n",
      "iteration 2440, dc_loss: 0.026146981865167618, tv_loss: 0.03352908045053482\n",
      "iteration 2441, dc_loss: 0.02613896131515503, tv_loss: 0.03353259339928627\n",
      "iteration 2442, dc_loss: 0.02612459287047386, tv_loss: 0.03354959189891815\n",
      "iteration 2443, dc_loss: 0.026130754500627518, tv_loss: 0.03354361280798912\n",
      "iteration 2444, dc_loss: 0.02612093649804592, tv_loss: 0.03354443609714508\n",
      "iteration 2445, dc_loss: 0.026118367910385132, tv_loss: 0.03355870023369789\n",
      "iteration 2446, dc_loss: 0.026113471016287804, tv_loss: 0.03355475515127182\n",
      "iteration 2447, dc_loss: 0.026117641478776932, tv_loss: 0.03353803977370262\n",
      "iteration 2448, dc_loss: 0.02610631473362446, tv_loss: 0.03354043886065483\n",
      "iteration 2449, dc_loss: 0.026104610413312912, tv_loss: 0.03353424370288849\n",
      "iteration 2450, dc_loss: 0.02609926648437977, tv_loss: 0.03353805094957352\n",
      "iteration 2451, dc_loss: 0.02608462981879711, tv_loss: 0.03355003148317337\n",
      "iteration 2452, dc_loss: 0.026100071147084236, tv_loss: 0.03354213014245033\n",
      "iteration 2453, dc_loss: 0.026089049875736237, tv_loss: 0.033561330288648605\n",
      "iteration 2454, dc_loss: 0.026080738753080368, tv_loss: 0.03355078771710396\n",
      "iteration 2455, dc_loss: 0.026076504960656166, tv_loss: 0.03353644162416458\n",
      "iteration 2456, dc_loss: 0.026073182001709938, tv_loss: 0.033537235110998154\n",
      "iteration 2457, dc_loss: 0.0260754507035017, tv_loss: 0.033527765423059464\n",
      "iteration 2458, dc_loss: 0.02606327086687088, tv_loss: 0.0335419587790966\n",
      "iteration 2459, dc_loss: 0.0260605551302433, tv_loss: 0.03355111926794052\n",
      "iteration 2460, dc_loss: 0.026060014963150024, tv_loss: 0.033556848764419556\n",
      "iteration 2461, dc_loss: 0.02605808712542057, tv_loss: 0.03354814276099205\n",
      "iteration 2462, dc_loss: 0.026044076308608055, tv_loss: 0.03354543074965477\n",
      "iteration 2463, dc_loss: 0.026042941957712173, tv_loss: 0.03353665769100189\n",
      "iteration 2464, dc_loss: 0.026043077930808067, tv_loss: 0.03353406861424446\n",
      "iteration 2465, dc_loss: 0.026038547977805138, tv_loss: 0.03353952616453171\n",
      "iteration 2466, dc_loss: 0.026033267378807068, tv_loss: 0.03355185315012932\n",
      "iteration 2467, dc_loss: 0.026029638946056366, tv_loss: 0.033549606800079346\n",
      "iteration 2468, dc_loss: 0.02603224851191044, tv_loss: 0.03353864699602127\n",
      "iteration 2469, dc_loss: 0.026015836745500565, tv_loss: 0.03354450687766075\n",
      "iteration 2470, dc_loss: 0.026011543348431587, tv_loss: 0.03354039415717125\n",
      "iteration 2471, dc_loss: 0.026016587391495705, tv_loss: 0.033533792942762375\n",
      "iteration 2472, dc_loss: 0.026005469262599945, tv_loss: 0.03353799879550934\n",
      "iteration 2473, dc_loss: 0.026008788496255875, tv_loss: 0.03354154899716377\n",
      "iteration 2474, dc_loss: 0.025998767465353012, tv_loss: 0.033558424562215805\n",
      "iteration 2475, dc_loss: 0.025999674573540688, tv_loss: 0.03355091065168381\n",
      "iteration 2476, dc_loss: 0.025992058217525482, tv_loss: 0.03354445472359657\n",
      "iteration 2477, dc_loss: 0.025992069393396378, tv_loss: 0.0335388258099556\n",
      "iteration 2478, dc_loss: 0.02598576433956623, tv_loss: 0.03354085981845856\n",
      "iteration 2479, dc_loss: 0.025971921160817146, tv_loss: 0.033546701073646545\n",
      "iteration 2480, dc_loss: 0.0259797852486372, tv_loss: 0.03353998437523842\n",
      "iteration 2481, dc_loss: 0.025973642244935036, tv_loss: 0.033538781106472015\n",
      "iteration 2482, dc_loss: 0.025969073176383972, tv_loss: 0.03353753313422203\n",
      "iteration 2483, dc_loss: 0.0259640384465456, tv_loss: 0.03354048728942871\n",
      "iteration 2484, dc_loss: 0.025955654680728912, tv_loss: 0.033546268939971924\n",
      "iteration 2485, dc_loss: 0.025958670303225517, tv_loss: 0.03354373201727867\n",
      "iteration 2486, dc_loss: 0.02596021257340908, tv_loss: 0.03354104235768318\n",
      "iteration 2487, dc_loss: 0.025941144675016403, tv_loss: 0.033554721623659134\n",
      "iteration 2488, dc_loss: 0.0259446669369936, tv_loss: 0.03353911265730858\n",
      "iteration 2489, dc_loss: 0.025944802910089493, tv_loss: 0.03354262188076973\n",
      "iteration 2490, dc_loss: 0.025930130854249, tv_loss: 0.03354617953300476\n",
      "iteration 2491, dc_loss: 0.02593277581036091, tv_loss: 0.03353315219283104\n",
      "iteration 2492, dc_loss: 0.025928374379873276, tv_loss: 0.03353903815150261\n",
      "iteration 2493, dc_loss: 0.02592139132320881, tv_loss: 0.03353684023022652\n",
      "iteration 2494, dc_loss: 0.025927355512976646, tv_loss: 0.03352818638086319\n",
      "iteration 2495, dc_loss: 0.025916405022144318, tv_loss: 0.03353670984506607\n",
      "iteration 2496, dc_loss: 0.025914466008543968, tv_loss: 0.03353821113705635\n",
      "iteration 2497, dc_loss: 0.025911642238497734, tv_loss: 0.03354470431804657\n",
      "iteration 2498, dc_loss: 0.025902509689331055, tv_loss: 0.03356682136654854\n",
      "iteration 2499, dc_loss: 0.025891518220305443, tv_loss: 0.03356422111392021\n",
      "iteration 2500, dc_loss: 0.02589842677116394, tv_loss: 0.03354267030954361\n",
      "iteration 2501, dc_loss: 0.025890182703733444, tv_loss: 0.03354034945368767\n",
      "iteration 2502, dc_loss: 0.0258872639387846, tv_loss: 0.03354885056614876\n",
      "iteration 2503, dc_loss: 0.02589421719312668, tv_loss: 0.03354746103286743\n",
      "iteration 2504, dc_loss: 0.025874806568026543, tv_loss: 0.033550944179296494\n",
      "iteration 2505, dc_loss: 0.02587125077843666, tv_loss: 0.03354915231466293\n",
      "iteration 2506, dc_loss: 0.0258789099752903, tv_loss: 0.033534109592437744\n",
      "iteration 2507, dc_loss: 0.025863518938422203, tv_loss: 0.033546268939971924\n",
      "iteration 2508, dc_loss: 0.025868363678455353, tv_loss: 0.0335400365293026\n",
      "iteration 2509, dc_loss: 0.02586648054420948, tv_loss: 0.033536724746227264\n",
      "iteration 2510, dc_loss: 0.02585393749177456, tv_loss: 0.033543672412633896\n",
      "iteration 2511, dc_loss: 0.025849733501672745, tv_loss: 0.03354620561003685\n",
      "iteration 2512, dc_loss: 0.025851290673017502, tv_loss: 0.03353794664144516\n",
      "iteration 2513, dc_loss: 0.025841709226369858, tv_loss: 0.033546581864356995\n",
      "iteration 2514, dc_loss: 0.025841282680630684, tv_loss: 0.033538494259119034\n",
      "iteration 2515, dc_loss: 0.02584250457584858, tv_loss: 0.03353191167116165\n",
      "iteration 2516, dc_loss: 0.02583083137869835, tv_loss: 0.03353811800479889\n",
      "iteration 2517, dc_loss: 0.02582363784313202, tv_loss: 0.03354622423648834\n",
      "iteration 2518, dc_loss: 0.0258327703922987, tv_loss: 0.03353643789887428\n",
      "iteration 2519, dc_loss: 0.025816652923822403, tv_loss: 0.03355291858315468\n",
      "iteration 2520, dc_loss: 0.02581964060664177, tv_loss: 0.03355271369218826\n",
      "iteration 2521, dc_loss: 0.025813955813646317, tv_loss: 0.033548157662153244\n",
      "iteration 2522, dc_loss: 0.025814687833189964, tv_loss: 0.03353901579976082\n",
      "iteration 2523, dc_loss: 0.025800473988056183, tv_loss: 0.033545102924108505\n",
      "iteration 2524, dc_loss: 0.02580251917243004, tv_loss: 0.033534176647663116\n",
      "iteration 2525, dc_loss: 0.025804946199059486, tv_loss: 0.0335334911942482\n",
      "iteration 2526, dc_loss: 0.025786958634853363, tv_loss: 0.03355316445231438\n",
      "iteration 2527, dc_loss: 0.02579508349299431, tv_loss: 0.0335434228181839\n",
      "iteration 2528, dc_loss: 0.02578211948275566, tv_loss: 0.03355156630277634\n",
      "iteration 2529, dc_loss: 0.02578170970082283, tv_loss: 0.033544089645147324\n",
      "iteration 2530, dc_loss: 0.025778846815228462, tv_loss: 0.03354137763381004\n",
      "iteration 2531, dc_loss: 0.02577519789338112, tv_loss: 0.033541712909936905\n",
      "iteration 2532, dc_loss: 0.02577584609389305, tv_loss: 0.033536966890096664\n",
      "iteration 2533, dc_loss: 0.025763677433133125, tv_loss: 0.033543650060892105\n",
      "iteration 2534, dc_loss: 0.02575944922864437, tv_loss: 0.03354198485612869\n",
      "iteration 2535, dc_loss: 0.025762889534235, tv_loss: 0.033534616231918335\n",
      "iteration 2536, dc_loss: 0.025759005919098854, tv_loss: 0.03353163227438927\n",
      "iteration 2537, dc_loss: 0.025751346722245216, tv_loss: 0.033532608300447464\n",
      "iteration 2538, dc_loss: 0.025741657242178917, tv_loss: 0.03354224935173988\n",
      "iteration 2539, dc_loss: 0.025740280747413635, tv_loss: 0.03353595361113548\n",
      "iteration 2540, dc_loss: 0.02574996091425419, tv_loss: 0.03354049101471901\n",
      "iteration 2541, dc_loss: 0.02573052980005741, tv_loss: 0.03356495872139931\n",
      "iteration 2542, dc_loss: 0.025733059272170067, tv_loss: 0.033553317189216614\n",
      "iteration 2543, dc_loss: 0.025724725797772408, tv_loss: 0.0335470549762249\n",
      "iteration 2544, dc_loss: 0.025725189596414566, tv_loss: 0.03353295475244522\n",
      "iteration 2545, dc_loss: 0.025725850835442543, tv_loss: 0.03352950140833855\n",
      "iteration 2546, dc_loss: 0.025712184607982635, tv_loss: 0.03353600203990936\n",
      "iteration 2547, dc_loss: 0.025712711736559868, tv_loss: 0.033535394817590714\n",
      "iteration 2548, dc_loss: 0.025707120075821877, tv_loss: 0.03353741392493248\n",
      "iteration 2549, dc_loss: 0.025701621547341347, tv_loss: 0.033562108874320984\n",
      "iteration 2550, dc_loss: 0.025708751752972603, tv_loss: 0.03356214612722397\n",
      "iteration 2551, dc_loss: 0.025693627074360847, tv_loss: 0.03354697301983833\n",
      "iteration 2552, dc_loss: 0.025691673159599304, tv_loss: 0.033549580723047256\n",
      "iteration 2553, dc_loss: 0.025699619203805923, tv_loss: 0.03355421870946884\n",
      "iteration 2554, dc_loss: 0.025677721947431564, tv_loss: 0.03355797380208969\n",
      "iteration 2555, dc_loss: 0.025675876066088676, tv_loss: 0.03354687616229057\n",
      "iteration 2556, dc_loss: 0.025683041661977768, tv_loss: 0.03354896232485771\n",
      "iteration 2557, dc_loss: 0.025670664384961128, tv_loss: 0.03357327729463577\n",
      "iteration 2558, dc_loss: 0.025673888623714447, tv_loss: 0.03355112299323082\n",
      "iteration 2559, dc_loss: 0.025665052235126495, tv_loss: 0.03354065492749214\n",
      "iteration 2560, dc_loss: 0.02567066065967083, tv_loss: 0.03354845568537712\n",
      "iteration 2561, dc_loss: 0.02565951459109783, tv_loss: 0.0335550382733345\n",
      "iteration 2562, dc_loss: 0.025644419714808464, tv_loss: 0.03355320543050766\n",
      "iteration 2563, dc_loss: 0.025663049891591072, tv_loss: 0.03353175148367882\n",
      "iteration 2564, dc_loss: 0.025645839050412178, tv_loss: 0.033541489392519\n",
      "iteration 2565, dc_loss: 0.025639917701482773, tv_loss: 0.033543799072504044\n",
      "iteration 2566, dc_loss: 0.025645896792411804, tv_loss: 0.03353298455476761\n",
      "iteration 2567, dc_loss: 0.02563490904867649, tv_loss: 0.03354819118976593\n",
      "iteration 2568, dc_loss: 0.025625592097640038, tv_loss: 0.033561933785676956\n",
      "iteration 2569, dc_loss: 0.025633318349719048, tv_loss: 0.03353997319936752\n",
      "iteration 2570, dc_loss: 0.025626327842473984, tv_loss: 0.0335441455245018\n",
      "iteration 2571, dc_loss: 0.025622284039855003, tv_loss: 0.03354327008128166\n",
      "iteration 2572, dc_loss: 0.025623667985200882, tv_loss: 0.033534541726112366\n",
      "iteration 2573, dc_loss: 0.025611964985728264, tv_loss: 0.033536188304424286\n",
      "iteration 2574, dc_loss: 0.02560245990753174, tv_loss: 0.03354731574654579\n",
      "iteration 2575, dc_loss: 0.025607772171497345, tv_loss: 0.03353918716311455\n",
      "iteration 2576, dc_loss: 0.025603724643588066, tv_loss: 0.03354490175843239\n",
      "iteration 2577, dc_loss: 0.02559811621904373, tv_loss: 0.03355110436677933\n",
      "iteration 2578, dc_loss: 0.02559511363506317, tv_loss: 0.033550556749105453\n",
      "iteration 2579, dc_loss: 0.02559744007885456, tv_loss: 0.03353935480117798\n",
      "iteration 2580, dc_loss: 0.025578975677490234, tv_loss: 0.033549316227436066\n",
      "iteration 2581, dc_loss: 0.025582697242498398, tv_loss: 0.03353586047887802\n",
      "iteration 2582, dc_loss: 0.025581836700439453, tv_loss: 0.0335393100976944\n",
      "iteration 2583, dc_loss: 0.025569403544068336, tv_loss: 0.03354335576295853\n",
      "iteration 2584, dc_loss: 0.025577975437045097, tv_loss: 0.033526793122291565\n",
      "iteration 2585, dc_loss: 0.02557295560836792, tv_loss: 0.03353065997362137\n",
      "iteration 2586, dc_loss: 0.025563344359397888, tv_loss: 0.03353838250041008\n",
      "iteration 2587, dc_loss: 0.025561757385730743, tv_loss: 0.03353375196456909\n",
      "iteration 2588, dc_loss: 0.025554073974490166, tv_loss: 0.03353654220700264\n",
      "iteration 2589, dc_loss: 0.025555776432156563, tv_loss: 0.033534906804561615\n",
      "iteration 2590, dc_loss: 0.025545654818415642, tv_loss: 0.03356888145208359\n",
      "iteration 2591, dc_loss: 0.02554982900619507, tv_loss: 0.03355839475989342\n",
      "iteration 2592, dc_loss: 0.025536412373185158, tv_loss: 0.033543799072504044\n",
      "iteration 2593, dc_loss: 0.02554292418062687, tv_loss: 0.03354097157716751\n",
      "iteration 2594, dc_loss: 0.025535086169838905, tv_loss: 0.03356997296214104\n",
      "iteration 2595, dc_loss: 0.025523649528622627, tv_loss: 0.03356751427054405\n",
      "iteration 2596, dc_loss: 0.02553441934287548, tv_loss: 0.033529527485370636\n",
      "iteration 2597, dc_loss: 0.025525201112031937, tv_loss: 0.033544834703207016\n",
      "iteration 2598, dc_loss: 0.025516586378216743, tv_loss: 0.0335703119635582\n",
      "iteration 2599, dc_loss: 0.025524763390421867, tv_loss: 0.03355355188250542\n",
      "iteration 2600, dc_loss: 0.02550557255744934, tv_loss: 0.03355163335800171\n",
      "iteration 2601, dc_loss: 0.025507302954792976, tv_loss: 0.03354532644152641\n",
      "iteration 2602, dc_loss: 0.02550044283270836, tv_loss: 0.0335635282099247\n",
      "iteration 2603, dc_loss: 0.025501884520053864, tv_loss: 0.03355540335178375\n",
      "iteration 2604, dc_loss: 0.025513725355267525, tv_loss: 0.03353763371706009\n",
      "iteration 2605, dc_loss: 0.02547893300652504, tv_loss: 0.0335676334798336\n",
      "iteration 2606, dc_loss: 0.025490213185548782, tv_loss: 0.033569153398275375\n",
      "iteration 2607, dc_loss: 0.02550644427537918, tv_loss: 0.03353668004274368\n",
      "iteration 2608, dc_loss: 0.025469059124588966, tv_loss: 0.03356368467211723\n",
      "iteration 2609, dc_loss: 0.025469625368714333, tv_loss: 0.033568594604730606\n",
      "iteration 2610, dc_loss: 0.025487996637821198, tv_loss: 0.03354413062334061\n",
      "iteration 2611, dc_loss: 0.025471149012446404, tv_loss: 0.03355258330702782\n",
      "iteration 2612, dc_loss: 0.025468403473496437, tv_loss: 0.033544886857271194\n",
      "iteration 2613, dc_loss: 0.02546967752277851, tv_loss: 0.03354005888104439\n",
      "iteration 2614, dc_loss: 0.025464003905653954, tv_loss: 0.03355695307254791\n",
      "iteration 2615, dc_loss: 0.025454595685005188, tv_loss: 0.03355836123228073\n",
      "iteration 2616, dc_loss: 0.025450075045228004, tv_loss: 0.03354748338460922\n",
      "iteration 2617, dc_loss: 0.025457536801695824, tv_loss: 0.03354083374142647\n",
      "iteration 2618, dc_loss: 0.025438543409109116, tv_loss: 0.03355865925550461\n",
      "iteration 2619, dc_loss: 0.02544192038476467, tv_loss: 0.033554285764694214\n",
      "iteration 2620, dc_loss: 0.025453150272369385, tv_loss: 0.03353891149163246\n",
      "iteration 2621, dc_loss: 0.025428244844079018, tv_loss: 0.033549223095178604\n",
      "iteration 2622, dc_loss: 0.0254297386854887, tv_loss: 0.033547159284353256\n",
      "iteration 2623, dc_loss: 0.025437628850340843, tv_loss: 0.033536262810230255\n",
      "iteration 2624, dc_loss: 0.02542000263929367, tv_loss: 0.033552076667547226\n",
      "iteration 2625, dc_loss: 0.025416400283575058, tv_loss: 0.03355105221271515\n",
      "iteration 2626, dc_loss: 0.02541813813149929, tv_loss: 0.03354395553469658\n",
      "iteration 2627, dc_loss: 0.025426601991057396, tv_loss: 0.03353239595890045\n",
      "iteration 2628, dc_loss: 0.02540031634271145, tv_loss: 0.03355348855257034\n",
      "iteration 2629, dc_loss: 0.025410853326320648, tv_loss: 0.03353460133075714\n",
      "iteration 2630, dc_loss: 0.025401022285223007, tv_loss: 0.03353686258196831\n",
      "iteration 2631, dc_loss: 0.02540036104619503, tv_loss: 0.03353442996740341\n",
      "iteration 2632, dc_loss: 0.025391362607479095, tv_loss: 0.033536646515131\n",
      "iteration 2633, dc_loss: 0.025394940748810768, tv_loss: 0.03353460878133774\n",
      "iteration 2634, dc_loss: 0.02539198100566864, tv_loss: 0.0335482619702816\n",
      "iteration 2635, dc_loss: 0.025392286479473114, tv_loss: 0.033565275371074677\n",
      "iteration 2636, dc_loss: 0.025368565693497658, tv_loss: 0.0335749089717865\n",
      "iteration 2637, dc_loss: 0.02539770118892193, tv_loss: 0.03352167829871178\n",
      "iteration 2638, dc_loss: 0.025376660749316216, tv_loss: 0.033553093671798706\n",
      "iteration 2639, dc_loss: 0.025379788130521774, tv_loss: 0.03355734050273895\n",
      "iteration 2640, dc_loss: 0.025376001372933388, tv_loss: 0.033568594604730606\n",
      "iteration 2641, dc_loss: 0.025396492332220078, tv_loss: 0.03353360295295715\n",
      "iteration 2642, dc_loss: 0.025383803993463516, tv_loss: 0.03355663642287254\n",
      "iteration 2643, dc_loss: 0.025408253073692322, tv_loss: 0.0335330031812191\n",
      "iteration 2644, dc_loss: 0.025388101115822792, tv_loss: 0.03357475623488426\n",
      "iteration 2645, dc_loss: 0.025430195033550262, tv_loss: 0.03351946547627449\n",
      "iteration 2646, dc_loss: 0.025366416200995445, tv_loss: 0.03357579559087753\n",
      "iteration 2647, dc_loss: 0.02538973279297352, tv_loss: 0.03352484479546547\n",
      "iteration 2648, dc_loss: 0.02534474991261959, tv_loss: 0.03354828804731369\n",
      "iteration 2649, dc_loss: 0.025352533906698227, tv_loss: 0.03352534770965576\n",
      "iteration 2650, dc_loss: 0.025317782536149025, tv_loss: 0.03354683518409729\n",
      "iteration 2651, dc_loss: 0.025335099548101425, tv_loss: 0.03353557735681534\n",
      "iteration 2652, dc_loss: 0.025354696437716484, tv_loss: 0.033525899052619934\n",
      "iteration 2653, dc_loss: 0.02531779371201992, tv_loss: 0.03357987478375435\n",
      "iteration 2654, dc_loss: 0.025343354791402817, tv_loss: 0.0335431769490242\n",
      "iteration 2655, dc_loss: 0.025308454409241676, tv_loss: 0.03354915603995323\n",
      "iteration 2656, dc_loss: 0.02531713806092739, tv_loss: 0.03353313356637955\n",
      "iteration 2657, dc_loss: 0.025299271568655968, tv_loss: 0.03354702144861221\n",
      "iteration 2658, dc_loss: 0.025306036695837975, tv_loss: 0.033556655049324036\n",
      "iteration 2659, dc_loss: 0.025314785540103912, tv_loss: 0.03353970870375633\n",
      "iteration 2660, dc_loss: 0.025291871279478073, tv_loss: 0.03355037048459053\n",
      "iteration 2661, dc_loss: 0.025298766791820526, tv_loss: 0.033532362431287766\n",
      "iteration 2662, dc_loss: 0.025290733203291893, tv_loss: 0.03354644775390625\n",
      "iteration 2663, dc_loss: 0.025293180719017982, tv_loss: 0.033544015139341354\n",
      "iteration 2664, dc_loss: 0.025265634059906006, tv_loss: 0.033574458211660385\n",
      "iteration 2665, dc_loss: 0.025280961766839027, tv_loss: 0.03354880213737488\n",
      "iteration 2666, dc_loss: 0.025286545976996422, tv_loss: 0.033536359667778015\n",
      "iteration 2667, dc_loss: 0.025269338861107826, tv_loss: 0.03354727476835251\n",
      "iteration 2668, dc_loss: 0.02527577616274357, tv_loss: 0.03354162350296974\n",
      "iteration 2669, dc_loss: 0.0252615325152874, tv_loss: 0.033550772815942764\n",
      "iteration 2670, dc_loss: 0.02527027390897274, tv_loss: 0.03354117274284363\n",
      "iteration 2671, dc_loss: 0.02524625137448311, tv_loss: 0.03355320543050766\n",
      "iteration 2672, dc_loss: 0.025253454223275185, tv_loss: 0.03354334458708763\n",
      "iteration 2673, dc_loss: 0.025259481742978096, tv_loss: 0.03353210166096687\n",
      "iteration 2674, dc_loss: 0.025244731456041336, tv_loss: 0.03354869782924652\n",
      "iteration 2675, dc_loss: 0.02524239383637905, tv_loss: 0.03353925049304962\n",
      "iteration 2676, dc_loss: 0.025246581062674522, tv_loss: 0.03353384882211685\n",
      "iteration 2677, dc_loss: 0.02523181214928627, tv_loss: 0.03354600444436073\n",
      "iteration 2678, dc_loss: 0.02523544616997242, tv_loss: 0.03354440629482269\n",
      "iteration 2679, dc_loss: 0.025221100077033043, tv_loss: 0.033558353781700134\n",
      "iteration 2680, dc_loss: 0.025228355079889297, tv_loss: 0.03354203701019287\n",
      "iteration 2681, dc_loss: 0.02522161230444908, tv_loss: 0.033542852848768234\n",
      "iteration 2682, dc_loss: 0.02522435411810875, tv_loss: 0.03353677690029144\n",
      "iteration 2683, dc_loss: 0.025210769847035408, tv_loss: 0.03354522958397865\n",
      "iteration 2684, dc_loss: 0.02520926482975483, tv_loss: 0.03354831412434578\n",
      "iteration 2685, dc_loss: 0.025207320228219032, tv_loss: 0.033542390912771225\n",
      "iteration 2686, dc_loss: 0.025209784507751465, tv_loss: 0.033539436757564545\n",
      "iteration 2687, dc_loss: 0.025192884728312492, tv_loss: 0.033546727150678635\n",
      "iteration 2688, dc_loss: 0.02520137093961239, tv_loss: 0.03353746980428696\n",
      "iteration 2689, dc_loss: 0.025192677974700928, tv_loss: 0.03353990614414215\n",
      "iteration 2690, dc_loss: 0.025187203660607338, tv_loss: 0.03354939445853233\n",
      "iteration 2691, dc_loss: 0.02519466169178486, tv_loss: 0.033536989241838455\n",
      "iteration 2692, dc_loss: 0.02518061362206936, tv_loss: 0.033547576516866684\n",
      "iteration 2693, dc_loss: 0.025175221264362335, tv_loss: 0.03355292230844498\n",
      "iteration 2694, dc_loss: 0.02518206089735031, tv_loss: 0.03353853151202202\n",
      "iteration 2695, dc_loss: 0.02517770417034626, tv_loss: 0.03354225307703018\n",
      "iteration 2696, dc_loss: 0.025167003273963928, tv_loss: 0.03354869410395622\n",
      "iteration 2697, dc_loss: 0.025164926424622536, tv_loss: 0.033549342304468155\n",
      "iteration 2698, dc_loss: 0.025165895000100136, tv_loss: 0.033546384423971176\n",
      "iteration 2699, dc_loss: 0.02515680342912674, tv_loss: 0.03354470431804657\n",
      "iteration 2700, dc_loss: 0.02516128681600094, tv_loss: 0.033533673733472824\n",
      "iteration 2701, dc_loss: 0.025152334943413734, tv_loss: 0.03354299068450928\n",
      "iteration 2702, dc_loss: 0.02514326199889183, tv_loss: 0.03354663401842117\n",
      "iteration 2703, dc_loss: 0.025156117975711823, tv_loss: 0.03353283554315567\n",
      "iteration 2704, dc_loss: 0.025146935135126114, tv_loss: 0.0335305780172348\n",
      "iteration 2705, dc_loss: 0.02512413077056408, tv_loss: 0.03355656936764717\n",
      "iteration 2706, dc_loss: 0.025153206661343575, tv_loss: 0.033523984253406525\n",
      "iteration 2707, dc_loss: 0.02513018436729908, tv_loss: 0.033546239137649536\n",
      "iteration 2708, dc_loss: 0.02515343390405178, tv_loss: 0.03351498395204544\n",
      "iteration 2709, dc_loss: 0.02512749284505844, tv_loss: 0.03354368358850479\n",
      "iteration 2710, dc_loss: 0.025142095983028412, tv_loss: 0.03353526443243027\n",
      "iteration 2711, dc_loss: 0.025128424167633057, tv_loss: 0.03357230871915817\n",
      "iteration 2712, dc_loss: 0.02516830712556839, tv_loss: 0.03355599194765091\n",
      "iteration 2713, dc_loss: 0.025118308141827583, tv_loss: 0.033569760620594025\n",
      "iteration 2714, dc_loss: 0.025171278044581413, tv_loss: 0.03350987285375595\n",
      "iteration 2715, dc_loss: 0.025118667632341385, tv_loss: 0.033595919609069824\n",
      "iteration 2716, dc_loss: 0.02513880282640457, tv_loss: 0.03355341777205467\n",
      "iteration 2717, dc_loss: 0.02510581910610199, tv_loss: 0.03356511518359184\n",
      "iteration 2718, dc_loss: 0.025133537128567696, tv_loss: 0.03357211872935295\n",
      "iteration 2719, dc_loss: 0.025109009817242622, tv_loss: 0.033576905727386475\n",
      "iteration 2720, dc_loss: 0.025121347978711128, tv_loss: 0.03353669121861458\n",
      "iteration 2721, dc_loss: 0.025107627734541893, tv_loss: 0.03357210382819176\n",
      "iteration 2722, dc_loss: 0.0250824186950922, tv_loss: 0.033575721085071564\n",
      "iteration 2723, dc_loss: 0.025078075006604195, tv_loss: 0.0335574634373188\n",
      "iteration 2724, dc_loss: 0.025078296661376953, tv_loss: 0.03356602042913437\n",
      "iteration 2725, dc_loss: 0.025070950388908386, tv_loss: 0.03356114402413368\n",
      "iteration 2726, dc_loss: 0.0250734630972147, tv_loss: 0.03355427086353302\n",
      "iteration 2727, dc_loss: 0.02507079765200615, tv_loss: 0.03355531021952629\n",
      "iteration 2728, dc_loss: 0.025066548958420753, tv_loss: 0.03356337174773216\n",
      "iteration 2729, dc_loss: 0.02506418339908123, tv_loss: 0.03354916349053383\n",
      "iteration 2730, dc_loss: 0.025051478296518326, tv_loss: 0.033547114580869675\n",
      "iteration 2731, dc_loss: 0.025063462555408478, tv_loss: 0.03353939577937126\n",
      "iteration 2732, dc_loss: 0.02503308095037937, tv_loss: 0.03356693685054779\n",
      "iteration 2733, dc_loss: 0.02505459263920784, tv_loss: 0.03353997692465782\n",
      "iteration 2734, dc_loss: 0.025050215423107147, tv_loss: 0.03353991359472275\n",
      "iteration 2735, dc_loss: 0.025027593597769737, tv_loss: 0.033553313463926315\n",
      "iteration 2736, dc_loss: 0.02503482811152935, tv_loss: 0.03353692591190338\n",
      "iteration 2737, dc_loss: 0.025034872815012932, tv_loss: 0.033536821603775024\n",
      "iteration 2738, dc_loss: 0.025028588250279427, tv_loss: 0.03353483974933624\n",
      "iteration 2739, dc_loss: 0.025017311796545982, tv_loss: 0.03354450687766075\n",
      "iteration 2740, dc_loss: 0.025036901235580444, tv_loss: 0.03351709991693497\n",
      "iteration 2741, dc_loss: 0.025011712685227394, tv_loss: 0.03353993222117424\n",
      "iteration 2742, dc_loss: 0.02501589059829712, tv_loss: 0.03353957459330559\n",
      "iteration 2743, dc_loss: 0.025007350370287895, tv_loss: 0.03355851024389267\n",
      "iteration 2744, dc_loss: 0.024998147040605545, tv_loss: 0.03356758877635002\n",
      "iteration 2745, dc_loss: 0.02501041628420353, tv_loss: 0.03353683277964592\n",
      "iteration 2746, dc_loss: 0.024997450411319733, tv_loss: 0.033541448414325714\n",
      "iteration 2747, dc_loss: 0.025008514523506165, tv_loss: 0.03353158012032509\n",
      "iteration 2748, dc_loss: 0.024987323209643364, tv_loss: 0.033542342483997345\n",
      "iteration 2749, dc_loss: 0.024991123005747795, tv_loss: 0.03353426977992058\n",
      "iteration 2750, dc_loss: 0.02498217672109604, tv_loss: 0.03353698179125786\n",
      "iteration 2751, dc_loss: 0.02498854324221611, tv_loss: 0.033526454120874405\n",
      "iteration 2752, dc_loss: 0.024976084008812904, tv_loss: 0.03354036062955856\n",
      "iteration 2753, dc_loss: 0.024979839101433754, tv_loss: 0.03353216499090195\n",
      "iteration 2754, dc_loss: 0.024971099570393562, tv_loss: 0.033546630293130875\n",
      "iteration 2755, dc_loss: 0.024975040927529335, tv_loss: 0.03355433791875839\n",
      "iteration 2756, dc_loss: 0.024962082505226135, tv_loss: 0.03356577828526497\n",
      "iteration 2757, dc_loss: 0.024968884885311127, tv_loss: 0.03353719785809517\n",
      "iteration 2758, dc_loss: 0.02495688572525978, tv_loss: 0.03355059027671814\n",
      "iteration 2759, dc_loss: 0.024968795478343964, tv_loss: 0.03352678194642067\n",
      "iteration 2760, dc_loss: 0.024955151602625847, tv_loss: 0.03354300931096077\n",
      "iteration 2761, dc_loss: 0.02496037445962429, tv_loss: 0.033532943576574326\n",
      "iteration 2762, dc_loss: 0.024947689846158028, tv_loss: 0.033542875200510025\n",
      "iteration 2763, dc_loss: 0.02496117353439331, tv_loss: 0.033526461571455\n",
      "iteration 2764, dc_loss: 0.024936307221651077, tv_loss: 0.0335579477250576\n",
      "iteration 2765, dc_loss: 0.024971771985292435, tv_loss: 0.033519402146339417\n",
      "iteration 2766, dc_loss: 0.024938397109508514, tv_loss: 0.03356151655316353\n",
      "iteration 2767, dc_loss: 0.02497265301644802, tv_loss: 0.03353393077850342\n",
      "iteration 2768, dc_loss: 0.024943824857473373, tv_loss: 0.033568836748600006\n",
      "iteration 2769, dc_loss: 0.02496807463467121, tv_loss: 0.033532045781612396\n",
      "iteration 2770, dc_loss: 0.02493198588490486, tv_loss: 0.0335669070482254\n",
      "iteration 2771, dc_loss: 0.024973241612315178, tv_loss: 0.03351327404379845\n",
      "iteration 2772, dc_loss: 0.024914689362049103, tv_loss: 0.033566661179065704\n",
      "iteration 2773, dc_loss: 0.02495354600250721, tv_loss: 0.033521078526973724\n",
      "iteration 2774, dc_loss: 0.024899885058403015, tv_loss: 0.03356550261378288\n",
      "iteration 2775, dc_loss: 0.024935346096754074, tv_loss: 0.033524394035339355\n",
      "iteration 2776, dc_loss: 0.024885376915335655, tv_loss: 0.03356429561972618\n",
      "iteration 2777, dc_loss: 0.024916257709264755, tv_loss: 0.03353409469127655\n",
      "iteration 2778, dc_loss: 0.024904249235987663, tv_loss: 0.03353644162416458\n",
      "iteration 2779, dc_loss: 0.02488991618156433, tv_loss: 0.03354034945368767\n",
      "iteration 2780, dc_loss: 0.024896837770938873, tv_loss: 0.033529993146657944\n",
      "iteration 2781, dc_loss: 0.024883519858121872, tv_loss: 0.03354470059275627\n",
      "iteration 2782, dc_loss: 0.02488935925066471, tv_loss: 0.03354373201727867\n",
      "iteration 2783, dc_loss: 0.02488010935485363, tv_loss: 0.033563800156116486\n",
      "iteration 2784, dc_loss: 0.024881748482584953, tv_loss: 0.03355251997709274\n",
      "iteration 2785, dc_loss: 0.024865420535206795, tv_loss: 0.03355368226766586\n",
      "iteration 2786, dc_loss: 0.024872373789548874, tv_loss: 0.03354579955339432\n",
      "iteration 2787, dc_loss: 0.024862593039870262, tv_loss: 0.033559802919626236\n",
      "iteration 2788, dc_loss: 0.024875063449144363, tv_loss: 0.03354194387793541\n",
      "iteration 2789, dc_loss: 0.024852417409420013, tv_loss: 0.0335586853325367\n",
      "iteration 2790, dc_loss: 0.024866372346878052, tv_loss: 0.03353433683514595\n",
      "iteration 2791, dc_loss: 0.024850869551301003, tv_loss: 0.03354354202747345\n",
      "iteration 2792, dc_loss: 0.024846354499459267, tv_loss: 0.03354484215378761\n",
      "iteration 2793, dc_loss: 0.024851711466908455, tv_loss: 0.03353043273091316\n",
      "iteration 2794, dc_loss: 0.024837225675582886, tv_loss: 0.033541131764650345\n",
      "iteration 2795, dc_loss: 0.024846184998750687, tv_loss: 0.03353176638484001\n",
      "iteration 2796, dc_loss: 0.024835366755723953, tv_loss: 0.03354025259613991\n",
      "iteration 2797, dc_loss: 0.024837905541062355, tv_loss: 0.0335511788725853\n",
      "iteration 2798, dc_loss: 0.024830663576722145, tv_loss: 0.03356045484542847\n",
      "iteration 2799, dc_loss: 0.024823619052767754, tv_loss: 0.03355928510427475\n",
      "iteration 2800, dc_loss: 0.024823466315865517, tv_loss: 0.033538151532411575\n",
      "iteration 2801, dc_loss: 0.024825667962431908, tv_loss: 0.03353995084762573\n",
      "iteration 2802, dc_loss: 0.02481626160442829, tv_loss: 0.033540546894073486\n",
      "iteration 2803, dc_loss: 0.024818871170282364, tv_loss: 0.03353036195039749\n",
      "iteration 2804, dc_loss: 0.024810828268527985, tv_loss: 0.03353413566946983\n",
      "iteration 2805, dc_loss: 0.024801116436719894, tv_loss: 0.033542610704898834\n",
      "iteration 2806, dc_loss: 0.024813709780573845, tv_loss: 0.0335254929959774\n",
      "iteration 2807, dc_loss: 0.0248066745698452, tv_loss: 0.033535297960042953\n",
      "iteration 2808, dc_loss: 0.024790050461888313, tv_loss: 0.03354129567742348\n",
      "iteration 2809, dc_loss: 0.024807052686810493, tv_loss: 0.03352292627096176\n",
      "iteration 2810, dc_loss: 0.024809157475829124, tv_loss: 0.03352123498916626\n",
      "iteration 2811, dc_loss: 0.024787483736872673, tv_loss: 0.03354240208864212\n",
      "iteration 2812, dc_loss: 0.02478909119963646, tv_loss: 0.03354041278362274\n",
      "iteration 2813, dc_loss: 0.02479381486773491, tv_loss: 0.03352483734488487\n",
      "iteration 2814, dc_loss: 0.02478010021150112, tv_loss: 0.03353237733244896\n",
      "iteration 2815, dc_loss: 0.024781381711363792, tv_loss: 0.033533692359924316\n",
      "iteration 2816, dc_loss: 0.024786800146102905, tv_loss: 0.03353465348482132\n",
      "iteration 2817, dc_loss: 0.024777773767709732, tv_loss: 0.03354327753186226\n",
      "iteration 2818, dc_loss: 0.024775618687272072, tv_loss: 0.03353515639901161\n",
      "iteration 2819, dc_loss: 0.02477259933948517, tv_loss: 0.03353080153465271\n",
      "iteration 2820, dc_loss: 0.024770183488726616, tv_loss: 0.03352821618318558\n",
      "iteration 2821, dc_loss: 0.024767450988292694, tv_loss: 0.03353343531489372\n",
      "iteration 2822, dc_loss: 0.024764394387602806, tv_loss: 0.03353890776634216\n",
      "iteration 2823, dc_loss: 0.02476799488067627, tv_loss: 0.0335308276116848\n",
      "iteration 2824, dc_loss: 0.02476116642355919, tv_loss: 0.033531010150909424\n",
      "iteration 2825, dc_loss: 0.024755509570240974, tv_loss: 0.033527810126543045\n",
      "iteration 2826, dc_loss: 0.024749087169766426, tv_loss: 0.03353209048509598\n",
      "iteration 2827, dc_loss: 0.024754781275987625, tv_loss: 0.03352472186088562\n",
      "iteration 2828, dc_loss: 0.024757923558354378, tv_loss: 0.03352532163262367\n",
      "iteration 2829, dc_loss: 0.024745378643274307, tv_loss: 0.033549025654792786\n",
      "iteration 2830, dc_loss: 0.024743743240833282, tv_loss: 0.03354152664542198\n",
      "iteration 2831, dc_loss: 0.024741314351558685, tv_loss: 0.03352950140833855\n",
      "iteration 2832, dc_loss: 0.024739759042859077, tv_loss: 0.03352850303053856\n",
      "iteration 2833, dc_loss: 0.024740180000662804, tv_loss: 0.03353731334209442\n",
      "iteration 2834, dc_loss: 0.02473258785903454, tv_loss: 0.03355211764574051\n",
      "iteration 2835, dc_loss: 0.02473517134785652, tv_loss: 0.03353065997362137\n",
      "iteration 2836, dc_loss: 0.02472832426428795, tv_loss: 0.03352965787053108\n",
      "iteration 2837, dc_loss: 0.024727897718548775, tv_loss: 0.03352721780538559\n",
      "iteration 2838, dc_loss: 0.024723559617996216, tv_loss: 0.03354177251458168\n",
      "iteration 2839, dc_loss: 0.02471904642879963, tv_loss: 0.03354378789663315\n",
      "iteration 2840, dc_loss: 0.024725303053855896, tv_loss: 0.03352523595094681\n",
      "iteration 2841, dc_loss: 0.02471441589295864, tv_loss: 0.0335291288793087\n",
      "iteration 2842, dc_loss: 0.024713382124900818, tv_loss: 0.03352630138397217\n",
      "iteration 2843, dc_loss: 0.02471579797565937, tv_loss: 0.03352293744683266\n",
      "iteration 2844, dc_loss: 0.024706002324819565, tv_loss: 0.03353465721011162\n",
      "iteration 2845, dc_loss: 0.02470744214951992, tv_loss: 0.033534806221723557\n",
      "iteration 2846, dc_loss: 0.02470630407333374, tv_loss: 0.03353581950068474\n",
      "iteration 2847, dc_loss: 0.02470715343952179, tv_loss: 0.03353327512741089\n",
      "iteration 2848, dc_loss: 0.02469714917242527, tv_loss: 0.03354065492749214\n",
      "iteration 2849, dc_loss: 0.02469496987760067, tv_loss: 0.033533696085214615\n",
      "iteration 2850, dc_loss: 0.024694137275218964, tv_loss: 0.03353036195039749\n",
      "iteration 2851, dc_loss: 0.024692310020327568, tv_loss: 0.03352881222963333\n",
      "iteration 2852, dc_loss: 0.024695396423339844, tv_loss: 0.03352268785238266\n",
      "iteration 2853, dc_loss: 0.02468135952949524, tv_loss: 0.03353705257177353\n",
      "iteration 2854, dc_loss: 0.024679530411958694, tv_loss: 0.03353980556130409\n",
      "iteration 2855, dc_loss: 0.024687839671969414, tv_loss: 0.033531926572322845\n",
      "iteration 2856, dc_loss: 0.024684345349669456, tv_loss: 0.03353225067257881\n",
      "iteration 2857, dc_loss: 0.024671204388141632, tv_loss: 0.03353689983487129\n",
      "iteration 2858, dc_loss: 0.02467193454504013, tv_loss: 0.03352751582860947\n",
      "iteration 2859, dc_loss: 0.024675264954566956, tv_loss: 0.03352734446525574\n",
      "iteration 2860, dc_loss: 0.02466774918138981, tv_loss: 0.03352737054228783\n",
      "iteration 2861, dc_loss: 0.024668822064995766, tv_loss: 0.03352561220526695\n",
      "iteration 2862, dc_loss: 0.02466725930571556, tv_loss: 0.03352164849638939\n",
      "iteration 2863, dc_loss: 0.02466592937707901, tv_loss: 0.03351685032248497\n",
      "iteration 2864, dc_loss: 0.02465798147022724, tv_loss: 0.03352373465895653\n",
      "iteration 2865, dc_loss: 0.02465326525270939, tv_loss: 0.03352981433272362\n",
      "iteration 2866, dc_loss: 0.02465542033314705, tv_loss: 0.03352215513586998\n",
      "iteration 2867, dc_loss: 0.024651361629366875, tv_loss: 0.03352269157767296\n",
      "iteration 2868, dc_loss: 0.024650555104017258, tv_loss: 0.03352228179574013\n",
      "iteration 2869, dc_loss: 0.02464793622493744, tv_loss: 0.033525701612234116\n",
      "iteration 2870, dc_loss: 0.02464212104678154, tv_loss: 0.03353925794363022\n",
      "iteration 2871, dc_loss: 0.024644406512379646, tv_loss: 0.033540092408657074\n",
      "iteration 2872, dc_loss: 0.024639122188091278, tv_loss: 0.033539462834596634\n",
      "iteration 2873, dc_loss: 0.02463695779442787, tv_loss: 0.033526476472616196\n",
      "iteration 2874, dc_loss: 0.024638714268803596, tv_loss: 0.03352485969662666\n",
      "iteration 2875, dc_loss: 0.024632278829813004, tv_loss: 0.03353391960263252\n",
      "iteration 2876, dc_loss: 0.0246241707354784, tv_loss: 0.033544693142175674\n",
      "iteration 2877, dc_loss: 0.024623336270451546, tv_loss: 0.03354063630104065\n",
      "iteration 2878, dc_loss: 0.024633368477225304, tv_loss: 0.03351948782801628\n",
      "iteration 2879, dc_loss: 0.02462727390229702, tv_loss: 0.03352503851056099\n",
      "iteration 2880, dc_loss: 0.02461080066859722, tv_loss: 0.03353599086403847\n",
      "iteration 2881, dc_loss: 0.024619081988930702, tv_loss: 0.03352765366435051\n",
      "iteration 2882, dc_loss: 0.024616343900561333, tv_loss: 0.033533282577991486\n",
      "iteration 2883, dc_loss: 0.02460995502769947, tv_loss: 0.03354093059897423\n",
      "iteration 2884, dc_loss: 0.02461085468530655, tv_loss: 0.033533498644828796\n",
      "iteration 2885, dc_loss: 0.024618228897452354, tv_loss: 0.033518433570861816\n",
      "iteration 2886, dc_loss: 0.024601612240076065, tv_loss: 0.033533692359924316\n",
      "iteration 2887, dc_loss: 0.024595534428954124, tv_loss: 0.033532071858644485\n",
      "iteration 2888, dc_loss: 0.024603070691227913, tv_loss: 0.03352132812142372\n",
      "iteration 2889, dc_loss: 0.02459908276796341, tv_loss: 0.03352706506848335\n",
      "iteration 2890, dc_loss: 0.024597251787781715, tv_loss: 0.03352862969040871\n",
      "iteration 2891, dc_loss: 0.024596985429525375, tv_loss: 0.03353648632764816\n",
      "iteration 2892, dc_loss: 0.024593347683548927, tv_loss: 0.033537279814481735\n",
      "iteration 2893, dc_loss: 0.024581488221883774, tv_loss: 0.033543575555086136\n",
      "iteration 2894, dc_loss: 0.024582678452134132, tv_loss: 0.03353244438767433\n",
      "iteration 2895, dc_loss: 0.02458730898797512, tv_loss: 0.03352458029985428\n",
      "iteration 2896, dc_loss: 0.024585450068116188, tv_loss: 0.03352274000644684\n",
      "iteration 2897, dc_loss: 0.024574104696512222, tv_loss: 0.033532995730638504\n",
      "iteration 2898, dc_loss: 0.02457783930003643, tv_loss: 0.03353071212768555\n",
      "iteration 2899, dc_loss: 0.024578969925642014, tv_loss: 0.03354017063975334\n",
      "iteration 2900, dc_loss: 0.024569453671574593, tv_loss: 0.03354094550013542\n",
      "iteration 2901, dc_loss: 0.024559732526540756, tv_loss: 0.033539045602083206\n",
      "iteration 2902, dc_loss: 0.024575654417276382, tv_loss: 0.03352015092968941\n",
      "iteration 2903, dc_loss: 0.0245716143399477, tv_loss: 0.03352878615260124\n",
      "iteration 2904, dc_loss: 0.024549642577767372, tv_loss: 0.03355371952056885\n",
      "iteration 2905, dc_loss: 0.024556534364819527, tv_loss: 0.03353570029139519\n",
      "iteration 2906, dc_loss: 0.024564053863286972, tv_loss: 0.03352617099881172\n",
      "iteration 2907, dc_loss: 0.024555224925279617, tv_loss: 0.033530343323946\n",
      "iteration 2908, dc_loss: 0.02455243654549122, tv_loss: 0.03353855758905411\n",
      "iteration 2909, dc_loss: 0.024545861408114433, tv_loss: 0.03354253992438316\n",
      "iteration 2910, dc_loss: 0.02454676665365696, tv_loss: 0.033534176647663116\n",
      "iteration 2911, dc_loss: 0.024544591084122658, tv_loss: 0.03352825343608856\n",
      "iteration 2912, dc_loss: 0.024537742137908936, tv_loss: 0.03354257717728615\n",
      "iteration 2913, dc_loss: 0.02454257197678089, tv_loss: 0.0335405133664608\n",
      "iteration 2914, dc_loss: 0.024544158950448036, tv_loss: 0.03352748975157738\n",
      "iteration 2915, dc_loss: 0.02454478293657303, tv_loss: 0.03352287784218788\n",
      "iteration 2916, dc_loss: 0.0245206318795681, tv_loss: 0.0335402749478817\n",
      "iteration 2917, dc_loss: 0.024519864469766617, tv_loss: 0.03354232758283615\n",
      "iteration 2918, dc_loss: 0.02454458549618721, tv_loss: 0.03351888060569763\n",
      "iteration 2919, dc_loss: 0.024521516636013985, tv_loss: 0.03353823721408844\n",
      "iteration 2920, dc_loss: 0.024514751508831978, tv_loss: 0.03354283422231674\n",
      "iteration 2921, dc_loss: 0.024526486173272133, tv_loss: 0.03352253511548042\n",
      "iteration 2922, dc_loss: 0.024526631459593773, tv_loss: 0.033521201461553574\n",
      "iteration 2923, dc_loss: 0.024513132870197296, tv_loss: 0.03352564200758934\n",
      "iteration 2924, dc_loss: 0.02451058104634285, tv_loss: 0.03353112190961838\n",
      "iteration 2925, dc_loss: 0.024510033428668976, tv_loss: 0.033530913293361664\n",
      "iteration 2926, dc_loss: 0.024507444351911545, tv_loss: 0.03353913873434067\n",
      "iteration 2927, dc_loss: 0.024503527209162712, tv_loss: 0.033545371145009995\n",
      "iteration 2928, dc_loss: 0.02450789138674736, tv_loss: 0.03352886065840721\n",
      "iteration 2929, dc_loss: 0.02449995093047619, tv_loss: 0.03353402763605118\n",
      "iteration 2930, dc_loss: 0.024502146989107132, tv_loss: 0.03352304548025131\n",
      "iteration 2931, dc_loss: 0.024492062628269196, tv_loss: 0.033532340079545975\n",
      "iteration 2932, dc_loss: 0.02448919788002968, tv_loss: 0.033537089824676514\n",
      "iteration 2933, dc_loss: 0.024501046165823936, tv_loss: 0.03352094069123268\n",
      "iteration 2934, dc_loss: 0.02449011243879795, tv_loss: 0.0335417166352272\n",
      "iteration 2935, dc_loss: 0.024485578760504723, tv_loss: 0.03353653848171234\n",
      "iteration 2936, dc_loss: 0.024488799273967743, tv_loss: 0.03352910280227661\n",
      "iteration 2937, dc_loss: 0.024481700733304024, tv_loss: 0.033527638763189316\n",
      "iteration 2938, dc_loss: 0.024473505094647408, tv_loss: 0.033546701073646545\n",
      "iteration 2939, dc_loss: 0.024474019184708595, tv_loss: 0.033546026796102524\n",
      "iteration 2940, dc_loss: 0.02448311448097229, tv_loss: 0.03352835401892662\n",
      "iteration 2941, dc_loss: 0.024471819400787354, tv_loss: 0.033531565219163895\n",
      "iteration 2942, dc_loss: 0.024468878284096718, tv_loss: 0.03352953493595123\n",
      "iteration 2943, dc_loss: 0.024469979107379913, tv_loss: 0.03353719785809517\n",
      "iteration 2944, dc_loss: 0.024463357403874397, tv_loss: 0.0335402712225914\n",
      "iteration 2945, dc_loss: 0.02446298487484455, tv_loss: 0.03353404998779297\n",
      "iteration 2946, dc_loss: 0.024457361549139023, tv_loss: 0.03353092446923256\n",
      "iteration 2947, dc_loss: 0.02446010150015354, tv_loss: 0.03352672606706619\n",
      "iteration 2948, dc_loss: 0.024460159242153168, tv_loss: 0.03352085500955582\n",
      "iteration 2949, dc_loss: 0.024453677237033844, tv_loss: 0.033522479236125946\n",
      "iteration 2950, dc_loss: 0.02444811724126339, tv_loss: 0.033524755388498306\n",
      "iteration 2951, dc_loss: 0.024449089542031288, tv_loss: 0.03352634981274605\n",
      "iteration 2952, dc_loss: 0.02445019967854023, tv_loss: 0.03352325037121773\n",
      "iteration 2953, dc_loss: 0.024439344182610512, tv_loss: 0.03354748338460922\n",
      "iteration 2954, dc_loss: 0.024440739303827286, tv_loss: 0.03354349359869957\n",
      "iteration 2955, dc_loss: 0.024442831054329872, tv_loss: 0.033529266715049744\n",
      "iteration 2956, dc_loss: 0.024436982348561287, tv_loss: 0.03352617099881172\n",
      "iteration 2957, dc_loss: 0.024433886632323265, tv_loss: 0.03353121504187584\n",
      "iteration 2958, dc_loss: 0.0244317464530468, tv_loss: 0.033548176288604736\n",
      "iteration 2959, dc_loss: 0.024434372782707214, tv_loss: 0.03354218229651451\n",
      "iteration 2960, dc_loss: 0.02442321740090847, tv_loss: 0.03353424742817879\n",
      "iteration 2961, dc_loss: 0.02441972680389881, tv_loss: 0.03353071212768555\n",
      "iteration 2962, dc_loss: 0.024426210671663284, tv_loss: 0.03353060781955719\n",
      "iteration 2963, dc_loss: 0.024421248584985733, tv_loss: 0.033544667065143585\n",
      "iteration 2964, dc_loss: 0.024415193125605583, tv_loss: 0.03354281559586525\n",
      "iteration 2965, dc_loss: 0.024413639679551125, tv_loss: 0.033535003662109375\n",
      "iteration 2966, dc_loss: 0.02442193776369095, tv_loss: 0.03352425619959831\n",
      "iteration 2967, dc_loss: 0.02441377006471157, tv_loss: 0.03353840485215187\n",
      "iteration 2968, dc_loss: 0.024394892156124115, tv_loss: 0.03355506807565689\n",
      "iteration 2969, dc_loss: 0.024407222867012024, tv_loss: 0.03353142738342285\n",
      "iteration 2970, dc_loss: 0.024408426135778427, tv_loss: 0.03352675214409828\n",
      "iteration 2971, dc_loss: 0.024398453533649445, tv_loss: 0.033538818359375\n",
      "iteration 2972, dc_loss: 0.02440701425075531, tv_loss: 0.03352594003081322\n",
      "iteration 2973, dc_loss: 0.024397842586040497, tv_loss: 0.033532146364450455\n",
      "iteration 2974, dc_loss: 0.02439018338918686, tv_loss: 0.03353225812315941\n",
      "iteration 2975, dc_loss: 0.024388255551457405, tv_loss: 0.03352746739983559\n",
      "iteration 2976, dc_loss: 0.024389322847127914, tv_loss: 0.03353123739361763\n",
      "iteration 2977, dc_loss: 0.024391593411564827, tv_loss: 0.033531296998262405\n",
      "iteration 2978, dc_loss: 0.024386441335082054, tv_loss: 0.03353599086403847\n",
      "iteration 2979, dc_loss: 0.0243915356695652, tv_loss: 0.03352459892630577\n",
      "iteration 2980, dc_loss: 0.024377066642045975, tv_loss: 0.033532802015542984\n",
      "iteration 2981, dc_loss: 0.024373339489102364, tv_loss: 0.033535201102495193\n",
      "iteration 2982, dc_loss: 0.024377377703785896, tv_loss: 0.033527299761772156\n",
      "iteration 2983, dc_loss: 0.024367809295654297, tv_loss: 0.033530786633491516\n",
      "iteration 2984, dc_loss: 0.024371588602662086, tv_loss: 0.033525001257658005\n",
      "iteration 2985, dc_loss: 0.024378444999456406, tv_loss: 0.03351269289851189\n",
      "iteration 2986, dc_loss: 0.024369576945900917, tv_loss: 0.03353133425116539\n",
      "iteration 2987, dc_loss: 0.024358827620744705, tv_loss: 0.03354620933532715\n",
      "iteration 2988, dc_loss: 0.024359706789255142, tv_loss: 0.03353923186659813\n",
      "iteration 2989, dc_loss: 0.024357862770557404, tv_loss: 0.03353440389037132\n",
      "iteration 2990, dc_loss: 0.02436029724776745, tv_loss: 0.03352152183651924\n",
      "iteration 2991, dc_loss: 0.024359792470932007, tv_loss: 0.03352576494216919\n",
      "iteration 2992, dc_loss: 0.024350149556994438, tv_loss: 0.03353347256779671\n",
      "iteration 2993, dc_loss: 0.024347709491848946, tv_loss: 0.0335361622273922\n",
      "iteration 2994, dc_loss: 0.024349868297576904, tv_loss: 0.03353380784392357\n",
      "iteration 2995, dc_loss: 0.024341048672795296, tv_loss: 0.03353549912571907\n",
      "iteration 2996, dc_loss: 0.024348793551325798, tv_loss: 0.03352035582065582\n",
      "iteration 2997, dc_loss: 0.024344662204384804, tv_loss: 0.03352493792772293\n",
      "iteration 2998, dc_loss: 0.024329792708158493, tv_loss: 0.03353935480117798\n",
      "iteration 2999, dc_loss: 0.024340126663446426, tv_loss: 0.033529072999954224\n",
      "iteration 3000, dc_loss: 0.024342482909560204, tv_loss: 0.033527810126543045\n",
      "iteration 3001, dc_loss: 0.02432708628475666, tv_loss: 0.033547256141901016\n",
      "iteration 3002, dc_loss: 0.024317415431141853, tv_loss: 0.03354630991816521\n",
      "iteration 3003, dc_loss: 0.024335471913218498, tv_loss: 0.03352005034685135\n",
      "iteration 3004, dc_loss: 0.024326469749212265, tv_loss: 0.033527445048093796\n",
      "iteration 3005, dc_loss: 0.0243221502751112, tv_loss: 0.03352782502770424\n",
      "iteration 3006, dc_loss: 0.02432551234960556, tv_loss: 0.033537011593580246\n",
      "iteration 3007, dc_loss: 0.02431659772992134, tv_loss: 0.03354398533701897\n",
      "iteration 3008, dc_loss: 0.024313362315297127, tv_loss: 0.033534761518239975\n",
      "iteration 3009, dc_loss: 0.02431437000632286, tv_loss: 0.0335269458591938\n",
      "iteration 3010, dc_loss: 0.024308912456035614, tv_loss: 0.03352655470371246\n",
      "iteration 3011, dc_loss: 0.024309249594807625, tv_loss: 0.03352334350347519\n",
      "iteration 3012, dc_loss: 0.02430785447359085, tv_loss: 0.03352726250886917\n",
      "iteration 3013, dc_loss: 0.024304861202836037, tv_loss: 0.03353326395153999\n",
      "iteration 3014, dc_loss: 0.024302583187818527, tv_loss: 0.03353974595665932\n",
      "iteration 3015, dc_loss: 0.02430451661348343, tv_loss: 0.033525947481393814\n",
      "iteration 3016, dc_loss: 0.024288151413202286, tv_loss: 0.03353983536362648\n",
      "iteration 3017, dc_loss: 0.024292588233947754, tv_loss: 0.03353463113307953\n",
      "iteration 3018, dc_loss: 0.024297267198562622, tv_loss: 0.03352325037121773\n",
      "iteration 3019, dc_loss: 0.024295825511217117, tv_loss: 0.0335373692214489\n",
      "iteration 3020, dc_loss: 0.02429274283349514, tv_loss: 0.03353022411465645\n",
      "iteration 3021, dc_loss: 0.024283649399876595, tv_loss: 0.033533237874507904\n",
      "iteration 3022, dc_loss: 0.02428312599658966, tv_loss: 0.03352789208292961\n",
      "iteration 3023, dc_loss: 0.02428310364484787, tv_loss: 0.03352732211351395\n",
      "iteration 3024, dc_loss: 0.02427082695066929, tv_loss: 0.0335402712225914\n",
      "iteration 3025, dc_loss: 0.024282658472657204, tv_loss: 0.03352558612823486\n",
      "iteration 3026, dc_loss: 0.0242849662899971, tv_loss: 0.03352741524577141\n",
      "iteration 3027, dc_loss: 0.024274900555610657, tv_loss: 0.03352952003479004\n",
      "iteration 3028, dc_loss: 0.024264715611934662, tv_loss: 0.03354313597083092\n",
      "iteration 3029, dc_loss: 0.024282336235046387, tv_loss: 0.03352026268839836\n",
      "iteration 3030, dc_loss: 0.02426377311348915, tv_loss: 0.033530548214912415\n",
      "iteration 3031, dc_loss: 0.024267148226499557, tv_loss: 0.03353029489517212\n",
      "iteration 3032, dc_loss: 0.024271514266729355, tv_loss: 0.033534616231918335\n",
      "iteration 3033, dc_loss: 0.02426636591553688, tv_loss: 0.033537011593580246\n",
      "iteration 3034, dc_loss: 0.024259934201836586, tv_loss: 0.03354330360889435\n",
      "iteration 3035, dc_loss: 0.0242624469101429, tv_loss: 0.033527571707963943\n",
      "iteration 3036, dc_loss: 0.024253083392977715, tv_loss: 0.033533595502376556\n",
      "iteration 3037, dc_loss: 0.0242511797696352, tv_loss: 0.03353426605463028\n",
      "iteration 3038, dc_loss: 0.024252505972981453, tv_loss: 0.033526282757520676\n",
      "iteration 3039, dc_loss: 0.024245111271739006, tv_loss: 0.03352785483002663\n",
      "iteration 3040, dc_loss: 0.024235086515545845, tv_loss: 0.03353403881192207\n",
      "iteration 3041, dc_loss: 0.024247806519269943, tv_loss: 0.0335184745490551\n",
      "iteration 3042, dc_loss: 0.024239853024482727, tv_loss: 0.033521924167871475\n",
      "iteration 3043, dc_loss: 0.024228522554039955, tv_loss: 0.03353407606482506\n",
      "iteration 3044, dc_loss: 0.024240510538220406, tv_loss: 0.033534642308950424\n",
      "iteration 3045, dc_loss: 0.024240953847765923, tv_loss: 0.033539336174726486\n",
      "iteration 3046, dc_loss: 0.02422039769589901, tv_loss: 0.03354834020137787\n",
      "iteration 3047, dc_loss: 0.024231769144535065, tv_loss: 0.03352395072579384\n",
      "iteration 3048, dc_loss: 0.02423039637506008, tv_loss: 0.03352528437972069\n",
      "iteration 3049, dc_loss: 0.02421545423567295, tv_loss: 0.033549170941114426\n",
      "iteration 3050, dc_loss: 0.024220841005444527, tv_loss: 0.033539339900016785\n",
      "iteration 3051, dc_loss: 0.02421353943645954, tv_loss: 0.03353486582636833\n",
      "iteration 3052, dc_loss: 0.02421349100768566, tv_loss: 0.03353257104754448\n",
      "iteration 3053, dc_loss: 0.024218160659074783, tv_loss: 0.03352400287985802\n",
      "iteration 3054, dc_loss: 0.02421039342880249, tv_loss: 0.033531103283166885\n",
      "iteration 3055, dc_loss: 0.0242097619920969, tv_loss: 0.03353489935398102\n",
      "iteration 3056, dc_loss: 0.024205001071095467, tv_loss: 0.03353885933756828\n",
      "iteration 3057, dc_loss: 0.024205900728702545, tv_loss: 0.03353209048509598\n",
      "iteration 3058, dc_loss: 0.024199681356549263, tv_loss: 0.03352728113532066\n",
      "iteration 3059, dc_loss: 0.024202857166528702, tv_loss: 0.03352344036102295\n",
      "iteration 3060, dc_loss: 0.02419470250606537, tv_loss: 0.033532921224832535\n",
      "iteration 3061, dc_loss: 0.0241925660520792, tv_loss: 0.033537235110998154\n",
      "iteration 3062, dc_loss: 0.024197056889533997, tv_loss: 0.033541321754455566\n",
      "iteration 3063, dc_loss: 0.02419089525938034, tv_loss: 0.03353668004274368\n",
      "iteration 3064, dc_loss: 0.024187356233596802, tv_loss: 0.033529117703437805\n",
      "iteration 3065, dc_loss: 0.024187130853533745, tv_loss: 0.03352876752614975\n",
      "iteration 3066, dc_loss: 0.024181002750992775, tv_loss: 0.03353894501924515\n",
      "iteration 3067, dc_loss: 0.02418125607073307, tv_loss: 0.033539559692144394\n",
      "iteration 3068, dc_loss: 0.024178745225071907, tv_loss: 0.03353181108832359\n",
      "iteration 3069, dc_loss: 0.024181243032217026, tv_loss: 0.033525146543979645\n",
      "iteration 3070, dc_loss: 0.024176646023988724, tv_loss: 0.033523257821798325\n",
      "iteration 3071, dc_loss: 0.024176355451345444, tv_loss: 0.033521462231874466\n",
      "iteration 3072, dc_loss: 0.02416977658867836, tv_loss: 0.03353770077228546\n",
      "iteration 3073, dc_loss: 0.024160360917448997, tv_loss: 0.03355349600315094\n",
      "iteration 3074, dc_loss: 0.024176638573408127, tv_loss: 0.03352553769946098\n",
      "iteration 3075, dc_loss: 0.02416599728167057, tv_loss: 0.0335262231528759\n",
      "iteration 3076, dc_loss: 0.024161234498023987, tv_loss: 0.03353198990225792\n",
      "iteration 3077, dc_loss: 0.024160781875252724, tv_loss: 0.03353419527411461\n",
      "iteration 3078, dc_loss: 0.024158990010619164, tv_loss: 0.033535514026880264\n",
      "iteration 3079, dc_loss: 0.024162251502275467, tv_loss: 0.03353337198495865\n",
      "iteration 3080, dc_loss: 0.024162476882338524, tv_loss: 0.033529169857501984\n",
      "iteration 3081, dc_loss: 0.024155093356966972, tv_loss: 0.033533137291669846\n",
      "iteration 3082, dc_loss: 0.02415984496474266, tv_loss: 0.0335286483168602\n",
      "iteration 3083, dc_loss: 0.02415887638926506, tv_loss: 0.03352931886911392\n",
      "iteration 3084, dc_loss: 0.02416849695146084, tv_loss: 0.03351776674389839\n",
      "iteration 3085, dc_loss: 0.024151677265763283, tv_loss: 0.033534660935401917\n",
      "iteration 3086, dc_loss: 0.024164263159036636, tv_loss: 0.03352228179574013\n",
      "iteration 3087, dc_loss: 0.024135321378707886, tv_loss: 0.03354917839169502\n",
      "iteration 3088, dc_loss: 0.024145178496837616, tv_loss: 0.0335320308804512\n",
      "iteration 3089, dc_loss: 0.024137718603014946, tv_loss: 0.03352666646242142\n",
      "iteration 3090, dc_loss: 0.024130256846547127, tv_loss: 0.033528681844472885\n",
      "iteration 3091, dc_loss: 0.024125130847096443, tv_loss: 0.03353165462613106\n",
      "iteration 3092, dc_loss: 0.02413380891084671, tv_loss: 0.03352522477507591\n",
      "iteration 3093, dc_loss: 0.024136824533343315, tv_loss: 0.03352367877960205\n",
      "iteration 3094, dc_loss: 0.024113668128848076, tv_loss: 0.03354312479496002\n",
      "iteration 3095, dc_loss: 0.02412794530391693, tv_loss: 0.03353011980652809\n",
      "iteration 3096, dc_loss: 0.02411738783121109, tv_loss: 0.03353438153862953\n",
      "iteration 3097, dc_loss: 0.024121444672346115, tv_loss: 0.033528562635183334\n",
      "iteration 3098, dc_loss: 0.024112362414598465, tv_loss: 0.03353235498070717\n",
      "iteration 3099, dc_loss: 0.02411505952477455, tv_loss: 0.03352690115571022\n",
      "iteration 3100, dc_loss: 0.0241114292293787, tv_loss: 0.033526819199323654\n",
      "iteration 3101, dc_loss: 0.02410372905433178, tv_loss: 0.03353274613618851\n",
      "iteration 3102, dc_loss: 0.024109244346618652, tv_loss: 0.03352893888950348\n",
      "iteration 3103, dc_loss: 0.024099700152873993, tv_loss: 0.03354451432824135\n",
      "iteration 3104, dc_loss: 0.02410716377198696, tv_loss: 0.03353071212768555\n",
      "iteration 3105, dc_loss: 0.024092573672533035, tv_loss: 0.03353769704699516\n",
      "iteration 3106, dc_loss: 0.024095816537737846, tv_loss: 0.0335269495844841\n",
      "iteration 3107, dc_loss: 0.024098997935652733, tv_loss: 0.03352078050374985\n",
      "iteration 3108, dc_loss: 0.024092091247439384, tv_loss: 0.033527158200740814\n",
      "iteration 3109, dc_loss: 0.024090910330414772, tv_loss: 0.03353143483400345\n",
      "iteration 3110, dc_loss: 0.02408159151673317, tv_loss: 0.03354180231690407\n",
      "iteration 3111, dc_loss: 0.024087395519018173, tv_loss: 0.03353521600365639\n",
      "iteration 3112, dc_loss: 0.024082079529762268, tv_loss: 0.03353281319141388\n",
      "iteration 3113, dc_loss: 0.02408011071383953, tv_loss: 0.03352561220526695\n",
      "iteration 3114, dc_loss: 0.02407710812985897, tv_loss: 0.03352827951312065\n",
      "iteration 3115, dc_loss: 0.024082260206341743, tv_loss: 0.03351936116814613\n",
      "iteration 3116, dc_loss: 0.02407403104007244, tv_loss: 0.03353269025683403\n",
      "iteration 3117, dc_loss: 0.024066396057605743, tv_loss: 0.033540572971105576\n",
      "iteration 3118, dc_loss: 0.024070316925644875, tv_loss: 0.03353589400649071\n",
      "iteration 3119, dc_loss: 0.024064749479293823, tv_loss: 0.03353193774819374\n",
      "iteration 3120, dc_loss: 0.024072999134659767, tv_loss: 0.03352188691496849\n",
      "iteration 3121, dc_loss: 0.024057231843471527, tv_loss: 0.033532433211803436\n",
      "iteration 3122, dc_loss: 0.024070249870419502, tv_loss: 0.03352026268839836\n",
      "iteration 3123, dc_loss: 0.02405550703406334, tv_loss: 0.033537592738866806\n",
      "iteration 3124, dc_loss: 0.024055160582065582, tv_loss: 0.03353981301188469\n",
      "iteration 3125, dc_loss: 0.024050232023000717, tv_loss: 0.03353724256157875\n",
      "iteration 3126, dc_loss: 0.024062572047114372, tv_loss: 0.0335213765501976\n",
      "iteration 3127, dc_loss: 0.024041548371315002, tv_loss: 0.03354246914386749\n",
      "iteration 3128, dc_loss: 0.024054504930973053, tv_loss: 0.03351873159408569\n",
      "iteration 3129, dc_loss: 0.024049172177910805, tv_loss: 0.03352916240692139\n",
      "iteration 3130, dc_loss: 0.024048589169979095, tv_loss: 0.03352758288383484\n",
      "iteration 3131, dc_loss: 0.02404073439538479, tv_loss: 0.033529132604599\n",
      "iteration 3132, dc_loss: 0.024041369557380676, tv_loss: 0.033523108810186386\n",
      "iteration 3133, dc_loss: 0.024031223729252815, tv_loss: 0.03353094682097435\n",
      "iteration 3134, dc_loss: 0.024049248546361923, tv_loss: 0.03351091220974922\n",
      "iteration 3135, dc_loss: 0.02403271198272705, tv_loss: 0.033520329743623734\n",
      "iteration 3136, dc_loss: 0.02403753623366356, tv_loss: 0.033516813069581985\n",
      "iteration 3137, dc_loss: 0.02402748353779316, tv_loss: 0.033522605895996094\n",
      "iteration 3138, dc_loss: 0.024035530164837837, tv_loss: 0.03351261466741562\n",
      "iteration 3139, dc_loss: 0.02401990257203579, tv_loss: 0.033537983894348145\n",
      "iteration 3140, dc_loss: 0.024035213515162468, tv_loss: 0.03354077786207199\n",
      "iteration 3141, dc_loss: 0.024012157693505287, tv_loss: 0.03355839475989342\n",
      "iteration 3142, dc_loss: 0.02401926927268505, tv_loss: 0.033524274826049805\n",
      "iteration 3143, dc_loss: 0.02400873973965645, tv_loss: 0.03353067860007286\n",
      "iteration 3144, dc_loss: 0.02401779219508171, tv_loss: 0.03354131057858467\n",
      "iteration 3145, dc_loss: 0.024012606590986252, tv_loss: 0.03354905545711517\n",
      "iteration 3146, dc_loss: 0.024008341133594513, tv_loss: 0.033530913293361664\n",
      "iteration 3147, dc_loss: 0.024009807035326958, tv_loss: 0.03353162109851837\n",
      "iteration 3148, dc_loss: 0.024001456797122955, tv_loss: 0.033555082976818085\n",
      "iteration 3149, dc_loss: 0.023999743163585663, tv_loss: 0.033548466861248016\n",
      "iteration 3150, dc_loss: 0.024012796580791473, tv_loss: 0.033520448952913284\n",
      "iteration 3151, dc_loss: 0.0240015871822834, tv_loss: 0.033533964306116104\n",
      "iteration 3152, dc_loss: 0.02399599179625511, tv_loss: 0.033558689057826996\n",
      "iteration 3153, dc_loss: 0.024009553715586662, tv_loss: 0.033536944538354874\n",
      "iteration 3154, dc_loss: 0.024009039625525475, tv_loss: 0.033528443425893784\n",
      "iteration 3155, dc_loss: 0.02399907074868679, tv_loss: 0.03355322778224945\n",
      "iteration 3156, dc_loss: 0.02403264492750168, tv_loss: 0.033536966890096664\n",
      "iteration 3157, dc_loss: 0.024012157693505287, tv_loss: 0.033547308295965195\n",
      "iteration 3158, dc_loss: 0.024025406688451767, tv_loss: 0.0335271991789341\n",
      "iteration 3159, dc_loss: 0.02400815859436989, tv_loss: 0.033547256141901016\n",
      "iteration 3160, dc_loss: 0.02401365339756012, tv_loss: 0.03352821245789528\n",
      "iteration 3161, dc_loss: 0.02397322468459606, tv_loss: 0.033549755811691284\n",
      "iteration 3162, dc_loss: 0.023989349603652954, tv_loss: 0.03351961821317673\n",
      "iteration 3163, dc_loss: 0.02397102117538452, tv_loss: 0.0335322767496109\n",
      "iteration 3164, dc_loss: 0.02397862821817398, tv_loss: 0.033531710505485535\n",
      "iteration 3165, dc_loss: 0.023982038721442223, tv_loss: 0.03352788835763931\n",
      "iteration 3166, dc_loss: 0.02397461235523224, tv_loss: 0.03353239595890045\n",
      "iteration 3167, dc_loss: 0.023988384753465652, tv_loss: 0.03351166471838951\n",
      "iteration 3168, dc_loss: 0.023953909054398537, tv_loss: 0.033537764102220535\n",
      "iteration 3169, dc_loss: 0.023965606465935707, tv_loss: 0.033518098294734955\n",
      "iteration 3170, dc_loss: 0.023959774523973465, tv_loss: 0.03352077677845955\n",
      "iteration 3171, dc_loss: 0.023956527933478355, tv_loss: 0.03352898359298706\n",
      "iteration 3172, dc_loss: 0.02396395616233349, tv_loss: 0.033537693321704865\n",
      "iteration 3173, dc_loss: 0.023940613493323326, tv_loss: 0.03355536237359047\n",
      "iteration 3174, dc_loss: 0.023960920050740242, tv_loss: 0.03352439031004906\n",
      "iteration 3175, dc_loss: 0.02395327389240265, tv_loss: 0.033522095531225204\n",
      "iteration 3176, dc_loss: 0.023944934830069542, tv_loss: 0.03352817893028259\n",
      "iteration 3177, dc_loss: 0.023944107815623283, tv_loss: 0.03352484479546547\n",
      "iteration 3178, dc_loss: 0.023945240303874016, tv_loss: 0.03352859988808632\n",
      "iteration 3179, dc_loss: 0.023946713656187057, tv_loss: 0.033529169857501984\n",
      "iteration 3180, dc_loss: 0.02392110601067543, tv_loss: 0.03354489430785179\n",
      "iteration 3181, dc_loss: 0.023941755294799805, tv_loss: 0.0335254967212677\n",
      "iteration 3182, dc_loss: 0.023937324061989784, tv_loss: 0.033523816615343094\n",
      "iteration 3183, dc_loss: 0.02392786182463169, tv_loss: 0.033532992005348206\n",
      "iteration 3184, dc_loss: 0.023932715877890587, tv_loss: 0.033528830856084824\n",
      "iteration 3185, dc_loss: 0.02393031306564808, tv_loss: 0.03353573754429817\n",
      "iteration 3186, dc_loss: 0.023923393338918686, tv_loss: 0.033539291471242905\n",
      "iteration 3187, dc_loss: 0.02391832508146763, tv_loss: 0.03353152796626091\n",
      "iteration 3188, dc_loss: 0.02393641695380211, tv_loss: 0.03351200371980667\n",
      "iteration 3189, dc_loss: 0.023902179673314095, tv_loss: 0.033546674996614456\n",
      "iteration 3190, dc_loss: 0.023922165855765343, tv_loss: 0.03351704776287079\n",
      "iteration 3191, dc_loss: 0.023917969316244125, tv_loss: 0.03352685272693634\n",
      "iteration 3192, dc_loss: 0.02391021139919758, tv_loss: 0.03353836014866829\n",
      "iteration 3193, dc_loss: 0.02390342950820923, tv_loss: 0.033541373908519745\n",
      "iteration 3194, dc_loss: 0.02391628921031952, tv_loss: 0.03352414816617966\n",
      "iteration 3195, dc_loss: 0.02390669286251068, tv_loss: 0.033528801053762436\n",
      "iteration 3196, dc_loss: 0.023905465379357338, tv_loss: 0.03352213650941849\n",
      "iteration 3197, dc_loss: 0.023906780406832695, tv_loss: 0.03352423384785652\n",
      "iteration 3198, dc_loss: 0.023893844336271286, tv_loss: 0.03352813050150871\n",
      "iteration 3199, dc_loss: 0.023895058780908585, tv_loss: 0.03353250026702881\n",
      "iteration 3200, dc_loss: 0.0239058006554842, tv_loss: 0.03351860120892525\n",
      "iteration 3201, dc_loss: 0.023882608860731125, tv_loss: 0.033542387187480927\n",
      "iteration 3202, dc_loss: 0.02389375865459442, tv_loss: 0.033520158380270004\n",
      "iteration 3203, dc_loss: 0.023899327963590622, tv_loss: 0.03350820392370224\n",
      "iteration 3204, dc_loss: 0.023886242881417274, tv_loss: 0.033520475029945374\n",
      "iteration 3205, dc_loss: 0.023879481479525566, tv_loss: 0.03352833911776543\n",
      "iteration 3206, dc_loss: 0.023881657049059868, tv_loss: 0.03351728990674019\n",
      "iteration 3207, dc_loss: 0.023884935304522514, tv_loss: 0.033515650779008865\n",
      "iteration 3208, dc_loss: 0.02388393133878708, tv_loss: 0.033512189984321594\n",
      "iteration 3209, dc_loss: 0.023874936625361443, tv_loss: 0.03352177143096924\n",
      "iteration 3210, dc_loss: 0.023877888917922974, tv_loss: 0.033515799790620804\n",
      "iteration 3211, dc_loss: 0.023878997191786766, tv_loss: 0.03351646661758423\n",
      "iteration 3212, dc_loss: 0.0238688625395298, tv_loss: 0.03352661058306694\n",
      "iteration 3213, dc_loss: 0.023870479315519333, tv_loss: 0.033526599407196045\n",
      "iteration 3214, dc_loss: 0.023878533393144608, tv_loss: 0.03351707011461258\n",
      "iteration 3215, dc_loss: 0.023869113996624947, tv_loss: 0.033518992364406586\n",
      "iteration 3216, dc_loss: 0.02386346086859703, tv_loss: 0.03352172672748566\n",
      "iteration 3217, dc_loss: 0.023860344663262367, tv_loss: 0.03352260962128639\n",
      "iteration 3218, dc_loss: 0.023865023627877235, tv_loss: 0.033517587929964066\n",
      "iteration 3219, dc_loss: 0.023866649717092514, tv_loss: 0.03351818397641182\n",
      "iteration 3220, dc_loss: 0.023857062682509422, tv_loss: 0.03352685272693634\n",
      "iteration 3221, dc_loss: 0.023856081068515778, tv_loss: 0.03352705016732216\n",
      "iteration 3222, dc_loss: 0.023857783526182175, tv_loss: 0.03351599723100662\n",
      "iteration 3223, dc_loss: 0.02385466918349266, tv_loss: 0.03351585566997528\n",
      "iteration 3224, dc_loss: 0.023854004219174385, tv_loss: 0.033514320850372314\n",
      "iteration 3225, dc_loss: 0.023853229358792305, tv_loss: 0.03351159021258354\n",
      "iteration 3226, dc_loss: 0.023850228637456894, tv_loss: 0.03351490572094917\n",
      "iteration 3227, dc_loss: 0.0238481592386961, tv_loss: 0.033526066690683365\n",
      "iteration 3228, dc_loss: 0.023845475167036057, tv_loss: 0.033531807363033295\n",
      "iteration 3229, dc_loss: 0.02384110540151596, tv_loss: 0.033528175204992294\n",
      "iteration 3230, dc_loss: 0.023848840966820717, tv_loss: 0.03350911661982536\n",
      "iteration 3231, dc_loss: 0.023842688649892807, tv_loss: 0.03351305052638054\n",
      "iteration 3232, dc_loss: 0.023835569620132446, tv_loss: 0.03352130949497223\n",
      "iteration 3233, dc_loss: 0.023837221786379814, tv_loss: 0.03352073207497597\n",
      "iteration 3234, dc_loss: 0.023839371278882027, tv_loss: 0.033522892743349075\n",
      "iteration 3235, dc_loss: 0.023838765919208527, tv_loss: 0.03352677449584007\n",
      "iteration 3236, dc_loss: 0.023827549070119858, tv_loss: 0.033528588712215424\n",
      "iteration 3237, dc_loss: 0.02383195050060749, tv_loss: 0.03351621702313423\n",
      "iteration 3238, dc_loss: 0.02383829839527607, tv_loss: 0.03350953757762909\n",
      "iteration 3239, dc_loss: 0.023821713402867317, tv_loss: 0.033525947481393814\n",
      "iteration 3240, dc_loss: 0.023823207244277, tv_loss: 0.0335187204182148\n",
      "iteration 3241, dc_loss: 0.02382936142385006, tv_loss: 0.033510543406009674\n",
      "iteration 3242, dc_loss: 0.023824533447623253, tv_loss: 0.033511918038129807\n",
      "iteration 3243, dc_loss: 0.023817012086510658, tv_loss: 0.033524323254823685\n",
      "iteration 3244, dc_loss: 0.023817921057343483, tv_loss: 0.033529132604599\n",
      "iteration 3245, dc_loss: 0.02382510155439377, tv_loss: 0.0335247740149498\n",
      "iteration 3246, dc_loss: 0.023819968104362488, tv_loss: 0.033515896648168564\n",
      "iteration 3247, dc_loss: 0.023812759667634964, tv_loss: 0.033519845455884933\n",
      "iteration 3248, dc_loss: 0.023809541016817093, tv_loss: 0.03352893888950348\n",
      "iteration 3249, dc_loss: 0.02380860224366188, tv_loss: 0.03352685272693634\n",
      "iteration 3250, dc_loss: 0.023816555738449097, tv_loss: 0.03351645544171333\n",
      "iteration 3251, dc_loss: 0.02380559779703617, tv_loss: 0.03352384641766548\n",
      "iteration 3252, dc_loss: 0.02380126342177391, tv_loss: 0.03354005143046379\n",
      "iteration 3253, dc_loss: 0.023811502382159233, tv_loss: 0.03352099657058716\n",
      "iteration 3254, dc_loss: 0.023803355172276497, tv_loss: 0.033517707139253616\n",
      "iteration 3255, dc_loss: 0.02380061335861683, tv_loss: 0.03352900221943855\n",
      "iteration 3256, dc_loss: 0.023802949115633965, tv_loss: 0.03353079408407211\n",
      "iteration 3257, dc_loss: 0.023793445900082588, tv_loss: 0.033527810126543045\n",
      "iteration 3258, dc_loss: 0.02379857562482357, tv_loss: 0.033516623079776764\n",
      "iteration 3259, dc_loss: 0.023802723735570908, tv_loss: 0.033524662256240845\n",
      "iteration 3260, dc_loss: 0.023786721751093864, tv_loss: 0.033536672592163086\n",
      "iteration 3261, dc_loss: 0.023789387196302414, tv_loss: 0.03352811560034752\n",
      "iteration 3262, dc_loss: 0.023797601461410522, tv_loss: 0.03351516276597977\n",
      "iteration 3263, dc_loss: 0.023783626034855843, tv_loss: 0.03353642672300339\n",
      "iteration 3264, dc_loss: 0.023782921954989433, tv_loss: 0.03353404626250267\n",
      "iteration 3265, dc_loss: 0.023790795356035233, tv_loss: 0.033513061702251434\n",
      "iteration 3266, dc_loss: 0.023789217695593834, tv_loss: 0.03351428732275963\n",
      "iteration 3267, dc_loss: 0.023780900985002518, tv_loss: 0.03352924436330795\n",
      "iteration 3268, dc_loss: 0.023773837834596634, tv_loss: 0.03353000432252884\n",
      "iteration 3269, dc_loss: 0.023785589262843132, tv_loss: 0.03351626917719841\n",
      "iteration 3270, dc_loss: 0.023778894916176796, tv_loss: 0.033518023788928986\n",
      "iteration 3271, dc_loss: 0.023771772161126137, tv_loss: 0.03352431207895279\n",
      "iteration 3272, dc_loss: 0.023774191737174988, tv_loss: 0.03352753445506096\n",
      "iteration 3273, dc_loss: 0.023773150518536568, tv_loss: 0.033520203083753586\n",
      "iteration 3274, dc_loss: 0.023775704205036163, tv_loss: 0.03351204842329025\n",
      "iteration 3275, dc_loss: 0.023770367726683617, tv_loss: 0.03351660817861557\n",
      "iteration 3276, dc_loss: 0.02375948801636696, tv_loss: 0.03352564573287964\n",
      "iteration 3277, dc_loss: 0.02377087064087391, tv_loss: 0.03351425752043724\n",
      "iteration 3278, dc_loss: 0.023768559098243713, tv_loss: 0.03351587802171707\n",
      "iteration 3279, dc_loss: 0.023755550384521484, tv_loss: 0.03352896124124527\n",
      "iteration 3280, dc_loss: 0.023759419098496437, tv_loss: 0.03352274000644684\n",
      "iteration 3281, dc_loss: 0.02376309223473072, tv_loss: 0.033515360206365585\n",
      "iteration 3282, dc_loss: 0.023758379742503166, tv_loss: 0.0335153266787529\n",
      "iteration 3283, dc_loss: 0.023756759241223335, tv_loss: 0.03351561352610588\n",
      "iteration 3284, dc_loss: 0.023756511509418488, tv_loss: 0.03351422771811485\n",
      "iteration 3285, dc_loss: 0.023752344772219658, tv_loss: 0.03352334350347519\n",
      "iteration 3286, dc_loss: 0.023751281201839447, tv_loss: 0.03352691978216171\n",
      "iteration 3287, dc_loss: 0.02374902181327343, tv_loss: 0.03352392837405205\n",
      "iteration 3288, dc_loss: 0.02375037409365177, tv_loss: 0.033515334129333496\n",
      "iteration 3289, dc_loss: 0.023742208257317543, tv_loss: 0.033516574651002884\n",
      "iteration 3290, dc_loss: 0.0237417109310627, tv_loss: 0.033525191247463226\n",
      "iteration 3291, dc_loss: 0.023749208077788353, tv_loss: 0.03352276235818863\n",
      "iteration 3292, dc_loss: 0.02373850904405117, tv_loss: 0.033536527305841446\n",
      "iteration 3293, dc_loss: 0.023739362135529518, tv_loss: 0.03351963311433792\n",
      "iteration 3294, dc_loss: 0.023742370307445526, tv_loss: 0.033513396978378296\n",
      "iteration 3295, dc_loss: 0.023737194016575813, tv_loss: 0.03352593258023262\n",
      "iteration 3296, dc_loss: 0.023732395842671394, tv_loss: 0.03353078290820122\n",
      "iteration 3297, dc_loss: 0.023730242624878883, tv_loss: 0.03352884203195572\n",
      "iteration 3298, dc_loss: 0.023736394941806793, tv_loss: 0.03351440280675888\n",
      "iteration 3299, dc_loss: 0.02372940629720688, tv_loss: 0.03352750465273857\n",
      "iteration 3300, dc_loss: 0.023721851408481598, tv_loss: 0.03354682773351669\n",
      "iteration 3301, dc_loss: 0.02373659797012806, tv_loss: 0.03351510316133499\n",
      "iteration 3302, dc_loss: 0.02373180165886879, tv_loss: 0.033515043556690216\n",
      "iteration 3303, dc_loss: 0.023713266476988792, tv_loss: 0.03354229778051376\n",
      "iteration 3304, dc_loss: 0.02371985651552677, tv_loss: 0.033538758754730225\n",
      "iteration 3305, dc_loss: 0.023721998557448387, tv_loss: 0.033519990742206573\n",
      "iteration 3306, dc_loss: 0.023718731477856636, tv_loss: 0.033523865044116974\n",
      "iteration 3307, dc_loss: 0.02371925115585327, tv_loss: 0.033522024750709534\n",
      "iteration 3308, dc_loss: 0.02371319569647312, tv_loss: 0.03352684900164604\n",
      "iteration 3309, dc_loss: 0.02371555007994175, tv_loss: 0.033520691096782684\n",
      "iteration 3310, dc_loss: 0.0237171221524477, tv_loss: 0.033517178148031235\n",
      "iteration 3311, dc_loss: 0.02370942384004593, tv_loss: 0.033519092947244644\n",
      "iteration 3312, dc_loss: 0.023706920444965363, tv_loss: 0.03352853283286095\n",
      "iteration 3313, dc_loss: 0.0237099751830101, tv_loss: 0.03351892530918121\n",
      "iteration 3314, dc_loss: 0.023706478998064995, tv_loss: 0.03352092579007149\n",
      "iteration 3315, dc_loss: 0.023699548095464706, tv_loss: 0.03352471441030502\n",
      "iteration 3316, dc_loss: 0.023706402629613876, tv_loss: 0.03351128473877907\n",
      "iteration 3317, dc_loss: 0.023702900856733322, tv_loss: 0.033515624701976776\n",
      "iteration 3318, dc_loss: 0.023696457967162132, tv_loss: 0.03352580592036247\n",
      "iteration 3319, dc_loss: 0.023700356483459473, tv_loss: 0.033528052270412445\n",
      "iteration 3320, dc_loss: 0.023695165291428566, tv_loss: 0.03352309763431549\n",
      "iteration 3321, dc_loss: 0.023695001378655434, tv_loss: 0.03351762145757675\n",
      "iteration 3322, dc_loss: 0.02369818091392517, tv_loss: 0.03351501747965813\n",
      "iteration 3323, dc_loss: 0.023687124252319336, tv_loss: 0.03352432698011398\n",
      "iteration 3324, dc_loss: 0.023686667904257774, tv_loss: 0.03352169319987297\n",
      "iteration 3325, dc_loss: 0.0236912053078413, tv_loss: 0.03351849317550659\n",
      "iteration 3326, dc_loss: 0.0236882995814085, tv_loss: 0.033516280353069305\n",
      "iteration 3327, dc_loss: 0.023681454360485077, tv_loss: 0.033524490892887115\n",
      "iteration 3328, dc_loss: 0.023685792461037636, tv_loss: 0.033516474068164825\n",
      "iteration 3329, dc_loss: 0.023687148466706276, tv_loss: 0.03351246938109398\n",
      "iteration 3330, dc_loss: 0.023678362369537354, tv_loss: 0.03351780027151108\n",
      "iteration 3331, dc_loss: 0.02367318794131279, tv_loss: 0.033521514385938644\n",
      "iteration 3332, dc_loss: 0.02367752604186535, tv_loss: 0.03351633623242378\n",
      "iteration 3333, dc_loss: 0.02367635816335678, tv_loss: 0.033514708280563354\n",
      "iteration 3334, dc_loss: 0.023673422634601593, tv_loss: 0.03351990506052971\n",
      "iteration 3335, dc_loss: 0.023676613345742226, tv_loss: 0.03351671248674393\n",
      "iteration 3336, dc_loss: 0.023674044758081436, tv_loss: 0.03352276608347893\n",
      "iteration 3337, dc_loss: 0.023663757368922234, tv_loss: 0.03352995961904526\n",
      "iteration 3338, dc_loss: 0.02366444282233715, tv_loss: 0.03352418541908264\n",
      "iteration 3339, dc_loss: 0.023667672649025917, tv_loss: 0.03351620212197304\n",
      "iteration 3340, dc_loss: 0.023667311295866966, tv_loss: 0.033514004200696945\n",
      "iteration 3341, dc_loss: 0.023663276806473732, tv_loss: 0.03351481631398201\n",
      "iteration 3342, dc_loss: 0.023657897487282753, tv_loss: 0.03351786360144615\n",
      "iteration 3343, dc_loss: 0.023659326136112213, tv_loss: 0.03351648896932602\n",
      "iteration 3344, dc_loss: 0.0236569382250309, tv_loss: 0.033515918999910355\n",
      "iteration 3345, dc_loss: 0.023656761273741722, tv_loss: 0.033525481820106506\n",
      "iteration 3346, dc_loss: 0.023656656965613365, tv_loss: 0.033522870391607285\n",
      "iteration 3347, dc_loss: 0.023657822981476784, tv_loss: 0.03351597115397453\n",
      "iteration 3348, dc_loss: 0.023651931434869766, tv_loss: 0.03351452574133873\n",
      "iteration 3349, dc_loss: 0.023646419867873192, tv_loss: 0.0335213877260685\n",
      "iteration 3350, dc_loss: 0.023648932576179504, tv_loss: 0.03351646289229393\n",
      "iteration 3351, dc_loss: 0.02364496700465679, tv_loss: 0.03351961076259613\n",
      "iteration 3352, dc_loss: 0.0236480962485075, tv_loss: 0.03351573646068573\n",
      "iteration 3353, dc_loss: 0.02364201284945011, tv_loss: 0.03351717069745064\n",
      "iteration 3354, dc_loss: 0.023641157895326614, tv_loss: 0.033517271280288696\n",
      "iteration 3355, dc_loss: 0.023642271757125854, tv_loss: 0.033512718975543976\n",
      "iteration 3356, dc_loss: 0.023638198152184486, tv_loss: 0.03351658582687378\n",
      "iteration 3357, dc_loss: 0.023637909442186356, tv_loss: 0.03351966664195061\n",
      "iteration 3358, dc_loss: 0.023635905236005783, tv_loss: 0.03351815044879913\n",
      "iteration 3359, dc_loss: 0.023632396012544632, tv_loss: 0.03352102264761925\n",
      "iteration 3360, dc_loss: 0.02363094501197338, tv_loss: 0.033523328602313995\n",
      "iteration 3361, dc_loss: 0.023630205541849136, tv_loss: 0.03351845592260361\n",
      "iteration 3362, dc_loss: 0.023632418364286423, tv_loss: 0.0335148349404335\n",
      "iteration 3363, dc_loss: 0.023631621152162552, tv_loss: 0.033511143177747726\n",
      "iteration 3364, dc_loss: 0.023624546825885773, tv_loss: 0.033517025411129\n",
      "iteration 3365, dc_loss: 0.023622484877705574, tv_loss: 0.03352184593677521\n",
      "iteration 3366, dc_loss: 0.023621825501322746, tv_loss: 0.033518362790346146\n",
      "iteration 3367, dc_loss: 0.02362043410539627, tv_loss: 0.03351900354027748\n",
      "iteration 3368, dc_loss: 0.023623637855052948, tv_loss: 0.033511970192193985\n",
      "iteration 3369, dc_loss: 0.023620445281267166, tv_loss: 0.03351332247257233\n",
      "iteration 3370, dc_loss: 0.02361450158059597, tv_loss: 0.033518388867378235\n",
      "iteration 3371, dc_loss: 0.023612041026353836, tv_loss: 0.03351925313472748\n",
      "iteration 3372, dc_loss: 0.023613503202795982, tv_loss: 0.033519014716148376\n",
      "iteration 3373, dc_loss: 0.023613229393959045, tv_loss: 0.033517416566610336\n",
      "iteration 3374, dc_loss: 0.02361205965280533, tv_loss: 0.03351718932390213\n",
      "iteration 3375, dc_loss: 0.0236066747456789, tv_loss: 0.033519893884658813\n",
      "iteration 3376, dc_loss: 0.023605067282915115, tv_loss: 0.03351891040802002\n",
      "iteration 3377, dc_loss: 0.02360425889492035, tv_loss: 0.033519621938467026\n",
      "iteration 3378, dc_loss: 0.023606358096003532, tv_loss: 0.03351353481411934\n",
      "iteration 3379, dc_loss: 0.023603837937116623, tv_loss: 0.03351817652583122\n",
      "iteration 3380, dc_loss: 0.023600192740559578, tv_loss: 0.033517152070999146\n",
      "iteration 3381, dc_loss: 0.023600919172167778, tv_loss: 0.03351246938109398\n",
      "iteration 3382, dc_loss: 0.023598410189151764, tv_loss: 0.033519197255373\n",
      "iteration 3383, dc_loss: 0.023591943085193634, tv_loss: 0.03352702036499977\n",
      "iteration 3384, dc_loss: 0.023595210164785385, tv_loss: 0.033523157238960266\n",
      "iteration 3385, dc_loss: 0.023592988029122353, tv_loss: 0.03352339193224907\n",
      "iteration 3386, dc_loss: 0.023591775447130203, tv_loss: 0.033512987196445465\n",
      "iteration 3387, dc_loss: 0.023593636229634285, tv_loss: 0.03351306542754173\n",
      "iteration 3388, dc_loss: 0.02358824759721756, tv_loss: 0.03351771458983421\n",
      "iteration 3389, dc_loss: 0.023581519722938538, tv_loss: 0.03351898491382599\n",
      "iteration 3390, dc_loss: 0.023590130731463432, tv_loss: 0.0335095152258873\n",
      "iteration 3391, dc_loss: 0.023583559319376945, tv_loss: 0.033518221229314804\n",
      "iteration 3392, dc_loss: 0.023579804226756096, tv_loss: 0.03352503851056099\n",
      "iteration 3393, dc_loss: 0.023580171167850494, tv_loss: 0.03353077173233032\n",
      "iteration 3394, dc_loss: 0.023582665249705315, tv_loss: 0.03351708501577377\n",
      "iteration 3395, dc_loss: 0.02357112616300583, tv_loss: 0.033523187041282654\n",
      "iteration 3396, dc_loss: 0.023575598374009132, tv_loss: 0.03352129086852074\n",
      "iteration 3397, dc_loss: 0.02358218841254711, tv_loss: 0.033519960939884186\n",
      "iteration 3398, dc_loss: 0.023570546880364418, tv_loss: 0.03352334350347519\n",
      "iteration 3399, dc_loss: 0.02357041835784912, tv_loss: 0.033523350954055786\n",
      "iteration 3400, dc_loss: 0.02357202023267746, tv_loss: 0.03351835161447525\n",
      "iteration 3401, dc_loss: 0.023564085364341736, tv_loss: 0.03352535516023636\n",
      "iteration 3402, dc_loss: 0.0235699824988842, tv_loss: 0.03352195397019386\n",
      "iteration 3403, dc_loss: 0.023570120334625244, tv_loss: 0.03351828455924988\n",
      "iteration 3404, dc_loss: 0.023557079955935478, tv_loss: 0.03352518379688263\n",
      "iteration 3405, dc_loss: 0.023558422923088074, tv_loss: 0.03351902961730957\n",
      "iteration 3406, dc_loss: 0.023570546880364418, tv_loss: 0.03351335600018501\n",
      "iteration 3407, dc_loss: 0.02355116605758667, tv_loss: 0.0335237942636013\n",
      "iteration 3408, dc_loss: 0.023554831743240356, tv_loss: 0.03351851925253868\n",
      "iteration 3409, dc_loss: 0.023570319637656212, tv_loss: 0.03350625932216644\n",
      "iteration 3410, dc_loss: 0.023548120632767677, tv_loss: 0.033522412180900574\n",
      "iteration 3411, dc_loss: 0.02354445680975914, tv_loss: 0.03352586552500725\n",
      "iteration 3412, dc_loss: 0.023556793108582497, tv_loss: 0.03350941091775894\n",
      "iteration 3413, dc_loss: 0.023550162091851234, tv_loss: 0.03351668640971184\n",
      "iteration 3414, dc_loss: 0.023544959723949432, tv_loss: 0.03351685032248497\n",
      "iteration 3415, dc_loss: 0.023548582568764687, tv_loss: 0.033512748777866364\n",
      "iteration 3416, dc_loss: 0.023548057302832603, tv_loss: 0.033510711044073105\n",
      "iteration 3417, dc_loss: 0.023538703098893166, tv_loss: 0.033514898270368576\n",
      "iteration 3418, dc_loss: 0.023540882393717766, tv_loss: 0.03351884335279465\n",
      "iteration 3419, dc_loss: 0.02355010434985161, tv_loss: 0.033513836562633514\n",
      "iteration 3420, dc_loss: 0.02353321760892868, tv_loss: 0.033533867448568344\n",
      "iteration 3421, dc_loss: 0.023530621081590652, tv_loss: 0.03353520482778549\n",
      "iteration 3422, dc_loss: 0.023539165034890175, tv_loss: 0.03351469710469246\n",
      "iteration 3423, dc_loss: 0.023540275171399117, tv_loss: 0.03351077064871788\n",
      "iteration 3424, dc_loss: 0.02352546527981758, tv_loss: 0.03352579474449158\n",
      "iteration 3425, dc_loss: 0.023534713312983513, tv_loss: 0.033511683344841\n",
      "iteration 3426, dc_loss: 0.02353077381849289, tv_loss: 0.03351670131087303\n",
      "iteration 3427, dc_loss: 0.023521509021520615, tv_loss: 0.03353483974933624\n",
      "iteration 3428, dc_loss: 0.02353445626795292, tv_loss: 0.03351091966032982\n",
      "iteration 3429, dc_loss: 0.023522501811385155, tv_loss: 0.03351539000868797\n",
      "iteration 3430, dc_loss: 0.02351418510079384, tv_loss: 0.03352600336074829\n",
      "iteration 3431, dc_loss: 0.02352886088192463, tv_loss: 0.03351055458188057\n",
      "iteration 3432, dc_loss: 0.023518366739153862, tv_loss: 0.03351537138223648\n",
      "iteration 3433, dc_loss: 0.023516066372394562, tv_loss: 0.033522170037031174\n",
      "iteration 3434, dc_loss: 0.023526588454842567, tv_loss: 0.03350715711712837\n",
      "iteration 3435, dc_loss: 0.023513920605182648, tv_loss: 0.03351757302880287\n",
      "iteration 3436, dc_loss: 0.023505518212914467, tv_loss: 0.033522382378578186\n",
      "iteration 3437, dc_loss: 0.023515816777944565, tv_loss: 0.03351743519306183\n",
      "iteration 3438, dc_loss: 0.023508843034505844, tv_loss: 0.03351651504635811\n",
      "iteration 3439, dc_loss: 0.02351096272468567, tv_loss: 0.033509671688079834\n",
      "iteration 3440, dc_loss: 0.023509755730628967, tv_loss: 0.03351408988237381\n",
      "iteration 3441, dc_loss: 0.023501785472035408, tv_loss: 0.03352124243974686\n",
      "iteration 3442, dc_loss: 0.023510197177529335, tv_loss: 0.03350832313299179\n",
      "iteration 3443, dc_loss: 0.023500945419073105, tv_loss: 0.03351389989256859\n",
      "iteration 3444, dc_loss: 0.023494025692343712, tv_loss: 0.03352578729391098\n",
      "iteration 3445, dc_loss: 0.02350524067878723, tv_loss: 0.03351735323667526\n",
      "iteration 3446, dc_loss: 0.0235044714063406, tv_loss: 0.033520545810461044\n",
      "iteration 3447, dc_loss: 0.02349018305540085, tv_loss: 0.033531785011291504\n",
      "iteration 3448, dc_loss: 0.02349613420665264, tv_loss: 0.03351472690701485\n",
      "iteration 3449, dc_loss: 0.023500319570302963, tv_loss: 0.033510446548461914\n",
      "iteration 3450, dc_loss: 0.02348499745130539, tv_loss: 0.033527374267578125\n",
      "iteration 3451, dc_loss: 0.023491952568292618, tv_loss: 0.03351796045899391\n",
      "iteration 3452, dc_loss: 0.023488853126764297, tv_loss: 0.03351866081357002\n",
      "iteration 3453, dc_loss: 0.023483898490667343, tv_loss: 0.03352721408009529\n",
      "iteration 3454, dc_loss: 0.023491017520427704, tv_loss: 0.03351098671555519\n",
      "iteration 3455, dc_loss: 0.023487895727157593, tv_loss: 0.033512674272060394\n",
      "iteration 3456, dc_loss: 0.02348192222416401, tv_loss: 0.033525872975587845\n",
      "iteration 3457, dc_loss: 0.023479897528886795, tv_loss: 0.03353191912174225\n",
      "iteration 3458, dc_loss: 0.02348172292113304, tv_loss: 0.03352515399456024\n",
      "iteration 3459, dc_loss: 0.023471496999263763, tv_loss: 0.03352836146950722\n",
      "iteration 3460, dc_loss: 0.023481810465455055, tv_loss: 0.03351485729217529\n",
      "iteration 3461, dc_loss: 0.023480143398046494, tv_loss: 0.03352268412709236\n",
      "iteration 3462, dc_loss: 0.023465923964977264, tv_loss: 0.033536117523908615\n",
      "iteration 3463, dc_loss: 0.02347516641020775, tv_loss: 0.033516161143779755\n",
      "iteration 3464, dc_loss: 0.023478006944060326, tv_loss: 0.03350713104009628\n",
      "iteration 3465, dc_loss: 0.02346261590719223, tv_loss: 0.0335264727473259\n",
      "iteration 3466, dc_loss: 0.023469077423214912, tv_loss: 0.03352871909737587\n",
      "iteration 3467, dc_loss: 0.023465504869818687, tv_loss: 0.03353101760149002\n",
      "iteration 3468, dc_loss: 0.023462293669581413, tv_loss: 0.03352594003081322\n",
      "iteration 3469, dc_loss: 0.023465711623430252, tv_loss: 0.03351489081978798\n",
      "iteration 3470, dc_loss: 0.023463359102606773, tv_loss: 0.03352582827210426\n",
      "iteration 3471, dc_loss: 0.023454340174794197, tv_loss: 0.03353336080908775\n",
      "iteration 3472, dc_loss: 0.023457741364836693, tv_loss: 0.03351958841085434\n",
      "iteration 3473, dc_loss: 0.023465633392333984, tv_loss: 0.03351396322250366\n",
      "iteration 3474, dc_loss: 0.023454204201698303, tv_loss: 0.03351863473653793\n",
      "iteration 3475, dc_loss: 0.023452872410416603, tv_loss: 0.03351948782801628\n",
      "iteration 3476, dc_loss: 0.02345730923116207, tv_loss: 0.033514320850372314\n",
      "iteration 3477, dc_loss: 0.023442458361387253, tv_loss: 0.03352515771985054\n",
      "iteration 3478, dc_loss: 0.023447241634130478, tv_loss: 0.03351634368300438\n",
      "iteration 3479, dc_loss: 0.023460375145077705, tv_loss: 0.033501025289297104\n",
      "iteration 3480, dc_loss: 0.023440295830368996, tv_loss: 0.03351825103163719\n",
      "iteration 3481, dc_loss: 0.023442594334483147, tv_loss: 0.0335165299475193\n",
      "iteration 3482, dc_loss: 0.023454517126083374, tv_loss: 0.03350010886788368\n",
      "iteration 3483, dc_loss: 0.02344083972275257, tv_loss: 0.033510494977235794\n",
      "iteration 3484, dc_loss: 0.023428944870829582, tv_loss: 0.03352511301636696\n",
      "iteration 3485, dc_loss: 0.02344616688787937, tv_loss: 0.03351189196109772\n",
      "iteration 3486, dc_loss: 0.023439785465598106, tv_loss: 0.03353884071111679\n",
      "iteration 3487, dc_loss: 0.023434698581695557, tv_loss: 0.033533159643411636\n",
      "iteration 3488, dc_loss: 0.023437034338712692, tv_loss: 0.03351374343037605\n",
      "iteration 3489, dc_loss: 0.023438038304448128, tv_loss: 0.03352993354201317\n",
      "iteration 3490, dc_loss: 0.023426424711942673, tv_loss: 0.03354775905609131\n",
      "iteration 3491, dc_loss: 0.023422803729772568, tv_loss: 0.033529866486787796\n",
      "iteration 3492, dc_loss: 0.023431895300745964, tv_loss: 0.033538877964019775\n",
      "iteration 3493, dc_loss: 0.023433169350028038, tv_loss: 0.0335342176258564\n",
      "iteration 3494, dc_loss: 0.023421257734298706, tv_loss: 0.03352833539247513\n",
      "iteration 3495, dc_loss: 0.023420829325914383, tv_loss: 0.033539604395627975\n",
      "iteration 3496, dc_loss: 0.02342488057911396, tv_loss: 0.03353635594248772\n",
      "iteration 3497, dc_loss: 0.02341521717607975, tv_loss: 0.03353021666407585\n",
      "iteration 3498, dc_loss: 0.023424837738275528, tv_loss: 0.03352884575724602\n",
      "iteration 3499, dc_loss: 0.02342495135962963, tv_loss: 0.03353190794587135\n",
      "iteration 3500, dc_loss: 0.02340969629585743, tv_loss: 0.03353089839220047\n",
      "iteration 3501, dc_loss: 0.02341691218316555, tv_loss: 0.03352731093764305\n",
      "iteration 3502, dc_loss: 0.023425152525305748, tv_loss: 0.033520057797431946\n",
      "iteration 3503, dc_loss: 0.023401940241456032, tv_loss: 0.03353477269411087\n",
      "iteration 3504, dc_loss: 0.023405779153108597, tv_loss: 0.03352304548025131\n",
      "iteration 3505, dc_loss: 0.023417208343744278, tv_loss: 0.033516775816679\n",
      "iteration 3506, dc_loss: 0.023404225707054138, tv_loss: 0.033530332148075104\n",
      "iteration 3507, dc_loss: 0.023401662707328796, tv_loss: 0.03352699428796768\n",
      "iteration 3508, dc_loss: 0.023416154086589813, tv_loss: 0.033504363149404526\n",
      "iteration 3509, dc_loss: 0.023403897881507874, tv_loss: 0.0335170179605484\n",
      "iteration 3510, dc_loss: 0.02339913696050644, tv_loss: 0.03352140262722969\n",
      "iteration 3511, dc_loss: 0.023406755179166794, tv_loss: 0.03351486474275589\n",
      "iteration 3512, dc_loss: 0.023400789126753807, tv_loss: 0.0335245281457901\n",
      "iteration 3513, dc_loss: 0.02338382601737976, tv_loss: 0.033531785011291504\n",
      "iteration 3514, dc_loss: 0.02340497262775898, tv_loss: 0.033508773893117905\n",
      "iteration 3515, dc_loss: 0.02339821495115757, tv_loss: 0.03351069986820221\n",
      "iteration 3516, dc_loss: 0.023386286571621895, tv_loss: 0.033529069274663925\n",
      "iteration 3517, dc_loss: 0.023393496870994568, tv_loss: 0.03351924940943718\n",
      "iteration 3518, dc_loss: 0.023396799340844154, tv_loss: 0.03351668640971184\n",
      "iteration 3519, dc_loss: 0.02338765375316143, tv_loss: 0.033522721379995346\n",
      "iteration 3520, dc_loss: 0.02338036522269249, tv_loss: 0.033523593097925186\n",
      "iteration 3521, dc_loss: 0.023387692868709564, tv_loss: 0.03351227566599846\n",
      "iteration 3522, dc_loss: 0.02338448166847229, tv_loss: 0.033517394214868546\n",
      "iteration 3523, dc_loss: 0.02338416315615177, tv_loss: 0.03351350873708725\n",
      "iteration 3524, dc_loss: 0.023381691426038742, tv_loss: 0.03351801633834839\n",
      "iteration 3525, dc_loss: 0.02337121032178402, tv_loss: 0.033526334911584854\n",
      "iteration 3526, dc_loss: 0.023385848850011826, tv_loss: 0.03350921347737312\n",
      "iteration 3527, dc_loss: 0.023374876007437706, tv_loss: 0.03351837024092674\n",
      "iteration 3528, dc_loss: 0.02337525226175785, tv_loss: 0.03351082280278206\n",
      "iteration 3529, dc_loss: 0.023378824815154076, tv_loss: 0.03351160138845444\n",
      "iteration 3530, dc_loss: 0.023366494104266167, tv_loss: 0.03351818397641182\n",
      "iteration 3531, dc_loss: 0.02337116375565529, tv_loss: 0.03350848704576492\n",
      "iteration 3532, dc_loss: 0.02337578684091568, tv_loss: 0.03350485861301422\n",
      "iteration 3533, dc_loss: 0.02335890755057335, tv_loss: 0.03352167084813118\n",
      "iteration 3534, dc_loss: 0.023368969559669495, tv_loss: 0.033512264490127563\n",
      "iteration 3535, dc_loss: 0.023363333195447922, tv_loss: 0.03352141007781029\n",
      "iteration 3536, dc_loss: 0.023367593064904213, tv_loss: 0.033525414764881134\n",
      "iteration 3537, dc_loss: 0.023355994373559952, tv_loss: 0.03352758660912514\n",
      "iteration 3538, dc_loss: 0.0233658105134964, tv_loss: 0.033518679440021515\n",
      "iteration 3539, dc_loss: 0.02336161583662033, tv_loss: 0.03351574018597603\n",
      "iteration 3540, dc_loss: 0.02336062863469124, tv_loss: 0.03352905809879303\n",
      "iteration 3541, dc_loss: 0.023351235315203667, tv_loss: 0.03353307023644447\n",
      "iteration 3542, dc_loss: 0.023354291915893555, tv_loss: 0.033524058759212494\n",
      "iteration 3543, dc_loss: 0.023362543433904648, tv_loss: 0.03352278098464012\n",
      "iteration 3544, dc_loss: 0.023364214226603508, tv_loss: 0.03352687880396843\n",
      "iteration 3545, dc_loss: 0.023339150473475456, tv_loss: 0.033537913113832474\n",
      "iteration 3546, dc_loss: 0.023356182500720024, tv_loss: 0.03352271765470505\n",
      "iteration 3547, dc_loss: 0.023350253701210022, tv_loss: 0.03353375941514969\n",
      "iteration 3548, dc_loss: 0.023351842537522316, tv_loss: 0.03352303430438042\n",
      "iteration 3549, dc_loss: 0.023346539586782455, tv_loss: 0.033525943756103516\n",
      "iteration 3550, dc_loss: 0.02335156686604023, tv_loss: 0.033518508076667786\n",
      "iteration 3551, dc_loss: 0.023337554186582565, tv_loss: 0.0335330106317997\n",
      "iteration 3552, dc_loss: 0.023341001942753792, tv_loss: 0.0335177443921566\n",
      "iteration 3553, dc_loss: 0.023341167718172073, tv_loss: 0.03352135047316551\n",
      "iteration 3554, dc_loss: 0.023330776020884514, tv_loss: 0.03353118523955345\n",
      "iteration 3555, dc_loss: 0.023333987221121788, tv_loss: 0.0335322767496109\n",
      "iteration 3556, dc_loss: 0.023340795189142227, tv_loss: 0.03351040929555893\n",
      "iteration 3557, dc_loss: 0.02333141304552555, tv_loss: 0.033522892743349075\n",
      "iteration 3558, dc_loss: 0.023329993709921837, tv_loss: 0.0335257463157177\n",
      "iteration 3559, dc_loss: 0.0233295951038599, tv_loss: 0.03351878747344017\n",
      "iteration 3560, dc_loss: 0.023325949907302856, tv_loss: 0.03352665901184082\n",
      "iteration 3561, dc_loss: 0.023329976946115494, tv_loss: 0.03351251780986786\n",
      "iteration 3562, dc_loss: 0.02332364208996296, tv_loss: 0.0335211381316185\n",
      "iteration 3563, dc_loss: 0.0233265683054924, tv_loss: 0.03351554647088051\n",
      "iteration 3564, dc_loss: 0.0233134962618351, tv_loss: 0.033521659672260284\n",
      "iteration 3565, dc_loss: 0.023325985297560692, tv_loss: 0.0335136316716671\n",
      "iteration 3566, dc_loss: 0.023325176909565926, tv_loss: 0.03350640833377838\n",
      "iteration 3567, dc_loss: 0.023308245465159416, tv_loss: 0.03352327644824982\n",
      "iteration 3568, dc_loss: 0.02331714890897274, tv_loss: 0.03351911902427673\n",
      "iteration 3569, dc_loss: 0.02332390658557415, tv_loss: 0.033520448952913284\n",
      "iteration 3570, dc_loss: 0.023304004222154617, tv_loss: 0.03353329747915268\n",
      "iteration 3571, dc_loss: 0.02330826409161091, tv_loss: 0.03351890668272972\n",
      "iteration 3572, dc_loss: 0.023315316066145897, tv_loss: 0.03351190686225891\n",
      "iteration 3573, dc_loss: 0.023308757692575455, tv_loss: 0.03352416306734085\n",
      "iteration 3574, dc_loss: 0.02330748923122883, tv_loss: 0.03352481871843338\n",
      "iteration 3575, dc_loss: 0.0233078021556139, tv_loss: 0.03351801261305809\n",
      "iteration 3576, dc_loss: 0.02330085076391697, tv_loss: 0.03351769596338272\n",
      "iteration 3577, dc_loss: 0.02329578809440136, tv_loss: 0.03352459520101547\n",
      "iteration 3578, dc_loss: 0.023307165130972862, tv_loss: 0.03350747376680374\n",
      "iteration 3579, dc_loss: 0.023301392793655396, tv_loss: 0.03352358192205429\n",
      "iteration 3580, dc_loss: 0.023293396458029747, tv_loss: 0.033525656908750534\n",
      "iteration 3581, dc_loss: 0.023299530148506165, tv_loss: 0.03351457417011261\n",
      "iteration 3582, dc_loss: 0.02329450473189354, tv_loss: 0.033520184457302094\n",
      "iteration 3583, dc_loss: 0.023293526843190193, tv_loss: 0.03352559357881546\n",
      "iteration 3584, dc_loss: 0.02329462394118309, tv_loss: 0.033526461571455\n",
      "iteration 3585, dc_loss: 0.02328934520483017, tv_loss: 0.033519282937049866\n",
      "iteration 3586, dc_loss: 0.02328077331185341, tv_loss: 0.03352726995944977\n",
      "iteration 3587, dc_loss: 0.023294953629374504, tv_loss: 0.03351680561900139\n",
      "iteration 3588, dc_loss: 0.02329232543706894, tv_loss: 0.03352617099881172\n",
      "iteration 3589, dc_loss: 0.023281477391719818, tv_loss: 0.03352653980255127\n",
      "iteration 3590, dc_loss: 0.02327982895076275, tv_loss: 0.033523786813020706\n",
      "iteration 3591, dc_loss: 0.023287389427423477, tv_loss: 0.033522382378578186\n",
      "iteration 3592, dc_loss: 0.023281637579202652, tv_loss: 0.03352368623018265\n",
      "iteration 3593, dc_loss: 0.023273272439837456, tv_loss: 0.033527176827192307\n",
      "iteration 3594, dc_loss: 0.02328270487487316, tv_loss: 0.03351493924856186\n",
      "iteration 3595, dc_loss: 0.023287458345294, tv_loss: 0.03350919485092163\n",
      "iteration 3596, dc_loss: 0.023266244679689407, tv_loss: 0.033534783869981766\n",
      "iteration 3597, dc_loss: 0.023282775655388832, tv_loss: 0.03351667895913124\n",
      "iteration 3598, dc_loss: 0.023268451914191246, tv_loss: 0.03352300450205803\n",
      "iteration 3599, dc_loss: 0.023272009566426277, tv_loss: 0.03351781144738197\n",
      "iteration 3600, dc_loss: 0.023278269916772842, tv_loss: 0.03351397067308426\n",
      "iteration 3601, dc_loss: 0.0232823695987463, tv_loss: 0.03351566568017006\n",
      "iteration 3602, dc_loss: 0.02326417714357376, tv_loss: 0.03352212533354759\n",
      "iteration 3603, dc_loss: 0.0232655368745327, tv_loss: 0.03350977227091789\n",
      "iteration 3604, dc_loss: 0.023276343941688538, tv_loss: 0.03350460156798363\n",
      "iteration 3605, dc_loss: 0.023264292627573013, tv_loss: 0.033521611243486404\n",
      "iteration 3606, dc_loss: 0.02326129376888275, tv_loss: 0.03351534157991409\n",
      "iteration 3607, dc_loss: 0.02326652966439724, tv_loss: 0.03350566327571869\n",
      "iteration 3608, dc_loss: 0.023260125890374184, tv_loss: 0.033515240997076035\n",
      "iteration 3609, dc_loss: 0.023259397596120834, tv_loss: 0.033515069633722305\n",
      "iteration 3610, dc_loss: 0.023259304463863373, tv_loss: 0.033509623259305954\n",
      "iteration 3611, dc_loss: 0.02325759083032608, tv_loss: 0.03351166471838951\n",
      "iteration 3612, dc_loss: 0.023258617147803307, tv_loss: 0.03350576013326645\n",
      "iteration 3613, dc_loss: 0.023249972611665726, tv_loss: 0.03350920230150223\n",
      "iteration 3614, dc_loss: 0.023250754922628403, tv_loss: 0.03350940719246864\n",
      "iteration 3615, dc_loss: 0.023256167769432068, tv_loss: 0.03350207582116127\n",
      "iteration 3616, dc_loss: 0.02324659377336502, tv_loss: 0.03351176530122757\n",
      "iteration 3617, dc_loss: 0.02325361594557762, tv_loss: 0.033505260944366455\n",
      "iteration 3618, dc_loss: 0.02325556054711342, tv_loss: 0.03350552171468735\n",
      "iteration 3619, dc_loss: 0.023242540657520294, tv_loss: 0.03351385146379471\n",
      "iteration 3620, dc_loss: 0.023240378126502037, tv_loss: 0.03351454809308052\n",
      "iteration 3621, dc_loss: 0.023250309750437737, tv_loss: 0.033502381294965744\n",
      "iteration 3622, dc_loss: 0.023242345079779625, tv_loss: 0.033504974097013474\n",
      "iteration 3623, dc_loss: 0.02324017882347107, tv_loss: 0.033507321029901505\n",
      "iteration 3624, dc_loss: 0.02324255369603634, tv_loss: 0.033503029495477676\n",
      "iteration 3625, dc_loss: 0.023241575807332993, tv_loss: 0.03350536897778511\n",
      "iteration 3626, dc_loss: 0.023242535069584846, tv_loss: 0.03350793942809105\n",
      "iteration 3627, dc_loss: 0.02323046140372753, tv_loss: 0.03351950645446777\n",
      "iteration 3628, dc_loss: 0.023233462125062943, tv_loss: 0.03351924195885658\n",
      "iteration 3629, dc_loss: 0.023240668699145317, tv_loss: 0.033502452075481415\n",
      "iteration 3630, dc_loss: 0.023232093080878258, tv_loss: 0.033511534333229065\n",
      "iteration 3631, dc_loss: 0.023225432261824608, tv_loss: 0.033521514385938644\n",
      "iteration 3632, dc_loss: 0.023236965760588646, tv_loss: 0.03350793197751045\n",
      "iteration 3633, dc_loss: 0.0232393741607666, tv_loss: 0.03349975124001503\n",
      "iteration 3634, dc_loss: 0.023223882541060448, tv_loss: 0.03351184353232384\n",
      "iteration 3635, dc_loss: 0.023221470415592194, tv_loss: 0.0335194356739521\n",
      "iteration 3636, dc_loss: 0.02323250100016594, tv_loss: 0.03351324796676636\n",
      "iteration 3637, dc_loss: 0.02322738617658615, tv_loss: 0.03350900113582611\n",
      "iteration 3638, dc_loss: 0.023219633847475052, tv_loss: 0.03351534157991409\n",
      "iteration 3639, dc_loss: 0.023220820352435112, tv_loss: 0.03351571783423424\n",
      "iteration 3640, dc_loss: 0.023223279044032097, tv_loss: 0.03351708501577377\n",
      "iteration 3641, dc_loss: 0.023223236203193665, tv_loss: 0.03351336717605591\n",
      "iteration 3642, dc_loss: 0.02322029322385788, tv_loss: 0.03350537642836571\n",
      "iteration 3643, dc_loss: 0.023219017311930656, tv_loss: 0.03351378068327904\n",
      "iteration 3644, dc_loss: 0.023218076676130295, tv_loss: 0.033519625663757324\n",
      "iteration 3645, dc_loss: 0.02321256697177887, tv_loss: 0.03351544216275215\n",
      "iteration 3646, dc_loss: 0.023218873888254166, tv_loss: 0.03350887820124626\n",
      "iteration 3647, dc_loss: 0.02321840077638626, tv_loss: 0.03351757302880287\n",
      "iteration 3648, dc_loss: 0.02320852503180504, tv_loss: 0.03351867198944092\n",
      "iteration 3649, dc_loss: 0.023211592808365822, tv_loss: 0.033507611602544785\n",
      "iteration 3650, dc_loss: 0.023212911561131477, tv_loss: 0.033506840467453\n",
      "iteration 3651, dc_loss: 0.0232080165296793, tv_loss: 0.03351350128650665\n",
      "iteration 3652, dc_loss: 0.023206086829304695, tv_loss: 0.033517930656671524\n",
      "iteration 3653, dc_loss: 0.023204883560538292, tv_loss: 0.033512674272060394\n",
      "iteration 3654, dc_loss: 0.02320975624024868, tv_loss: 0.033501580357551575\n",
      "iteration 3655, dc_loss: 0.023208435624837875, tv_loss: 0.033505234867334366\n",
      "iteration 3656, dc_loss: 0.023200925439596176, tv_loss: 0.03351403400301933\n",
      "iteration 3657, dc_loss: 0.023203041404485703, tv_loss: 0.03351340815424919\n",
      "iteration 3658, dc_loss: 0.023201990872621536, tv_loss: 0.033509548753499985\n",
      "iteration 3659, dc_loss: 0.02319924905896187, tv_loss: 0.03350905701518059\n",
      "iteration 3660, dc_loss: 0.02319849096238613, tv_loss: 0.03350633755326271\n",
      "iteration 3661, dc_loss: 0.02320234477519989, tv_loss: 0.0335041880607605\n",
      "iteration 3662, dc_loss: 0.023196246474981308, tv_loss: 0.033513352274894714\n",
      "iteration 3663, dc_loss: 0.02318963035941124, tv_loss: 0.0335155688226223\n",
      "iteration 3664, dc_loss: 0.023196637630462646, tv_loss: 0.03350713104009628\n",
      "iteration 3665, dc_loss: 0.023197568953037262, tv_loss: 0.03350413218140602\n",
      "iteration 3666, dc_loss: 0.023191744461655617, tv_loss: 0.03350444510579109\n",
      "iteration 3667, dc_loss: 0.023187432438135147, tv_loss: 0.03351077437400818\n",
      "iteration 3668, dc_loss: 0.023189391940832138, tv_loss: 0.03350850194692612\n",
      "iteration 3669, dc_loss: 0.02319309301674366, tv_loss: 0.033499933779239655\n",
      "iteration 3670, dc_loss: 0.02318873628973961, tv_loss: 0.03350752592086792\n",
      "iteration 3671, dc_loss: 0.02318463660776615, tv_loss: 0.03350730985403061\n",
      "iteration 3672, dc_loss: 0.02318514697253704, tv_loss: 0.03350559249520302\n",
      "iteration 3673, dc_loss: 0.02318566106259823, tv_loss: 0.033503416925668716\n",
      "iteration 3674, dc_loss: 0.023182988166809082, tv_loss: 0.03350500762462616\n",
      "iteration 3675, dc_loss: 0.02317875251173973, tv_loss: 0.03350790962576866\n",
      "iteration 3676, dc_loss: 0.023183055222034454, tv_loss: 0.03350374102592468\n",
      "iteration 3677, dc_loss: 0.023184720426797867, tv_loss: 0.03350649029016495\n",
      "iteration 3678, dc_loss: 0.023173123598098755, tv_loss: 0.03351582959294319\n",
      "iteration 3679, dc_loss: 0.02317258156836033, tv_loss: 0.03351669758558273\n",
      "iteration 3680, dc_loss: 0.023182157427072525, tv_loss: 0.03350134938955307\n",
      "iteration 3681, dc_loss: 0.0231741052120924, tv_loss: 0.03350405395030975\n",
      "iteration 3682, dc_loss: 0.023169072344899178, tv_loss: 0.03351110219955444\n",
      "iteration 3683, dc_loss: 0.023177290335297585, tv_loss: 0.033503081649541855\n",
      "iteration 3684, dc_loss: 0.023175641894340515, tv_loss: 0.033508095890283585\n",
      "iteration 3685, dc_loss: 0.02317107655107975, tv_loss: 0.03351600095629692\n",
      "iteration 3686, dc_loss: 0.023162784054875374, tv_loss: 0.03352191299200058\n",
      "iteration 3687, dc_loss: 0.023169441148638725, tv_loss: 0.03350941464304924\n",
      "iteration 3688, dc_loss: 0.023171257227659225, tv_loss: 0.03350457176566124\n",
      "iteration 3689, dc_loss: 0.023166704922914505, tv_loss: 0.033511850982904434\n",
      "iteration 3690, dc_loss: 0.02316526509821415, tv_loss: 0.03351816162467003\n",
      "iteration 3691, dc_loss: 0.02316109836101532, tv_loss: 0.03350944072008133\n",
      "iteration 3692, dc_loss: 0.023162800818681717, tv_loss: 0.03351180627942085\n",
      "iteration 3693, dc_loss: 0.02316322736442089, tv_loss: 0.033505573868751526\n",
      "iteration 3694, dc_loss: 0.023155923932790756, tv_loss: 0.03351674601435661\n",
      "iteration 3695, dc_loss: 0.023159505799412727, tv_loss: 0.03350456431508064\n",
      "iteration 3696, dc_loss: 0.023163754492998123, tv_loss: 0.03350572660565376\n",
      "iteration 3697, dc_loss: 0.023156045004725456, tv_loss: 0.03350992128252983\n",
      "iteration 3698, dc_loss: 0.023154618218541145, tv_loss: 0.033509671688079834\n",
      "iteration 3699, dc_loss: 0.02315642684698105, tv_loss: 0.033512625843286514\n",
      "iteration 3700, dc_loss: 0.0231504924595356, tv_loss: 0.033509477972984314\n",
      "iteration 3701, dc_loss: 0.023154588416218758, tv_loss: 0.033505629748106\n",
      "iteration 3702, dc_loss: 0.02315533347427845, tv_loss: 0.03350963816046715\n",
      "iteration 3703, dc_loss: 0.023146698251366615, tv_loss: 0.033515267074108124\n",
      "iteration 3704, dc_loss: 0.02314644493162632, tv_loss: 0.0335099883377552\n",
      "iteration 3705, dc_loss: 0.02315480075776577, tv_loss: 0.03350329399108887\n",
      "iteration 3706, dc_loss: 0.023143790662288666, tv_loss: 0.03350701555609703\n",
      "iteration 3707, dc_loss: 0.023142486810684204, tv_loss: 0.03350816294550896\n",
      "iteration 3708, dc_loss: 0.023151837289333344, tv_loss: 0.03350216895341873\n",
      "iteration 3709, dc_loss: 0.023141810670495033, tv_loss: 0.03350909799337387\n",
      "iteration 3710, dc_loss: 0.02314087748527527, tv_loss: 0.03350960835814476\n",
      "iteration 3711, dc_loss: 0.02314150705933571, tv_loss: 0.03351103886961937\n",
      "iteration 3712, dc_loss: 0.023140788078308105, tv_loss: 0.03350895270705223\n",
      "iteration 3713, dc_loss: 0.023137887939810753, tv_loss: 0.0335056446492672\n",
      "iteration 3714, dc_loss: 0.02313845604658127, tv_loss: 0.03350742161273956\n",
      "iteration 3715, dc_loss: 0.023136399686336517, tv_loss: 0.033507008105516434\n",
      "iteration 3716, dc_loss: 0.023139819502830505, tv_loss: 0.0335003137588501\n",
      "iteration 3717, dc_loss: 0.023134510964155197, tv_loss: 0.03350387513637543\n",
      "iteration 3718, dc_loss: 0.023129157721996307, tv_loss: 0.033507779240608215\n",
      "iteration 3719, dc_loss: 0.023134896531701088, tv_loss: 0.03350280970335007\n",
      "iteration 3720, dc_loss: 0.02312943898141384, tv_loss: 0.03350397199392319\n",
      "iteration 3721, dc_loss: 0.023130757734179497, tv_loss: 0.03350263833999634\n",
      "iteration 3722, dc_loss: 0.023131195455789566, tv_loss: 0.03351105749607086\n",
      "iteration 3723, dc_loss: 0.023129373788833618, tv_loss: 0.03351551294326782\n",
      "iteration 3724, dc_loss: 0.02312324568629265, tv_loss: 0.03351639211177826\n",
      "iteration 3725, dc_loss: 0.02312655560672283, tv_loss: 0.0335044264793396\n",
      "iteration 3726, dc_loss: 0.023126259446144104, tv_loss: 0.03350302577018738\n",
      "iteration 3727, dc_loss: 0.023122195154428482, tv_loss: 0.03350850194692612\n",
      "iteration 3728, dc_loss: 0.023121673613786697, tv_loss: 0.03351057320833206\n",
      "iteration 3729, dc_loss: 0.023119540885090828, tv_loss: 0.033514413982629776\n",
      "iteration 3730, dc_loss: 0.023123662918806076, tv_loss: 0.03350451961159706\n",
      "iteration 3731, dc_loss: 0.02311825007200241, tv_loss: 0.03350580856204033\n",
      "iteration 3732, dc_loss: 0.023112406954169273, tv_loss: 0.033508967608213425\n",
      "iteration 3733, dc_loss: 0.02311931550502777, tv_loss: 0.03350120782852173\n",
      "iteration 3734, dc_loss: 0.023120049387216568, tv_loss: 0.033501334488391876\n",
      "iteration 3735, dc_loss: 0.023111704736948013, tv_loss: 0.033509403467178345\n",
      "iteration 3736, dc_loss: 0.023111935704946518, tv_loss: 0.03351464122533798\n",
      "iteration 3737, dc_loss: 0.02311551384627819, tv_loss: 0.03350787237286568\n",
      "iteration 3738, dc_loss: 0.02310958318412304, tv_loss: 0.03351243585348129\n",
      "iteration 3739, dc_loss: 0.023105375468730927, tv_loss: 0.03350932151079178\n",
      "iteration 3740, dc_loss: 0.023114318028092384, tv_loss: 0.033500995486974716\n",
      "iteration 3741, dc_loss: 0.023108873516321182, tv_loss: 0.033516619354486465\n",
      "iteration 3742, dc_loss: 0.02309923619031906, tv_loss: 0.03351825848221779\n",
      "iteration 3743, dc_loss: 0.023106534034013748, tv_loss: 0.033505771309137344\n",
      "iteration 3744, dc_loss: 0.02311578579246998, tv_loss: 0.0334973968565464\n",
      "iteration 3745, dc_loss: 0.023098068311810493, tv_loss: 0.03351516276597977\n",
      "iteration 3746, dc_loss: 0.02309403009712696, tv_loss: 0.033522527664899826\n",
      "iteration 3747, dc_loss: 0.023110877722501755, tv_loss: 0.03350178152322769\n",
      "iteration 3748, dc_loss: 0.02310119941830635, tv_loss: 0.03350706398487091\n",
      "iteration 3749, dc_loss: 0.023090912029147148, tv_loss: 0.03351671248674393\n",
      "iteration 3750, dc_loss: 0.023101598024368286, tv_loss: 0.03351699188351631\n",
      "iteration 3751, dc_loss: 0.023096684366464615, tv_loss: 0.033510223031044006\n",
      "iteration 3752, dc_loss: 0.023088810965418816, tv_loss: 0.03351917490363121\n",
      "iteration 3753, dc_loss: 0.023094812408089638, tv_loss: 0.03350809961557388\n",
      "iteration 3754, dc_loss: 0.02309730276465416, tv_loss: 0.03351050242781639\n",
      "iteration 3755, dc_loss: 0.02309376932680607, tv_loss: 0.03350541740655899\n",
      "iteration 3756, dc_loss: 0.023090921342372894, tv_loss: 0.03351093828678131\n",
      "iteration 3757, dc_loss: 0.02308754250407219, tv_loss: 0.03351354971528053\n",
      "iteration 3758, dc_loss: 0.023088719695806503, tv_loss: 0.03350897133350372\n",
      "iteration 3759, dc_loss: 0.023086892440915108, tv_loss: 0.03351130336523056\n",
      "iteration 3760, dc_loss: 0.023085476830601692, tv_loss: 0.03350670263171196\n",
      "iteration 3761, dc_loss: 0.02308613993227482, tv_loss: 0.03350457921624184\n",
      "iteration 3762, dc_loss: 0.023082369938492775, tv_loss: 0.0335065983235836\n",
      "iteration 3763, dc_loss: 0.023082289844751358, tv_loss: 0.03351221978664398\n",
      "iteration 3764, dc_loss: 0.023088183254003525, tv_loss: 0.03349970281124115\n",
      "iteration 3765, dc_loss: 0.023079322651028633, tv_loss: 0.03350750729441643\n",
      "iteration 3766, dc_loss: 0.023074576631188393, tv_loss: 0.033517707139253616\n",
      "iteration 3767, dc_loss: 0.023086659610271454, tv_loss: 0.03350154682993889\n",
      "iteration 3768, dc_loss: 0.023081110790371895, tv_loss: 0.033506911247968674\n",
      "iteration 3769, dc_loss: 0.023064401000738144, tv_loss: 0.03352026641368866\n",
      "iteration 3770, dc_loss: 0.02307814732193947, tv_loss: 0.033505164086818695\n",
      "iteration 3771, dc_loss: 0.02308589220046997, tv_loss: 0.033499013632535934\n",
      "iteration 3772, dc_loss: 0.02306368574500084, tv_loss: 0.0335262306034565\n",
      "iteration 3773, dc_loss: 0.023068411275744438, tv_loss: 0.033509183675050735\n",
      "iteration 3774, dc_loss: 0.02307741716504097, tv_loss: 0.033503033220767975\n",
      "iteration 3775, dc_loss: 0.023071197792887688, tv_loss: 0.03351068124175072\n",
      "iteration 3776, dc_loss: 0.023068109527230263, tv_loss: 0.03351498395204544\n",
      "iteration 3777, dc_loss: 0.02306814305484295, tv_loss: 0.0335138663649559\n",
      "iteration 3778, dc_loss: 0.023068435490131378, tv_loss: 0.03350329026579857\n",
      "iteration 3779, dc_loss: 0.02306106872856617, tv_loss: 0.03351035341620445\n",
      "iteration 3780, dc_loss: 0.02306288480758667, tv_loss: 0.03351453319191933\n",
      "iteration 3781, dc_loss: 0.023067409172654152, tv_loss: 0.03351042419672012\n",
      "iteration 3782, dc_loss: 0.02306215465068817, tv_loss: 0.03351395204663277\n",
      "iteration 3783, dc_loss: 0.023061860352754593, tv_loss: 0.03350777179002762\n",
      "iteration 3784, dc_loss: 0.023058760911226273, tv_loss: 0.033507220447063446\n",
      "iteration 3785, dc_loss: 0.02305760607123375, tv_loss: 0.03351670131087303\n",
      "iteration 3786, dc_loss: 0.023060377687215805, tv_loss: 0.03351622447371483\n",
      "iteration 3787, dc_loss: 0.023053985089063644, tv_loss: 0.033513229340314865\n",
      "iteration 3788, dc_loss: 0.02305128052830696, tv_loss: 0.03351227194070816\n",
      "iteration 3789, dc_loss: 0.023060491308569908, tv_loss: 0.03350617736577988\n",
      "iteration 3790, dc_loss: 0.023051314055919647, tv_loss: 0.03351529687643051\n",
      "iteration 3791, dc_loss: 0.023047933354973793, tv_loss: 0.03351340815424919\n",
      "iteration 3792, dc_loss: 0.023059386759996414, tv_loss: 0.03349771350622177\n",
      "iteration 3793, dc_loss: 0.023052353411912918, tv_loss: 0.033502720296382904\n",
      "iteration 3794, dc_loss: 0.023042064160108566, tv_loss: 0.033516790717840195\n",
      "iteration 3795, dc_loss: 0.023051371797919273, tv_loss: 0.03350519761443138\n",
      "iteration 3796, dc_loss: 0.02305205911397934, tv_loss: 0.033503998070955276\n",
      "iteration 3797, dc_loss: 0.02303951419889927, tv_loss: 0.0335153229534626\n",
      "iteration 3798, dc_loss: 0.023043567314743996, tv_loss: 0.033507056534290314\n",
      "iteration 3799, dc_loss: 0.023044629022479057, tv_loss: 0.03350277990102768\n",
      "iteration 3800, dc_loss: 0.02304147556424141, tv_loss: 0.03350653126835823\n",
      "iteration 3801, dc_loss: 0.023041490465402603, tv_loss: 0.03350559249520302\n",
      "iteration 3802, dc_loss: 0.023037055507302284, tv_loss: 0.03350567817687988\n",
      "iteration 3803, dc_loss: 0.02304476872086525, tv_loss: 0.03350182995200157\n",
      "iteration 3804, dc_loss: 0.023039713501930237, tv_loss: 0.033506862819194794\n",
      "iteration 3805, dc_loss: 0.023033607751131058, tv_loss: 0.03350928798317909\n",
      "iteration 3806, dc_loss: 0.023033857345581055, tv_loss: 0.033508241176605225\n",
      "iteration 3807, dc_loss: 0.02303725853562355, tv_loss: 0.03350052982568741\n",
      "iteration 3808, dc_loss: 0.023032817989587784, tv_loss: 0.03350546211004257\n",
      "iteration 3809, dc_loss: 0.023028071969747543, tv_loss: 0.033518243581056595\n",
      "iteration 3810, dc_loss: 0.023036042228341103, tv_loss: 0.03350745141506195\n",
      "iteration 3811, dc_loss: 0.023034481331706047, tv_loss: 0.03350527584552765\n",
      "iteration 3812, dc_loss: 0.023023223504424095, tv_loss: 0.03351151570677757\n",
      "iteration 3813, dc_loss: 0.023025821894407272, tv_loss: 0.03350409120321274\n",
      "iteration 3814, dc_loss: 0.023032132536172867, tv_loss: 0.03350049629807472\n",
      "iteration 3815, dc_loss: 0.023026125505566597, tv_loss: 0.033510562032461166\n",
      "iteration 3816, dc_loss: 0.023020615801215172, tv_loss: 0.03351416811347008\n",
      "iteration 3817, dc_loss: 0.02302713319659233, tv_loss: 0.03350507840514183\n",
      "iteration 3818, dc_loss: 0.023020992055535316, tv_loss: 0.033510033041238785\n",
      "iteration 3819, dc_loss: 0.023016909137368202, tv_loss: 0.033506520092487335\n",
      "iteration 3820, dc_loss: 0.02302107959985733, tv_loss: 0.03350420668721199\n",
      "iteration 3821, dc_loss: 0.023021262139081955, tv_loss: 0.03350570425391197\n",
      "iteration 3822, dc_loss: 0.023022742941975594, tv_loss: 0.03350423648953438\n",
      "iteration 3823, dc_loss: 0.023015286773443222, tv_loss: 0.03351297229528427\n",
      "iteration 3824, dc_loss: 0.02301664650440216, tv_loss: 0.03350730240345001\n",
      "iteration 3825, dc_loss: 0.023013653233647346, tv_loss: 0.03350815922021866\n",
      "iteration 3826, dc_loss: 0.02300785854458809, tv_loss: 0.03351205214858055\n",
      "iteration 3827, dc_loss: 0.023019522428512573, tv_loss: 0.03350267931818962\n",
      "iteration 3828, dc_loss: 0.023013291880488396, tv_loss: 0.03350844979286194\n",
      "iteration 3829, dc_loss: 0.023004110902547836, tv_loss: 0.033514440059661865\n",
      "iteration 3830, dc_loss: 0.023006679490208626, tv_loss: 0.033510759472846985\n",
      "iteration 3831, dc_loss: 0.023012841120362282, tv_loss: 0.03349805623292923\n",
      "iteration 3832, dc_loss: 0.02300916239619255, tv_loss: 0.03350489214062691\n",
      "iteration 3833, dc_loss: 0.023002395406365395, tv_loss: 0.033513184636831284\n",
      "iteration 3834, dc_loss: 0.023006649687886238, tv_loss: 0.03350908309221268\n",
      "iteration 3835, dc_loss: 0.023006321862339973, tv_loss: 0.03350456804037094\n",
      "iteration 3836, dc_loss: 0.023002266883850098, tv_loss: 0.03350423648953438\n",
      "iteration 3837, dc_loss: 0.02300184592604637, tv_loss: 0.03350422531366348\n",
      "iteration 3838, dc_loss: 0.022998306900262833, tv_loss: 0.03350377455353737\n",
      "iteration 3839, dc_loss: 0.02300027199089527, tv_loss: 0.03350113332271576\n",
      "iteration 3840, dc_loss: 0.023000990971922874, tv_loss: 0.03349870443344116\n",
      "iteration 3841, dc_loss: 0.022994276136159897, tv_loss: 0.03350558504462242\n",
      "iteration 3842, dc_loss: 0.022995900362730026, tv_loss: 0.03350195288658142\n",
      "iteration 3843, dc_loss: 0.02299531176686287, tv_loss: 0.03350474685430527\n",
      "iteration 3844, dc_loss: 0.02299482561647892, tv_loss: 0.033502012491226196\n",
      "iteration 3845, dc_loss: 0.02299283817410469, tv_loss: 0.033506590873003006\n",
      "iteration 3846, dc_loss: 0.022990547120571136, tv_loss: 0.03351140022277832\n",
      "iteration 3847, dc_loss: 0.022991513833403587, tv_loss: 0.03350505605340004\n",
      "iteration 3848, dc_loss: 0.022992772981524467, tv_loss: 0.03350811451673508\n",
      "iteration 3849, dc_loss: 0.022984579205513, tv_loss: 0.03351017087697983\n",
      "iteration 3850, dc_loss: 0.022985301911830902, tv_loss: 0.03351007029414177\n",
      "iteration 3851, dc_loss: 0.022989753633737564, tv_loss: 0.03350009769201279\n",
      "iteration 3852, dc_loss: 0.022986559197306633, tv_loss: 0.03350592777132988\n",
      "iteration 3853, dc_loss: 0.022985335439443588, tv_loss: 0.033501867204904556\n",
      "iteration 3854, dc_loss: 0.02298288606107235, tv_loss: 0.03349986672401428\n",
      "iteration 3855, dc_loss: 0.02298242412507534, tv_loss: 0.033503446727991104\n",
      "iteration 3856, dc_loss: 0.022981274873018265, tv_loss: 0.033503130078315735\n",
      "iteration 3857, dc_loss: 0.022979598492383957, tv_loss: 0.03350336477160454\n",
      "iteration 3858, dc_loss: 0.02297952212393284, tv_loss: 0.033509183675050735\n",
      "iteration 3859, dc_loss: 0.022974062711000443, tv_loss: 0.03351333364844322\n",
      "iteration 3860, dc_loss: 0.022975724190473557, tv_loss: 0.033510010689496994\n",
      "iteration 3861, dc_loss: 0.02297959104180336, tv_loss: 0.033501386642456055\n",
      "iteration 3862, dc_loss: 0.022981863468885422, tv_loss: 0.03349912911653519\n",
      "iteration 3863, dc_loss: 0.022964777424931526, tv_loss: 0.03351554647088051\n",
      "iteration 3864, dc_loss: 0.022969407960772514, tv_loss: 0.033506184816360474\n",
      "iteration 3865, dc_loss: 0.022977329790592194, tv_loss: 0.03349914774298668\n",
      "iteration 3866, dc_loss: 0.02296513505280018, tv_loss: 0.03351180627942085\n",
      "iteration 3867, dc_loss: 0.02296925149857998, tv_loss: 0.03350439295172691\n",
      "iteration 3868, dc_loss: 0.022973114624619484, tv_loss: 0.03350256010890007\n",
      "iteration 3869, dc_loss: 0.02296280488371849, tv_loss: 0.03351328894495964\n",
      "iteration 3870, dc_loss: 0.02296352945268154, tv_loss: 0.03350740671157837\n",
      "iteration 3871, dc_loss: 0.02297430858016014, tv_loss: 0.0334964394569397\n",
      "iteration 3872, dc_loss: 0.0229618139564991, tv_loss: 0.03350895643234253\n",
      "iteration 3873, dc_loss: 0.022958479821681976, tv_loss: 0.03350802883505821\n",
      "iteration 3874, dc_loss: 0.022963857278227806, tv_loss: 0.03350021317601204\n",
      "iteration 3875, dc_loss: 0.02296087145805359, tv_loss: 0.03350800648331642\n",
      "iteration 3876, dc_loss: 0.022958677262067795, tv_loss: 0.03351159766316414\n",
      "iteration 3877, dc_loss: 0.022956108674407005, tv_loss: 0.0335078239440918\n",
      "iteration 3878, dc_loss: 0.022957971319556236, tv_loss: 0.03351151943206787\n",
      "iteration 3879, dc_loss: 0.022954870015382767, tv_loss: 0.03350619599223137\n",
      "iteration 3880, dc_loss: 0.022957706823945045, tv_loss: 0.033503204584121704\n",
      "iteration 3881, dc_loss: 0.022956607863307, tv_loss: 0.033503711223602295\n",
      "iteration 3882, dc_loss: 0.02294834889471531, tv_loss: 0.03350837528705597\n",
      "iteration 3883, dc_loss: 0.022950680926442146, tv_loss: 0.03350835293531418\n",
      "iteration 3884, dc_loss: 0.022959575057029724, tv_loss: 0.033491816371679306\n",
      "iteration 3885, dc_loss: 0.022952532395720482, tv_loss: 0.03350111097097397\n",
      "iteration 3886, dc_loss: 0.022938480600714684, tv_loss: 0.03351334482431412\n",
      "iteration 3887, dc_loss: 0.022950921207666397, tv_loss: 0.03349658474326134\n",
      "iteration 3888, dc_loss: 0.022951524704694748, tv_loss: 0.033499009907245636\n",
      "iteration 3889, dc_loss: 0.022937925532460213, tv_loss: 0.03351282328367233\n",
      "iteration 3890, dc_loss: 0.02294626086950302, tv_loss: 0.033507753163576126\n",
      "iteration 3891, dc_loss: 0.02294670045375824, tv_loss: 0.03351392224431038\n",
      "iteration 3892, dc_loss: 0.022938642650842667, tv_loss: 0.0335138700902462\n",
      "iteration 3893, dc_loss: 0.022937770932912827, tv_loss: 0.033509183675050735\n",
      "iteration 3894, dc_loss: 0.02295261062681675, tv_loss: 0.03349439799785614\n",
      "iteration 3895, dc_loss: 0.022937249392271042, tv_loss: 0.03350917994976044\n",
      "iteration 3896, dc_loss: 0.022930385544896126, tv_loss: 0.03351648151874542\n",
      "iteration 3897, dc_loss: 0.022944994270801544, tv_loss: 0.0335034504532814\n",
      "iteration 3898, dc_loss: 0.022933410480618477, tv_loss: 0.033509511500597\n",
      "iteration 3899, dc_loss: 0.02292686328291893, tv_loss: 0.03351600840687752\n",
      "iteration 3900, dc_loss: 0.022938169538974762, tv_loss: 0.03350371867418289\n",
      "iteration 3901, dc_loss: 0.022938240319490433, tv_loss: 0.033495109528303146\n",
      "iteration 3902, dc_loss: 0.022927913814783096, tv_loss: 0.03351213410496712\n",
      "iteration 3903, dc_loss: 0.02292863465845585, tv_loss: 0.03351235017180443\n",
      "iteration 3904, dc_loss: 0.022932231426239014, tv_loss: 0.03350730985403061\n",
      "iteration 3905, dc_loss: 0.022924985736608505, tv_loss: 0.033510129898786545\n",
      "iteration 3906, dc_loss: 0.02293151058256626, tv_loss: 0.033500559628009796\n",
      "iteration 3907, dc_loss: 0.022926073521375656, tv_loss: 0.03350810334086418\n",
      "iteration 3908, dc_loss: 0.022920871153473854, tv_loss: 0.03351416811347008\n",
      "iteration 3909, dc_loss: 0.022928200662136078, tv_loss: 0.03350389748811722\n",
      "iteration 3910, dc_loss: 0.022925419732928276, tv_loss: 0.03349876403808594\n",
      "iteration 3911, dc_loss: 0.022918494418263435, tv_loss: 0.03350917249917984\n",
      "iteration 3912, dc_loss: 0.02292242832481861, tv_loss: 0.033504415303468704\n",
      "iteration 3913, dc_loss: 0.022918235510587692, tv_loss: 0.03350236639380455\n",
      "iteration 3914, dc_loss: 0.022917192429304123, tv_loss: 0.033505771309137344\n",
      "iteration 3915, dc_loss: 0.022920941933989525, tv_loss: 0.03350132331252098\n",
      "iteration 3916, dc_loss: 0.022914759814739227, tv_loss: 0.03350493684411049\n",
      "iteration 3917, dc_loss: 0.0229156706482172, tv_loss: 0.03350076451897621\n",
      "iteration 3918, dc_loss: 0.02291501872241497, tv_loss: 0.0335015207529068\n",
      "iteration 3919, dc_loss: 0.022908620536327362, tv_loss: 0.03350696340203285\n",
      "iteration 3920, dc_loss: 0.022915391251444817, tv_loss: 0.03349858894944191\n",
      "iteration 3921, dc_loss: 0.02291497029364109, tv_loss: 0.03350045531988144\n",
      "iteration 3922, dc_loss: 0.02290879748761654, tv_loss: 0.03351568803191185\n",
      "iteration 3923, dc_loss: 0.022904591634869576, tv_loss: 0.033515967428684235\n",
      "iteration 3924, dc_loss: 0.022914785891771317, tv_loss: 0.03350488469004631\n",
      "iteration 3925, dc_loss: 0.02290606126189232, tv_loss: 0.033508963882923126\n",
      "iteration 3926, dc_loss: 0.0229047704488039, tv_loss: 0.033512361347675323\n",
      "iteration 3927, dc_loss: 0.02290692739188671, tv_loss: 0.03351645916700363\n",
      "iteration 3928, dc_loss: 0.02289704605937004, tv_loss: 0.03351892903447151\n",
      "iteration 3929, dc_loss: 0.022901708260178566, tv_loss: 0.033509548753499985\n",
      "iteration 3930, dc_loss: 0.02290823496878147, tv_loss: 0.033505771309137344\n",
      "iteration 3931, dc_loss: 0.02289753593504429, tv_loss: 0.033516403287649155\n",
      "iteration 3932, dc_loss: 0.02289731428027153, tv_loss: 0.03351503610610962\n",
      "iteration 3933, dc_loss: 0.02290402539074421, tv_loss: 0.033501241356134415\n",
      "iteration 3934, dc_loss: 0.022898707538843155, tv_loss: 0.033509209752082825\n",
      "iteration 3935, dc_loss: 0.02289232425391674, tv_loss: 0.033514853566884995\n",
      "iteration 3936, dc_loss: 0.02289634756743908, tv_loss: 0.03350616618990898\n",
      "iteration 3937, dc_loss: 0.02289699763059616, tv_loss: 0.03350122645497322\n",
      "iteration 3938, dc_loss: 0.022890478372573853, tv_loss: 0.03350900113582611\n",
      "iteration 3939, dc_loss: 0.022892354056239128, tv_loss: 0.033506330102682114\n",
      "iteration 3940, dc_loss: 0.02289395034313202, tv_loss: 0.033502474427223206\n",
      "iteration 3941, dc_loss: 0.022889019921422005, tv_loss: 0.0335065983235836\n",
      "iteration 3942, dc_loss: 0.022890282794833183, tv_loss: 0.033501796424388885\n",
      "iteration 3943, dc_loss: 0.02288767881691456, tv_loss: 0.03350182995200157\n",
      "iteration 3944, dc_loss: 0.0228850357234478, tv_loss: 0.03350221365690231\n",
      "iteration 3945, dc_loss: 0.022888002917170525, tv_loss: 0.03349928930401802\n",
      "iteration 3946, dc_loss: 0.022887878119945526, tv_loss: 0.03349859267473221\n",
      "iteration 3947, dc_loss: 0.022880371659994125, tv_loss: 0.033505525439977646\n",
      "iteration 3948, dc_loss: 0.022886278107762337, tv_loss: 0.03349647670984268\n",
      "iteration 3949, dc_loss: 0.022880299016833305, tv_loss: 0.03350130468606949\n",
      "iteration 3950, dc_loss: 0.02287684753537178, tv_loss: 0.033510833978652954\n",
      "iteration 3951, dc_loss: 0.02288118377327919, tv_loss: 0.03350724279880524\n",
      "iteration 3952, dc_loss: 0.022884374484419823, tv_loss: 0.03350576385855675\n",
      "iteration 3953, dc_loss: 0.0228738971054554, tv_loss: 0.03350840136408806\n",
      "iteration 3954, dc_loss: 0.022876670584082603, tv_loss: 0.033501990139484406\n",
      "iteration 3955, dc_loss: 0.02287277765572071, tv_loss: 0.03350798785686493\n",
      "iteration 3956, dc_loss: 0.022872287780046463, tv_loss: 0.03350351005792618\n",
      "iteration 3957, dc_loss: 0.022879380732774734, tv_loss: 0.03349698707461357\n",
      "iteration 3958, dc_loss: 0.02286936342716217, tv_loss: 0.033504046499729156\n",
      "iteration 3959, dc_loss: 0.02286743000149727, tv_loss: 0.03351195901632309\n",
      "iteration 3960, dc_loss: 0.02287573739886284, tv_loss: 0.03349924832582474\n",
      "iteration 3961, dc_loss: 0.02287311851978302, tv_loss: 0.03350353240966797\n",
      "iteration 3962, dc_loss: 0.022860337048768997, tv_loss: 0.033513110131025314\n",
      "iteration 3963, dc_loss: 0.022870179265737534, tv_loss: 0.0335024893283844\n",
      "iteration 3964, dc_loss: 0.022869376465678215, tv_loss: 0.03350082039833069\n",
      "iteration 3965, dc_loss: 0.02286052145063877, tv_loss: 0.03350617736577988\n",
      "iteration 3966, dc_loss: 0.022863101214170456, tv_loss: 0.033502690494060516\n",
      "iteration 3967, dc_loss: 0.022867988795042038, tv_loss: 0.03349745273590088\n",
      "iteration 3968, dc_loss: 0.02285890281200409, tv_loss: 0.03350343927741051\n",
      "iteration 3969, dc_loss: 0.022862384095788002, tv_loss: 0.033499784767627716\n",
      "iteration 3970, dc_loss: 0.02285531349480152, tv_loss: 0.03350421413779259\n",
      "iteration 3971, dc_loss: 0.02285972237586975, tv_loss: 0.03349907696247101\n",
      "iteration 3972, dc_loss: 0.02286648005247116, tv_loss: 0.033495839685201645\n",
      "iteration 3973, dc_loss: 0.022852016612887383, tv_loss: 0.033503394573926926\n",
      "iteration 3974, dc_loss: 0.02285357005894184, tv_loss: 0.03350269794464111\n",
      "iteration 3975, dc_loss: 0.02285945415496826, tv_loss: 0.03349977359175682\n",
      "iteration 3976, dc_loss: 0.022851036861538887, tv_loss: 0.03351158648729324\n",
      "iteration 3977, dc_loss: 0.022848960012197495, tv_loss: 0.03351328894495964\n",
      "iteration 3978, dc_loss: 0.02285933308303356, tv_loss: 0.0335029736161232\n",
      "iteration 3979, dc_loss: 0.022846855223178864, tv_loss: 0.03351123631000519\n",
      "iteration 3980, dc_loss: 0.02284553274512291, tv_loss: 0.0335116982460022\n",
      "iteration 3981, dc_loss: 0.022854551672935486, tv_loss: 0.03349822387099266\n",
      "iteration 3982, dc_loss: 0.022844621911644936, tv_loss: 0.033507898449897766\n",
      "iteration 3983, dc_loss: 0.022843170911073685, tv_loss: 0.03350916504859924\n",
      "iteration 3984, dc_loss: 0.022849105298519135, tv_loss: 0.03350936248898506\n",
      "iteration 3985, dc_loss: 0.02284267172217369, tv_loss: 0.0335111990571022\n",
      "iteration 3986, dc_loss: 0.02283814735710621, tv_loss: 0.0335114561021328\n",
      "iteration 3987, dc_loss: 0.022846298292279243, tv_loss: 0.0335017628967762\n",
      "iteration 3988, dc_loss: 0.022845061495900154, tv_loss: 0.03350379690527916\n",
      "iteration 3989, dc_loss: 0.02283359505236149, tv_loss: 0.033517010509967804\n",
      "iteration 3990, dc_loss: 0.022844867780804634, tv_loss: 0.033502865582704544\n",
      "iteration 3991, dc_loss: 0.02284104749560356, tv_loss: 0.03350324183702469\n",
      "iteration 3992, dc_loss: 0.022831998765468597, tv_loss: 0.033511094748973846\n",
      "iteration 3993, dc_loss: 0.022839142009615898, tv_loss: 0.03350229188799858\n",
      "iteration 3994, dc_loss: 0.02283349446952343, tv_loss: 0.033508505672216415\n",
      "iteration 3995, dc_loss: 0.02282790280878544, tv_loss: 0.03351297602057457\n",
      "iteration 3996, dc_loss: 0.0228362325578928, tv_loss: 0.033499788492918015\n",
      "iteration 3997, dc_loss: 0.022838616743683815, tv_loss: 0.03349504992365837\n",
      "iteration 3998, dc_loss: 0.02282591536641121, tv_loss: 0.03350866585969925\n",
      "iteration 3999, dc_loss: 0.022827569395303726, tv_loss: 0.03350362181663513\n",
      "iteration 4000, dc_loss: 0.022835399955511093, tv_loss: 0.03349271044135094\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n"
     ]
    }
   ],
   "source": [
    "for dataset, ar in zip(datasets_list, [1, 2, 4, 8]):\n",
    "    device = torch.device(\"cuda\")\n",
    "    size = (640, 368)\n",
    "    from ese5934_project.models.SIREN import Siren, get_coordinates\n",
    "\n",
    "    coords = get_coordinates(size)\n",
    "    kspace, (mean, std), masked_kspace, mask, csm = dataset[15]\n",
    "    field = Siren(\n",
    "        size,\n",
    "        mean.to(device),\n",
    "        std.to(device),\n",
    "        in_features=2,\n",
    "        out_features=2,\n",
    "        hidden_features=256,\n",
    "        hidden_layers=4,\n",
    "        outermost_linear=True,\n",
    "        first_omega_0=25,\n",
    "        hidden_omega_0=25,\n",
    "    )\n",
    "    lr_scheduler = lambda t: 0.8 ** (t // 400) * 1e-4\n",
    "    optimizer = torchopt.adamw(lr=lr_scheduler)\n",
    "    # 1e-4 1.092077389\n",
    "    # 1e-3 0.08540542\n",
    "    params, image_list_SIREN = reconstruct(\n",
    "        field,\n",
    "        coords,\n",
    "        masked_kspace,\n",
    "        csm,\n",
    "        mask,\n",
    "        alpha=0.01,\n",
    "        optimizer=optimizer,\n",
    "        iterations=4000,\n",
    "        device=device,\n",
    "    )\n",
    "    psnr, ssim = Evaluate_MT1(image_gt, image_list_ADAM[-1])\n",
    "    SIREN_list.append((psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [40, 24]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 40, 24])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> total parameters:  361264\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [40, 24]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 40, 24])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 34.87135346651343\n",
      "SSIM Value mt1: 0.7606510936766294\n",
      "=====> total parameters:  361264\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [40, 24]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 40, 24])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 34.87135346651343\n",
      "SSIM Value mt1: 0.7606510936766294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> total parameters:  361264\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [40, 24]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 40, 24])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 34.87135346651343\n",
      "SSIM Value mt1: 0.7606510936766294\n",
      "=====> total parameters:  361264\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "PSNR Value mt1: 34.87135346651343\n",
      "SSIM Value mt1: 0.7606510936766294\n"
     ]
    }
   ],
   "source": [
    "from ese5934_project.models.FactorFields import DictField, get_coordinates\n",
    "\n",
    "base_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/defaults.yaml\")\n",
    "second_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/image.yaml\")\n",
    "cfg = OmegaConf.merge(\n",
    "    base_conf,\n",
    "    second_conf,\n",
    ")\n",
    "for dataset, ar in zip(datasets_list, [1, 2, 4, 8]):\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    size = (640, 368)\n",
    "    model = DictField(cfg, size, device)\n",
    "\n",
    "    coords = get_coordinates(size)\n",
    "    kspace, (mean, std), masked_kspace, mask, csm = dataset[15]\n",
    "    scheduler = lambda t: 0.8 ** (t // 400) * 5e-3\n",
    "    optimizer = torchopt.adam(lr=scheduler)\n",
    "\n",
    "    params, image_list = reconstruct(\n",
    "        model,\n",
    "        coords,\n",
    "        masked_kspace,\n",
    "        csm,\n",
    "        mask,\n",
    "        alpha=0.005,\n",
    "        optimizer=optimizer,\n",
    "        iterations=4000,\n",
    "        device=device,\n",
    "    )\n",
    "    psnr, ssim = Evaluate_MT1(image_gt, image_list_ADAM[-1])\n",
    "    DF_list.append((psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(34.87135346651343, 0.7606510936766294), (34.87135346651343, 0.7606510936766294), (34.87135346651343, 0.7606510936766294), (34.87135346651343, 0.7606510936766294)]\n"
     ]
    }
   ],
   "source": [
    "print(DF_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\"ADAM\": ADAM_list, \"SIREN\": SIREN_list, \"DF\": DF_list}, \"results_AR_1_2_4_8.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35.239742876024216, 0.7594775679989919), (35.239742876024216, 0.7594775679989919), (35.239742876024216, 0.7594775679989919), (35.239742876024216, 0.7594775679989919), (35.77876077524474, 0.7715722252164936), (35.77876077524474, 0.7715722252164936), (35.77876077524474, 0.7715722252164936), (35.77876077524474, 0.7715722252164936)]\n"
     ]
    }
   ],
   "source": [
    "print(DF_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8db1953410>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([d[0] for d in ADAM_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiments/AR_results.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/core/generic.py:2414\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2403\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2404\u001b[0m     df,\n\u001b[1;32m   2405\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2413\u001b[0m )\n\u001b[0;32m-> 2414\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/io/formats/excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[1;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     62\u001b[0m         path,\n\u001b[1;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "results.to_excel(\"experiments/AR_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAM\n",
      "PSNR Value mt1: 35.51589318336404\n",
      "SSIM Value mt1: 0.7751314202416238\n",
      "SIREN\n",
      "PSNR Value mt1: 28.169658116720203\n",
      "SSIM Value mt1: 0.8022582995267628\n",
      "DF\n",
      "PSNR Value mt1: 38.49677393524084\n",
      "SSIM Value mt1: 0.9359180332422214\n",
      "gt\n",
      "PSNR Value mt1: inf\n",
      "SSIM Value mt1: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/skimage/metrics/simple_metrics.py:163: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 10 * np.log10((data_range ** 2) / err)\n"
     ]
    }
   ],
   "source": [
    "for k, v in results.items():\n",
    "    print(k)\n",
    "    Evaluate_MT1(image_gt, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt2: 37.331352482059785\n",
      "SSIM Value mt2: 0.9279717622379655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37.331352482059785, 0.9279717622379655)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate_MT2(image_gt, image_list[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
